2023-08-03 14:32:14,394 - logger name:exp/ECL-PatchTST2023-08-03-14:32:14.394535/ECL-PatchTST.log
2023-08-03 14:32:14,395 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-14:32:14.394535', 'path': 'exp/ECL-PatchTST2023-08-03-14:32:14.394535', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 14:32:14,395 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 14:32:14,619 - [*] phase 0 Dataset load!
2023-08-03 14:32:15,955 - [*] phase 0 Training start
train 7969
2023-08-03 14:32:42,629 - epoch:0, training loss:0.7789 validation loss:0.4190
train 7969
vs, vt 0.41895488537847997 0.4132383428514004
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.3884594913572073 0.3885953791439533
2023-08-03 14:33:37,260 - epoch:1, training loss:0.7827 validation loss:0.3885
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.37636158131062986 0.38732904344797137
2023-08-03 14:34:14,133 - epoch:2, training loss:0.7247 validation loss:0.3764
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.3692531656473875 0.37150229774415494
2023-08-03 14:34:49,421 - epoch:3, training loss:0.6853 validation loss:0.3693
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.37501056008040906 0.37064487636089327
2023-08-03 14:35:25,261 - epoch:4, training loss:0.6570 validation loss:0.3750
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.38222029991447926 0.3795190904289484
2023-08-03 14:36:05,651 - epoch:5, training loss:0.6274 validation loss:0.3822
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.384800049290061 0.3743063505738974
2023-08-03 14:36:46,121 - epoch:6, training loss:0.6018 validation loss:0.3848
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.39033303037285805 0.3742960184812546
2023-08-03 14:37:28,480 - epoch:7, training loss:0.5840 validation loss:0.3903
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.4042418375611305 0.3683088950812817
2023-08-03 14:38:08,388 - epoch:8, training loss:0.5665 validation loss:0.4042
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.39587954208254816 0.36832850240170956
2023-08-03 14:38:47,140 - epoch:9, training loss:0.5552 validation loss:0.3959
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.40352830700576303 0.3688770061358809
2023-08-03 14:39:24,259 - epoch:10, training loss:0.5437 validation loss:0.4035
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.40291605815291404 0.37274851333349945
2023-08-03 14:40:00,187 - epoch:11, training loss:0.5367 validation loss:0.4029
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.40562997534871104 0.37250636890530586
2023-08-03 14:40:38,403 - epoch:12, training loss:0.5288 validation loss:0.4056
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.4082068149000406 0.3741697113960981
2023-08-03 14:41:18,740 - epoch:13, training loss:0.5233 validation loss:0.4082
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.40224927887320516 0.3716515524312854
2023-08-03 14:42:00,623 - epoch:14, training loss:0.5162 validation loss:0.4022
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.40842701122164726 0.3760084010660648
2023-08-03 14:42:42,591 - epoch:15, training loss:0.5125 validation loss:0.4084
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.4020721226930618 0.37496607322245834
2023-08-03 14:43:21,234 - epoch:16, training loss:0.5067 validation loss:0.4021
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.4106238901615143 0.37596512362360957
2023-08-03 14:43:59,217 - epoch:17, training loss:0.5031 validation loss:0.4106
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.41638905704021456 0.37749884389340876
2023-08-03 14:44:35,346 - epoch:18, training loss:0.5001 validation loss:0.4164
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.41248416937887666 0.3823992099612951
2023-08-03 14:45:13,579 - epoch:19, training loss:0.4973 validation loss:0.4125
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.41345476284623145 0.38106851149350407
2023-08-03 14:45:53,394 - epoch:20, training loss:0.4938 validation loss:0.4135
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.41314056776463987 0.37940026093274354
2023-08-03 14:46:34,894 - epoch:21, training loss:0.4908 validation loss:0.4131
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.4144771359860897 0.38237633034586904
2023-08-03 14:47:17,963 - epoch:22, training loss:0.4901 validation loss:0.4145
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.4146151874214411 0.3790848825126886
2023-08-03 14:48:01,062 - epoch:23, training loss:0.4890 validation loss:0.4146
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.4166548874229193 0.3805908065289259
2023-08-03 14:48:39,833 - epoch:24, training loss:0.4869 validation loss:0.4167
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.4161531891673803 0.3811688283458352
2023-08-03 14:49:17,600 - epoch:25, training loss:0.4857 validation loss:0.4162
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.4165823206305504 0.3809694429859519
2023-08-03 14:50:00,366 - epoch:26, training loss:0.4853 validation loss:0.4166
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.41638774834573267 0.379313001409173
2023-08-03 14:50:46,412 - epoch:27, training loss:0.4847 validation loss:0.4164
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.4166882984340191 0.38182540629059075
2023-08-03 14:51:33,879 - epoch:28, training loss:0.4850 validation loss:0.4167
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.4169592048972845 0.3810563111677766
2023-08-03 14:52:21,239 - epoch:29, training loss:0.4859 validation loss:0.4170
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-14:32:14.394535/0/0.3693_epoch_3.pkl  &  0.3683088950812817
2023-08-03 14:52:27,020 - [*] loss:0.3693
2023-08-03 14:52:27,034 - [*] phase 0, testing
2023-08-03 14:52:27,755 - T:336	MAE	0.399641	RMSE	0.364573	MAPE	164.044392
2023-08-03 14:52:27,756 - 336	mae	0.3996	
2023-08-03 14:52:27,756 - 336	rmse	0.3646	
2023-08-03 14:52:27,756 - 336	mape	164.0444	
2023-08-03 14:52:32,373 - [*] loss:0.3683
2023-08-03 14:52:32,382 - [*] phase 0, testing
2023-08-03 14:52:33,043 - T:336	MAE	0.396014	RMSE	0.364173	MAPE	156.963718
2023-08-03 14:52:33,044 - 336	mae	0.3960	
2023-08-03 14:52:33,044 - 336	rmse	0.3642	
2023-08-03 14:52:33,044 - 336	mape	156.9637	
2023-08-03 14:52:35,825 - logger name:exp/ECL-PatchTST2023-08-03-14:52:35.824833/ECL-PatchTST.log
2023-08-03 14:52:35,825 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 3, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-14:52:35.824833', 'path': 'exp/ECL-PatchTST2023-08-03-14:52:35.824833', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 14:52:35,825 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 14:52:36,117 - [*] phase 0 Dataset load!
2023-08-03 14:52:37,235 - [*] phase 0 Training start
train 7969
2023-08-03 14:53:03,664 - epoch:0, training loss:0.7267 validation loss:0.4004
train 7969
vs, vt 0.40042925663292406 0.40652801282703876
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.3817365407943726 0.3820631384849548
2023-08-03 14:54:06,073 - epoch:1, training loss:8.3287 validation loss:0.3817
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.3727871794253588 0.36100388020277024
2023-08-03 14:54:53,102 - epoch:2, training loss:6.3633 validation loss:0.3728
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.3641559544950724 0.35917276218533517
2023-08-03 14:55:40,615 - epoch:3, training loss:4.0729 validation loss:0.3642
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.3659325923770666 0.358781972900033
2023-08-03 14:56:28,146 - epoch:4, training loss:2.8061 validation loss:0.3659
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.368446359410882 0.3660051830112934
2023-08-03 14:57:17,264 - epoch:5, training loss:2.4084 validation loss:0.3684
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.3699640292674303 0.37055621966719626
2023-08-03 14:58:04,181 - epoch:6, training loss:2.2685 validation loss:0.3700
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.37404194436967375 0.3629697557538748
2023-08-03 14:58:51,501 - epoch:7, training loss:2.2398 validation loss:0.3740
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.3700234517455101 0.3719156028702855
2023-08-03 14:59:37,762 - epoch:8, training loss:2.1921 validation loss:0.3700
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.36953861713409425 0.3680699959397316
2023-08-03 15:00:25,864 - epoch:9, training loss:2.1197 validation loss:0.3695
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.37357829585671426 0.37341509237885473
2023-08-03 15:01:13,632 - epoch:10, training loss:1.9911 validation loss:0.3736
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.37600971907377245 0.36887283250689507
2023-08-03 15:02:01,050 - epoch:11, training loss:1.9290 validation loss:0.3760
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.37607965283095834 0.37209227252751587
2023-08-03 15:02:48,858 - epoch:12, training loss:1.7866 validation loss:0.3761
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.37782846987247465 0.367181565053761
2023-08-03 15:03:36,363 - epoch:13, training loss:1.7961 validation loss:0.3778
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.3725086208432913 0.3735627427697182
2023-08-03 15:04:24,053 - epoch:14, training loss:1.7689 validation loss:0.3725
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.3782374855130911 0.37027135863900185
2023-08-03 15:05:12,735 - epoch:15, training loss:1.8226 validation loss:0.3782
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.38035629242658614 0.37248214371502397
2023-08-03 15:05:59,232 - epoch:16, training loss:1.7602 validation loss:0.3804
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.3781389217823744 0.37141972705721854
2023-08-03 15:06:46,986 - epoch:17, training loss:1.7354 validation loss:0.3781
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.3764084469527006 0.37432384956628084
2023-08-03 15:07:34,510 - epoch:18, training loss:1.7663 validation loss:0.3764
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.38222819566726685 0.37462137416005137
2023-08-03 15:08:22,890 - epoch:19, training loss:1.7464 validation loss:0.3822
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.38192065209150317 0.3752113949507475
2023-08-03 15:09:09,942 - epoch:20, training loss:1.7212 validation loss:0.3819
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.38105298653244973 0.3755831252783537
2023-08-03 15:09:57,709 - epoch:21, training loss:1.7509 validation loss:0.3811
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.3795709043741226 0.3768599022179842
2023-08-03 15:10:45,574 - epoch:22, training loss:1.7656 validation loss:0.3796
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.38283358290791514 0.37603242583572866
2023-08-03 15:11:32,980 - epoch:23, training loss:1.7696 validation loss:0.3828
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.3816397275775671 0.3778599929064512
2023-08-03 15:12:19,839 - epoch:24, training loss:1.7549 validation loss:0.3816
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.3813490532338619 0.3787209201604128
2023-08-03 15:13:08,131 - epoch:25, training loss:1.7632 validation loss:0.3813
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.3837870877236128 0.3792259130626917
2023-08-03 15:13:55,078 - epoch:26, training loss:1.7854 validation loss:0.3838
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.38107874281704424 0.37722606025636196
2023-08-03 15:14:42,868 - epoch:27, training loss:1.7375 validation loss:0.3811
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.3812083758413792 0.37804316692054274
2023-08-03 15:15:30,676 - epoch:28, training loss:1.7626 validation loss:0.3812
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.3823201820254326 0.3773680154234171
2023-08-03 15:16:19,164 - epoch:29, training loss:1.7894 validation loss:0.3823
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-14:52:35.824833/0/0.3642_epoch_3.pkl  &  0.358781972900033
2023-08-03 15:16:23,922 - [*] loss:0.3642
2023-08-03 15:16:23,932 - [*] phase 0, testing
2023-08-03 15:16:24,089 - T:336	MAE	0.397195	RMSE	0.359621	MAPE	164.178407
2023-08-03 15:16:24,090 - 336	mae	0.3972	
2023-08-03 15:16:24,090 - 336	rmse	0.3596	
2023-08-03 15:16:24,090 - 336	mape	164.1784	
2023-08-03 15:16:29,301 - [*] loss:0.3588
2023-08-03 15:16:29,310 - [*] phase 0, testing
2023-08-03 15:16:29,456 - T:336	MAE	0.392363	RMSE	0.354252	MAPE	159.008849
2023-08-03 15:16:29,458 - 336	mae	0.3924	
2023-08-03 15:16:29,458 - 336	rmse	0.3543	
2023-08-03 15:16:29,458 - 336	mape	159.0088	
2023-08-03 15:16:32,231 - logger name:exp/ECL-PatchTST2023-08-03-15:16:32.230464/ECL-PatchTST.log
2023-08-03 15:16:32,231 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 3, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-15:16:32.230464', 'path': 'exp/ECL-PatchTST2023-08-03-15:16:32.230464', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 15:16:32,232 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 15:16:32,547 - [*] phase 0 Dataset load!
2023-08-03 15:16:33,849 - [*] phase 0 Training start
train 7969
2023-08-03 15:17:03,966 - epoch:0, training loss:0.2540 validation loss:0.1794
train 7969
vs, vt 0.1794192695990205 0.18175389114767312
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.17219245620071888 0.1707881884649396
2023-08-03 15:18:07,232 - epoch:1, training loss:0.6968 validation loss:0.1722
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.16645541805773972 0.1619849346578121
2023-08-03 15:18:52,336 - epoch:2, training loss:0.6325 validation loss:0.1665
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.1644762043841183 0.16056026853621005
2023-08-03 15:19:35,579 - epoch:3, training loss:0.5576 validation loss:0.1645
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16858129426836968 0.16112911626696585
2023-08-03 15:20:20,699 - epoch:4, training loss:0.4899 validation loss:0.1686
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.16776248011738062 0.1623426300473511
2023-08-03 15:21:06,447 - epoch:5, training loss:0.4576 validation loss:0.1678
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.17293408047407866 0.16894247923046352
2023-08-03 15:21:52,559 - epoch:6, training loss:0.4430 validation loss:0.1729
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.16832183822989463 0.16601962838321924
2023-08-03 15:22:41,181 - epoch:7, training loss:0.4368 validation loss:0.1683
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.17069347947835922 0.1652716943062842
2023-08-03 15:23:30,133 - epoch:8, training loss:0.4311 validation loss:0.1707
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.16783525962382556 0.16504459362477064
2023-08-03 15:24:16,469 - epoch:9, training loss:0.4236 validation loss:0.1678
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.16847845632582903 0.16634646328166128
2023-08-03 15:25:01,633 - epoch:10, training loss:0.4185 validation loss:0.1685
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.1702960379421711 0.16552058598026634
2023-08-03 15:25:45,911 - epoch:11, training loss:0.4125 validation loss:0.1703
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.1711767066270113 0.16624961318448186
2023-08-03 15:26:30,799 - epoch:12, training loss:0.4097 validation loss:0.1712
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.16918242834508418 0.1640816773287952
2023-08-03 15:27:17,079 - epoch:13, training loss:0.4058 validation loss:0.1692
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.16902255564928054 0.16621865751221776
2023-08-03 15:28:03,636 - epoch:14, training loss:0.4042 validation loss:0.1690
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.17141032144427298 0.16583790872246026
2023-08-03 15:28:52,275 - epoch:15, training loss:0.4010 validation loss:0.1714
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.1702234748750925 0.16646230556070804
2023-08-03 15:29:39,815 - epoch:16, training loss:0.3995 validation loss:0.1702
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.17104079723358154 0.16656024064868688
2023-08-03 15:30:25,721 - epoch:17, training loss:0.3973 validation loss:0.1710
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.17109924480319022 0.1680300365202129
2023-08-03 15:31:10,936 - epoch:18, training loss:0.3961 validation loss:0.1711
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.17081003300845624 0.16754103312268853
2023-08-03 15:31:53,023 - epoch:19, training loss:0.3949 validation loss:0.1708
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.17090033572167157 0.16798321362584828
2023-08-03 15:32:36,646 - epoch:20, training loss:0.3936 validation loss:0.1709
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.17149447575211524 0.1673157167620957
2023-08-03 15:33:20,406 - epoch:21, training loss:0.3923 validation loss:0.1715
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.17109071519225835 0.16773984376341106
2023-08-03 15:34:06,576 - epoch:22, training loss:0.3915 validation loss:0.1711
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.1714202580973506 0.16743239341303706
2023-08-03 15:34:53,859 - epoch:23, training loss:0.3913 validation loss:0.1714
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.17146263998001815 0.16762749375775457
2023-08-03 15:35:41,980 - epoch:24, training loss:0.3902 validation loss:0.1715
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.17134692203253507 0.16787741743028164
2023-08-03 15:36:30,460 - epoch:25, training loss:0.3900 validation loss:0.1713
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.17158559076488017 0.1684064638800919
2023-08-03 15:37:18,769 - epoch:26, training loss:0.3899 validation loss:0.1716
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.17119739037007092 0.16740667494013906
2023-08-03 15:38:04,866 - epoch:27, training loss:0.3896 validation loss:0.1712
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.17147894185036422 0.16788331689313055
2023-08-03 15:38:51,013 - epoch:28, training loss:0.3889 validation loss:0.1715
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.17179145980626345 0.16772461999207736
2023-08-03 15:39:36,301 - epoch:29, training loss:0.3905 validation loss:0.1718
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-15:16:32.230464/0/0.1645_epoch_3.pkl  &  0.16056026853621005
2023-08-03 15:39:42,332 - [*] loss:0.3697
2023-08-03 15:39:42,343 - [*] phase 0, testing
2023-08-03 15:39:42,491 - T:336	MAE	0.398368	RMSE	0.365050	MAPE	162.002993
2023-08-03 15:39:42,492 - 336	mae	0.3984	
2023-08-03 15:39:42,492 - 336	rmse	0.3650	
2023-08-03 15:39:42,492 - 336	mape	162.0030	
2023-08-03 15:39:48,624 - [*] loss:0.3593
2023-08-03 15:39:48,634 - [*] phase 0, testing
2023-08-03 15:39:48,788 - T:336	MAE	0.394383	RMSE	0.354730	MAPE	159.379101
2023-08-03 15:39:48,790 - 336	mae	0.3944	
2023-08-03 15:39:48,790 - 336	rmse	0.3547	
2023-08-03 15:39:48,790 - 336	mape	159.3791	
2023-08-03 15:39:51,166 - logger name:exp/ECL-PatchTST2023-08-03-15:39:51.166427/ECL-PatchTST.log
2023-08-03 15:39:51,167 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-15:39:51.166427', 'path': 'exp/ECL-PatchTST2023-08-03-15:39:51.166427', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 15:39:51,167 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 15:39:51,381 - [*] phase 0 Dataset load!
2023-08-03 15:39:52,470 - [*] phase 0 Training start
train 7969
2023-08-03 15:40:21,752 - epoch:0, training loss:0.2705 validation loss:0.1867
train 7969
vs, vt 0.1866797858849168 0.18489858955144883
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.17678104415535928 0.17481709364801645
2023-08-03 15:41:20,003 - epoch:1, training loss:1.4124 validation loss:0.1768
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.16952955555170773 0.17689544036984445
2023-08-03 15:42:00,718 - epoch:2, training loss:1.1377 validation loss:0.1695
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.1644246056675911 0.16581294257193804
2023-08-03 15:42:38,637 - epoch:3, training loss:0.8792 validation loss:0.1644
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16372620156034828 0.16539810718968512
2023-08-03 15:43:15,430 - epoch:4, training loss:0.7474 validation loss:0.1637
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.1632874934002757 0.16635963264852763
2023-08-03 15:43:55,841 - epoch:5, training loss:0.6861 validation loss:0.1633
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.16148472214117646 0.16544905388727785
2023-08-03 15:44:37,825 - epoch:6, training loss:0.6598 validation loss:0.1615
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.1622985566034913 0.16407940313220024
2023-08-03 15:45:23,439 - epoch:7, training loss:0.6434 validation loss:0.1623
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.16188363786786794 0.16041282834485174
2023-08-03 15:46:05,114 - epoch:8, training loss:0.6374 validation loss:0.1619
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.16193614676594734 0.16076799649745227
2023-08-03 15:46:44,840 - epoch:9, training loss:0.6321 validation loss:0.1619
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.16221563573926687 0.1636753600090742
2023-08-03 15:47:22,938 - epoch:10, training loss:0.6264 validation loss:0.1622
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.16264732275158167 0.16302890479564666
2023-08-03 15:48:01,386 - epoch:11, training loss:0.6214 validation loss:0.1626
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.16355696013197302 0.16167438495904207
2023-08-03 15:48:42,504 - epoch:12, training loss:0.6229 validation loss:0.1636
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.16250956617295742 0.16466804621741177
2023-08-03 15:49:23,259 - epoch:13, training loss:0.6194 validation loss:0.1625
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.1618585217744112 0.1627862320281565
2023-08-03 15:50:06,606 - epoch:14, training loss:0.6178 validation loss:0.1619
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.16316726254299282 0.16416067853569985
2023-08-03 15:50:47,134 - epoch:15, training loss:0.6201 validation loss:0.1632
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.1643098789267242 0.16468064542859792
2023-08-03 15:51:25,961 - epoch:16, training loss:0.6128 validation loss:0.1643
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.16434050016105176 0.16435703029856086
2023-08-03 15:52:04,638 - epoch:17, training loss:0.6119 validation loss:0.1643
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.16379408026114106 0.1648788282647729
2023-08-03 15:52:42,361 - epoch:18, training loss:0.6091 validation loss:0.1638
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.16418391605839133 0.16572296293452382
2023-08-03 15:53:22,193 - epoch:19, training loss:0.6116 validation loss:0.1642
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.16321089528501034 0.16581326387822629
2023-08-03 15:54:04,097 - epoch:20, training loss:0.6126 validation loss:0.1632
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.16385364858433604 0.16567146992310883
2023-08-03 15:54:46,206 - epoch:21, training loss:0.6106 validation loss:0.1639
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.16389599265530705 0.1655285419896245
2023-08-03 15:55:31,784 - epoch:22, training loss:0.6107 validation loss:0.1639
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.16441094120964408 0.16507648406550288
2023-08-03 15:56:12,185 - epoch:23, training loss:0.6100 validation loss:0.1644
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.1643658610060811 0.16567727392539383
2023-08-03 15:56:50,506 - epoch:24, training loss:0.6079 validation loss:0.1644
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.16417109863832594 0.16550177475437522
2023-08-03 15:57:25,658 - epoch:25, training loss:0.6086 validation loss:0.1642
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.16413126466795802 0.16529113203287124
2023-08-03 15:58:04,123 - epoch:26, training loss:0.6090 validation loss:0.1641
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.16426505902782082 0.16530020479112864
2023-08-03 15:58:45,356 - epoch:27, training loss:0.6082 validation loss:0.1643
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.1642352889291942 0.16559553537517785
2023-08-03 15:59:27,602 - epoch:28, training loss:0.6082 validation loss:0.1642
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.16450080815702678 0.16569589553400874
2023-08-03 16:00:11,212 - epoch:29, training loss:0.6094 validation loss:0.1645
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-15:39:51.166427/0/0.1615_epoch_6.pkl  &  0.16041282834485174
2023-08-03 16:00:16,637 - [*] loss:0.3635
2023-08-03 16:00:16,645 - [*] phase 0, testing
2023-08-03 16:00:16,803 - T:336	MAE	0.392836	RMSE	0.358821	MAPE	159.532630
2023-08-03 16:00:16,803 - 336	mae	0.3928	
2023-08-03 16:00:16,803 - 336	rmse	0.3588	
2023-08-03 16:00:16,804 - 336	mape	159.5326	
2023-08-03 16:00:22,442 - [*] loss:0.3622
2023-08-03 16:00:22,451 - [*] phase 0, testing
2023-08-03 16:00:22,626 - T:336	MAE	0.390547	RMSE	0.357590	MAPE	155.586922
2023-08-03 16:00:22,627 - 336	mae	0.3905	
2023-08-03 16:00:22,627 - 336	rmse	0.3576	
2023-08-03 16:00:22,627 - 336	mape	155.5869	
2023-08-03 16:00:25,046 - logger name:exp/ECL-PatchTST2023-08-03-16:00:25.045696/ECL-PatchTST.log
2023-08-03 16:00:25,046 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 3, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-16:00:25.045696', 'path': 'exp/ECL-PatchTST2023-08-03-16:00:25.045696', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 16:00:25,047 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 16:00:25,276 - [*] phase 0 Dataset load!
2023-08-03 16:00:26,393 - [*] phase 0 Training start
train 7969
2023-08-03 16:00:58,175 - epoch:0, training loss:0.2540 validation loss:0.1794
train 7969
vs, vt 0.1794192695990205 0.18175389114767312
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.1726553848013282 0.1707838514819741
2023-08-03 16:02:01,856 - epoch:1, training loss:3.0720 validation loss:0.1727
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.16876038089394568 0.1618475504219532
2023-08-03 16:02:47,636 - epoch:2, training loss:2.4359 validation loss:0.1688
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.16466639805585145 0.16115978173911572
2023-08-03 16:03:32,576 - epoch:3, training loss:1.6024 validation loss:0.1647
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16529536973685027 0.16233243411406875
2023-08-03 16:04:15,464 - epoch:4, training loss:1.0961 validation loss:0.1653
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.16443589925765992 0.1623446609824896
2023-08-03 16:04:59,420 - epoch:5, training loss:0.9289 validation loss:0.1644
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.16355646206066013 0.1658447954803705
2023-08-03 16:05:44,856 - epoch:6, training loss:0.8652 validation loss:0.1636
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.163021039031446 0.16051811892539264
2023-08-03 16:06:33,207 - epoch:7, training loss:0.8340 validation loss:0.1630
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.16392435571178793 0.1634375996887684
2023-08-03 16:07:21,164 - epoch:8, training loss:0.8060 validation loss:0.1639
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.16232145763933659 0.16447215834632517
2023-08-03 16:08:07,556 - epoch:9, training loss:0.7702 validation loss:0.1623
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.1652420441620052 0.1629269919358194
2023-08-03 16:08:53,066 - epoch:10, training loss:0.7356 validation loss:0.1652
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.16304980050772427 0.16140788076445461
2023-08-03 16:09:37,121 - epoch:11, training loss:0.7085 validation loss:0.1630
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.1672424143180251 0.16161118661984802
2023-08-03 16:10:21,424 - epoch:12, training loss:0.6682 validation loss:0.1672
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.16381643591448664 0.16231942912563682
2023-08-03 16:11:07,945 - epoch:13, training loss:0.6650 validation loss:0.1638
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.1650613217614591 0.16212378898635507
2023-08-03 16:11:55,563 - epoch:14, training loss:0.6634 validation loss:0.1651
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.16560871666297317 0.16166583122685552
2023-08-03 16:12:44,285 - epoch:15, training loss:0.6675 validation loss:0.1656
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.16408183546736838 0.1619580236263573
2023-08-03 16:13:32,331 - epoch:16, training loss:0.6684 validation loss:0.1641
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.16491519175469876 0.16140213711187243
2023-08-03 16:14:17,679 - epoch:17, training loss:0.6691 validation loss:0.1649
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.1650694955140352 0.16270528193563222
2023-08-03 16:15:01,420 - epoch:18, training loss:0.6739 validation loss:0.1651
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.1653009472414851 0.1631381562910974
2023-08-03 16:15:46,865 - epoch:19, training loss:0.6693 validation loss:0.1653
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.16569429626688362 0.1630512916482985
2023-08-03 16:16:32,913 - epoch:20, training loss:0.6645 validation loss:0.1657
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.16548738637939095 0.16269694715738298
2023-08-03 16:17:20,381 - epoch:21, training loss:0.6643 validation loss:0.1655
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.1652210557833314 0.16284234542399645
2023-08-03 16:18:08,768 - epoch:22, training loss:0.6721 validation loss:0.1652
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.1657900813035667 0.16284742970019578
2023-08-03 16:18:57,103 - epoch:23, training loss:0.6778 validation loss:0.1658
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.1652425780892372 0.16301339119672775
2023-08-03 16:19:41,753 - epoch:24, training loss:0.6681 validation loss:0.1652
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.16541614551097156 0.1631583644077182
2023-08-03 16:20:24,874 - epoch:25, training loss:0.6780 validation loss:0.1654
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.16611873814836145 0.16356095299124718
2023-08-03 16:21:11,349 - epoch:26, training loss:0.6784 validation loss:0.1661
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.1652687189169228 0.16290587466210127
2023-08-03 16:21:53,820 - epoch:27, training loss:0.6688 validation loss:0.1653
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.16544675529003144 0.16331424470990896
2023-08-03 16:22:41,346 - epoch:28, training loss:0.6718 validation loss:0.1654
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.1654344817623496 0.16305771078914405
2023-08-03 16:23:28,689 - epoch:29, training loss:0.6786 validation loss:0.1654
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-16:00:25.045696/0/0.1623_epoch_9.pkl  &  0.16051811892539264
2023-08-03 16:23:32,701 - [*] loss:0.3647
2023-08-03 16:23:32,731 - [*] phase 0, testing
2023-08-03 16:23:33,624 - T:336	MAE	0.394625	RMSE	0.360322	MAPE	161.190128
2023-08-03 16:23:33,625 - 336	mae	0.3946	
2023-08-03 16:23:33,625 - 336	rmse	0.3603	
2023-08-03 16:23:33,625 - 336	mape	161.1901	
2023-08-03 16:23:38,255 - [*] loss:0.3616
2023-08-03 16:23:38,263 - [*] phase 0, testing
2023-08-03 16:23:39,045 - T:336	MAE	0.392755	RMSE	0.357254	MAPE	159.754384
2023-08-03 16:23:39,046 - 336	mae	0.3928	
2023-08-03 16:23:39,046 - 336	rmse	0.3573	
2023-08-03 16:23:39,046 - 336	mape	159.7544	
2023-08-03 16:23:41,496 - logger name:exp/ECL-PatchTST2023-08-03-16:23:41.495756/ECL-PatchTST.log
2023-08-03 16:23:41,496 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-16:23:41.495756', 'path': 'exp/ECL-PatchTST2023-08-03-16:23:41.495756', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 16:23:41,496 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 16:23:41,755 - [*] phase 0 Dataset load!
2023-08-03 16:23:42,924 - [*] phase 0 Training start
train 7969
2023-08-03 16:24:14,841 - epoch:0, training loss:0.7817 validation loss:0.4167
train 7969
vs, vt 0.4167296431958675 0.4123847097158432
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.3876314040273428 0.3871305737644434
2023-08-03 16:25:08,242 - epoch:1, training loss:0.7850 validation loss:0.3876
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.37420373409986496 0.38321346528828143
2023-08-03 16:25:49,308 - epoch:2, training loss:0.7256 validation loss:0.3742
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.383265845105052 0.37648963592946527
2023-08-03 16:26:32,221 - epoch:3, training loss:0.6805 validation loss:0.3833
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.3827069777995348 0.37858289033174514
2023-08-03 16:27:13,252 - epoch:4, training loss:0.6548 validation loss:0.3827
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.3871807973831892 0.37936418838799
2023-08-03 16:27:51,654 - epoch:5, training loss:0.6296 validation loss:0.3872
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.39621306136250495 0.3698203571140766
2023-08-03 16:28:29,378 - epoch:6, training loss:0.6046 validation loss:0.3962
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.3946853868663311 0.3632135380059481
2023-08-03 16:29:07,298 - epoch:7, training loss:0.5824 validation loss:0.3947
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.39806302525103093 0.3680301671847701
2023-08-03 16:29:46,039 - epoch:8, training loss:0.5665 validation loss:0.3981
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.3968516692519188 0.36661980152130125
2023-08-03 16:30:26,362 - epoch:9, training loss:0.5552 validation loss:0.3969
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.39830633625388145 0.3673460818827152
2023-08-03 16:31:09,264 - epoch:10, training loss:0.5446 validation loss:0.3983
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.4062670383602381 0.36994571927934883
2023-08-03 16:31:50,391 - epoch:11, training loss:0.5369 validation loss:0.4063
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.4073169704526663 0.3746752716600895
2023-08-03 16:32:28,153 - epoch:12, training loss:0.5289 validation loss:0.4073
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.40702265202999116 0.37205852307379245
2023-08-03 16:33:05,651 - epoch:13, training loss:0.5229 validation loss:0.4070
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.40798113904893396 0.3718140233308077
2023-08-03 16:33:43,885 - epoch:14, training loss:0.5173 validation loss:0.4080
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.4137137483805418 0.3720414288341999
2023-08-03 16:34:23,459 - epoch:15, training loss:0.5121 validation loss:0.4137
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.4082370851188898 0.37868351098150016
2023-08-03 16:35:05,271 - epoch:16, training loss:0.5073 validation loss:0.4082
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.4129898238927126 0.3762074578553438
2023-08-03 16:35:47,791 - epoch:17, training loss:0.5045 validation loss:0.4130
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.4131840441375971 0.379788401350379
2023-08-03 16:36:30,272 - epoch:18, training loss:0.5002 validation loss:0.4132
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.412611224129796 0.3769471328705549
2023-08-03 16:37:10,789 - epoch:19, training loss:0.4970 validation loss:0.4126
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.4163555298000574 0.3786321684718132
2023-08-03 16:37:50,170 - epoch:20, training loss:0.4935 validation loss:0.4164
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.4164810419082642 0.3801243081688881
2023-08-03 16:38:25,722 - epoch:21, training loss:0.4929 validation loss:0.4165
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.4154406826943159 0.3767362147569656
2023-08-03 16:39:03,019 - epoch:22, training loss:0.4908 validation loss:0.4154
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.4173882454633713 0.3798141680657864
2023-08-03 16:39:44,253 - epoch:23, training loss:0.4896 validation loss:0.4174
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.4151085015386343 0.37885707803070545
2023-08-03 16:40:27,043 - epoch:24, training loss:0.4890 validation loss:0.4151
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.41749700978398324 0.3788635164499283
2023-08-03 16:41:08,791 - epoch:25, training loss:0.4875 validation loss:0.4175
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.41619018092751503 0.378339447453618
2023-08-03 16:41:49,374 - epoch:26, training loss:0.4869 validation loss:0.4162
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.4186307225376368 0.3795242987573147
2023-08-03 16:42:27,671 - epoch:27, training loss:0.4876 validation loss:0.4186
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.4185335271060467 0.3785880718380213
2023-08-03 16:43:05,802 - epoch:28, training loss:0.4858 validation loss:0.4185
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.41656267531216146 0.3783268123865128
2023-08-03 16:43:44,903 - epoch:29, training loss:0.4858 validation loss:0.4166
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-16:23:41.495756/0/0.3742_epoch_2.pkl  &  0.3632135380059481
2023-08-03 16:43:49,443 - [*] loss:0.3742
2023-08-03 16:43:49,451 - [*] phase 0, testing
2023-08-03 16:43:50,126 - T:336	MAE	0.405891	RMSE	0.369685	MAPE	169.481814
2023-08-03 16:43:50,126 - 336	mae	0.4059	
2023-08-03 16:43:50,127 - 336	rmse	0.3697	
2023-08-03 16:43:50,127 - 336	mape	169.4818	
2023-08-03 16:43:55,914 - [*] loss:0.3632
2023-08-03 16:43:55,923 - [*] phase 0, testing
2023-08-03 16:43:56,657 - T:336	MAE	0.393712	RMSE	0.359039	MAPE	158.533692
2023-08-03 16:43:56,658 - 336	mae	0.3937	
2023-08-03 16:43:56,658 - 336	rmse	0.3590	
2023-08-03 16:43:56,658 - 336	mape	158.5337	
2023-08-03 16:43:59,204 - logger name:exp/ECL-PatchTST2023-08-03-16:43:59.203866/ECL-PatchTST.log
2023-08-03 16:43:59,204 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 3, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-16:43:59.203866', 'path': 'exp/ECL-PatchTST2023-08-03-16:43:59.203866', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 16:43:59,205 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 16:43:59,467 - [*] phase 0 Dataset load!
2023-08-03 16:44:00,725 - [*] phase 0 Training start
train 7969
2023-08-03 16:44:32,727 - epoch:0, training loss:0.7274 validation loss:0.4006
train 7969
vs, vt 0.4006344098597765 0.40311049185693265
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.38278107792139054 0.3785765178501606
2023-08-03 16:45:30,668 - epoch:1, training loss:8.0051 validation loss:0.3828
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.3717516142874956 0.35979535803198814
2023-08-03 16:46:13,976 - epoch:2, training loss:6.0388 validation loss:0.3718
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.374500360712409 0.36515850871801375
2023-08-03 16:46:56,533 - epoch:3, training loss:3.7902 validation loss:0.3745
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.3661212593317032 0.37107359282672403
2023-08-03 16:47:39,248 - epoch:4, training loss:2.6654 validation loss:0.3661
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.3681700900197029 0.3633160825818777
2023-08-03 16:48:21,231 - epoch:5, training loss:2.3295 validation loss:0.3682
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.36947649307549 0.36309602819383147
2023-08-03 16:49:04,211 - epoch:6, training loss:2.2484 validation loss:0.3695
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.3649595957249403 0.3726449962705374
2023-08-03 16:49:46,633 - epoch:7, training loss:2.1881 validation loss:0.3650
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.3721823714673519 0.377641748264432
2023-08-03 16:50:28,683 - epoch:8, training loss:2.1353 validation loss:0.3722
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.3654684633016586 0.3678641924634576
2023-08-03 16:51:10,968 - epoch:9, training loss:2.0971 validation loss:0.3655
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.36863492913544177 0.3732446406036615
2023-08-03 16:51:52,557 - epoch:10, training loss:1.9156 validation loss:0.3686
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.37208973094820974 0.36732310578227045
2023-08-03 16:52:34,408 - epoch:11, training loss:1.8932 validation loss:0.3721
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.3707676358520985 0.37125636301934717
2023-08-03 16:53:16,300 - epoch:12, training loss:1.8773 validation loss:0.3708
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.37394495531916616 0.3758563924580812
2023-08-03 16:54:00,239 - epoch:13, training loss:1.8639 validation loss:0.3739
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.378185436502099 0.3782406523823738
2023-08-03 16:54:43,980 - epoch:14, training loss:1.8044 validation loss:0.3782
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.3789943367242813 0.37975929472595454
2023-08-03 16:55:28,198 - epoch:15, training loss:1.7859 validation loss:0.3790
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.3786923099309206 0.3833677712827921
2023-08-03 16:56:12,392 - epoch:16, training loss:1.8054 validation loss:0.3787
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.37849221266806127 0.3818175319582224
2023-08-03 16:56:54,582 - epoch:17, training loss:1.7889 validation loss:0.3785
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.3783392108976841 0.3788463331758976
2023-08-03 16:57:41,433 - epoch:18, training loss:1.7547 validation loss:0.3783
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.3822516709566116 0.37909388486295936
2023-08-03 16:58:25,966 - epoch:19, training loss:1.7404 validation loss:0.3823
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.3780580922961235 0.38434222973883153
2023-08-03 16:59:14,653 - epoch:20, training loss:1.7301 validation loss:0.3781
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.38195620365440847 0.3826632671058178
2023-08-03 17:00:06,410 - epoch:21, training loss:1.7233 validation loss:0.3820
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.38078772388398646 0.382678822055459
2023-08-03 17:00:51,006 - epoch:22, training loss:1.7422 validation loss:0.3808
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.38072658255696296 0.3848198974505067
2023-08-03 17:01:33,667 - epoch:23, training loss:1.7403 validation loss:0.3807
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.38406244181096555 0.3841231931000948
2023-08-03 17:02:17,714 - epoch:24, training loss:1.7191 validation loss:0.3841
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.382540288567543 0.383509392850101
2023-08-03 17:03:00,428 - epoch:25, training loss:1.7037 validation loss:0.3825
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.38158134631812574 0.38350620605051516
2023-08-03 17:03:44,789 - epoch:26, training loss:1.7170 validation loss:0.3816
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.38130404315888883 0.38271879851818086
2023-08-03 17:04:28,283 - epoch:27, training loss:1.7192 validation loss:0.3813
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.3822822865098715 0.38438023775815966
2023-08-03 17:05:12,435 - epoch:28, training loss:1.7258 validation loss:0.3823
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.38144002221524714 0.3834470685571432
2023-08-03 17:05:56,253 - epoch:29, training loss:1.7157 validation loss:0.3814
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-16:43:59.203866/0/0.365_epoch_7.pkl  &  0.35979535803198814
2023-08-03 17:06:02,063 - [*] loss:0.3650
2023-08-03 17:06:02,071 - [*] phase 0, testing
2023-08-03 17:06:02,210 - T:336	MAE	0.398846	RMSE	0.360663	MAPE	166.677141
2023-08-03 17:06:02,211 - 336	mae	0.3988	
2023-08-03 17:06:02,211 - 336	rmse	0.3607	
2023-08-03 17:06:02,211 - 336	mape	166.6771	
2023-08-03 17:06:08,153 - [*] loss:0.3598
2023-08-03 17:06:08,162 - [*] phase 0, testing
2023-08-03 17:06:08,301 - T:336	MAE	0.402198	RMSE	0.355411	MAPE	166.261864
2023-08-03 17:06:08,302 - 336	mae	0.4022	
2023-08-03 17:06:08,302 - 336	rmse	0.3554	
2023-08-03 17:06:08,302 - 336	mape	166.2619	
2023-08-03 17:06:10,781 - logger name:exp/ECL-PatchTST2023-08-03-17:06:10.780964/ECL-PatchTST.log
2023-08-03 17:06:10,782 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 3, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-17:06:10.780964', 'path': 'exp/ECL-PatchTST2023-08-03-17:06:10.780964', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 17:06:10,782 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 17:06:11,068 - [*] phase 0 Dataset load!
2023-08-03 17:06:12,225 - [*] phase 0 Training start
train 7969
2023-08-03 17:06:41,985 - epoch:0, training loss:0.2543 validation loss:0.1796
train 7969
vs, vt 0.17958344966173173 0.1804242931306362
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.17191553264856338 0.1693336084485054
2023-08-03 17:07:42,370 - epoch:1, training loss:0.7124 validation loss:0.1719
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.1664147825911641 0.1611674040555954
2023-08-03 17:08:27,999 - epoch:2, training loss:0.6457 validation loss:0.1664
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.16777601409703494 0.1630656125023961
2023-08-03 17:09:15,638 - epoch:3, training loss:0.5627 validation loss:0.1678
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.1690609699115157 0.16463087052106856
2023-08-03 17:10:04,954 - epoch:4, training loss:0.4944 validation loss:0.1691
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.16831473391503096 0.16279305024072527
2023-08-03 17:10:52,202 - epoch:5, training loss:0.4623 validation loss:0.1683
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.16986332572996615 0.1636613524518907
2023-08-03 17:11:37,213 - epoch:6, training loss:0.4477 validation loss:0.1699
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.16865558903664352 0.16301287906244397
2023-08-03 17:12:20,735 - epoch:7, training loss:0.4380 validation loss:0.1687
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.17048779223114252 0.1607391812838614
2023-08-03 17:13:04,634 - epoch:8, training loss:0.4327 validation loss:0.1705
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.1709996249526739 0.16158136911690235
2023-08-03 17:13:49,155 - epoch:9, training loss:0.4278 validation loss:0.1710
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.17041796240955592 0.16328059146180748
2023-08-03 17:14:35,516 - epoch:10, training loss:0.4207 validation loss:0.1704
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.17358865980058907 0.16136064287275076
2023-08-03 17:15:23,045 - epoch:11, training loss:0.4153 validation loss:0.1736
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.1727815793827176 0.1619145456701517
2023-08-03 17:16:12,150 - epoch:12, training loss:0.4102 validation loss:0.1728
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.1710619829595089 0.1623569334857166
2023-08-03 17:17:00,725 - epoch:13, training loss:0.4073 validation loss:0.1711
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.1725564409047365 0.16128063425421715
2023-08-03 17:17:47,722 - epoch:14, training loss:0.4057 validation loss:0.1726
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.17358474098145962 0.16229886943474411
2023-08-03 17:18:31,179 - epoch:15, training loss:0.4037 validation loss:0.1736
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.17416374944150448 0.16142202988266946
2023-08-03 17:19:15,625 - epoch:16, training loss:0.4017 validation loss:0.1742
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.17446104791015388 0.16162440776824952
2023-08-03 17:20:00,575 - epoch:17, training loss:0.3991 validation loss:0.1745
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.17436025030910968 0.16180320139974355
2023-08-03 17:20:46,392 - epoch:18, training loss:0.3974 validation loss:0.1744
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.17419263180345296 0.16261358885094523
2023-08-03 17:21:33,734 - epoch:19, training loss:0.3967 validation loss:0.1742
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.17498129103332757 0.16224857997149228
2023-08-03 17:22:22,996 - epoch:20, training loss:0.3951 validation loss:0.1750
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.17540215998888015 0.16303626531735063
2023-08-03 17:23:10,555 - epoch:21, training loss:0.3952 validation loss:0.1754
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.1745798572897911 0.16314233150333166
2023-08-03 17:23:56,216 - epoch:22, training loss:0.3936 validation loss:0.1746
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.17473838459700347 0.16251824535429477
2023-08-03 17:24:39,911 - epoch:23, training loss:0.3932 validation loss:0.1747
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.17452678717672826 0.16301703779026866
2023-08-03 17:25:24,174 - epoch:24, training loss:0.3925 validation loss:0.1745
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.1750728277489543 0.16262340666726233
2023-08-03 17:26:08,757 - epoch:25, training loss:0.3928 validation loss:0.1751
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.17493668477982283 0.16291178427636624
2023-08-03 17:26:54,807 - epoch:26, training loss:0.3922 validation loss:0.1749
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.17517040446400642 0.16259889593347907
2023-08-03 17:27:41,832 - epoch:27, training loss:0.3922 validation loss:0.1752
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.1750170709565282 0.1629536421969533
2023-08-03 17:28:22,905 - epoch:28, training loss:0.3916 validation loss:0.1750
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.17459122221916915 0.16284648664295673
2023-08-03 17:29:04,595 - epoch:29, training loss:0.3912 validation loss:0.1746
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-17:06:10.780964/0/0.1664_epoch_2.pkl  &  0.1607391812838614
2023-08-03 17:29:10,046 - [*] loss:0.3734
2023-08-03 17:29:10,056 - [*] phase 0, testing
2023-08-03 17:29:10,218 - T:336	MAE	0.401481	RMSE	0.368763	MAPE	164.305961
2023-08-03 17:29:10,219 - 336	mae	0.4015	
2023-08-03 17:29:10,219 - 336	rmse	0.3688	
2023-08-03 17:29:10,219 - 336	mape	164.3060	
2023-08-03 17:29:16,083 - [*] loss:0.3632
2023-08-03 17:29:16,092 - [*] phase 0, testing
2023-08-03 17:29:16,249 - T:336	MAE	0.391219	RMSE	0.358679	MAPE	155.383301
2023-08-03 17:29:16,250 - 336	mae	0.3912	
2023-08-03 17:29:16,250 - 336	rmse	0.3587	
2023-08-03 17:29:16,250 - 336	mape	155.3833	
2023-08-03 17:29:18,867 - logger name:exp/ECL-PatchTST2023-08-03-17:29:18.866821/ECL-PatchTST.log
2023-08-03 17:29:18,867 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-17:29:18.866821', 'path': 'exp/ECL-PatchTST2023-08-03-17:29:18.866821', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 17:29:18,868 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 17:29:19,133 - [*] phase 0 Dataset load!
2023-08-03 17:29:20,267 - [*] phase 0 Training start
train 7969
2023-08-03 17:29:47,550 - epoch:0, training loss:0.2713 validation loss:0.1861
train 7969
vs, vt 0.18606640696525573 0.18458142075687647
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.1762928307056427 0.17420527450740336
2023-08-03 17:30:40,888 - epoch:1, training loss:1.3929 validation loss:0.1763
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.16739275064319373 0.17534079179167747
2023-08-03 17:31:19,517 - epoch:2, training loss:1.1043 validation loss:0.1674
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.1687181556597352 0.1679232569411397
2023-08-03 17:31:59,642 - epoch:3, training loss:0.8374 validation loss:0.1687
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16563968267291784 0.1687107871286571
2023-08-03 17:32:41,446 - epoch:4, training loss:0.7209 validation loss:0.1656
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.16499768001958728 0.1676021119579673
2023-08-03 17:33:24,909 - epoch:5, training loss:0.6704 validation loss:0.1650
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.1634133467450738 0.16348740756511687
2023-08-03 17:34:06,823 - epoch:6, training loss:0.6463 validation loss:0.1634
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.1618995669297874 0.16268926532939076
2023-08-03 17:34:45,816 - epoch:7, training loss:0.6359 validation loss:0.1619
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.1665754488669336 0.16075872350484133
2023-08-03 17:35:24,128 - epoch:8, training loss:0.6257 validation loss:0.1666
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.16380771351978182 0.16415417324751616
2023-08-03 17:36:01,571 - epoch:9, training loss:0.6190 validation loss:0.1638
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.16333988131955265 0.162934457603842
2023-08-03 17:36:40,266 - epoch:10, training loss:0.6163 validation loss:0.1633
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.16276204260066152 0.16289323307573794
2023-08-03 17:37:21,458 - epoch:11, training loss:0.6173 validation loss:0.1628
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.16266918675974013 0.16166053730994462
2023-08-03 17:38:04,783 - epoch:12, training loss:0.6186 validation loss:0.1627
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.16246092915534974 0.16291156262159348
2023-08-03 17:38:46,726 - epoch:13, training loss:0.6172 validation loss:0.1625
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.16395515538752078 0.1627190263941884
2023-08-03 17:39:25,216 - epoch:14, training loss:0.6198 validation loss:0.1640
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.1636816463433206 0.16326403841376305
2023-08-03 17:40:02,140 - epoch:15, training loss:0.6167 validation loss:0.1637
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.1644517251290381 0.16392281800508499
2023-08-03 17:40:41,487 - epoch:16, training loss:0.6166 validation loss:0.1645
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.16465001637116075 0.1638239251449704
2023-08-03 17:41:21,755 - epoch:17, training loss:0.6162 validation loss:0.1647
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.16389794703572988 0.16426971983164548
2023-08-03 17:42:03,904 - epoch:18, training loss:0.6148 validation loss:0.1639
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.1648701958358288 0.1638320659287274
2023-08-03 17:42:47,535 - epoch:19, training loss:0.6150 validation loss:0.1649
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.16562306974083185 0.1644819712266326
2023-08-03 17:43:28,850 - epoch:20, training loss:0.6157 validation loss:0.1656
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.16556838443502783 0.16569376904517413
2023-08-03 17:44:08,419 - epoch:21, training loss:0.6131 validation loss:0.1656
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.1656686750240624 0.1646505633369088
2023-08-03 17:44:45,207 - epoch:22, training loss:0.6125 validation loss:0.1657
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.16569384541362525 0.165397072955966
2023-08-03 17:45:23,410 - epoch:23, training loss:0.6113 validation loss:0.1657
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.16632671589031817 0.16545774145051836
2023-08-03 17:46:04,306 - epoch:24, training loss:0.6097 validation loss:0.1663
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.1657762685790658 0.16562071396037936
2023-08-03 17:46:46,221 - epoch:25, training loss:0.6102 validation loss:0.1658
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.16596592366695403 0.1651960125193
2023-08-03 17:47:30,580 - epoch:26, training loss:0.6094 validation loss:0.1660
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.1661278628744185 0.16541545931249857
2023-08-03 17:48:13,432 - epoch:27, training loss:0.6096 validation loss:0.1661
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.16606737915426492 0.16522494796663523
2023-08-03 17:48:53,257 - epoch:28, training loss:0.6096 validation loss:0.1661
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.16592862978577613 0.16515134163200856
2023-08-03 17:49:30,645 - epoch:29, training loss:0.6095 validation loss:0.1659
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-17:29:18.866821/0/0.1619_epoch_7.pkl  &  0.16075872350484133
2023-08-03 17:49:36,629 - [*] loss:0.3639
2023-08-03 17:49:36,638 - [*] phase 0, testing
2023-08-03 17:49:36,783 - T:336	MAE	0.393510	RMSE	0.359204	MAPE	161.981463
2023-08-03 17:49:36,783 - 336	mae	0.3935	
2023-08-03 17:49:36,784 - 336	rmse	0.3592	
2023-08-03 17:49:36,784 - 336	mape	161.9815	
2023-08-03 17:49:42,438 - [*] loss:0.3623
2023-08-03 17:49:42,447 - [*] phase 0, testing
2023-08-03 17:49:42,594 - T:336	MAE	0.391202	RMSE	0.357824	MAPE	156.752467
2023-08-03 17:49:42,595 - 336	mae	0.3912	
2023-08-03 17:49:42,595 - 336	rmse	0.3578	
2023-08-03 17:49:42,595 - 336	mape	156.7525	
2023-08-03 17:49:45,152 - logger name:exp/ECL-PatchTST2023-08-03-17:49:45.136312/ECL-PatchTST.log
2023-08-03 17:49:45,153 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 3, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-17:49:45.136312', 'path': 'exp/ECL-PatchTST2023-08-03-17:49:45.136312', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 17:49:45,153 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 17:49:45,401 - [*] phase 0 Dataset load!
2023-08-03 17:49:46,708 - [*] phase 0 Training start
train 7969
2023-08-03 17:50:14,779 - epoch:0, training loss:0.2543 validation loss:0.1796
train 7969
vs, vt 0.17958344966173173 0.1804242931306362
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.17305813133716583 0.16933334283530713
2023-08-03 17:51:21,144 - epoch:1, training loss:2.9718 validation loss:0.1731
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.1679888240993023 0.1613508477807045
2023-08-03 17:52:06,351 - epoch:2, training loss:2.3250 validation loss:0.1680
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.16939649768173695 0.16246030535548925
2023-08-03 17:52:53,102 - epoch:3, training loss:1.4925 validation loss:0.1694
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.1650604521855712 0.1656001160852611
2023-08-03 17:53:43,162 - epoch:4, training loss:1.0479 validation loss:0.1651
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.16513466769829394 0.1631157411262393
2023-08-03 17:54:31,068 - epoch:5, training loss:0.9035 validation loss:0.1651
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.16398216569796203 0.16487593501806258
2023-08-03 17:55:17,554 - epoch:6, training loss:0.8389 validation loss:0.1640
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.16270792707800866 0.16504884604364634
2023-08-03 17:56:03,277 - epoch:7, training loss:0.7959 validation loss:0.1627
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.1629871795885265 0.16300399228930473
2023-08-03 17:56:47,605 - epoch:8, training loss:0.7540 validation loss:0.1630
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.16350639704614878 0.16404656926169991
2023-08-03 17:57:32,333 - epoch:9, training loss:0.7188 validation loss:0.1635
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.1625247210264206 0.16712896917015313
2023-08-03 17:58:17,798 - epoch:10, training loss:0.6730 validation loss:0.1625
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.16564674898982049 0.16148926317691803
2023-08-03 17:59:04,994 - epoch:11, training loss:0.6612 validation loss:0.1656
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.16225274726748468 0.16306446455419063
2023-08-03 17:59:54,944 - epoch:12, training loss:0.6426 validation loss:0.1623
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.16442139903083444 0.16375915231183172
2023-08-03 18:00:43,786 - epoch:13, training loss:0.6528 validation loss:0.1644
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.16392778484150766 0.1633092410862446
2023-08-03 18:01:31,054 - epoch:14, training loss:0.6378 validation loss:0.1639
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.16456849426031112 0.16427301783114673
2023-08-03 18:02:15,545 - epoch:15, training loss:0.6339 validation loss:0.1646
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.1640336373820901 0.16434953548014164
2023-08-03 18:02:58,503 - epoch:16, training loss:0.6311 validation loss:0.1640
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.1639627086929977 0.16382463229820132
2023-08-03 18:03:42,664 - epoch:17, training loss:0.6277 validation loss:0.1640
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.16354704787954688 0.16472962396219373
2023-08-03 18:04:28,742 - epoch:18, training loss:0.6167 validation loss:0.1635
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.16317371586337687 0.16467001661658287
2023-08-03 18:05:16,912 - epoch:19, training loss:0.6180 validation loss:0.1632
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.16492474302649499 0.1660853517241776
2023-08-03 18:06:06,240 - epoch:20, training loss:0.6138 validation loss:0.1649
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.16481194039806724 0.16588542917743326
2023-08-03 18:06:54,356 - epoch:21, training loss:0.6118 validation loss:0.1648
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.1645117306150496 0.1659783580340445
2023-08-03 18:07:39,528 - epoch:22, training loss:0.6102 validation loss:0.1645
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.16417894316837192 0.165318791475147
2023-08-03 18:08:24,134 - epoch:23, training loss:0.6116 validation loss:0.1642
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.16492042653262615 0.16604453660547733
2023-08-03 18:09:09,944 - epoch:24, training loss:0.6077 validation loss:0.1649
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.16452755620703102 0.16573846181854607
2023-08-03 18:09:54,968 - epoch:25, training loss:0.6057 validation loss:0.1645
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.1644538045860827 0.165950433164835
2023-08-03 18:10:41,609 - epoch:26, training loss:0.6080 validation loss:0.1645
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.16396787678822874 0.16541171325370668
2023-08-03 18:11:29,910 - epoch:27, training loss:0.6059 validation loss:0.1640
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.16452585402876138 0.16600211868062614
2023-08-03 18:12:18,205 - epoch:28, training loss:0.6117 validation loss:0.1645
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.16439580488950015 0.1657215070910752
2023-08-03 18:13:01,977 - epoch:29, training loss:0.6052 validation loss:0.1644
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-17:49:45.136312/0/0.1623_epoch_12.pkl  &  0.1613508477807045
2023-08-03 18:13:07,653 - [*] loss:0.3647
2023-08-03 18:13:07,661 - [*] phase 0, testing
2023-08-03 18:13:07,821 - T:336	MAE	0.395042	RMSE	0.360340	MAPE	160.510492
2023-08-03 18:13:07,822 - 336	mae	0.3950	
2023-08-03 18:13:07,822 - 336	rmse	0.3603	
2023-08-03 18:13:07,822 - 336	mape	160.5105	
2023-08-03 18:13:14,240 - [*] loss:0.3581
2023-08-03 18:13:14,266 - [*] phase 0, testing
2023-08-03 18:13:14,415 - T:336	MAE	0.401147	RMSE	0.353738	MAPE	166.398835
2023-08-03 18:13:14,416 - 336	mae	0.4011	
2023-08-03 18:13:14,416 - 336	rmse	0.3537	
2023-08-03 18:13:14,416 - 336	mape	166.3988	
2023-08-03 18:13:17,197 - logger name:exp/ECL-PatchTST2023-08-03-18:13:17.196452/ECL-PatchTST.log
2023-08-03 18:13:17,197 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-18:13:17.196452', 'path': 'exp/ECL-PatchTST2023-08-03-18:13:17.196452', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 18:13:17,198 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 18:13:17,468 - [*] phase 0 Dataset load!
2023-08-03 18:13:18,563 - [*] phase 0 Training start
train 7969
2023-08-03 18:13:46,370 - epoch:0, training loss:0.7805 validation loss:0.4173
train 7969
vs, vt 0.4172559309750795 0.41229391992092135
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.38917614556849 0.38716056309640406
2023-08-03 18:14:41,476 - epoch:1, training loss:0.7839 validation loss:0.3892
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.3803667314350605 0.38423193134367467
2023-08-03 18:15:18,860 - epoch:2, training loss:0.7261 validation loss:0.3804
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.3722642567008734 0.377781293168664
2023-08-03 18:15:57,177 - epoch:3, training loss:0.6820 validation loss:0.3723
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.38287182301282885 0.37348058968782427
2023-08-03 18:16:37,509 - epoch:4, training loss:0.6527 validation loss:0.3829
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.3829612024128437 0.37500788550823927
2023-08-03 18:17:18,044 - epoch:5, training loss:0.6204 validation loss:0.3830
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.3854151040315628 0.37295592948794365
2023-08-03 18:18:01,846 - epoch:6, training loss:0.5989 validation loss:0.3854
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.3870586175471544 0.3672395724803209
2023-08-03 18:18:44,214 - epoch:7, training loss:0.5824 validation loss:0.3871
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.40181018859148027 0.36394464075565336
2023-08-03 18:19:24,635 - epoch:8, training loss:0.5680 validation loss:0.4018
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.39775267615914345 0.3693282788619399
2023-08-03 18:20:02,840 - epoch:9, training loss:0.5551 validation loss:0.3978
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.4018056817352772 0.36097380686551334
2023-08-03 18:20:40,686 - epoch:10, training loss:0.5443 validation loss:0.4018
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.4130539521574974 0.36427293829619883
2023-08-03 18:21:20,177 - epoch:11, training loss:0.5378 validation loss:0.4131
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.40830092169344423 0.3694477269425988
2023-08-03 18:22:01,475 - epoch:12, training loss:0.5284 validation loss:0.4083
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.41110352501273156 0.36893864534795284
2023-08-03 18:22:44,098 - epoch:13, training loss:0.5207 validation loss:0.4111
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.41804004870355127 0.37255688905715945
2023-08-03 18:23:27,367 - epoch:14, training loss:0.5145 validation loss:0.4180
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.4172793820500374 0.37109893318265674
2023-08-03 18:24:08,259 - epoch:15, training loss:0.5101 validation loss:0.4173
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.4202317051589489 0.3736016919836402
2023-08-03 18:24:46,678 - epoch:16, training loss:0.5060 validation loss:0.4202
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.4224001131951809 0.37213861756026745
2023-08-03 18:25:23,820 - epoch:17, training loss:0.5026 validation loss:0.4224
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.42374623827636243 0.37284641098231075
2023-08-03 18:26:02,561 - epoch:18, training loss:0.5005 validation loss:0.4237
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.421118275821209 0.37549552638083694
2023-08-03 18:26:43,913 - epoch:19, training loss:0.4953 validation loss:0.4211
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.42665219455957415 0.3763864504173398
2023-08-03 18:27:26,439 - epoch:20, training loss:0.4934 validation loss:0.4267
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.42312016487121584 0.3764932433143258
2023-08-03 18:28:09,601 - epoch:21, training loss:0.4901 validation loss:0.4231
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.4251875415444374 0.3755166307091713
2023-08-03 18:28:50,390 - epoch:22, training loss:0.4883 validation loss:0.4252
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.42617694698274133 0.3752763407304883
2023-08-03 18:29:29,658 - epoch:23, training loss:0.4893 validation loss:0.4262
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.42525718063116075 0.3754185041412711
2023-08-03 18:30:06,665 - epoch:24, training loss:0.4874 validation loss:0.4253
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.426384674012661 0.3747838204726577
2023-08-03 18:30:46,897 - epoch:25, training loss:0.4860 validation loss:0.4264
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.42705991566181184 0.3754644563421607
2023-08-03 18:31:27,512 - epoch:26, training loss:0.4856 validation loss:0.4271
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.4267853945493698 0.37713833134621383
2023-08-03 18:32:08,416 - epoch:27, training loss:0.4850 validation loss:0.4268
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.4266431175172329 0.3760843396186829
2023-08-03 18:32:51,952 - epoch:28, training loss:0.4853 validation loss:0.4266
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.4256879899650812 0.3754329835996032
2023-08-03 18:33:35,759 - epoch:29, training loss:0.4842 validation loss:0.4257
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-18:13:17.196452/0/0.3723_epoch_3.pkl  &  0.36097380686551334
2023-08-03 18:33:39,704 - [*] loss:0.3723
2023-08-03 18:33:39,713 - [*] phase 0, testing
2023-08-03 18:33:39,846 - T:336	MAE	0.400734	RMSE	0.367559	MAPE	164.076805
2023-08-03 18:33:39,847 - 336	mae	0.4007	
2023-08-03 18:33:39,847 - 336	rmse	0.3676	
2023-08-03 18:33:39,847 - 336	mape	164.0768	
2023-08-03 18:33:43,944 - [*] loss:0.3610
2023-08-03 18:33:43,953 - [*] phase 0, testing
2023-08-03 18:33:44,093 - T:336	MAE	0.392455	RMSE	0.356934	MAPE	158.236301
2023-08-03 18:33:44,094 - 336	mae	0.3925	
2023-08-03 18:33:44,094 - 336	rmse	0.3569	
2023-08-03 18:33:44,094 - 336	mape	158.2363	
2023-08-03 18:33:46,499 - logger name:exp/ECL-PatchTST2023-08-03-18:33:46.499410/ECL-PatchTST.log
2023-08-03 18:33:46,500 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 3, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-18:33:46.499410', 'path': 'exp/ECL-PatchTST2023-08-03-18:33:46.499410', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 18:33:46,500 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 18:33:46,731 - [*] phase 0 Dataset load!
2023-08-03 18:33:47,993 - [*] phase 0 Training start
train 7969
2023-08-03 18:34:21,753 - epoch:0, training loss:0.7280 validation loss:0.3987
train 7969
vs, vt 0.3987369067966938 0.4035313956439495
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.38288254849612713 0.37890348844230176
2023-08-03 18:35:25,648 - epoch:1, training loss:8.2050 validation loss:0.3829
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.375345816090703 0.36270985305309295
2023-08-03 18:36:11,669 - epoch:2, training loss:6.1060 validation loss:0.3753
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.3677504524588585 0.3593352634459734
2023-08-03 18:36:58,924 - epoch:3, training loss:3.8624 validation loss:0.3678
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.3705785073339939 0.3607057802379131
2023-08-03 18:37:46,373 - epoch:4, training loss:2.7417 validation loss:0.3706
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.36728604808449744 0.3723960805684328
2023-08-03 18:38:33,702 - epoch:5, training loss:2.3755 validation loss:0.3673
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.3693811409175396 0.3598071113228798
2023-08-03 18:39:20,332 - epoch:6, training loss:2.2487 validation loss:0.3694
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.3682960193604231 0.36353707760572435
2023-08-03 18:40:07,795 - epoch:7, training loss:2.2547 validation loss:0.3683
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.37202358394861224 0.3682019401341677
2023-08-03 18:40:55,827 - epoch:8, training loss:2.2175 validation loss:0.3720
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.3688572373241186 0.36556340791285036
2023-08-03 18:41:43,697 - epoch:9, training loss:2.1907 validation loss:0.3689
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.37177293710410597 0.36653806306421755
2023-08-03 18:42:32,344 - epoch:10, training loss:2.0762 validation loss:0.3718
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.3728836476802826 0.3626207845285535
2023-08-03 18:43:20,602 - epoch:11, training loss:2.1115 validation loss:0.3729
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.37553734704852104 0.3664537787437439
2023-08-03 18:44:08,031 - epoch:12, training loss:2.0515 validation loss:0.3755
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.3698994904756546 0.3703626230359077
2023-08-03 18:44:55,904 - epoch:13, training loss:2.0260 validation loss:0.3699
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.3744242399930954 0.3636403899639845
2023-08-03 18:45:44,515 - epoch:14, training loss:2.0084 validation loss:0.3744
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.3763920087367296 0.3695836640894413
2023-08-03 18:46:32,729 - epoch:15, training loss:1.9833 validation loss:0.3764
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.3759804848581553 0.3697833281010389
2023-08-03 18:47:20,441 - epoch:16, training loss:2.0052 validation loss:0.3760
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.3798442307859659 0.36705303005874157
2023-08-03 18:48:09,034 - epoch:17, training loss:1.9921 validation loss:0.3798
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.37505609430372716 0.37299371138215065
2023-08-03 18:48:57,577 - epoch:18, training loss:1.9569 validation loss:0.3751
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.376350000500679 0.36911544278264047
2023-08-03 18:49:46,880 - epoch:19, training loss:1.9870 validation loss:0.3764
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.3778066825121641 0.37093532308936117
2023-08-03 18:50:36,777 - epoch:20, training loss:1.9966 validation loss:0.3778
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.37881916761398315 0.3704017367213964
2023-08-03 18:51:25,621 - epoch:21, training loss:2.0181 validation loss:0.3788
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.38015677891671656 0.3714053977280855
2023-08-03 18:52:15,493 - epoch:22, training loss:1.9828 validation loss:0.3802
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.3778587695211172 0.3697025176137686
2023-08-03 18:53:04,544 - epoch:23, training loss:1.9877 validation loss:0.3779
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.3782902657985687 0.37073296047747134
2023-08-03 18:53:53,938 - epoch:24, training loss:1.9986 validation loss:0.3783
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.380088946595788 0.3708268865942955
2023-08-03 18:54:42,796 - epoch:25, training loss:2.0215 validation loss:0.3801
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.3790157616138458 0.37119439281523225
2023-08-03 18:55:31,843 - epoch:26, training loss:2.0078 validation loss:0.3790
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.380215148255229 0.3725341219455004
2023-08-03 18:56:21,668 - epoch:27, training loss:2.0162 validation loss:0.3802
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.3797599658370018 0.372286482155323
2023-08-03 18:57:10,440 - epoch:28, training loss:2.0025 validation loss:0.3798
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.37909437492489817 0.3711492374539375
2023-08-03 18:57:58,989 - epoch:29, training loss:2.0029 validation loss:0.3791
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-18:33:46.499410/0/0.3673_epoch_5.pkl  &  0.3593352634459734
2023-08-03 18:58:02,719 - [*] loss:0.3673
2023-08-03 18:58:02,727 - [*] phase 0, testing
2023-08-03 18:58:02,866 - T:336	MAE	0.399242	RMSE	0.362828	MAPE	165.048814
2023-08-03 18:58:02,866 - 336	mae	0.3992	
2023-08-03 18:58:02,866 - 336	rmse	0.3628	
2023-08-03 18:58:02,866 - 336	mape	165.0488	
2023-08-03 18:58:06,908 - [*] loss:0.3593
2023-08-03 18:58:06,917 - [*] phase 0, testing
2023-08-03 18:58:07,062 - T:336	MAE	0.394180	RMSE	0.354761	MAPE	157.670200
2023-08-03 18:58:07,063 - 336	mae	0.3942	
2023-08-03 18:58:07,063 - 336	rmse	0.3548	
2023-08-03 18:58:07,063 - 336	mape	157.6702	
2023-08-03 18:58:09,248 - logger name:exp/ECL-PatchTST2023-08-03-18:58:09.248547/ECL-PatchTST.log
2023-08-03 18:58:09,249 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 3, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-18:58:09.248547', 'path': 'exp/ECL-PatchTST2023-08-03-18:58:09.248547', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 18:58:09,249 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 18:58:09,463 - [*] phase 0 Dataset load!
2023-08-03 18:58:10,526 - [*] phase 0 Training start
train 7969
2023-08-03 18:58:45,121 - epoch:0, training loss:0.2547 validation loss:0.1787
train 7969
vs, vt 0.1787222580984235 0.1805749973282218
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.17305824793875219 0.16953253019601106
2023-08-03 18:59:46,773 - epoch:1, training loss:0.7015 validation loss:0.1731
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.16796913407742978 0.16195313781499862
2023-08-03 19:00:34,811 - epoch:2, training loss:0.6358 validation loss:0.1680
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.1646741496399045 0.16038671638816596
2023-08-03 19:01:22,580 - epoch:3, training loss:0.5591 validation loss:0.1647
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16836639791727065 0.16096524186432362
2023-08-03 19:02:11,194 - epoch:4, training loss:0.4921 validation loss:0.1684
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.16929046381264926 0.164984998293221
2023-08-03 19:02:54,616 - epoch:5, training loss:0.4624 validation loss:0.1693
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.1676641261205077 0.16437200121581555
2023-08-03 19:03:39,698 - epoch:6, training loss:0.4502 validation loss:0.1677
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.1675277978181839 0.16041261665523052
2023-08-03 19:04:25,925 - epoch:7, training loss:0.4442 validation loss:0.1675
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.17045570500195026 0.16217595525085926
2023-08-03 19:05:13,153 - epoch:8, training loss:0.4341 validation loss:0.1705
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.17163525242358446 0.16273277383297682
2023-08-03 19:06:01,414 - epoch:9, training loss:0.4286 validation loss:0.1716
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.17141811475157737 0.1625160539522767
2023-08-03 19:06:50,983 - epoch:10, training loss:0.4227 validation loss:0.1714
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.17413195464760065 0.16251763701438904
2023-08-03 19:07:37,920 - epoch:11, training loss:0.4186 validation loss:0.1741
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.17418824099004268 0.16258111763745547
2023-08-03 19:08:22,754 - epoch:12, training loss:0.4131 validation loss:0.1742
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.17470890320837498 0.16514421720057726
2023-08-03 19:09:06,641 - epoch:13, training loss:0.4099 validation loss:0.1747
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.17438969239592553 0.16435654014348983
2023-08-03 19:09:51,225 - epoch:14, training loss:0.4073 validation loss:0.1744
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.17481835708022117 0.16570703964680433
2023-08-03 19:10:38,394 - epoch:15, training loss:0.4041 validation loss:0.1748
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.17658787555992603 0.1653207816183567
2023-08-03 19:11:27,390 - epoch:16, training loss:0.4016 validation loss:0.1766
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.17614571321755648 0.1658869670704007
2023-08-03 19:12:16,791 - epoch:17, training loss:0.3991 validation loss:0.1761
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.17590867280960082 0.16719016898423433
2023-08-03 19:13:05,230 - epoch:18, training loss:0.3982 validation loss:0.1759
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.17681088503450154 0.16568423639982938
2023-08-03 19:13:51,058 - epoch:19, training loss:0.3963 validation loss:0.1768
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.1768304342404008 0.1660621227696538
2023-08-03 19:14:34,977 - epoch:20, training loss:0.3948 validation loss:0.1768
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.1767255797982216 0.16615588329732417
2023-08-03 19:15:19,406 - epoch:21, training loss:0.3925 validation loss:0.1767
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.17693001572042705 0.16541845463216304
2023-08-03 19:16:05,530 - epoch:22, training loss:0.3916 validation loss:0.1769
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.17724072746932507 0.16627285797148944
2023-08-03 19:16:52,300 - epoch:23, training loss:0.3923 validation loss:0.1772
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.177006771042943 0.16570745073258877
2023-08-03 19:17:40,459 - epoch:24, training loss:0.3911 validation loss:0.1770
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.17696904763579369 0.1661810375750065
2023-08-03 19:18:29,635 - epoch:25, training loss:0.3904 validation loss:0.1770
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.17728197928518058 0.16599802169948816
2023-08-03 19:19:17,112 - epoch:26, training loss:0.3906 validation loss:0.1773
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.1771661492064595 0.16631340216845275
2023-08-03 19:20:03,248 - epoch:27, training loss:0.3898 validation loss:0.1772
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.1771961661055684 0.1665181066840887
2023-08-03 19:20:50,943 - epoch:28, training loss:0.3904 validation loss:0.1772
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.17706939224153756 0.16631678137928246
2023-08-03 19:21:37,456 - epoch:29, training loss:0.3891 validation loss:0.1771
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-18:58:09.248547/0/0.1647_epoch_3.pkl  &  0.16038671638816596
2023-08-03 19:21:41,682 - [*] loss:0.3701
2023-08-03 19:21:41,691 - [*] phase 0, testing
2023-08-03 19:21:41,829 - T:336	MAE	0.398425	RMSE	0.365401	MAPE	162.490666
2023-08-03 19:21:41,829 - 336	mae	0.3984	
2023-08-03 19:21:41,829 - 336	rmse	0.3654	
2023-08-03 19:21:41,829 - 336	mape	162.4907	
2023-08-03 19:21:45,818 - [*] loss:0.3586
2023-08-03 19:21:45,827 - [*] phase 0, testing
2023-08-03 19:21:45,967 - T:336	MAE	0.396014	RMSE	0.354060	MAPE	160.632813
2023-08-03 19:21:45,968 - 336	mae	0.3960	
2023-08-03 19:21:45,968 - 336	rmse	0.3541	
2023-08-03 19:21:45,968 - 336	mape	160.6328	
2023-08-03 19:21:48,455 - logger name:exp/ECL-PatchTST2023-08-03-19:21:48.454923/ECL-PatchTST.log
2023-08-03 19:21:48,455 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-19:21:48.454923', 'path': 'exp/ECL-PatchTST2023-08-03-19:21:48.454923', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 19:21:48,455 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 19:21:48,695 - [*] phase 0 Dataset load!
2023-08-03 19:21:49,826 - [*] phase 0 Training start
train 7969
2023-08-03 19:22:24,215 - epoch:0, training loss:0.2710 validation loss:0.1861
train 7969
vs, vt 0.18605846632272005 0.18454548604786397
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.17688768710941077 0.17427780106663704
2023-08-03 19:23:19,945 - epoch:1, training loss:1.3975 validation loss:0.1769
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.17194924522191285 0.17435979880392552
2023-08-03 19:24:02,430 - epoch:2, training loss:1.1183 validation loss:0.1719
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.16503252349793912 0.1702175797894597
2023-08-03 19:24:45,320 - epoch:3, training loss:0.8545 validation loss:0.1650
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16707939319312573 0.1643549669533968
2023-08-03 19:25:27,897 - epoch:4, training loss:0.7257 validation loss:0.1671
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.16220704158768057 0.1686804009601474
2023-08-03 19:26:08,152 - epoch:5, training loss:0.6756 validation loss:0.1622
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.162848265375942 0.16468003932386638
2023-08-03 19:26:48,013 - epoch:6, training loss:0.6504 validation loss:0.1628
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.16298832399770619 0.16216658540070056
2023-08-03 19:27:28,291 - epoch:7, training loss:0.6383 validation loss:0.1630
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.16300293039530517 0.16289858324453235
2023-08-03 19:28:09,466 - epoch:8, training loss:0.6319 validation loss:0.1630
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.1612774054519832 0.16273256167769432
2023-08-03 19:28:51,665 - epoch:9, training loss:0.6295 validation loss:0.1613
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.16239668894559145 0.16223184252157807
2023-08-03 19:29:34,893 - epoch:10, training loss:0.6304 validation loss:0.1624
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.16252904217690228 0.16383077269420027
2023-08-03 19:30:19,160 - epoch:11, training loss:0.6174 validation loss:0.1625
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.16244338024407626 0.1622148615308106
2023-08-03 19:31:00,843 - epoch:12, training loss:0.6167 validation loss:0.1624
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.1608428828418255 0.16226842096075417
2023-08-03 19:31:40,305 - epoch:13, training loss:0.6140 validation loss:0.1608
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.16280612554401158 0.16256899842992426
2023-08-03 19:32:17,943 - epoch:14, training loss:0.6095 validation loss:0.1628
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.16336882822215557 0.1641359436325729
2023-08-03 19:32:57,571 - epoch:15, training loss:0.6144 validation loss:0.1634
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.16212287349626422 0.16462653866037727
2023-08-03 19:33:38,901 - epoch:16, training loss:0.6097 validation loss:0.1621
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.16310923034325242 0.16308294301852583
2023-08-03 19:34:21,971 - epoch:17, training loss:0.6114 validation loss:0.1631
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.16143835978582502 0.16420977925881744
2023-08-03 19:35:05,817 - epoch:18, training loss:0.6088 validation loss:0.1614
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.16211921647191047 0.16373780267313123
2023-08-03 19:35:47,475 - epoch:19, training loss:0.6106 validation loss:0.1621
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.16237533278763294 0.16427757600322365
2023-08-03 19:36:26,565 - epoch:20, training loss:0.6073 validation loss:0.1624
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.16221587331965565 0.16422054236754774
2023-08-03 19:37:05,813 - epoch:21, training loss:0.6094 validation loss:0.1622
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.16269083647057414 0.16399339521303774
2023-08-03 19:37:45,010 - epoch:22, training loss:0.6073 validation loss:0.1627
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.1624288296326995 0.1639121918939054
2023-08-03 19:38:24,477 - epoch:23, training loss:0.6061 validation loss:0.1624
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.16221911823377014 0.16389756398275496
2023-08-03 19:39:05,688 - epoch:24, training loss:0.6056 validation loss:0.1622
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.16255047712475063 0.16379508785903454
2023-08-03 19:39:50,055 - epoch:25, training loss:0.6062 validation loss:0.1626
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.16251574084162712 0.16432893937453627
2023-08-03 19:40:35,006 - epoch:26, training loss:0.6063 validation loss:0.1625
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.16241018818691372 0.16438646810129284
2023-08-03 19:41:17,337 - epoch:27, training loss:0.6065 validation loss:0.1624
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.16274578869342804 0.16412308998405933
2023-08-03 19:41:58,232 - epoch:28, training loss:0.6069 validation loss:0.1627
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.16262675495818257 0.1641228304244578
2023-08-03 19:42:38,100 - epoch:29, training loss:0.6061 validation loss:0.1626
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-19:21:48.454923/0/0.1608_epoch_13.pkl  &  0.16216658540070056
2023-08-03 19:42:43,879 - [*] loss:0.3607
2023-08-03 19:42:43,889 - [*] phase 0, testing
2023-08-03 19:42:44,102 - T:336	MAE	0.393470	RMSE	0.356230	MAPE	161.754656
2023-08-03 19:42:44,102 - 336	mae	0.3935	
2023-08-03 19:42:44,102 - 336	rmse	0.3562	
2023-08-03 19:42:44,102 - 336	mape	161.7547	
2023-08-03 19:42:50,205 - [*] loss:0.3675
2023-08-03 19:42:50,214 - [*] phase 0, testing
2023-08-03 19:42:50,367 - T:336	MAE	0.392331	RMSE	0.362802	MAPE	155.423713
2023-08-03 19:42:50,368 - 336	mae	0.3923	
2023-08-03 19:42:50,368 - 336	rmse	0.3628	
2023-08-03 19:42:50,368 - 336	mape	155.4237	
2023-08-03 19:42:53,000 - logger name:exp/ECL-PatchTST2023-08-03-19:42:53.000293/ECL-PatchTST.log
2023-08-03 19:42:53,001 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 3, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-19:42:53.000293', 'path': 'exp/ECL-PatchTST2023-08-03-19:42:53.000293', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 19:42:53,001 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 19:42:53,249 - [*] phase 0 Dataset load!
2023-08-03 19:42:54,409 - [*] phase 0 Training start
train 7969
2023-08-03 19:43:22,584 - epoch:0, training loss:0.2547 validation loss:0.1787
train 7969
vs, vt 0.1787222580984235 0.1805749973282218
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.1732732964679599 0.1695312213152647
2023-08-03 19:44:27,044 - epoch:1, training loss:3.0341 validation loss:0.1733
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.1700681932270527 0.16175763681530952
2023-08-03 19:44:54,805 - epoch:2, training loss:2.3494 validation loss:0.1701
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.16589608378708362 0.16009943149983882
2023-08-03 19:45:19,091 - epoch:3, training loss:1.5213 validation loss:0.1659
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16804306618869305 0.16134371235966682
2023-08-03 19:45:43,999 - epoch:4, training loss:1.0618 validation loss:0.1680
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.16371125262230635 0.16500571239739656
2023-08-03 19:46:08,742 - epoch:5, training loss:0.9055 validation loss:0.1637
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.16341051179915667 0.16220790091902018
2023-08-03 19:46:33,458 - epoch:6, training loss:0.8475 validation loss:0.1634
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.16416217535734176 0.15885431412607431
2023-08-03 19:46:57,931 - epoch:7, training loss:0.8176 validation loss:0.1642
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.16483078943565488 0.16050019338726998
2023-08-03 19:47:23,084 - epoch:8, training loss:0.7692 validation loss:0.1648
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.16278683384880424 0.15957402801141143
2023-08-03 19:47:47,295 - epoch:9, training loss:0.7457 validation loss:0.1628
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.16376845091581343 0.15941582210361957
2023-08-03 19:48:12,525 - epoch:10, training loss:0.6990 validation loss:0.1638
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.16607978399842976 0.1623213139362633
2023-08-03 19:48:36,982 - epoch:11, training loss:0.6903 validation loss:0.1661
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.16367563121020795 0.16181668220087886
2023-08-03 19:49:01,863 - epoch:12, training loss:0.6761 validation loss:0.1637
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.1631688628345728 0.1642641771584749
2023-08-03 19:49:27,299 - epoch:13, training loss:0.6719 validation loss:0.1632
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.1626010864973068 0.162520317081362
2023-08-03 19:49:51,849 - epoch:14, training loss:0.6493 validation loss:0.1626
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.1631672997958958 0.16401215959340334
2023-08-03 19:50:16,402 - epoch:15, training loss:0.6507 validation loss:0.1632
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.16381507804617285 0.1645620498806238
2023-08-03 19:50:41,102 - epoch:16, training loss:0.6592 validation loss:0.1638
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.1632101614959538 0.16479176208376883
2023-08-03 19:51:06,139 - epoch:17, training loss:0.6548 validation loss:0.1632
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.16281911646947264 0.16588754374533893
2023-08-03 19:51:30,767 - epoch:18, training loss:0.6531 validation loss:0.1628
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.16304454170167446 0.16549716265872122
2023-08-03 19:51:54,817 - epoch:19, training loss:0.6544 validation loss:0.1630
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.16257900660857558 0.16578548382967712
2023-08-03 19:52:19,148 - epoch:20, training loss:0.6452 validation loss:0.1626
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.16269140001386403 0.16603123843669892
2023-08-03 19:52:44,746 - epoch:21, training loss:0.6490 validation loss:0.1627
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.16356263598427176 0.16629451708868145
2023-08-03 19:53:08,595 - epoch:22, training loss:0.6483 validation loss:0.1636
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.16263912450522183 0.16612447509542108
2023-08-03 19:53:33,763 - epoch:23, training loss:0.6400 validation loss:0.1626
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.16299613742157817 0.16640934171155095
2023-08-03 19:53:57,597 - epoch:24, training loss:0.6487 validation loss:0.1630
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.16301188403740524 0.16640254743397237
2023-08-03 19:54:21,080 - epoch:25, training loss:0.6488 validation loss:0.1630
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.16314427871257067 0.1666882480494678
2023-08-03 19:54:46,543 - epoch:26, training loss:0.6466 validation loss:0.1631
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.16320911850780248 0.16703289262950421
2023-08-03 19:55:11,794 - epoch:27, training loss:0.6427 validation loss:0.1632
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.1632235878147185 0.16681452067568897
2023-08-03 19:55:36,085 - epoch:28, training loss:0.6459 validation loss:0.1632
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.16298388624563814 0.16655906504020096
2023-08-03 19:56:00,704 - epoch:29, training loss:0.6505 validation loss:0.1630
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-19:42:53.000293/0/0.1626_epoch_20.pkl  &  0.15885431412607431
2023-08-03 19:56:02,446 - [*] loss:0.3657
2023-08-03 19:56:02,455 - [*] phase 0, testing
2023-08-03 19:56:02,590 - T:336	MAE	0.395594	RMSE	0.361190	MAPE	159.294081
2023-08-03 19:56:02,591 - 336	mae	0.3956	
2023-08-03 19:56:02,591 - 336	rmse	0.3612	
2023-08-03 19:56:02,591 - 336	mape	159.2941	
2023-08-03 19:56:04,540 - [*] loss:0.3584
2023-08-03 19:56:04,549 - [*] phase 0, testing
2023-08-03 19:56:04,678 - T:336	MAE	0.389264	RMSE	0.353942	MAPE	155.839515
2023-08-03 19:56:04,679 - 336	mae	0.3893	
2023-08-03 19:56:04,679 - 336	rmse	0.3539	
2023-08-03 19:56:04,679 - 336	mape	155.8395	
