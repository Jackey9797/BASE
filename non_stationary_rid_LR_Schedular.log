2023-06-15 18:05:11,176 - logger name:exp/ECL-Informer2023-06-15-18:05:11.175797/ECL-Informer.log
2023-06-15 18:05:11,176 - params : {'conf': 'ECL-Informer', 'data_name': 'electricity', 'iteration': 1, 'auto_test': 1, 'load': False, 'build_graph': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'end_phase': 1, 'x_len': 96, 'y_len': 96, 'device': device(type='cuda', index=0), '/* model related args*/': '//', 'model_name': 'informer', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'test_model_path': '/Disk/fhyega/code/PatchTST/PatchTST_supervised/checkpoints/electricity_96_96_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0/checkpoint.pth', 'e_layers': 2, 'd_layers': 1, 'factor': 3, 'n_heads': 8, 'd_model': 512, 'd_ff': 2048, 'dropout': 0.05, 'fc_dropout': 0.05, 'head_dropout': 0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 0, '/*train related args*/': '//', 'train': True, 'epoch': 100, 'batch_size': 128, 'lr': 0.0001, 'loss': 'mse', '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': False, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 1, 'phase_len_ratio': 0.7, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-Informer', 'time': '2023-06-15-18:05:11.175797', 'path': 'exp/ECL-Informer2023-06-15-18:05:11.175797', 'pred_len': 96, 'logger': <Logger __main__ (INFO)>}
2023-06-15 18:05:11,176 - [*] phase 1 start training
2023-06-15 18:05:54,312 - [*] phase 1 Dataset load!
2023-06-15 18:08:26,822 - [*] phase 1 Training start
2023-06-15 18:09:18,278 - epoch:0, training loss:1.2186 validation loss:0.9681
Updating learning rate to 0.0001
2023-06-15 18:10:03,955 - epoch:1, training loss:0.7881 validation loss:0.5665
Updating learning rate to 0.0001
2023-06-15 18:10:49,032 - epoch:2, training loss:0.4990 validation loss:0.4819
Updating learning rate to 0.0001
2023-06-15 18:11:38,186 - epoch:3, training loss:0.3992 validation loss:0.4176
Updating learning rate to 9e-05
2023-06-15 18:12:27,880 - epoch:4, training loss:0.2958 validation loss:0.3355
Updating learning rate to 8.1e-05
2023-06-15 18:13:18,481 - epoch:5, training loss:0.2462 validation loss:0.3175
Updating learning rate to 7.290000000000001e-05
2023-06-15 18:14:08,029 - epoch:6, training loss:0.2223 validation loss:0.3046
Updating learning rate to 6.561e-05
2023-06-15 18:15:00,155 - epoch:7, training loss:0.2091 validation loss:0.2980
Updating learning rate to 5.904900000000001e-05
2023-06-15 18:15:56,933 - epoch:8, training loss:0.1996 validation loss:0.2830
Updating learning rate to 5.3144100000000005e-05
2023-06-15 18:16:52,718 - epoch:9, training loss:0.1928 validation loss:0.2715
Updating learning rate to 4.782969000000001e-05
2023-06-15 18:17:55,707 - epoch:10, training loss:0.1873 validation loss:0.2589
Updating learning rate to 4.304672100000001e-05
2023-06-15 18:18:59,726 - epoch:11, training loss:0.1827 validation loss:0.2633
Updating learning rate to 3.874204890000001e-05
2023-06-15 18:20:05,740 - epoch:12, training loss:0.1791 validation loss:0.2626
Updating learning rate to 3.486784401000001e-05
2023-06-15 18:21:17,098 - epoch:13, training loss:0.1760 validation loss:0.2608
Updating learning rate to 3.138105960900001e-05
2023-06-15 18:22:33,239 - epoch:14, training loss:0.1727 validation loss:0.2506
Updating learning rate to 2.824295364810001e-05
2023-06-15 18:23:53,891 - epoch:15, training loss:0.1701 validation loss:0.2525
Updating learning rate to 2.541865828329001e-05
2023-06-15 18:25:11,693 - epoch:16, training loss:0.1679 validation loss:0.2526
Updating learning rate to 2.287679245496101e-05
2023-06-15 18:26:33,791 - epoch:17, training loss:0.1658 validation loss:0.2507
Updating learning rate to 2.0589113209464907e-05
2023-06-15 18:27:57,537 - epoch:18, training loss:0.1644 validation loss:0.2496
Updating learning rate to 1.8530201888518416e-05
2023-06-15 18:29:24,946 - epoch:19, training loss:0.1626 validation loss:0.2443
Updating learning rate to 1.6677181699666577e-05
2023-06-15 18:30:47,556 - epoch:20, training loss:0.1611 validation loss:0.2449
Updating learning rate to 1.5009463529699919e-05
2023-06-15 18:32:12,367 - epoch:21, training loss:0.1596 validation loss:0.2455
Updating learning rate to 1.3508517176729929e-05
2023-06-15 18:33:40,757 - epoch:22, training loss:0.1588 validation loss:0.2426
Updating learning rate to 1.2157665459056936e-05
2023-06-15 18:35:04,307 - epoch:23, training loss:0.1577 validation loss:0.2432
Updating learning rate to 1.0941898913151242e-05
2023-06-15 18:36:34,703 - epoch:24, training loss:0.1570 validation loss:0.2434
Updating learning rate to 9.847709021836118e-06
2023-06-15 18:38:05,212 - epoch:25, training loss:0.1561 validation loss:0.2425
Updating learning rate to 8.862938119652508e-06
2023-06-15 18:39:31,554 - epoch:26, training loss:0.1553 validation loss:0.2408
Updating learning rate to 7.976644307687255e-06
2023-06-15 18:40:57,099 - epoch:27, training loss:0.1545 validation loss:0.2415
Updating learning rate to 7.178979876918531e-06
2023-06-15 18:42:24,892 - epoch:28, training loss:0.1539 validation loss:0.2418
Updating learning rate to 6.4610818892266776e-06
2023-06-15 18:43:57,229 - epoch:29, training loss:0.1533 validation loss:0.2419
Updating learning rate to 5.8149737003040096e-06
2023-06-15 18:45:26,719 - epoch:30, training loss:0.1527 validation loss:0.2392
Updating learning rate to 5.23347633027361e-06
2023-06-15 18:46:58,926 - epoch:31, training loss:0.1526 validation loss:0.2405
Updating learning rate to 4.710128697246249e-06
2023-06-15 18:48:30,409 - epoch:32, training loss:0.1519 validation loss:0.2407
Updating learning rate to 4.239115827521624e-06
2023-06-15 18:50:05,253 - epoch:33, training loss:0.1516 validation loss:0.2393
Updating learning rate to 3.815204244769462e-06
2023-06-15 18:51:38,790 - epoch:34, training loss:0.1513 validation loss:0.2381
Updating learning rate to 3.4336838202925152e-06
2023-06-15 18:53:08,644 - epoch:35, training loss:0.1510 validation loss:0.2388
Updating learning rate to 3.090315438263264e-06
2023-06-15 18:54:37,731 - epoch:36, training loss:0.1506 validation loss:0.2378
Updating learning rate to 2.7812838944369375e-06
2023-06-15 18:56:10,629 - epoch:37, training loss:0.1506 validation loss:0.2367
Updating learning rate to 2.503155504993244e-06
2023-06-15 18:57:41,755 - epoch:38, training loss:0.1502 validation loss:0.2393
Updating learning rate to 2.2528399544939195e-06
2023-06-15 18:59:10,098 - epoch:39, training loss:0.1500 validation loss:0.2374
Updating learning rate to 2.0275559590445276e-06
2023-06-15 19:00:36,714 - epoch:40, training loss:0.1499 validation loss:0.2382
Updating learning rate to 1.8248003631400751e-06
2023-06-15 19:01:59,467 - epoch:41, training loss:0.1497 validation loss:0.2382
Updating learning rate to 1.6423203268260676e-06
2023-06-15 19:03:04,976 - epoch:42, training loss:0.1496 validation loss:0.2390
Updating learning rate to 1.4780882941434609e-06
2023-06-15 19:04:02,691 - epoch:43, training loss:0.1494 validation loss:0.2386
Updating learning rate to 1.3302794647291146e-06
2023-06-15 19:04:55,796 - epoch:44, training loss:0.1493 validation loss:0.2384
Updating learning rate to 1.1972515182562034e-06
2023-06-15 19:05:46,127 - epoch:45, training loss:0.1491 validation loss:0.2377
Updating learning rate to 1.077526366430583e-06
2023-06-15 19:06:34,237 - epoch:46, training loss:0.1490 validation loss:0.2382
Updating learning rate to 9.697737297875248e-07
2023-06-15 19:07:20,518 - epoch:47, training loss:0.1489 validation loss:0.2376
Updating learning rate to 8.727963568087723e-07
2023-06-15 19:08:07,675 - epoch:48, training loss:0.1489 validation loss:0.2377
2023-06-15 19:08:14,259 - [*] loss:0.3906
2023-06-15 19:08:14,462 - [*] phase 1, testing
2023-06-15 19:08:15,483 - T:96	MAE	0.4987	RMSE	0.4568	MAPE	863.8345
2023-06-15 19:08:15,485 - 96	mae	0.4987	
2023-06-15 19:08:15,485 - 96	rmse	0.4568	
2023-06-15 19:08:15,485 - 96	mape	863.8345	
