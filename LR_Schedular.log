2023-06-15 18:05:51,366 - logger name:exp/ECL-Informer2023-06-15-18:05:51.366376/ECL-Informer.log
2023-06-15 18:05:51,366 - params : {'conf': 'ECL-Informer', 'data_name': 'electricity', 'iteration': 1, 'auto_test': 1, 'load': False, 'build_graph': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'end_phase': 1, 'x_len': 96, 'y_len': 96, 'device': device(type='cuda', index=1), '/* model related args*/': '//', 'model_name': 'informer', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'test_model_path': '/Disk/fhyega/code/PatchTST/PatchTST_supervised/checkpoints/electricity_96_96_Informer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0/checkpoint.pth', 'e_layers': 2, 'd_layers': 1, 'factor': 3, 'n_heads': 8, 'd_model': 512, 'd_ff': 2048, 'dropout': 0.05, 'fc_dropout': 0.05, 'head_dropout': 0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 0, '/*train related args*/': '//', 'train': True, 'epoch': 100, 'batch_size': 128, 'lr': 0.0001, 'loss': 'mse', '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': False, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 1, 'phase_len_ratio': 0.7, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-Informer', 'time': '2023-06-15-18:05:51.366376', 'path': 'exp/ECL-Informer2023-06-15-18:05:51.366376', 'pred_len': 96, 'logger': <Logger __main__ (INFO)>}
2023-06-15 18:05:51,367 - [*] phase 1 start training
2023-06-15 18:05:54,463 - [*] phase 1 Dataset load!
2023-06-15 18:08:26,823 - [*] phase 1 Training start
2023-06-15 18:09:21,467 - epoch:0, training loss:1.2137 validation loss:0.9630
Updating learning rate to 0.0001
2023-06-15 18:10:11,567 - epoch:1, training loss:0.7759 validation loss:0.5604
Updating learning rate to 0.0001
2023-06-15 18:11:02,521 - epoch:2, training loss:0.4903 validation loss:0.4693
Updating learning rate to 0.0001
2023-06-15 18:11:53,473 - epoch:3, training loss:0.3978 validation loss:0.4129
Updating learning rate to 9e-05
2023-06-15 18:12:45,101 - epoch:4, training loss:0.2961 validation loss:0.3501
Updating learning rate to 8.1e-05
2023-06-15 18:13:36,354 - epoch:5, training loss:0.2419 validation loss:0.3021
Updating learning rate to 7.290000000000001e-05
2023-06-15 18:14:27,959 - epoch:6, training loss:0.2180 validation loss:0.2990
Updating learning rate to 6.561e-05
2023-06-15 18:15:20,831 - epoch:7, training loss:0.2057 validation loss:0.2877
Updating learning rate to 5.904900000000001e-05
2023-06-15 18:16:12,888 - epoch:8, training loss:0.1964 validation loss:0.2757
Updating learning rate to 5.3144100000000005e-05
2023-06-15 18:17:06,139 - epoch:9, training loss:0.1895 validation loss:0.2749
Updating learning rate to 4.782969000000001e-05
2023-06-15 18:17:58,260 - epoch:10, training loss:0.1839 validation loss:0.2739
Updating learning rate to 4.304672100000001e-05
2023-06-15 18:18:50,338 - epoch:11, training loss:0.1792 validation loss:0.2649
Updating learning rate to 3.874204890000001e-05
2023-06-15 18:19:43,153 - epoch:12, training loss:0.1761 validation loss:0.2568
Updating learning rate to 3.486784401000001e-05
2023-06-15 18:20:35,236 - epoch:13, training loss:0.1720 validation loss:0.2589
Updating learning rate to 3.138105960900001e-05
2023-06-15 18:21:28,089 - epoch:14, training loss:0.1689 validation loss:0.2543
Updating learning rate to 2.824295364810001e-05
2023-06-15 18:22:21,365 - epoch:15, training loss:0.1666 validation loss:0.2513
Updating learning rate to 2.541865828329001e-05
2023-06-15 18:23:14,817 - epoch:16, training loss:0.1643 validation loss:0.2530
Updating learning rate to 2.287679245496101e-05
2023-06-15 18:24:07,085 - epoch:17, training loss:0.1621 validation loss:0.2550
Updating learning rate to 2.0589113209464907e-05
2023-06-15 18:24:59,445 - epoch:18, training loss:0.1607 validation loss:0.2511
Updating learning rate to 1.8530201888518416e-05
2023-06-15 18:25:52,909 - epoch:19, training loss:0.1590 validation loss:0.2519
Updating learning rate to 1.6677181699666577e-05
2023-06-15 18:26:46,162 - epoch:20, training loss:0.1572 validation loss:0.2512
Updating learning rate to 1.5009463529699919e-05
2023-06-15 18:27:39,734 - epoch:21, training loss:0.1563 validation loss:0.2480
Updating learning rate to 1.3508517176729929e-05
2023-06-15 18:28:33,617 - epoch:22, training loss:0.1552 validation loss:0.2476
Updating learning rate to 1.2157665459056936e-05
2023-06-15 18:29:26,491 - epoch:23, training loss:0.1541 validation loss:0.2451
Updating learning rate to 1.0941898913151242e-05
2023-06-15 18:30:20,868 - epoch:24, training loss:0.1533 validation loss:0.2435
Updating learning rate to 9.847709021836118e-06
2023-06-15 18:31:14,304 - epoch:25, training loss:0.1525 validation loss:0.2436
Updating learning rate to 8.862938119652508e-06
2023-06-15 18:32:08,845 - epoch:26, training loss:0.1518 validation loss:0.2448
Updating learning rate to 7.976644307687255e-06
2023-06-15 18:33:03,212 - epoch:27, training loss:0.1512 validation loss:0.2422
Updating learning rate to 7.178979876918531e-06
2023-06-15 18:33:57,482 - epoch:28, training loss:0.1505 validation loss:0.2421
Updating learning rate to 6.4610818892266776e-06
2023-06-15 18:34:52,371 - epoch:29, training loss:0.1501 validation loss:0.2438
Updating learning rate to 5.8149737003040096e-06
2023-06-15 18:35:46,599 - epoch:30, training loss:0.1496 validation loss:0.2443
Updating learning rate to 5.23347633027361e-06
2023-06-15 18:36:40,293 - epoch:31, training loss:0.1491 validation loss:0.2421
Updating learning rate to 4.710128697246249e-06
2023-06-15 18:37:33,140 - epoch:32, training loss:0.1488 validation loss:0.2419
Updating learning rate to 4.239115827521624e-06
2023-06-15 18:38:27,530 - epoch:33, training loss:0.1483 validation loss:0.2437
Updating learning rate to 3.815204244769462e-06
2023-06-15 18:39:20,523 - epoch:34, training loss:0.1481 validation loss:0.2419
Updating learning rate to 3.4336838202925152e-06
2023-06-15 18:40:15,058 - epoch:35, training loss:0.1478 validation loss:0.2420
Updating learning rate to 3.090315438263264e-06
2023-06-15 18:41:09,561 - epoch:36, training loss:0.1476 validation loss:0.2396
Updating learning rate to 2.7812838944369375e-06
2023-06-15 18:42:02,425 - epoch:37, training loss:0.1473 validation loss:0.2413
Updating learning rate to 2.503155504993244e-06
2023-06-15 18:42:55,053 - epoch:38, training loss:0.1471 validation loss:0.2413
Updating learning rate to 2.2528399544939195e-06
2023-06-15 18:43:48,635 - epoch:39, training loss:0.1469 validation loss:0.2418
Updating learning rate to 2.0275559590445276e-06
2023-06-15 18:44:42,623 - epoch:40, training loss:0.1468 validation loss:0.2396
Updating learning rate to 1.8248003631400751e-06
2023-06-15 18:45:37,212 - epoch:41, training loss:0.1467 validation loss:0.2406
Updating learning rate to 1.6423203268260676e-06
2023-06-15 18:46:31,452 - epoch:42, training loss:0.1464 validation loss:0.2404
Updating learning rate to 1.4780882941434609e-06
2023-06-15 18:47:25,974 - epoch:43, training loss:0.1462 validation loss:0.2404
Updating learning rate to 1.3302794647291146e-06
2023-06-15 18:48:21,313 - epoch:44, training loss:0.1461 validation loss:0.2403
Updating learning rate to 1.1972515182562034e-06
2023-06-15 18:49:14,636 - epoch:45, training loss:0.1462 validation loss:0.2406
Updating learning rate to 1.077526366430583e-06
2023-06-15 18:50:09,578 - epoch:46, training loss:0.1460 validation loss:0.2407
Updating learning rate to 9.697737297875248e-07
2023-06-15 18:51:03,485 - epoch:47, training loss:0.1459 validation loss:0.2390
Updating learning rate to 8.727963568087723e-07
2023-06-15 18:51:57,625 - epoch:48, training loss:0.1458 validation loss:0.2403
Updating learning rate to 7.855167211278951e-07
2023-06-15 18:52:52,141 - epoch:49, training loss:0.1457 validation loss:0.2405
Updating learning rate to 7.069650490151056e-07
2023-06-15 18:53:45,061 - epoch:50, training loss:0.1457 validation loss:0.2401
Updating learning rate to 6.36268544113595e-07
2023-06-15 18:54:40,379 - epoch:51, training loss:0.1457 validation loss:0.2399
Updating learning rate to 5.726416897022355e-07
2023-06-15 18:55:34,284 - epoch:52, training loss:0.1455 validation loss:0.2401
Updating learning rate to 5.15377520732012e-07
2023-06-15 18:56:28,275 - epoch:53, training loss:0.1455 validation loss:0.2393
Updating learning rate to 4.6383976865881085e-07
2023-06-15 18:57:22,767 - epoch:54, training loss:0.1455 validation loss:0.2394
Updating learning rate to 4.174557917929298e-07
2023-06-15 18:58:16,956 - epoch:55, training loss:0.1453 validation loss:0.2403
Updating learning rate to 3.7571021261363677e-07
2023-06-15 18:59:12,009 - epoch:56, training loss:0.1454 validation loss:0.2403
Updating learning rate to 3.381391913522731e-07
2023-06-15 19:00:05,686 - epoch:57, training loss:0.1454 validation loss:0.2392
Updating learning rate to 3.043252722170458e-07
2023-06-15 19:01:00,452 - epoch:58, training loss:0.1453 validation loss:0.2394
2023-06-15 19:01:12,003 - [*] loss:0.3805
2023-06-15 19:01:14,586 - [*] phase 1, testing
2023-06-15 19:01:20,746 - T:96	MAE	0.4919	RMSE	0.4478	MAPE	815.5513
2023-06-15 19:01:20,803 - 96	mae	0.4919	
2023-06-15 19:01:20,803 - 96	rmse	0.4478	
2023-06-15 19:01:20,803 - 96	mape	815.5513	
