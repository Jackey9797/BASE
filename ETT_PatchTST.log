2023-08-28 08:47:15,983 - logger name:exp/ECL-PatchTST2023-08-28-08:47:15.983569/ECL-PatchTST.log
2023-08-28 08:47:15,984 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.3, 'fc_dropout': 0.3, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 128, 'd_model': 16, 'n_heads': 4, 'seq_len': 336, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 7.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-08:47:15.983569', 'path': 'exp/ECL-PatchTST2023-08-28-08:47:15.983569', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 08:47:15,984 - [*] phase 0 start training
0 17420
train 8209
val 2785
test 2785
2023-08-28 08:47:16,172 - [*] phase 0 Dataset load!
2023-08-28 08:47:17,066 - [*] phase 0 Training start
train 8209
2023-08-28 08:47:20,619 - epoch:0, training loss:0.6737 validation loss:0.4187
train 8209
vs, vt 0.4187476401301948 0.4207193878563968
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.3940334794196216 0.39874145490202034
need align? ->  False 0.39874145490202034
2023-08-28 08:47:28,613 - epoch:1, training loss:0.7239 validation loss:0.3940
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.3552334325557405 0.36355446008118714
need align? ->  False 0.36355446008118714
2023-08-28 08:47:34,109 - epoch:2, training loss:0.6644 validation loss:0.3552
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.3177804909646511 0.3294023241509091
need align? ->  False 0.3294023241509091
2023-08-28 08:47:39,599 - epoch:3, training loss:0.5897 validation loss:0.3178
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.2973285361447118 0.3047365366735242
need align? ->  False 0.3047365366735242
2023-08-28 08:47:45,119 - epoch:4, training loss:0.5320 validation loss:0.2973
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.2885437428273938 0.2910778551277789
need align? ->  False 0.2910778551277789
2023-08-28 08:47:50,770 - epoch:5, training loss:0.4971 validation loss:0.2885
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.28345041535794735 0.28503565659577196
need align? ->  False 0.28503565659577196
2023-08-28 08:47:56,463 - epoch:6, training loss:0.4798 validation loss:0.2835
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.28032505190507934 0.2809093083170327
need align? ->  False 0.2809093083170327
2023-08-28 08:48:02,157 - epoch:7, training loss:0.4683 validation loss:0.2803
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.2791411802172661 0.2785537654364651
need align? ->  True 0.2785537654364651
2023-08-28 08:48:07,943 - epoch:8, training loss:0.4627 validation loss:0.2791
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.2780290836976333 0.2770587857812643
need align? ->  True 0.2770587857812643
2023-08-28 08:48:13,559 - epoch:9, training loss:0.4583 validation loss:0.2780
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.27706586197018623 0.2768986382606355
need align? ->  True 0.2768986382606355
2023-08-28 08:48:19,539 - epoch:10, training loss:0.4550 validation loss:0.2771
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.2763522374020381 0.2755636589770967
need align? ->  True 0.2755636589770967
2023-08-28 08:48:25,436 - epoch:11, training loss:0.4520 validation loss:0.2764
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.27708762630142947 0.2745611159638925
need align? ->  True 0.2745611159638925
2023-08-28 08:48:31,015 - epoch:12, training loss:0.4500 validation loss:0.2771
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.27575199983336707 0.2747614085674286
need align? ->  True 0.2745611159638925
2023-08-28 08:48:36,449 - epoch:13, training loss:0.4477 validation loss:0.2758
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.2755885842171582 0.2736536059528589
need align? ->  True 0.2736536059528589
2023-08-28 08:48:42,356 - epoch:14, training loss:0.4476 validation loss:0.2756
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.27608131244778633 0.27420853671025147
need align? ->  True 0.2736536059528589
2023-08-28 08:48:47,934 - epoch:15, training loss:0.4454 validation loss:0.2761
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.2751957659016956 0.2745474038476294
need align? ->  True 0.2736536059528589
2023-08-28 08:48:53,253 - epoch:16, training loss:0.4427 validation loss:0.2752
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.2755678560246121 0.273398930714889
need align? ->  True 0.273398930714889
2023-08-28 08:48:59,103 - epoch:17, training loss:0.4430 validation loss:0.2756
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.27567613074048 0.27408006973564625
need align? ->  True 0.273398930714889
2023-08-28 08:49:04,579 - epoch:18, training loss:0.4424 validation loss:0.2757
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.2751977379349145 0.2743819057941437
need align? ->  True 0.273398930714889
2023-08-28 08:49:10,301 - epoch:19, training loss:0.4409 validation loss:0.2752
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.2755852087654851 0.273317078975114
need align? ->  True 0.273317078975114
2023-08-28 08:49:16,141 - epoch:20, training loss:0.4406 validation loss:0.2756
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.27532770298421383 0.2742771970277483
need align? ->  True 0.273317078975114
2023-08-28 08:49:21,702 - epoch:21, training loss:0.4396 validation loss:0.2753
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.2756402048874985 0.2737315109168941
need align? ->  True 0.273317078975114
2023-08-28 08:49:27,209 - epoch:22, training loss:0.4394 validation loss:0.2756
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.2753808662634004 0.2737109229307283
need align? ->  True 0.273317078975114
2023-08-28 08:49:32,659 - epoch:23, training loss:0.4385 validation loss:0.2754
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.27535373311151157 0.2736239114945585
need align? ->  True 0.273317078975114
2023-08-28 08:49:38,302 - epoch:24, training loss:0.4392 validation loss:0.2754
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.27535435811362485 0.27374910343099723
need align? ->  True 0.273317078975114
2023-08-28 08:49:43,976 - epoch:25, training loss:0.4383 validation loss:0.2754
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.2754324207251722 0.2737615472552451
need align? ->  True 0.273317078975114
2023-08-28 08:49:49,390 - epoch:26, training loss:0.4393 validation loss:0.2754
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.27542213232002477 0.27373029782690783
need align? ->  True 0.273317078975114
2023-08-28 08:49:54,986 - epoch:27, training loss:0.4368 validation loss:0.2754
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.27545369060879404 0.27376938154074276
need align? ->  True 0.273317078975114
2023-08-28 08:50:00,570 - epoch:28, training loss:0.4383 validation loss:0.2755
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.27541755292225967 0.2737221661955118
need align? ->  True 0.273317078975114
2023-08-28 08:50:06,333 - epoch:29, training loss:0.4378 validation loss:0.2754
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-28-08:47:15.983569/0/0.2752_epoch_16.pkl  &  0.273317078975114
2023-08-28 08:50:06,811 - [*] loss:0.2752
2023-08-28 08:50:06,814 - [*] phase 0, testing
2023-08-28 08:50:06,851 - T:96	MAE	0.338210	RMSE	0.274974	MAPE	137.943518
2023-08-28 08:50:06,852 - 96	mae	0.3382	
2023-08-28 08:50:06,852 - 96	rmse	0.2750	
2023-08-28 08:50:06,852 - 96	mape	137.9435	
2023-08-28 08:50:07,264 - [*] loss:0.2752
2023-08-28 08:50:07,266 - [*] phase 0, testing
2023-08-28 08:50:07,304 - T:96	MAE	0.338210	RMSE	0.274974	MAPE	137.943518
2023-08-28 08:50:07,795 - [*] loss:0.2768
2023-08-28 08:50:07,799 - [*] phase 0, testing
2023-08-28 08:50:07,835 - T:96	MAE	0.340070	RMSE	0.276193	MAPE	135.961461
2023-08-28 08:50:08,282 - [*] loss:0.2733
2023-08-28 08:50:08,285 - [*] phase 0, testing
2023-08-28 08:50:08,323 - T:96	MAE	0.332999	RMSE	0.273292	MAPE	134.399796
2023-08-28 08:50:08,324 - 96	mae	0.3330	
2023-08-28 08:50:08,324 - 96	rmse	0.2733	
2023-08-28 08:50:08,324 - 96	mape	134.3998	
2023-08-28 08:50:10,334 - logger name:exp/ECL-PatchTST2023-08-28-08:50:10.333993/ECL-PatchTST.log
2023-08-28 08:50:10,334 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.3, 'fc_dropout': 0.3, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 128, 'd_model': 16, 'n_heads': 4, 'seq_len': 336, 'pred_len': 192, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 7.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-08:50:10.333993', 'path': 'exp/ECL-PatchTST2023-08-28-08:50:10.333993', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 08:50:10,334 - [*] phase 0 start training
0 17420
train 8113
val 2689
test 2689
2023-08-28 08:50:10,522 - [*] phase 0 Dataset load!
2023-08-28 08:50:11,358 - [*] phase 0 Training start
train 8113
2023-08-28 08:50:15,159 - epoch:0, training loss:0.7331 validation loss:0.4600
train 8113
vs, vt 0.4600439657541839 0.4600964073430408
Updating learning rate to 1.0464153247552845e-05
Updating learning rate to 1.0464153247552845e-05
train 8113
vs, vt 0.44274392893368547 0.4414124813946811
need align? ->  True 0.4414124813946811
2023-08-28 08:50:23,211 - epoch:1, training loss:0.8018 validation loss:0.4427
Updating learning rate to 2.8115559773217685e-05
Updating learning rate to 2.8115559773217685e-05
train 8113
vs, vt 0.4176787503741004 0.4128020697019317
need align? ->  True 0.4128020697019317
2023-08-28 08:50:28,900 - epoch:2, training loss:0.7501 validation loss:0.4177
Updating learning rate to 5.2199994709629883e-05
Updating learning rate to 5.2199994709629883e-05
train 8113
vs, vt 0.39314413883469324 0.3906300975517793
need align? ->  True 0.3906300975517793
2023-08-28 08:50:34,362 - epoch:3, training loss:0.6872 validation loss:0.3931
Updating learning rate to 7.623056312721927e-05
Updating learning rate to 7.623056312721927e-05
train 8113
vs, vt 0.37758510830727493 0.37762234630909836
need align? ->  False 0.37762234630909836
2023-08-28 08:50:39,953 - epoch:4, training loss:0.6361 validation loss:0.3776
Updating learning rate to 9.373487848943999e-05
Updating learning rate to 9.373487848943999e-05
train 8113
vs, vt 0.37053591825745325 0.36796878345988016
need align? ->  True 0.36796878345988016
2023-08-28 08:50:45,767 - epoch:5, training loss:0.6088 validation loss:0.3705
Updating learning rate to 9.999989207196297e-05
Updating learning rate to 9.999989207196297e-05
train 8113
vs, vt 0.36701786958358507 0.36421472596173937
need align? ->  True 0.36421472596173937
2023-08-28 08:50:51,793 - epoch:6, training loss:0.5928 validation loss:0.3670
Updating learning rate to 9.955857764964711e-05
Updating learning rate to 9.955857764964711e-05
train 8113
vs, vt 0.3629649637097662 0.3611853700131178
need align? ->  True 0.3611853700131178
2023-08-28 08:50:57,802 - epoch:7, training loss:0.5811 validation loss:0.3630
Updating learning rate to 9.826930564556767e-05
Updating learning rate to 9.826930564556767e-05
train 8113
vs, vt 0.36128287931734865 0.3582122783091935
need align? ->  True 0.3582122783091935
2023-08-28 08:51:03,673 - epoch:8, training loss:0.5749 validation loss:0.3613
Updating learning rate to 9.61541358611682e-05
Updating learning rate to 9.61541358611682e-05
train 8113
vs, vt 0.3603512560102073 0.357855635271831
need align? ->  True 0.357855635271831
2023-08-28 08:51:09,578 - epoch:9, training loss:0.5691 validation loss:0.3604
Updating learning rate to 9.324925943789559e-05
Updating learning rate to 9.324925943789559e-05
train 8113
vs, vt 0.3579793169417165 0.358108508146622
need align? ->  True 0.357855635271831
2023-08-28 08:51:15,250 - epoch:10, training loss:0.5669 validation loss:0.3580
Updating learning rate to 8.960437961673599e-05
Updating learning rate to 8.960437961673599e-05
train 8113
vs, vt 0.3560271506959742 0.3551797191189094
need align? ->  True 0.3551797191189094
2023-08-28 08:51:21,242 - epoch:11, training loss:0.5604 validation loss:0.3560
Updating learning rate to 8.528186130198099e-05
Updating learning rate to 8.528186130198099e-05
train 8113
vs, vt 0.3558777106756514 0.3552394819191911
need align? ->  True 0.3551797191189094
2023-08-28 08:51:26,944 - epoch:12, training loss:0.5618 validation loss:0.3559
Updating learning rate to 8.035566398042457e-05
Updating learning rate to 8.035566398042457e-05
train 8113
vs, vt 0.35557023774493823 0.3546913919801062
need align? ->  True 0.3546913919801062
2023-08-28 08:51:32,810 - epoch:13, training loss:0.5560 validation loss:0.3556
Updating learning rate to 7.491007625403847e-05
Updating learning rate to 7.491007625403847e-05
train 8113
vs, vt 0.3544002036479386 0.3548767348243432
need align? ->  False 0.3546913919801062
2023-08-28 08:51:38,634 - epoch:14, training loss:0.5500 validation loss:0.3544
Updating learning rate to 6.903827363862332e-05
Updating learning rate to 6.903827363862332e-05
train 8113
vs, vt 0.35350464995611797 0.35506226122379303
need align? ->  False 0.3546913919801062
2023-08-28 08:51:44,265 - epoch:15, training loss:0.5459 validation loss:0.3535
Updating learning rate to 6.284072430490012e-05
Updating learning rate to 6.284072430490012e-05
train 8113
vs, vt 0.35345893928950484 0.35493369468233804
need align? ->  False 0.3546913919801062
2023-08-28 08:51:49,772 - epoch:16, training loss:0.5432 validation loss:0.3535
Updating learning rate to 5.642347004025414e-05
Updating learning rate to 5.642347004025414e-05
train 8113
vs, vt 0.3538202318278226 0.35518942011350935
need align? ->  False 0.3546913919801062
2023-08-28 08:51:55,483 - epoch:17, training loss:0.5437 validation loss:0.3538
Updating learning rate to 4.989631184435254e-05
Updating learning rate to 4.989631184435254e-05
train 8113
vs, vt 0.35408595305952156 0.3557478076016361
need align? ->  False 0.3546913919801062
2023-08-28 08:52:01,113 - epoch:18, training loss:0.5394 validation loss:0.3541
Updating learning rate to 4.337093120359729e-05
Updating learning rate to 4.337093120359729e-05
train 8113
vs, vt 0.35402684794230893 0.3549435545097698
need align? ->  False 0.3546913919801062
2023-08-28 08:52:06,828 - epoch:19, training loss:0.5380 validation loss:0.3540
Updating learning rate to 3.695897918992905e-05
Updating learning rate to 3.695897918992905e-05
train 8113
vs, vt 0.3544428988613866 0.35530428486791527
need align? ->  False 0.3546913919801062
2023-08-28 08:52:12,604 - epoch:20, training loss:0.5388 validation loss:0.3544
Updating learning rate to 3.077016608003061e-05
Updating learning rate to 3.077016608003061e-05
train 8113
vs, vt 0.35444682226939633 0.35535877756774426
need align? ->  False 0.3546913919801062
2023-08-28 08:52:18,196 - epoch:21, training loss:0.5387 validation loss:0.3544
Updating learning rate to 2.4910384182075503e-05
Updating learning rate to 2.4910384182075503e-05
train 8113
vs, vt 0.354190355674787 0.355559680102901
need align? ->  False 0.3546913919801062
2023-08-28 08:52:23,810 - epoch:22, training loss:0.5379 validation loss:0.3542
Updating learning rate to 1.947989598897622e-05
Updating learning rate to 1.947989598897622e-05
train 8113
vs, vt 0.35459167273207143 0.355242916975509
need align? ->  False 0.3546913919801062
2023-08-28 08:52:29,407 - epoch:23, training loss:0.5366 validation loss:0.3546
Updating learning rate to 1.4571618659332437e-05
Updating learning rate to 1.4571618659332437e-05
train 8113
vs, vt 0.35457366637208243 0.3551430417732759
need align? ->  False 0.3546913919801062
2023-08-28 08:52:34,975 - epoch:24, training loss:0.5365 validation loss:0.3546
Updating learning rate to 1.0269534179085959e-05
Updating learning rate to 1.0269534179085959e-05
train 8113
vs, vt 0.35454202138564805 0.3552365421571515
need align? ->  False 0.3546913919801062
2023-08-28 08:52:40,609 - epoch:25, training loss:0.5357 validation loss:0.3545
Updating learning rate to 6.647252406456951e-06
Updating learning rate to 6.647252406456951e-06
train 8113
vs, vt 0.35460223934867163 0.3553191306577487
need align? ->  False 0.3546913919801062
2023-08-28 08:52:46,395 - epoch:26, training loss:0.5345 validation loss:0.3546
Updating learning rate to 3.7667515868613148e-06
Updating learning rate to 3.7667515868613148e-06
train 8113
vs, vt 0.3545751172033223 0.3553053272718733
need align? ->  False 0.3546913919801062
2023-08-28 08:52:51,886 - epoch:27, training loss:0.5380 validation loss:0.3546
Updating learning rate to 1.677317887948057e-06
Updating learning rate to 1.677317887948057e-06
train 8113
vs, vt 0.35465672408992593 0.35550588386302645
need align? ->  False 0.3546913919801062
2023-08-28 08:52:57,484 - epoch:28, training loss:0.5341 validation loss:0.3547
Updating learning rate to 4.1470209960604436e-07
Updating learning rate to 4.1470209960604436e-07
train 8113
vs, vt 0.3543860638006167 0.3551980448378758
need align? ->  False 0.3546913919801062
2023-08-28 08:53:03,244 - epoch:29, training loss:0.5355 validation loss:0.3544
Updating learning rate to 5.079280370371978e-10
Updating learning rate to 5.079280370371978e-10
check exp/ECL-PatchTST2023-08-28-08:50:10.333993/0/0.3535_epoch_16.pkl  &  0.3546913919801062
2023-08-28 08:53:03,762 - [*] loss:0.3535
2023-08-28 08:53:03,767 - [*] phase 0, testing
2023-08-28 08:53:03,860 - T:192	MAE	0.375425	RMSE	0.337565	MAPE	145.363843
2023-08-28 08:53:03,860 - 192	mae	0.3754	
2023-08-28 08:53:03,860 - 192	rmse	0.3376	
2023-08-28 08:53:03,860 - 192	mape	145.3638	
2023-08-28 08:53:04,412 - [*] loss:0.3535
2023-08-28 08:53:04,416 - [*] phase 0, testing
2023-08-28 08:53:04,499 - T:192	MAE	0.375425	RMSE	0.337565	MAPE	145.363843
2023-08-28 08:53:04,940 - [*] loss:0.3548
2023-08-28 08:53:04,944 - [*] phase 0, testing
2023-08-28 08:53:05,026 - T:192	MAE	0.376913	RMSE	0.335520	MAPE	145.012581
2023-08-28 08:53:05,554 - [*] loss:0.3546
2023-08-28 08:53:05,558 - [*] phase 0, testing
2023-08-28 08:53:05,640 - T:192	MAE	0.373740	RMSE	0.335766	MAPE	144.705629
2023-08-28 08:53:05,640 - 192	mae	0.3737	
2023-08-28 08:53:05,640 - 192	rmse	0.3358	
2023-08-28 08:53:05,640 - 192	mape	144.7056	
2023-08-28 08:53:07,622 - logger name:exp/ECL-PatchTST2023-08-28-08:53:07.621908/ECL-PatchTST.log
2023-08-28 08:53:07,622 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.3, 'fc_dropout': 0.3, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 128, 'd_model': 16, 'n_heads': 4, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 7.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-08:53:07.621908', 'path': 'exp/ECL-PatchTST2023-08-28-08:53:07.621908', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 08:53:07,622 - [*] phase 0 start training
0 17420
train 7969
val 2545
test 2545
2023-08-28 08:53:07,814 - [*] phase 0 Dataset load!
2023-08-28 08:53:08,651 - [*] phase 0 Training start
train 7969
2023-08-28 08:53:12,448 - epoch:0, training loss:0.8100 validation loss:0.4298
train 7969
vs, vt 0.4297566764056683 0.4285577926784754
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.41942953057587146 0.4144891481846571
need align? ->  True 0.4144891481846571
2023-08-28 08:53:20,512 - epoch:1, training loss:0.8962 validation loss:0.4194
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.4054512597620487 0.3955612938851118
need align? ->  True 0.3955612938851118
2023-08-28 08:53:26,139 - epoch:2, training loss:0.8492 validation loss:0.4055
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.392085749283433 0.3868342272937298
need align? ->  True 0.3868342272937298
2023-08-28 08:53:32,073 - epoch:3, training loss:0.7929 validation loss:0.3921
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.3829761154949665 0.38347429111599923
need align? ->  False 0.38347429111599923
2023-08-28 08:53:38,046 - epoch:4, training loss:0.7479 validation loss:0.3830
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.3781587578356266 0.3812903743237257
need align? ->  False 0.3812903743237257
2023-08-28 08:53:44,013 - epoch:5, training loss:0.7216 validation loss:0.3782
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.37568094208836555 0.37824235670268536
need align? ->  False 0.37824235670268536
2023-08-28 08:53:49,679 - epoch:6, training loss:0.7077 validation loss:0.3757
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.3726761095225811 0.375393944978714
need align? ->  False 0.375393944978714
2023-08-28 08:53:55,833 - epoch:7, training loss:0.6960 validation loss:0.3727
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.3714116908609867 0.37386410385370256
need align? ->  False 0.37386410385370256
2023-08-28 08:54:01,911 - epoch:8, training loss:0.6907 validation loss:0.3714
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.3696848139166832 0.37381849735975264
need align? ->  False 0.37381849735975264
2023-08-28 08:54:08,151 - epoch:9, training loss:0.6867 validation loss:0.3697
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.3667420521378517 0.37029490508139135
need align? ->  False 0.37029490508139135
2023-08-28 08:54:14,259 - epoch:10, training loss:0.6818 validation loss:0.3667
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.3670852784067392 0.36968976333737374
need align? ->  False 0.36968976333737374
2023-08-28 08:54:20,386 - epoch:11, training loss:0.6791 validation loss:0.3671
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.36605111956596376 0.3687377341091633
need align? ->  False 0.3687377341091633
2023-08-28 08:54:26,314 - epoch:12, training loss:0.6774 validation loss:0.3661
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.36816409304738046 0.36834983974695207
need align? ->  False 0.36834983974695207
2023-08-28 08:54:32,277 - epoch:13, training loss:0.6747 validation loss:0.3682
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.3678190641105175 0.3719172950834036
need align? ->  False 0.36834983974695207
2023-08-28 08:54:37,951 - epoch:14, training loss:0.6724 validation loss:0.3678
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.3680079527199268 0.37123357094824316
need align? ->  False 0.36834983974695207
2023-08-28 08:54:43,772 - epoch:15, training loss:0.6680 validation loss:0.3680
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.3649260368198156 0.3722645279020071
need align? ->  False 0.36834983974695207
2023-08-28 08:54:49,441 - epoch:16, training loss:0.6639 validation loss:0.3649
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.36705692782998084 0.3705803744494915
need align? ->  False 0.36834983974695207
2023-08-28 08:54:55,242 - epoch:17, training loss:0.6595 validation loss:0.3671
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.3660998590290546 0.37494365908205507
need align? ->  False 0.36834983974695207
2023-08-28 08:55:01,060 - epoch:18, training loss:0.6577 validation loss:0.3661
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.36724289059638976 0.37471817396581175
need align? ->  False 0.36834983974695207
2023-08-28 08:55:07,029 - epoch:19, training loss:0.6545 validation loss:0.3672
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.3678599577397108 0.3778206452727318
need align? ->  False 0.36834983974695207
2023-08-28 08:55:12,887 - epoch:20, training loss:0.6515 validation loss:0.3679
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.36890885457396505 0.37974687330424783
need align? ->  True 0.36834983974695207
2023-08-28 08:55:18,560 - epoch:21, training loss:0.6523 validation loss:0.3689
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.368882142752409 0.38105699308216573
need align? ->  True 0.36834983974695207
2023-08-28 08:55:24,252 - epoch:22, training loss:0.6509 validation loss:0.3689
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.36890597753226756 0.38068657331168654
need align? ->  True 0.36834983974695207
2023-08-28 08:55:30,158 - epoch:23, training loss:0.6491 validation loss:0.3689
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.36976159922778606 0.3814900379627943
need align? ->  True 0.36834983974695207
2023-08-28 08:55:35,928 - epoch:24, training loss:0.6493 validation loss:0.3698
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.369615238904953 0.3815072275698185
need align? ->  True 0.36834983974695207
2023-08-28 08:55:41,903 - epoch:25, training loss:0.6478 validation loss:0.3696
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.3696525122970343 0.38127704076468943
need align? ->  True 0.36834983974695207
2023-08-28 08:55:47,770 - epoch:26, training loss:0.6496 validation loss:0.3697
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.3696906104683876 0.38131052665412424
need align? ->  True 0.36834983974695207
2023-08-28 08:55:53,542 - epoch:27, training loss:0.6490 validation loss:0.3697
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.36962797306478024 0.38146246075630186
need align? ->  True 0.36834983974695207
2023-08-28 08:55:59,479 - epoch:28, training loss:0.6496 validation loss:0.3696
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.36874192394316196 0.38121695071458817
need align? ->  True 0.36834983974695207
2023-08-28 08:56:06,051 - epoch:29, training loss:0.6499 validation loss:0.3687
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-28-08:53:07.621908/0/0.3649_epoch_16.pkl  &  0.36834983974695207
2023-08-28 08:56:06,608 - [*] loss:0.3649
2023-08-28 08:56:06,616 - [*] phase 0, testing
2023-08-28 08:56:06,995 - T:336	MAE	0.398350	RMSE	0.360447	MAPE	166.295302
2023-08-28 08:56:06,996 - 336	mae	0.3984	
2023-08-28 08:56:06,996 - 336	rmse	0.3604	
2023-08-28 08:56:06,996 - 336	mape	166.2953	
2023-08-28 08:56:07,489 - [*] loss:0.3649
2023-08-28 08:56:07,497 - [*] phase 0, testing
2023-08-28 08:56:07,711 - T:336	MAE	0.398350	RMSE	0.360447	MAPE	166.295302
2023-08-28 08:56:08,321 - [*] loss:0.3658
2023-08-28 08:56:08,331 - [*] phase 0, testing
2023-08-28 08:56:08,667 - T:336	MAE	0.399882	RMSE	0.361374	MAPE	164.197087
2023-08-28 08:56:09,272 - [*] loss:0.3682
2023-08-28 08:56:09,280 - [*] phase 0, testing
2023-08-28 08:56:09,563 - T:336	MAE	0.397623	RMSE	0.363663	MAPE	162.210155
2023-08-28 08:56:09,564 - 336	mae	0.3976	
2023-08-28 08:56:09,564 - 336	rmse	0.3637	
2023-08-28 08:56:09,564 - 336	mape	162.2102	
2023-08-28 08:56:11,570 - logger name:exp/ECL-PatchTST2023-08-28-08:56:11.569851/ECL-PatchTST.log
2023-08-28 08:56:11,570 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.3, 'fc_dropout': 0.3, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 128, 'd_model': 16, 'n_heads': 4, 'seq_len': 336, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 7.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-08:56:11.569851', 'path': 'exp/ECL-PatchTST2023-08-28-08:56:11.569851', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 08:56:11,570 - [*] phase 0 start training
0 17420
train 7585
val 2161
test 2161
2023-08-28 08:56:11,758 - [*] phase 0 Dataset load!
2023-08-28 08:56:12,612 - [*] phase 0 Training start
train 7585
2023-08-28 08:56:16,637 - epoch:0, training loss:0.9788 validation loss:0.4634
train 7585
vs, vt 0.46344279716996584 0.4636097664342207
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.4516289348111433 0.45138005561688366
need align? ->  True 0.45138005561688366
2023-08-28 08:56:24,832 - epoch:1, training loss:1.1091 validation loss:0.4516
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.43385083535138297 0.4337094846893759
need align? ->  True 0.4337094846893759
2023-08-28 08:56:30,777 - epoch:2, training loss:1.0681 validation loss:0.4339
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.4170829513493706 0.4210218822254854
need align? ->  False 0.4210218822254854
2023-08-28 08:56:36,635 - epoch:3, training loss:1.0180 validation loss:0.4171
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.4078448327148662 0.4148818596320994
need align? ->  False 0.4148818596320994
2023-08-28 08:56:42,712 - epoch:4, training loss:0.9749 validation loss:0.4078
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.4027206029962091 0.40929658097379346
need align? ->  False 0.40929658097379346
2023-08-28 08:56:48,594 - epoch:5, training loss:0.9474 validation loss:0.4027
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.3994979310561629 0.40320519577054414
need align? ->  False 0.40320519577054414
2023-08-28 08:56:54,595 - epoch:6, training loss:0.9338 validation loss:0.3995
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.39787759500391345 0.39993034697630825
need align? ->  False 0.39993034697630825
2023-08-28 08:57:00,771 - epoch:7, training loss:0.9244 validation loss:0.3979
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.3967781518312061 0.40080525112502713
need align? ->  False 0.39993034697630825
2023-08-28 08:57:06,586 - epoch:8, training loss:0.9174 validation loss:0.3968
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.39530175792820316 0.39984162151813507
need align? ->  False 0.39984162151813507
2023-08-28 08:57:12,873 - epoch:9, training loss:0.9122 validation loss:0.3953
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.3940308260567048 0.39775148077922706
need align? ->  False 0.39775148077922706
2023-08-28 08:57:18,771 - epoch:10, training loss:0.9100 validation loss:0.3940
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.3935337079798474 0.3973480151856647
need align? ->  False 0.3973480151856647
2023-08-28 08:57:25,203 - epoch:11, training loss:0.9057 validation loss:0.3935
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.39371559023857117 0.3974615957807092
need align? ->  False 0.3973480151856647
2023-08-28 08:57:31,226 - epoch:12, training loss:0.9032 validation loss:0.3937
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.3937547745950082 0.3988102858557421
need align? ->  False 0.3973480151856647
2023-08-28 08:57:36,979 - epoch:13, training loss:0.9028 validation loss:0.3938
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.39234646848019433 0.39919905452167287
need align? ->  False 0.3973480151856647
2023-08-28 08:57:43,373 - epoch:14, training loss:0.8982 validation loss:0.3923
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.39199210265103507 0.39717745123540654
need align? ->  False 0.39717745123540654
2023-08-28 08:57:49,747 - epoch:15, training loss:0.8965 validation loss:0.3920
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.39091984182596207 0.39743153181146174
need align? ->  False 0.39717745123540654
2023-08-28 08:57:55,840 - epoch:16, training loss:0.8931 validation loss:0.3909
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.38992797451860767 0.39748875840621833
need align? ->  False 0.39717745123540654
2023-08-28 08:58:01,880 - epoch:17, training loss:0.8914 validation loss:0.3899
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.38863745463245053 0.3990875942742123
need align? ->  False 0.39717745123540654
2023-08-28 08:58:07,848 - epoch:18, training loss:0.8876 validation loss:0.3886
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.38787363557254567 0.3997926098458907
need align? ->  False 0.39717745123540654
2023-08-28 08:58:13,909 - epoch:19, training loss:0.8853 validation loss:0.3879
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.387391761821859 0.40065957386704054
need align? ->  False 0.39717745123540654
2023-08-28 08:58:19,924 - epoch:20, training loss:0.8829 validation loss:0.3874
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.38715429209610996 0.4011318144552848
need align? ->  False 0.39717745123540654
2023-08-28 08:58:25,839 - epoch:21, training loss:0.8803 validation loss:0.3872
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.38687799739487033 0.40173765447209864
need align? ->  False 0.39717745123540654
2023-08-28 08:58:31,727 - epoch:22, training loss:0.8805 validation loss:0.3869
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.3869438977802501 0.4016445634119651
need align? ->  False 0.39717745123540654
2023-08-28 08:58:37,695 - epoch:23, training loss:0.8784 validation loss:0.3869
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.3871890302090084 0.401608893976492
need align? ->  False 0.39717745123540654
2023-08-28 08:58:43,990 - epoch:24, training loss:0.8790 validation loss:0.3872
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.38712111641378966 0.4018472862594268
need align? ->  False 0.39717745123540654
2023-08-28 08:58:49,953 - epoch:25, training loss:0.8782 validation loss:0.3871
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.38710472119205136 0.4014585035688737
need align? ->  False 0.39717745123540654
2023-08-28 08:58:56,190 - epoch:26, training loss:0.8777 validation loss:0.3871
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.3871548806919771 0.40184016175129833
need align? ->  False 0.39717745123540654
2023-08-28 08:59:02,053 - epoch:27, training loss:0.8780 validation loss:0.3872
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.3870608074700131 0.4017264426631086
need align? ->  False 0.39717745123540654
2023-08-28 08:59:07,963 - epoch:28, training loss:0.8777 validation loss:0.3871
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.3869970011360505 0.4017327072865823
need align? ->  False 0.39717745123540654
2023-08-28 08:59:14,007 - epoch:29, training loss:0.8783 validation loss:0.3870
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-28-08:56:11.569851/0/0.3869_epoch_22.pkl  &  0.39717745123540654
2023-08-28 08:59:14,607 - [*] loss:0.3869
2023-08-28 08:59:14,623 - [*] phase 0, testing
2023-08-28 08:59:15,724 - T:720	MAE	0.423868	RMSE	0.385330	MAPE	193.642569
2023-08-28 08:59:15,725 - 720	mae	0.4239	
2023-08-28 08:59:15,725 - 720	rmse	0.3853	
2023-08-28 08:59:15,725 - 720	mape	193.6426	
2023-08-28 08:59:16,179 - [*] loss:0.3869
2023-08-28 08:59:16,195 - [*] phase 0, testing
2023-08-28 08:59:16,913 - T:720	MAE	0.423868	RMSE	0.385330	MAPE	193.642569
2023-08-28 08:59:17,406 - [*] loss:0.3985
2023-08-28 08:59:17,423 - [*] phase 0, testing
2023-08-28 08:59:18,030 - T:720	MAE	0.431247	RMSE	0.396913	MAPE	196.756220
2023-08-28 08:59:18,560 - [*] loss:0.3970
2023-08-28 08:59:18,577 - [*] phase 0, testing
2023-08-28 08:59:19,391 - T:720	MAE	0.428373	RMSE	0.395425	MAPE	194.160056
2023-08-28 08:59:19,392 - 720	mae	0.4284	
2023-08-28 08:59:19,392 - 720	rmse	0.3954	
2023-08-28 08:59:19,392 - 720	mape	194.1601	
2023-08-28 08:59:21,382 - logger name:exp/ECL-PatchTST2023-08-28-08:59:21.382190/ECL-PatchTST.log
2023-08-28 08:59:21,382 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.3, 'fc_dropout': 0.3, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 128, 'd_model': 16, 'n_heads': 4, 'seq_len': 336, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 7.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-08:59:21.382190', 'path': 'exp/ECL-PatchTST2023-08-28-08:59:21.382190', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 08:59:21,382 - [*] phase 0 start training
0 17420
train 8209
val 2785
test 2785
2023-08-28 08:59:21,573 - [*] phase 0 Dataset load!
2023-08-28 08:59:22,433 - [*] phase 0 Training start
train 8209
2023-08-28 08:59:26,187 - epoch:0, training loss:0.2394 validation loss:0.1858
train 8209
vs, vt 0.18582458370788532 0.18694469027898528
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.17145451022820038 0.17773056182671676
need align? ->  False 0.17773056182671676
2023-08-28 08:59:54,443 - epoch:1, training loss:15.9737 validation loss:0.1715
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.15177412195639176 0.161906158212911
need align? ->  False 0.161906158212911
2023-08-28 09:00:20,796 - epoch:2, training loss:10.8920 validation loss:0.1518
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.13972689990292897 0.14616333718665622
need align? ->  False 0.14616333718665622
2023-08-28 09:00:46,438 - epoch:3, training loss:7.6868 validation loss:0.1397
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.13234800515188413 0.13537380310960792
need align? ->  False 0.13537380310960792
2023-08-28 09:01:12,849 - epoch:4, training loss:6.0301 validation loss:0.1323
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12849361322481523 0.12987571421333335
need align? ->  False 0.12987571421333335
2023-08-28 09:01:38,733 - epoch:5, training loss:4.9237 validation loss:0.1285
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12686059293760496 0.1275771768255667
need align? ->  False 0.1275771768255667
2023-08-28 09:02:04,679 - epoch:6, training loss:4.1764 validation loss:0.1269
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12571482953022828 0.1262465598569675
need align? ->  False 0.1262465598569675
2023-08-28 09:02:30,397 - epoch:7, training loss:3.7216 validation loss:0.1257
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12536663257262923 0.12543321976607497
need align? ->  False 0.12543321976607497
2023-08-28 09:02:56,734 - epoch:8, training loss:3.4377 validation loss:0.1254
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12498426623642445 0.12492856400256808
need align? ->  True 0.12492856400256808
2023-08-28 09:03:22,384 - epoch:9, training loss:3.2353 validation loss:0.1250
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12455125051465901 0.12475012835453857
need align? ->  False 0.12475012835453857
2023-08-28 09:03:48,473 - epoch:10, training loss:3.0833 validation loss:0.1246
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12473862296478315 0.12426511054350571
need align? ->  True 0.12426511054350571
2023-08-28 09:04:14,894 - epoch:11, training loss:2.9885 validation loss:0.1247
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.1250256956131621 0.12417423758994449
need align? ->  True 0.12417423758994449
2023-08-28 09:04:40,692 - epoch:12, training loss:2.8709 validation loss:0.1250
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12428071129728448 0.1244227434085174
need align? ->  True 0.12417423758994449
2023-08-28 09:05:06,467 - epoch:13, training loss:2.8060 validation loss:0.1243
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12419642287899148 0.12367738859558647
need align? ->  True 0.12367738859558647
2023-08-28 09:05:32,673 - epoch:14, training loss:2.7418 validation loss:0.1242
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.1246433225883679 0.12385370023548603
need align? ->  True 0.12367738859558647
2023-08-28 09:05:58,472 - epoch:15, training loss:2.6685 validation loss:0.1246
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12409083392809737 0.12413035604086789
need align? ->  True 0.12367738859558647
2023-08-28 09:06:24,578 - epoch:16, training loss:2.6293 validation loss:0.1241
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.1244028564542532 0.12359740356491371
need align? ->  True 0.12359740356491371
2023-08-28 09:06:50,553 - epoch:17, training loss:2.5967 validation loss:0.1244
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12437064894898371 0.12409580693664876
need align? ->  True 0.12359740356491371
2023-08-28 09:07:16,472 - epoch:18, training loss:2.5415 validation loss:0.1244
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12417509643868967 0.12398146025159142
need align? ->  True 0.12359740356491371
2023-08-28 09:07:42,224 - epoch:19, training loss:2.5269 validation loss:0.1242
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12429111544042826 0.12362610675733197
need align? ->  True 0.12359740356491371
2023-08-28 09:08:07,922 - epoch:20, training loss:2.5209 validation loss:0.1243
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12429871723394502 0.12391879494217309
need align? ->  True 0.12359740356491371
2023-08-28 09:08:33,934 - epoch:21, training loss:2.5122 validation loss:0.1243
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12435728370804679 0.12391016412187707
need align? ->  True 0.12359740356491371
2023-08-28 09:08:59,673 - epoch:22, training loss:2.4997 validation loss:0.1244
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12424436880444939 0.12384547894312577
need align? ->  True 0.12359740356491371
2023-08-28 09:09:25,587 - epoch:23, training loss:2.4832 validation loss:0.1242
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12422492448240519 0.12383054467764767
need align? ->  True 0.12359740356491371
2023-08-28 09:09:51,253 - epoch:24, training loss:2.4754 validation loss:0.1242
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.1242591632868756 0.1238348064944148
need align? ->  True 0.12359740356491371
2023-08-28 09:10:17,272 - epoch:25, training loss:2.4669 validation loss:0.1243
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12433401165022091 0.12384486020627347
need align? ->  True 0.12359740356491371
2023-08-28 09:10:43,238 - epoch:26, training loss:2.4782 validation loss:0.1243
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12431627664376389 0.12383906102993271
need align? ->  True 0.12359740356491371
2023-08-28 09:11:09,103 - epoch:27, training loss:2.4741 validation loss:0.1243
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12434011689302596 0.12386189011687582
need align? ->  True 0.12359740356491371
2023-08-28 09:11:34,956 - epoch:28, training loss:2.4667 validation loss:0.1243
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12430868458680132 0.12383825721388514
need align? ->  True 0.12359740356491371
2023-08-28 09:12:00,657 - epoch:29, training loss:2.4668 validation loss:0.1243
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-28-08:59:21.382190/0/0.1241_epoch_16.pkl  &  0.12359740356491371
2023-08-28 09:12:03,495 - [*] loss:0.2752
2023-08-28 09:12:03,499 - [*] phase 0, testing
2023-08-28 09:12:03,535 - T:96	MAE	0.334221	RMSE	0.275333	MAPE	138.031471
2023-08-28 09:12:03,536 - 96	mae	0.3342	
2023-08-28 09:12:03,536 - 96	rmse	0.2753	
2023-08-28 09:12:03,536 - 96	mape	138.0315	
2023-08-28 09:12:04,037 - [*] loss:0.2754
2023-08-28 09:12:04,040 - [*] phase 0, testing
2023-08-28 09:12:04,086 - T:96	MAE	0.334102	RMSE	0.275571	MAPE	137.179768
2023-08-28 09:12:06,667 - [*] loss:0.2759
2023-08-28 09:12:06,669 - [*] phase 0, testing
2023-08-28 09:12:06,705 - T:96	MAE	0.333100	RMSE	0.276134	MAPE	136.735678
2023-08-28 09:12:07,226 - [*] loss:0.2748
2023-08-28 09:12:07,229 - [*] phase 0, testing
2023-08-28 09:12:07,266 - T:96	MAE	0.332867	RMSE	0.274918	MAPE	135.012090
2023-08-28 09:12:07,268 - 96	mae	0.3329	
2023-08-28 09:12:07,268 - 96	rmse	0.2749	
2023-08-28 09:12:07,268 - 96	mape	135.0121	
2023-08-28 09:12:09,369 - logger name:exp/ECL-PatchTST2023-08-28-09:12:09.366849/ECL-PatchTST.log
2023-08-28 09:12:09,370 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.3, 'fc_dropout': 0.3, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 128, 'd_model': 16, 'n_heads': 4, 'seq_len': 336, 'pred_len': 192, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 7.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-09:12:09.366849', 'path': 'exp/ECL-PatchTST2023-08-28-09:12:09.366849', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 09:12:09,370 - [*] phase 0 start training
0 17420
train 8113
val 2689
test 2689
2023-08-28 09:12:09,560 - [*] phase 0 Dataset load!
2023-08-28 09:12:10,387 - [*] phase 0 Training start
train 8113
2023-08-28 09:12:14,374 - epoch:0, training loss:0.2583 validation loss:0.2030
train 8113
vs, vt 0.20304405740038914 0.20339454371820798
Updating learning rate to 1.0464153247552845e-05
Updating learning rate to 1.0464153247552845e-05
train 8113
vs, vt 0.19294514320790768 0.19565812362865967
need align? ->  False 0.19565812362865967
2023-08-28 09:12:41,768 - epoch:1, training loss:16.0073 validation loss:0.1929
Updating learning rate to 2.8115559773217685e-05
Updating learning rate to 2.8115559773217685e-05
train 8113
vs, vt 0.17967884564264255 0.18284890580583701
need align? ->  False 0.18284890580583701
2023-08-28 09:13:07,043 - epoch:2, training loss:10.9860 validation loss:0.1797
Updating learning rate to 5.2199994709629883e-05
Updating learning rate to 5.2199994709629883e-05
train 8113
vs, vt 0.17056400769136168 0.17390273105014453
need align? ->  False 0.17390273105014453
2023-08-28 09:13:32,703 - epoch:3, training loss:7.7529 validation loss:0.1706
Updating learning rate to 7.623056312721927e-05
Updating learning rate to 7.623056312721927e-05
train 8113
vs, vt 0.1647593340074474 0.1666687899692492
need align? ->  False 0.1666687899692492
2023-08-28 09:13:57,910 - epoch:4, training loss:6.0721 validation loss:0.1648
Updating learning rate to 9.373487848943999e-05
Updating learning rate to 9.373487848943999e-05
train 8113
vs, vt 0.16214105690067465 0.16226602980697696
need align? ->  False 0.16226602980697696
2023-08-28 09:14:23,708 - epoch:5, training loss:4.9920 validation loss:0.1621
Updating learning rate to 9.999989207196297e-05
Updating learning rate to 9.999989207196297e-05
train 8113
vs, vt 0.16073439143259416 0.16098800995810467
need align? ->  False 0.16098800995810467
2023-08-28 09:14:49,322 - epoch:6, training loss:4.3179 validation loss:0.1607
Updating learning rate to 9.955857764964711e-05
Updating learning rate to 9.955857764964711e-05
train 8113
vs, vt 0.15948727024211126 0.15995093384249645
need align? ->  False 0.15995093384249645
2023-08-28 09:15:14,958 - epoch:7, training loss:3.8819 validation loss:0.1595
Updating learning rate to 9.826930564556767e-05
Updating learning rate to 9.826930564556767e-05
train 8113
vs, vt 0.15950118115341122 0.15861167276108806
need align? ->  True 0.15861167276108806
2023-08-28 09:15:40,312 - epoch:8, training loss:3.5857 validation loss:0.1595
Updating learning rate to 9.61541358611682e-05
Updating learning rate to 9.61541358611682e-05
train 8113
vs, vt 0.15925883256237616 0.158800830217925
need align? ->  True 0.15861167276108806
2023-08-28 09:16:06,233 - epoch:9, training loss:3.3823 validation loss:0.1593
Updating learning rate to 9.324925943789559e-05
Updating learning rate to 9.324925943789559e-05
train 8113
vs, vt 0.15856517377224835 0.1585625602271069
need align? ->  True 0.1585625602271069
2023-08-28 09:16:31,638 - epoch:10, training loss:3.2823 validation loss:0.1586
Updating learning rate to 8.960437961673599e-05
Updating learning rate to 8.960437961673599e-05
train 8113
vs, vt 0.15862434916198254 0.15825072159482675
need align? ->  True 0.15825072159482675
2023-08-28 09:16:57,282 - epoch:11, training loss:3.1324 validation loss:0.1586
Updating learning rate to 8.528186130198099e-05
Updating learning rate to 8.528186130198099e-05
train 8113
vs, vt 0.1577079946344549 0.15853140071373095
need align? ->  False 0.15825072159482675
2023-08-28 09:17:22,569 - epoch:12, training loss:3.0251 validation loss:0.1577
Updating learning rate to 8.035566398042457e-05
Updating learning rate to 8.035566398042457e-05
train 8113
vs, vt 0.15823311942883514 0.15711724233220925
need align? ->  True 0.15711724233220925
2023-08-28 09:17:48,366 - epoch:13, training loss:2.9503 validation loss:0.1582
Updating learning rate to 7.491007625403847e-05
Updating learning rate to 7.491007625403847e-05
train 8113
vs, vt 0.15765631207349626 0.1572954241525043
need align? ->  True 0.15711724233220925
2023-08-28 09:18:14,515 - epoch:14, training loss:2.8766 validation loss:0.1577
Updating learning rate to 6.903827363862332e-05
Updating learning rate to 6.903827363862332e-05
train 8113
vs, vt 0.15804620434276082 0.15668742782012982
need align? ->  True 0.15668742782012982
2023-08-28 09:18:40,632 - epoch:15, training loss:2.8320 validation loss:0.1580
Updating learning rate to 6.284072430490012e-05
Updating learning rate to 6.284072430490012e-05
train 8113
vs, vt 0.1573358436369083 0.15748339747502046
need align? ->  True 0.15668742782012982
2023-08-28 09:19:06,036 - epoch:16, training loss:2.7490 validation loss:0.1573
Updating learning rate to 5.642347004025414e-05
Updating learning rate to 5.642347004025414e-05
train 8113
vs, vt 0.15745692776346748 0.15685937604443592
need align? ->  True 0.15668742782012982
2023-08-28 09:19:31,554 - epoch:17, training loss:2.7446 validation loss:0.1575
Updating learning rate to 4.989631184435254e-05
Updating learning rate to 4.989631184435254e-05
train 8113
vs, vt 0.15801290312612598 0.15702088431201197
need align? ->  True 0.15668742782012982
2023-08-28 09:19:57,009 - epoch:18, training loss:2.7003 validation loss:0.1580
Updating learning rate to 4.337093120359729e-05
Updating learning rate to 4.337093120359729e-05
train 8113
vs, vt 0.15767148154025729 0.1571978558362885
need align? ->  True 0.15668742782012982
2023-08-28 09:20:22,297 - epoch:19, training loss:2.6979 validation loss:0.1577
Updating learning rate to 3.695897918992905e-05
Updating learning rate to 3.695897918992905e-05
train 8113
vs, vt 0.15762140991335566 0.15707546870478176
need align? ->  True 0.15668742782012982
2023-08-28 09:20:48,002 - epoch:20, training loss:2.6744 validation loss:0.1576
Updating learning rate to 3.077016608003061e-05
Updating learning rate to 3.077016608003061e-05
train 8113
vs, vt 0.15738168502734465 0.15706425117836756
need align? ->  True 0.15668742782012982
2023-08-28 09:21:13,902 - epoch:21, training loss:2.6721 validation loss:0.1574
Updating learning rate to 2.4910384182075503e-05
Updating learning rate to 2.4910384182075503e-05
train 8113
vs, vt 0.15729719823734326 0.15680535163053058
need align? ->  True 0.15668742782012982
2023-08-28 09:21:39,224 - epoch:22, training loss:2.6523 validation loss:0.1573
Updating learning rate to 1.947989598897622e-05
Updating learning rate to 1.947989598897622e-05
train 8113
vs, vt 0.1574067496271296 0.15673435953530399
need align? ->  True 0.15668742782012982
2023-08-28 09:22:04,556 - epoch:23, training loss:2.6432 validation loss:0.1574
Updating learning rate to 1.4571618659332437e-05
Updating learning rate to 1.4571618659332437e-05
train 8113
vs, vt 0.1573640104721893 0.1567695534534075
need align? ->  True 0.15668742782012982
2023-08-28 09:22:30,027 - epoch:24, training loss:2.6457 validation loss:0.1574
Updating learning rate to 1.0269534179085959e-05
Updating learning rate to 1.0269534179085959e-05
train 8113
vs, vt 0.15731762476604094 0.15677878302945333
need align? ->  True 0.15668742782012982
2023-08-28 09:22:55,480 - epoch:25, training loss:2.6252 validation loss:0.1573
Updating learning rate to 6.647252406456951e-06
Updating learning rate to 6.647252406456951e-06
train 8113
vs, vt 0.1573754106224938 0.15677574039860206
need align? ->  True 0.15668742782012982
2023-08-28 09:23:20,612 - epoch:26, training loss:2.6342 validation loss:0.1574
Updating learning rate to 3.7667515868613148e-06
Updating learning rate to 3.7667515868613148e-06
train 8113
vs, vt 0.15739027097482572 0.15678191828456792
need align? ->  True 0.15668742782012982
2023-08-28 09:23:46,302 - epoch:27, training loss:2.6182 validation loss:0.1574
Updating learning rate to 1.677317887948057e-06
Updating learning rate to 1.677317887948057e-06
train 8113
vs, vt 0.15737852615050293 0.15681553445756435
need align? ->  True 0.15668742782012982
2023-08-28 09:24:11,684 - epoch:28, training loss:2.6197 validation loss:0.1574
Updating learning rate to 4.1470209960604436e-07
Updating learning rate to 4.1470209960604436e-07
train 8113
vs, vt 0.1573478835211559 0.15675831653855063
need align? ->  True 0.15668742782012982
2023-08-28 09:24:37,246 - epoch:29, training loss:2.6408 validation loss:0.1573
Updating learning rate to 5.079280370371978e-10
Updating learning rate to 5.079280370371978e-10
check exp/ECL-PatchTST2023-08-28-09:12:09.366849/0/0.1573_epoch_22.pkl  &  0.15668742782012982
2023-08-28 09:24:39,962 - [*] loss:0.3573
2023-08-28 09:24:39,967 - [*] phase 0, testing
2023-08-28 09:24:40,052 - T:192	MAE	0.376748	RMSE	0.340199	MAPE	150.794268
2023-08-28 09:24:40,052 - 192	mae	0.3767	
2023-08-28 09:24:40,052 - 192	rmse	0.3402	
2023-08-28 09:24:40,052 - 192	mape	150.7943	
2023-08-28 09:24:40,588 - [*] loss:0.3575
2023-08-28 09:24:40,593 - [*] phase 0, testing
2023-08-28 09:24:40,671 - T:192	MAE	0.376570	RMSE	0.340752	MAPE	150.633013
2023-08-28 09:24:43,142 - [*] loss:0.3589
2023-08-28 09:24:43,147 - [*] phase 0, testing
2023-08-28 09:24:43,230 - T:192	MAE	0.375481	RMSE	0.340607	MAPE	146.936345
2023-08-28 09:24:43,731 - [*] loss:0.3567
2023-08-28 09:24:43,735 - [*] phase 0, testing
2023-08-28 09:24:43,822 - T:192	MAE	0.373993	RMSE	0.338661	MAPE	145.185351
2023-08-28 09:24:43,822 - 192	mae	0.3740	
2023-08-28 09:24:43,822 - 192	rmse	0.3387	
2023-08-28 09:24:43,822 - 192	mape	145.1854	
2023-08-28 09:24:45,792 - logger name:exp/ECL-PatchTST2023-08-28-09:24:45.792735/ECL-PatchTST.log
2023-08-28 09:24:45,793 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.3, 'fc_dropout': 0.3, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 128, 'd_model': 16, 'n_heads': 4, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 7.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-09:24:45.792735', 'path': 'exp/ECL-PatchTST2023-08-28-09:24:45.792735', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 09:24:45,793 - [*] phase 0 start training
0 17420
train 7969
val 2545
test 2545
2023-08-28 09:24:45,981 - [*] phase 0 Dataset load!
2023-08-28 09:24:46,826 - [*] phase 0 Training start
train 7969
2023-08-28 09:24:50,628 - epoch:0, training loss:0.2827 validation loss:0.1916
train 7969
vs, vt 0.19156976398080588 0.19124496970325708
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.18632086385041474 0.18526889737695457
need align? ->  True 0.18526889737695457
2023-08-28 09:25:17,679 - epoch:1, training loss:16.0097 validation loss:0.1863
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.1802918940782547 0.1772845845669508
need align? ->  True 0.1772845845669508
2023-08-28 09:25:42,700 - epoch:2, training loss:11.0298 validation loss:0.1803
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.17562947273254395 0.1760431993752718
need align? ->  False 0.1760431993752718
2023-08-28 09:26:08,331 - epoch:3, training loss:7.8263 validation loss:0.1756
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.17252311799675227 0.17383522912859917
need align? ->  False 0.17383522912859917
2023-08-28 09:26:33,619 - epoch:4, training loss:6.1490 validation loss:0.1725
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.17117222752422095 0.17190465405583383
need align? ->  False 0.17190465405583383
2023-08-28 09:26:58,985 - epoch:5, training loss:5.0727 validation loss:0.1712
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.16908572036772967 0.17053159438073634
need align? ->  False 0.17053159438073634
2023-08-28 09:27:24,344 - epoch:6, training loss:4.3974 validation loss:0.1691
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.16835862454026937 0.16883376706391573
need align? ->  False 0.16883376706391573
2023-08-28 09:27:49,506 - epoch:7, training loss:3.9830 validation loss:0.1684
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.16803399175405503 0.1686125250533223
need align? ->  False 0.1686125250533223
2023-08-28 09:28:15,186 - epoch:8, training loss:3.7239 validation loss:0.1680
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.16788330040872096 0.16817351747304202
need align? ->  False 0.16817351747304202
2023-08-28 09:28:40,120 - epoch:9, training loss:3.5265 validation loss:0.1679
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.16649130694568157 0.1668765589594841
need align? ->  False 0.1668765589594841
2023-08-28 09:29:05,588 - epoch:10, training loss:3.3679 validation loss:0.1665
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.1670625561848283 0.16669888980686665
need align? ->  True 0.16669888980686665
2023-08-28 09:29:30,650 - epoch:11, training loss:3.2400 validation loss:0.1671
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.16655269209295512 0.1666325400583446
need align? ->  False 0.1666325400583446
2023-08-28 09:29:56,162 - epoch:12, training loss:3.1439 validation loss:0.1666
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.16731479093432428 0.16605954384431243
need align? ->  True 0.16605954384431243
2023-08-28 09:30:21,673 - epoch:13, training loss:3.0665 validation loss:0.1673
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.16739507168531417 0.16705130077898503
need align? ->  True 0.16605954384431243
2023-08-28 09:30:46,921 - epoch:14, training loss:2.9861 validation loss:0.1674
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.1670899299904704 0.16704478543251752
need align? ->  True 0.16605954384431243
2023-08-28 09:31:11,921 - epoch:15, training loss:2.9290 validation loss:0.1671
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.16629810612648727 0.16646774895489216
need align? ->  True 0.16605954384431243
2023-08-28 09:31:37,062 - epoch:16, training loss:2.9039 validation loss:0.1663
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.16695658601820468 0.16529006427153944
need align? ->  True 0.16529006427153944
2023-08-28 09:32:02,665 - epoch:17, training loss:2.8803 validation loss:0.1670
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.16690177731215955 0.16601675003767014
need align? ->  True 0.16529006427153944
2023-08-28 09:32:28,134 - epoch:18, training loss:2.8138 validation loss:0.1669
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.16628434788435698 0.16591376755386592
need align? ->  True 0.16529006427153944
2023-08-28 09:32:56,818 - epoch:19, training loss:2.8003 validation loss:0.1663
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.1663068702444434 0.16564854886382818
need align? ->  True 0.16529006427153944
2023-08-28 09:33:24,379 - epoch:20, training loss:2.7707 validation loss:0.1663
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.1671767622232437 0.16594053180888296
need align? ->  True 0.16529006427153944
2023-08-28 09:33:49,582 - epoch:21, training loss:2.7561 validation loss:0.1672
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.1666650591418147 0.16637479588389398
need align? ->  True 0.16529006427153944
2023-08-28 09:34:15,331 - epoch:22, training loss:2.7504 validation loss:0.1667
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.16666617505252362 0.1658909846097231
need align? ->  True 0.16529006427153944
2023-08-28 09:34:40,610 - epoch:23, training loss:2.7500 validation loss:0.1667
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.16690718065947294 0.1660621680319309
need align? ->  True 0.16529006427153944
2023-08-28 09:35:09,811 - epoch:24, training loss:2.7313 validation loss:0.1669
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.16685619316995143 0.16601298972964287
need align? ->  True 0.16529006427153944
2023-08-28 09:35:34,904 - epoch:25, training loss:2.7454 validation loss:0.1669
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.16680561285465956 0.1659746509976685
need align? ->  True 0.16529006427153944
2023-08-28 09:36:21,517 - epoch:26, training loss:2.7240 validation loss:0.1668
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.16680327355861663 0.16595694422721863
need align? ->  True 0.16529006427153944
2023-08-28 09:36:47,573 - epoch:27, training loss:2.7194 validation loss:0.1668
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.1668225659057498 0.1660115818493068
need align? ->  True 0.16529006427153944
2023-08-28 09:37:20,131 - epoch:28, training loss:2.7259 validation loss:0.1668
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.1667352182790637 0.16594256833195686
need align? ->  True 0.16529006427153944
2023-08-28 09:37:46,029 - epoch:29, training loss:2.7200 validation loss:0.1667
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-28-09:24:45.792735/0/0.1663_epoch_19.pkl  &  0.16529006427153944
2023-08-28 09:37:48,596 - [*] loss:0.3721
2023-08-28 09:37:48,604 - [*] phase 0, testing
2023-08-28 09:37:48,930 - T:336	MAE	0.400955	RMSE	0.367705	MAPE	168.425488
2023-08-28 09:37:48,930 - 336	mae	0.4010	
2023-08-28 09:37:48,930 - 336	rmse	0.3677	
2023-08-28 09:37:48,930 - 336	mape	168.4255	
2023-08-28 09:37:49,526 - [*] loss:0.3720
2023-08-28 09:37:49,534 - [*] phase 0, testing
2023-08-28 09:37:49,666 - T:336	MAE	0.400814	RMSE	0.367597	MAPE	168.267190
2023-08-28 09:37:52,175 - [*] loss:0.3735
2023-08-28 09:37:52,183 - [*] phase 0, testing
2023-08-28 09:37:52,313 - T:336	MAE	0.398918	RMSE	0.368911	MAPE	161.995757
2023-08-28 09:37:53,034 - [*] loss:0.3726
2023-08-28 09:37:53,042 - [*] phase 0, testing
2023-08-28 09:37:53,170 - T:336	MAE	0.397670	RMSE	0.367929	MAPE	160.235834
2023-08-28 09:37:53,171 - 336	mae	0.3977	
2023-08-28 09:37:53,171 - 336	rmse	0.3679	
2023-08-28 09:37:53,171 - 336	mape	160.2358	
2023-08-28 09:37:55,183 - logger name:exp/ECL-PatchTST2023-08-28-09:37:55.183689/ECL-PatchTST.log
2023-08-28 09:37:55,184 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.3, 'fc_dropout': 0.3, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 128, 'd_model': 16, 'n_heads': 4, 'seq_len': 336, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 7.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-09:37:55.183689', 'path': 'exp/ECL-PatchTST2023-08-28-09:37:55.183689', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 09:37:55,184 - [*] phase 0 start training
0 17420
train 7585
val 2161
test 2161
2023-08-28 09:37:55,376 - [*] phase 0 Dataset load!
2023-08-28 09:37:56,233 - [*] phase 0 Training start
train 7585
2023-08-28 09:38:00,724 - epoch:0, training loss:0.3378 validation loss:0.2059
train 7585
vs, vt 0.20587393509991028 0.2059927810640896
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.19952202106223388 0.2007338317001567
need align? ->  False 0.2007338317001567
2023-08-28 09:38:27,238 - epoch:1, training loss:16.2562 validation loss:0.1995
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.19071998184218125 0.19312962714363546
need align? ->  False 0.19312962714363546
2023-08-28 09:38:51,418 - epoch:2, training loss:11.3711 validation loss:0.1907
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.1851839081329458 0.18918011293691747
need align? ->  False 0.18918011293691747
2023-08-28 09:39:15,717 - epoch:3, training loss:8.0461 validation loss:0.1852
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.18215032557354255 0.18675638779121287
need align? ->  False 0.18675638779121287
2023-08-28 09:39:40,060 - epoch:4, training loss:6.3548 validation loss:0.1822
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.18046994967495694 0.1843333838178831
need align? ->  False 0.1843333838178831
2023-08-28 09:40:04,261 - epoch:5, training loss:5.2954 validation loss:0.1805
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.1793086162823088 0.18233776004875407
need align? ->  False 0.18233776004875407
2023-08-28 09:40:28,537 - epoch:6, training loss:4.6248 validation loss:0.1793
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.1787316198734676 0.1815235527122722
need align? ->  False 0.1815235527122722
2023-08-28 09:40:52,901 - epoch:7, training loss:4.2248 validation loss:0.1787
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.17872113208560383 0.1816711005042581
need align? ->  False 0.1815235527122722
2023-08-28 09:41:20,425 - epoch:8, training loss:3.9340 validation loss:0.1787
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.17834475943270853 0.18102503765155287
need align? ->  False 0.18102503765155287
2023-08-28 09:41:45,085 - epoch:9, training loss:3.7921 validation loss:0.1783
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.1777619439012864 0.180387184900396
need align? ->  False 0.180387184900396
2023-08-28 09:42:09,734 - epoch:10, training loss:3.5700 validation loss:0.1778
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.17779444114250295 0.1802121383302352
need align? ->  False 0.1802121383302352
2023-08-28 09:42:34,439 - epoch:11, training loss:3.4246 validation loss:0.1778
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.1778810149168267 0.18066684069002376
need align? ->  False 0.1802121383302352
2023-08-28 09:42:59,113 - epoch:12, training loss:3.3310 validation loss:0.1779
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.17800347550826914 0.1804943012402338
need align? ->  False 0.1802121383302352
2023-08-28 09:43:23,382 - epoch:13, training loss:3.2787 validation loss:0.1780
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.17753086414407282 0.18104899784221368
need align? ->  False 0.1802121383302352
2023-08-28 09:43:47,899 - epoch:14, training loss:3.2264 validation loss:0.1775
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.17731258733307614 0.17974978378590414
need align? ->  False 0.17974978378590414
2023-08-28 09:44:13,090 - epoch:15, training loss:3.1915 validation loss:0.1773
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.1774440681671395 0.1793642401256982
need align? ->  False 0.1793642401256982
2023-08-28 09:44:40,157 - epoch:16, training loss:3.0907 validation loss:0.1774
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.17762966760817697 0.1799351857865558
need align? ->  False 0.1793642401256982
2023-08-28 09:45:04,336 - epoch:17, training loss:3.0473 validation loss:0.1776
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.17750716450459816 0.18016873365815947
need align? ->  False 0.1793642401256982
2023-08-28 09:45:28,565 - epoch:18, training loss:3.0140 validation loss:0.1775
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.17755410623024492 0.1800072324626586
need align? ->  False 0.1793642401256982
2023-08-28 09:45:52,921 - epoch:19, training loss:2.9872 validation loss:0.1776
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.1774975006632945 0.1801273787722868
need align? ->  False 0.1793642401256982
2023-08-28 09:46:17,273 - epoch:20, training loss:2.9876 validation loss:0.1775
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.17748301147538073 0.18016403365661116
need align? ->  False 0.1793642401256982
2023-08-28 09:46:42,122 - epoch:21, training loss:2.9812 validation loss:0.1775
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.17745658821042845 0.1802251448526102
need align? ->  False 0.1793642401256982
2023-08-28 09:47:07,077 - epoch:22, training loss:2.9636 validation loss:0.1775
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.17748577980434194 0.1801461340749965
need align? ->  False 0.1793642401256982
2023-08-28 09:47:36,042 - epoch:23, training loss:2.9538 validation loss:0.1775
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.17756809316137256 0.18018662688486717
need align? ->  False 0.1793642401256982
2023-08-28 09:48:01,027 - epoch:24, training loss:2.9535 validation loss:0.1776
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.1776014428366633 0.1803346918348004
need align? ->  False 0.1793642401256982
2023-08-28 09:48:25,699 - epoch:25, training loss:2.9295 validation loss:0.1776
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.1775462640558972 0.18023678024902062
need align? ->  False 0.1793642401256982
2023-08-28 09:48:49,954 - epoch:26, training loss:2.9498 validation loss:0.1775
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.1775921898729661 0.1803660451927606
need align? ->  False 0.1793642401256982
2023-08-28 09:49:14,817 - epoch:27, training loss:2.9308 validation loss:0.1776
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.17757419058505228 0.18033363582456813
need align? ->  False 0.1793642401256982
2023-08-28 09:49:39,127 - epoch:28, training loss:2.9396 validation loss:0.1776
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.177553540424389 0.18034561679643743
need align? ->  False 0.1793642401256982
2023-08-28 09:50:03,415 - epoch:29, training loss:2.9601 validation loss:0.1776
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-28-09:37:55.183689/0/0.1773_epoch_15.pkl  &  0.1793642401256982
2023-08-28 09:50:05,749 - [*] loss:0.3938
2023-08-28 09:50:05,764 - [*] phase 0, testing
2023-08-28 09:50:06,410 - T:720	MAE	0.425838	RMSE	0.392210	MAPE	195.153022
2023-08-28 09:50:06,411 - 720	mae	0.4258	
2023-08-28 09:50:06,411 - 720	rmse	0.3922	
2023-08-28 09:50:06,411 - 720	mape	195.1530	
2023-08-28 09:50:06,927 - [*] loss:0.3936
2023-08-28 09:50:06,941 - [*] phase 0, testing
2023-08-28 09:50:07,381 - T:720	MAE	0.425915	RMSE	0.391926	MAPE	194.511819
2023-08-28 09:50:09,693 - [*] loss:0.4033
2023-08-28 09:50:09,707 - [*] phase 0, testing
2023-08-28 09:50:10,153 - T:720	MAE	0.428702	RMSE	0.401719	MAPE	190.126443
2023-08-28 09:50:10,712 - [*] loss:0.3998
2023-08-28 09:50:10,726 - [*] phase 0, testing
2023-08-28 09:50:11,210 - T:720	MAE	0.427217	RMSE	0.398149	MAPE	188.944983
2023-08-28 09:50:11,211 - 720	mae	0.4272	
2023-08-28 09:50:11,211 - 720	rmse	0.3981	
2023-08-28 09:50:11,211 - 720	mape	188.9450	
