2023-07-24 14:31:02,376 - logger name:exp/ECL-PatchTST2023-07-24-14:31:02.376670/ECL-PatchTST.log
2023-07-24 14:31:02,377 - params : {'conf': 'ECL-PatchTST', 'data_name': 'electricity', 'iteration': 1, 'load': True, 'build_graph': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.1, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': 89, 'aligner': 0, 'refiner': 1, 'seed': 2023, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'batch_size': 16, 'lr': 0.0001, 'loss': 'mse', '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': False, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-07-24-14:31:02.376670', 'path': 'exp/ECL-PatchTST2023-07-24-14:31:02.376670', 'num_workers': 4, 'start_train': 0, 'logger': <Logger __main__ (INFO)>}
2023-07-24 14:31:02,377 - [*] phase 0 start training
18412 2632 5260 0.7 0.2 26304
train 17981
18412 2632 5260 0.7 0.2 26304
val 2537
18412 2632 5260 0.7 0.2 26304
test 5165
2023-07-24 14:31:05,560 - [*] phase 0 Dataset load!
2023-07-24 14:31:06,387 - [*] phase 0 Training start
18412 2632 5260 0.7 0.2 26304
train 17981
2023-07-24 14:31:27,870 - epoch:0, training loss:0.2406 validation loss:0.0657
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.06568460851767154 0.06531941004495763
Updating learning rate to 1.0432646020812621e-05
Updating learning rate to 1.0432646020812621e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.0519558518785257 0.05229377317822204
2023-07-24 14:32:19,004 - epoch:1, training loss:0.2341 validation loss:0.0520
Updating learning rate to 2.8006461798705615e-05
Updating learning rate to 2.8006461798705615e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04878269994057387 0.04821535864692626
2023-07-24 14:32:53,263 - epoch:2, training loss:0.1846 validation loss:0.0488
Updating learning rate to 5.201119166142244e-05
Updating learning rate to 5.201119166142244e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.049384776895291776 0.04867465663467001
2023-07-24 14:33:27,923 - epoch:3, training loss:0.1703 validation loss:0.0494
Updating learning rate to 7.601292185762613e-05
Updating learning rate to 7.601292185762613e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04876040425971619 0.050634300137090985
2023-07-24 14:34:02,561 - epoch:4, training loss:0.1632 validation loss:0.0488
Updating learning rate to 9.357854262734886e-05
Updating learning rate to 9.357854262734886e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.046507857581776266 0.046122955275500345
2023-07-24 14:34:37,093 - epoch:5, training loss:0.1580 validation loss:0.0465
Updating learning rate to 9.999999966033092e-05
Updating learning rate to 9.999999966033092e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04371385097081931 0.044194233275762516
2023-07-24 14:35:11,790 - epoch:6, training loss:0.1490 validation loss:0.0437
Updating learning rate to 9.957148372299848e-05
Updating learning rate to 9.957148372299848e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04419648819515165 0.04452607561043411
2023-07-24 14:35:46,252 - epoch:7, training loss:0.1428 validation loss:0.0442
Updating learning rate to 9.829478937740917e-05
Updating learning rate to 9.829478937740917e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.044907883735104175 0.04676361308790415
2023-07-24 14:36:20,844 - epoch:8, training loss:0.1360 validation loss:0.0449
Updating learning rate to 9.619176121778259e-05
Updating learning rate to 9.619176121778259e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04680759501907061 0.045415232045892276
2023-07-24 14:36:55,359 - epoch:9, training loss:0.1298 validation loss:0.0468
Updating learning rate to 9.32983826389995e-05
Updating learning rate to 9.32983826389995e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04307684523153043 0.044345510818954534
2023-07-24 14:37:29,646 - epoch:10, training loss:0.1253 validation loss:0.0431
Updating learning rate to 8.966416015073898e-05
Updating learning rate to 8.966416015073898e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.041701121753632275 0.041486238715174435
2023-07-24 14:38:04,144 - epoch:11, training loss:0.1215 validation loss:0.0417
Updating learning rate to 8.5351276307372e-05
Updating learning rate to 8.5351276307372e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04221892175293943 0.04218201155909015
2023-07-24 14:38:38,851 - epoch:12, training loss:0.1179 validation loss:0.0422
Updating learning rate to 8.043352574721588e-05
Updating learning rate to 8.043352574721588e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.043496717933167076 0.04452869191226907
2023-07-24 14:39:13,523 - epoch:13, training loss:0.1149 validation loss:0.0435
Updating learning rate to 7.499505254581293e-05
Updating learning rate to 7.499505254581293e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.042290930050489665 0.043285125304224355
2023-07-24 14:39:48,272 - epoch:14, training loss:0.1117 validation loss:0.0423
Updating learning rate to 6.912891048746876e-05
Updating learning rate to 6.912891048746876e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04134568023501233 0.04195279188149568
2023-07-24 14:40:23,375 - epoch:15, training loss:0.1089 validation loss:0.0413
Updating learning rate to 6.293547088920353e-05
Updating learning rate to 6.293547088920353e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.040617171232735026 0.04043872143967815
2023-07-24 14:40:58,158 - epoch:16, training loss:0.1061 validation loss:0.0406
Updating learning rate to 5.652070521968939e-05
Updating learning rate to 5.652070521968939e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04070126217842664 0.041341021358263945
2023-07-24 14:41:33,244 - epoch:17, training loss:0.1038 validation loss:0.0407
Updating learning rate to 4.999437189804077e-05
Updating learning rate to 4.999437189804077e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03998869763534961 0.04015136327676522
2023-07-24 14:42:08,192 - epoch:18, training loss:0.1015 validation loss:0.0400
Updating learning rate to 4.346813829683254e-05
Updating learning rate to 4.346813829683254e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03957127480703236 0.0388317163418627
2023-07-24 14:42:43,007 - epoch:19, training loss:0.0993 validation loss:0.0396
Updating learning rate to 3.705367008239516e-05
Updating learning rate to 3.705367008239516e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.039766010579377786 0.03938201707699951
2023-07-24 14:43:17,628 - epoch:20, training loss:0.0973 validation loss:0.0398
Updating learning rate to 3.086072058430419e-05
Updating learning rate to 3.086072058430419e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03978927347973083 0.0398957499730512
2023-07-24 14:43:52,072 - epoch:21, training loss:0.0955 validation loss:0.0398
Updating learning rate to 2.499525288548201e-05
Updating learning rate to 2.499525288548201e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03951493824053111 0.038681570396600466
2023-07-24 14:44:26,863 - epoch:22, training loss:0.0937 validation loss:0.0395
Updating learning rate to 1.9557626764470303e-05
Updating learning rate to 1.9557626764470303e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03904006207572683 0.038524444194498866
2023-07-24 14:45:01,606 - epoch:23, training loss:0.0928 validation loss:0.0390
Updating learning rate to 1.4640881511794355e-05
Updating learning rate to 1.4640881511794355e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03885276064440702 0.038345872289823286
2023-07-24 14:45:36,479 - epoch:24, training loss:0.0912 validation loss:0.0389
Updating learning rate to 1.0329144001906798e-05
Updating learning rate to 1.0329144001906798e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03877772820480871 0.03816448761817982
2023-07-24 14:46:11,170 - epoch:25, training loss:0.0903 validation loss:0.0388
Updating learning rate to 6.696189259041272e-06
Updating learning rate to 6.696189259041272e-06
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03924866982167528 0.038904519499800866
2023-07-24 14:46:45,876 - epoch:26, training loss:0.0896 validation loss:0.0392
Updating learning rate to 3.804178146093549e-06
Updating learning rate to 3.804178146093549e-06
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.038745286168659444 0.038285739983449564
2023-07-24 14:47:20,741 - epoch:27, training loss:0.0891 validation loss:0.0387
Updating learning rate to 1.7025937750231399e-06
Updating learning rate to 1.7025937750231399e-06
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.038740776690109724 0.03832462490331264
2023-07-24 14:47:55,770 - epoch:28, training loss:0.0886 validation loss:0.0387
Updating learning rate to 4.2739483708831805e-07
Updating learning rate to 4.2739483708831805e-07
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03870613974337495 0.03822158157251447
2023-07-24 14:48:31,025 - epoch:29, training loss:0.0883 validation loss:0.0387
Updating learning rate to 4.003396690830917e-10
Updating learning rate to 4.003396690830917e-10
2023-07-24 14:48:32,593 - [*] loss:0.0883
2023-07-24 14:48:32,595 - [*] phase 0, testing
2023-07-24 14:48:32,606 - T:96	MAE	0.186928	RMSE	0.088344	MAPE	184.182286
2023-07-24 14:48:32,606 - 96	mae	0.1869	
2023-07-24 14:48:32,606 - 96	rmse	0.0883	
2023-07-24 14:48:32,606 - 96	mape	184.1823	
