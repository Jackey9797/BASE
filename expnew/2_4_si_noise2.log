2023-07-25 15:29:23,811 - logger name:exp/ECL-PatchTST2023-07-25-15:29:23.811124/ECL-PatchTST.log
2023-07-25 15:29:23,811 - params : {'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.2, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'refiner': 0, 'enhance': 0, 'seed': 2022, 'batch_size': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, 'loss': 'mse', '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-07-25-15:29:23.811124', 'path': 'exp/ECL-PatchTST2023-07-25-15:29:23.811124', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-07-25 15:29:23,811 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-07-25 15:29:24,003 - [*] phase 0 Dataset load!
2023-07-25 15:29:24,944 - [*] phase 0 Training start
train 8209
2023-07-25 15:29:34,412 - epoch:0, training loss:0.6260 validation loss:0.2945
train 8209
vs, vt 0.29448264092206955 0.2952709983695637
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.2408587278967554 0.2446668449450623
2023-07-25 15:29:56,783 - epoch:1, training loss:0.6598 validation loss:0.2409
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.216844419864091 0.2210905406285416
2023-07-25 15:30:12,934 - epoch:2, training loss:0.5674 validation loss:0.2168
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.21459245783361522 0.21325841004198248
2023-07-25 15:30:29,586 - epoch:3, training loss:0.5214 validation loss:0.2146
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.21007295562462372 0.21288785101337868
2023-07-25 15:30:46,774 - epoch:4, training loss:0.5712 validation loss:0.2101
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.2098986025560986 0.21187522093003447
2023-07-25 15:31:03,474 - epoch:5, training loss:0.4779 validation loss:0.2099
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.2137127396735278 0.2179513472047719
2023-07-25 15:31:20,442 - epoch:6, training loss:0.4546 validation loss:0.2137
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.21506977250630205 0.22004298188469626
2023-07-25 15:31:37,286 - epoch:7, training loss:0.4358 validation loss:0.2151
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.21278889748183163 0.223310090940107
2023-07-25 15:31:54,294 - epoch:8, training loss:0.4188 validation loss:0.2128
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.21508167751810767 0.20743740620938214
2023-07-25 15:32:11,533 - epoch:9, training loss:0.4072 validation loss:0.2151
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.21324199946089226 0.20897640762003986
2023-07-25 15:32:28,804 - epoch:10, training loss:0.4655 validation loss:0.2132
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.21712420169602742 0.20770548182454976
2023-07-25 15:32:45,892 - epoch:11, training loss:0.4473 validation loss:0.2171
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.21865567327900368 0.20547299933704463
2023-07-25 15:33:03,307 - epoch:12, training loss:0.4368 validation loss:0.2187
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.22147717428478328 0.20574777878143571
2023-07-25 15:33:20,378 - epoch:13, training loss:0.4294 validation loss:0.2215
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.2196501110765067 0.2079154330898415
2023-07-25 15:33:37,376 - epoch:14, training loss:0.4231 validation loss:0.2197
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.22175406258214603 0.2062448855828155
2023-07-25 15:33:54,578 - epoch:15, training loss:0.4181 validation loss:0.2218
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.21727483888918703 0.20709437436678194
2023-07-25 15:34:11,495 - epoch:16, training loss:0.4141 validation loss:0.2173
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.22107849168506535 0.2066093168475411
2023-07-25 15:34:28,587 - epoch:17, training loss:0.4096 validation loss:0.2211
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.22186230834234844 0.2098672210492871
2023-07-25 15:34:45,977 - epoch:18, training loss:0.4042 validation loss:0.2219
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.22331651774319736 0.20970662513917143
2023-07-25 15:35:03,745 - epoch:19, training loss:0.4020 validation loss:0.2233
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.21987350251186977 0.21010953695936638
2023-07-25 15:35:47,435 - epoch:20, training loss:0.3994 validation loss:0.2199
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.2206442511894486 0.20771770958196034
2023-07-25 15:36:12,644 - epoch:21, training loss:0.3974 validation loss:0.2206
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.21997288479046387 0.20875762538476425
2023-07-25 15:36:51,763 - epoch:22, training loss:0.3946 validation loss:0.2200
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.22012510760263962 0.2081268856471235
2023-07-25 15:37:32,413 - epoch:23, training loss:0.3941 validation loss:0.2201
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.21843394027514892 0.2084086605093696
2023-07-25 15:38:12,246 - epoch:24, training loss:0.3917 validation loss:0.2184
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.21993018009445883 0.2080488293008371
2023-07-25 15:38:52,946 - epoch:25, training loss:0.3917 validation loss:0.2199
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.2193807881664146 0.20815588466145776
2023-07-25 15:39:34,736 - epoch:26, training loss:0.3921 validation loss:0.2194
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.21979928118261424 0.20821342414075678
2023-07-25 15:40:15,465 - epoch:27, training loss:0.3900 validation loss:0.2198
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.21964362263679504 0.20840774408795618
2023-07-25 15:40:57,168 - epoch:28, training loss:0.3902 validation loss:0.2196
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.21968379786068742 0.20845132964578542
2023-07-25 15:41:38,388 - epoch:29, training loss:0.3899 validation loss:0.2197
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-07-25-15:29:23.811124/0/0.2099_epoch_5.pkl  &  0.20547299933704463
2023-07-25 15:41:42,974 - [*] loss:0.2886
2023-07-25 15:41:42,978 - [*] phase 0, testing
2023-07-25 15:41:43,018 - T:96	MAE	0.345851	RMSE	0.288659	MAPE	138.208532
2023-07-25 15:41:43,020 - 96	mae	0.3459	
2023-07-25 15:41:43,020 - 96	rmse	0.2887	
2023-07-25 15:41:43,020 - 96	mape	138.2085	
2023-07-25 15:41:47,647 - [*] loss:0.2900
2023-07-25 15:41:47,650 - [*] phase 0, testing
2023-07-25 15:41:47,689 - T:96	MAE	0.343159	RMSE	0.290285	MAPE	138.360810
2023-07-25 15:41:47,690 - 96	mae	0.3432	
2023-07-25 15:41:47,690 - 96	rmse	0.2903	
2023-07-25 15:41:47,690 - 96	mape	138.3608	
