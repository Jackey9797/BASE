2023-07-25 05:21:53,293 - logger name:exp/ECL-PatchTST2023-07-25-05:21:53.292989/ECL-PatchTST.log
2023-07-25 05:21:53,293 - params : {'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'grad_norm': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.1, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'refiner': 1, 'enhance': 0, 'seed': 2023, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'batch_size': 128, 'lr': 0.0001, 'loss': 'mse', '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-07-25-05:21:53.292989', 'path': 'exp/ECL-PatchTST2023-07-25-05:21:53.292989', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-07-25 05:21:53,294 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-07-25 05:21:53,514 - [*] phase 0 Dataset load!
2023-07-25 05:21:54,620 - [*] phase 0 Training start
train 8209
2023-07-25 05:22:25,530 - epoch:0, training loss:0.6271 validation loss:0.2960
train 8209
vs, vt 0.2960200756788254 0.2961532703854821
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.24034825576977295 0.24800641563805667
2023-07-25 05:23:22,308 - epoch:1, training loss:0.7205 validation loss:0.2403
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.21824759888378056 0.22276111929254097
2023-07-25 05:24:02,330 - epoch:2, training loss:0.6236 validation loss:0.2182
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.21351343020796776 0.21577812765132298
2023-07-25 05:24:46,855 - epoch:3, training loss:0.5717 validation loss:0.2135
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.21114866062998772 0.21860459582372146
2023-07-25 05:25:30,729 - epoch:4, training loss:0.5432 validation loss:0.2111
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.20933366363698785 0.22071925110437654
2023-07-25 05:26:12,956 - epoch:5, training loss:0.5248 validation loss:0.2093
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.21235340291803534 0.21883471377871253
2023-07-25 05:26:56,924 - epoch:6, training loss:0.5032 validation loss:0.2124
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.21521543914621527 0.21761612119999799
2023-07-25 05:27:40,642 - epoch:7, training loss:0.4805 validation loss:0.2152
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.21835159408775243 0.2096088674258102
2023-07-25 05:28:21,076 - epoch:8, training loss:0.4646 validation loss:0.2184
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.22183563966642728 0.2078924375501546
2023-07-25 05:29:05,076 - epoch:9, training loss:0.4527 validation loss:0.2218
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.2196145264262503 0.20960059389472008
2023-07-25 05:29:50,181 - epoch:10, training loss:0.4401 validation loss:0.2196
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.21655608239499005 0.20973886752670462
2023-07-25 05:30:34,651 - epoch:11, training loss:0.4279 validation loss:0.2166
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.21896107731895012 0.21109780296683311
2023-07-25 05:31:16,406 - epoch:12, training loss:0.4198 validation loss:0.2190
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.21889642118053002 0.20901411974971945
2023-07-25 05:32:00,225 - epoch:13, training loss:0.4107 validation loss:0.2189
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.22053458744829352 0.21011315489357169
2023-07-25 05:32:43,131 - epoch:14, training loss:0.4058 validation loss:0.2205
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.21946797181259503 0.2128034190020778
2023-07-25 05:33:23,171 - epoch:15, training loss:0.4020 validation loss:0.2195
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.21987244148146023 0.20976513759656387
2023-07-25 05:34:05,809 - epoch:16, training loss:0.3973 validation loss:0.2199
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.21781995181332936 0.21097571165724235
2023-07-25 05:34:48,906 - epoch:17, training loss:0.3918 validation loss:0.2178
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.21825869957154448 0.20893888988278128
2023-07-25 05:35:41,641 - epoch:18, training loss:0.3884 validation loss:0.2183
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.22135198251767593 0.20900311693549156
2023-07-25 05:36:26,565 - epoch:19, training loss:0.3849 validation loss:0.2214
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.21912405504421753 0.21220522170717065
2023-07-25 05:37:09,623 - epoch:20, training loss:0.3855 validation loss:0.2191
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.21960856528444725 0.20958870649337769
2023-07-25 05:37:58,779 - epoch:21, training loss:0.3834 validation loss:0.2196
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.219577856361866 0.21038974144242026
2023-07-25 05:38:40,956 - epoch:22, training loss:0.3799 validation loss:0.2196
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.2195109443908388 0.21029995652762326
2023-07-25 05:39:25,109 - epoch:23, training loss:0.3793 validation loss:0.2195
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.21872488070618024 0.20891116153110156
2023-07-25 05:40:08,880 - epoch:24, training loss:0.3783 validation loss:0.2187
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.21876088428226384 0.21003181419589304
2023-07-25 05:40:50,078 - epoch:25, training loss:0.3761 validation loss:0.2188
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.21879195015538822 0.21021254326809535
2023-07-25 05:41:32,777 - epoch:26, training loss:0.3771 validation loss:0.2188
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.2186089876023206 0.20950362221761185
2023-07-25 05:42:17,368 - epoch:27, training loss:0.3755 validation loss:0.2186
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.21869263154539195 0.2100259983404116
2023-07-25 05:42:58,725 - epoch:28, training loss:0.3748 validation loss:0.2187
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.21864888105880131 0.21001377735625615
2023-07-25 05:43:39,770 - epoch:29, training loss:0.3760 validation loss:0.2186
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
2023-07-25 05:43:44,112 - [*] loss:0.2898
2023-07-25 05:43:44,117 - [*] phase 0, testing
2023-07-25 05:43:44,158 - T:96	MAE	0.347439	RMSE	0.289647	MAPE	140.549338
2023-07-25 05:43:44,159 - 96	mae	0.3474	
2023-07-25 05:43:44,160 - 96	rmse	0.2896	
2023-07-25 05:43:44,160 - 96	mape	140.5493	
2023-07-25 05:43:49,635 - [*] loss:0.2907
2023-07-25 05:43:49,638 - [*] phase 0, testing
2023-07-25 05:43:49,678 - T:96	MAE	0.344528	RMSE	0.290983	MAPE	138.032806
2023-07-25 05:43:49,679 - 96	mae	0.3445	
2023-07-25 05:43:49,679 - 96	rmse	0.2910	
2023-07-25 05:43:49,679 - 96	mape	138.0328	
