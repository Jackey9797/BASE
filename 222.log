2023-09-04 01:55:19,746 - logger name:exp/ECL-PatchTST2023-09-04-01:55:19.746412/ECL-PatchTST.log
2023-09-04 01:55:19,747 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'abl_tmp_context': 1, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-04-01:55:19.746412', 'path': 'exp/ECL-PatchTST2023-09-04-01:55:19.746412', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 01:55:19,747 - [*] phase 0 start training
0 69680
train 34129
val 11425
test 11425
2023-09-04 01:55:20,534 - [*] phase 0 Dataset load!
2023-09-04 01:55:21,514 - [*] phase 0 Training start
train 34129
2023-09-04 01:56:57,798 - epoch:0, training loss:0.1882 validation loss:0.1820
train 34129
vs, vt 0.18196401625043815 0.18696492351591587
Updating learning rate to 1.0438661460315324e-05
Updating learning rate to 1.0438661460315324e-05
train 34129
vs, vt 0.1676863262222873 0.16719366245799594
need align? ->  True 0.16719366245799594
2023-09-04 02:01:10,595 - epoch:1, training loss:11.1585 validation loss:0.1677
Updating learning rate to 2.8027297449571736e-05
Updating learning rate to 2.8027297449571736e-05
train 34129
vs, vt 0.16761041093203757 0.1660238788773616
need align? ->  True 0.1660238788773616
2023-09-04 02:04:20,172 - epoch:2, training loss:6.5976 validation loss:0.1676
Updating learning rate to 5.204727160595503e-05
Updating learning rate to 5.204727160595503e-05
train 34129
vs, vt 0.1685772467404604 0.1716311715957191
need align? ->  True 0.1660238788773616
2023-09-04 02:07:29,843 - epoch:3, training loss:2.4336 validation loss:0.1686
Updating learning rate to 7.605456385119542e-05
Updating learning rate to 7.605456385119542e-05
train 34129
vs, vt 0.1623684476647112 0.1669574427521891
need align? ->  False 0.1660238788773616
2023-09-04 02:10:39,356 - epoch:4, training loss:1.6778 validation loss:0.1624
Updating learning rate to 9.360855637921138e-05
Updating learning rate to 9.360855637921138e-05
train 34129
vs, vt 0.16480485391285685 0.1651841964572668
need align? ->  False 0.1651841964572668
2023-09-04 02:13:47,142 - epoch:5, training loss:1.4983 validation loss:0.1648
Updating learning rate to 9.99999939458629e-05
Updating learning rate to 9.99999939458629e-05
train 34129
vs, vt 0.1639927579710881 0.16258331139882407
need align? ->  True 0.16258331139882407
2023-09-04 02:16:54,565 - epoch:6, training loss:1.2652 validation loss:0.1640
Updating learning rate to 9.956902716655286e-05
Updating learning rate to 9.956902716655286e-05
train 34129
vs, vt 0.15810138628714615 0.15826666833211978
need align? ->  False 0.15826666833211978
2023-09-04 02:20:00,561 - epoch:7, training loss:1.1158 validation loss:0.1581
Updating learning rate to 9.828992401134782e-05
Updating learning rate to 9.828992401134782e-05
train 34129
vs, vt 0.15909049780004555 0.15920749993787872
need align? ->  True 0.15826666833211978
2023-09-04 02:23:06,562 - epoch:8, training loss:1.0618 validation loss:0.1591
Updating learning rate to 9.618457028986775e-05
Updating learning rate to 9.618457028986775e-05
train 34129
vs, vt 0.15830254939695199 0.1577066842466593
need align? ->  True 0.1577066842466593
2023-09-04 02:26:12,451 - epoch:9, training loss:0.9743 validation loss:0.1583
Updating learning rate to 9.32889891880015e-05
Updating learning rate to 9.32889891880015e-05
train 34129
vs, vt 0.15858047650092177 0.15822368365608983
need align? ->  True 0.1577066842466593
2023-09-04 02:29:18,101 - epoch:10, training loss:0.9927 validation loss:0.1586
Updating learning rate to 8.965272490120875e-05
Updating learning rate to 8.965272490120875e-05
train 34129
vs, vt 0.15702530108392238 0.1567795037602385
need align? ->  True 0.1567795037602385
2023-09-04 02:32:24,037 - epoch:11, training loss:0.9406 validation loss:0.1570
Updating learning rate to 8.533799491959945e-05
Updating learning rate to 8.533799491959945e-05
train 34129
vs, vt 0.1563206669771009 0.15535698864195083
need align? ->  True 0.15535698864195083
2023-09-04 02:35:31,777 - epoch:12, training loss:0.9461 validation loss:0.1563
Updating learning rate to 8.041862546942808e-05
Updating learning rate to 8.041862546942808e-05
train 34129
vs, vt 0.15629835774501163 0.1548796746051974
need align? ->  True 0.1548796746051974
2023-09-04 02:38:37,083 - epoch:13, training loss:0.9470 validation loss:0.1563
Updating learning rate to 7.497878832589398e-05
Updating learning rate to 7.497878832589398e-05
train 34129
vs, vt 0.15525386105808947 0.1540991488016314
need align? ->  True 0.1540991488016314
2023-09-04 02:41:44,504 - epoch:14, training loss:0.9408 validation loss:0.1553
Updating learning rate to 6.911156061073078e-05
Updating learning rate to 6.911156061073078e-05
train 34129
vs, vt 0.15608183323509164 0.15422027309735617
need align? ->  True 0.1540991488016314
2023-09-04 02:44:51,614 - epoch:15, training loss:0.9378 validation loss:0.1561
Updating learning rate to 6.291733221684778e-05
Updating learning rate to 6.291733221684778e-05
train 34129
vs, vt 0.1549798321806722 0.1530921424428622
need align? ->  True 0.1530921424428622
2023-09-04 02:47:58,585 - epoch:16, training loss:0.9198 validation loss:0.1550
Updating learning rate to 5.650208810942889e-05
Updating learning rate to 5.650208810942889e-05
train 34129
vs, vt 0.1555996387783024 0.15355076959563627
need align? ->  True 0.1530921424428622
2023-09-04 02:51:08,016 - epoch:17, training loss:0.9462 validation loss:0.1556
Updating learning rate to 4.9975594893793686e-05
Updating learning rate to 4.9975594893793686e-05
train 34129
vs, vt 0.1545007288042042 0.15439894042081304
need align? ->  True 0.1530921424428622
2023-09-04 02:54:16,713 - epoch:18, training loss:0.9295 validation loss:0.1545
Updating learning rate to 4.3449522678347527e-05
Updating learning rate to 4.3449522678347527e-05
train 34129
vs, vt 0.15488784768515163 0.15207666295270125
need align? ->  True 0.15207666295270125
2023-09-04 02:57:28,231 - epoch:19, training loss:0.9173 validation loss:0.1549
Updating learning rate to 3.7035534368065714e-05
Updating learning rate to 3.7035534368065714e-05
train 34129
vs, vt 0.15415059617824026 0.15204048073954052
need align? ->  True 0.15204048073954052
2023-09-04 03:00:41,382 - epoch:20, training loss:0.9453 validation loss:0.1542
Updating learning rate to 3.084337508123067e-05
Updating learning rate to 3.084337508123067e-05
train 34129
vs, vt 0.15428752274148994 0.15292624458670617
need align? ->  True 0.15204048073954052
2023-09-04 03:03:54,126 - epoch:21, training loss:0.9523 validation loss:0.1543
Updating learning rate to 2.497899438003108e-05
Updating learning rate to 2.497899438003108e-05
train 34129
vs, vt 0.15391831791235341 0.15215450757079654
need align? ->  True 0.15204048073954052
2023-09-04 03:07:05,873 - epoch:22, training loss:0.9445 validation loss:0.1539
Updating learning rate to 1.9542733444177923e-05
Updating learning rate to 1.9542733444177923e-05
train 34129
vs, vt 0.1542401634156704 0.1521846362286144
need align? ->  True 0.15204048073954052
2023-09-04 03:10:16,447 - epoch:23, training loss:0.9383 validation loss:0.1542
Updating learning rate to 1.4627608205499963e-05
Updating learning rate to 1.4627608205499963e-05
train 34129
vs, vt 0.15383968485726252 0.15192776665919358
need align? ->  True 0.15192776665919358
2023-09-04 03:13:30,158 - epoch:24, training loss:0.9334 validation loss:0.1538
Updating learning rate to 1.0317717819561129e-05
Updating learning rate to 1.0317717819561129e-05
train 34129
vs, vt 0.15458932080202634 0.15184656675491068
need align? ->  True 0.15184656675491068
2023-09-04 03:16:44,283 - epoch:25, training loss:0.9560 validation loss:0.1546
Updating learning rate to 6.686805705792195e-06
Updating learning rate to 6.686805705792195e-06
train 34129
vs, vt 0.15390575979318882 0.15217368565499784
need align? ->  True 0.15184656675491068
2023-09-04 03:19:57,259 - epoch:26, training loss:0.9579 validation loss:0.1539
Updating learning rate to 3.79699777713878e-06
Updating learning rate to 3.79699777713878e-06
train 34129
vs, vt 0.1536541918085681 0.15191623373991914
need align? ->  True 0.15184656675491068
2023-09-04 03:23:08,578 - epoch:27, training loss:0.9565 validation loss:0.1537
Updating learning rate to 1.6977394484662593e-06
Updating learning rate to 1.6977394484662593e-06
train 34129
vs, vt 0.1538107090526157 0.15187745214336448
need align? ->  True 0.15184656675491068
2023-09-04 03:26:18,734 - epoch:28, training loss:0.9553 validation loss:0.1538
Updating learning rate to 4.2494961180259127e-07
Updating learning rate to 4.2494961180259127e-07
train 34129
vs, vt 0.15384378934072124 0.15193043847878773
need align? ->  True 0.15184656675491068
2023-09-04 03:29:28,308 - epoch:29, training loss:0.9554 validation loss:0.1538
Updating learning rate to 4.0605413710007e-10
Updating learning rate to 4.0605413710007e-10
check exp/ECL-PatchTST2023-09-04-01:55:19.746412/0/0.1537_epoch_27.pkl  &  0.15184656675491068
2023-09-04 03:30:01,881 - [*] loss:0.2887
2023-09-04 03:30:01,892 - [*] phase 0, testing
2023-09-04 03:30:02,076 - T:96	MAE	0.333791	RMSE	0.290243	MAPE	205.822372
2023-09-04 03:30:02,077 - 96	mae	0.3338	
2023-09-04 03:30:02,077 - 96	rmse	0.2902	
2023-09-04 03:30:02,077 - 96	mape	205.8224	
----*-----
2023-09-04 03:30:30,201 - [*] loss:0.2887
2023-09-04 03:30:30,211 - [*] phase 0, testing
2023-09-04 03:30:30,390 - T:96	MAE	0.333791	RMSE	0.290243	MAPE	205.822372
2023-09-04 03:30:55,607 - [*] loss:0.3060
2023-09-04 03:30:55,617 - [*] phase 0, testing
2023-09-04 03:30:55,796 - T:96	MAE	0.358709	RMSE	0.307672	MAPE	224.408722
2023-09-04 03:31:18,655 - [*] loss:0.3961
2023-09-04 03:31:18,681 - [*] phase 0, testing
2023-09-04 03:31:18,854 - T:96	MAE	0.411171	RMSE	0.398378	MAPE	264.134407
2023-09-04 03:31:45,184 - [*] loss:0.2946
2023-09-04 03:31:45,194 - [*] phase 0, testing
2023-09-04 03:31:45,370 - T:96	MAE	0.342153	RMSE	0.296379	MAPE	224.904823
2023-09-04 03:32:15,347 - [*] loss:0.3557
2023-09-04 03:32:15,356 - [*] phase 0, testing
2023-09-04 03:32:15,535 - T:96	MAE	0.394254	RMSE	0.357514	MAPE	224.974799
2023-09-04 03:32:47,014 - [*] loss:0.3365
2023-09-04 03:32:47,025 - [*] phase 0, testing
2023-09-04 03:32:47,213 - T:96	MAE	0.373687	RMSE	0.338121	MAPE	194.527972
2023-09-04 03:33:20,556 - [*] loss:0.2945
2023-09-04 03:33:20,566 - [*] phase 0, testing
2023-09-04 03:33:20,748 - T:96	MAE	0.342629	RMSE	0.296116	MAPE	212.572002
2023-09-04 03:33:54,582 - [*] loss:0.3056
2023-09-04 03:33:54,592 - [*] phase 0, testing
2023-09-04 03:33:54,775 - T:96	MAE	0.353778	RMSE	0.307216	MAPE	194.701660
----*-----
2023-09-04 03:34:16,416 - [*] loss:0.2977
2023-09-04 03:34:16,427 - [*] phase 0, testing
2023-09-04 03:34:16,605 - T:96	MAE	0.351181	RMSE	0.298093	MAPE	203.824973
2023-09-04 03:34:51,257 - [*] loss:0.3237
2023-09-04 03:34:51,267 - [*] phase 0, testing
2023-09-04 03:34:51,461 - T:96	MAE	0.363040	RMSE	0.325385	MAPE	192.191398
2023-09-04 03:35:13,446 - [*] loss:0.3001
2023-09-04 03:35:13,457 - [*] phase 0, testing
2023-09-04 03:35:13,637 - T:96	MAE	0.346079	RMSE	0.301015	MAPE	200.262713
2023-09-04 03:35:13,641 - 96	mae	0.3461	
2023-09-04 03:35:13,641 - 96	rmse	0.3010	
2023-09-04 03:35:13,641 - 96	mape	200.2627	
2023-09-04 03:35:16,027 - logger name:exp/ECL-PatchTST2023-09-04-03:35:16.027129/ECL-PatchTST.log
2023-09-04 03:35:16,028 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'abl_tmp_context': 1, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-04-03:35:16.027129', 'path': 'exp/ECL-PatchTST2023-09-04-03:35:16.027129', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 03:35:16,028 - [*] phase 0 start training
0 69680
train 33889
val 11185
test 11185
2023-09-04 03:35:17,045 - [*] phase 0 Dataset load!
2023-09-04 03:35:18,121 - [*] phase 0 Training start
train 33889
2023-09-04 03:37:01,007 - epoch:0, training loss:0.2104 validation loss:0.2622
train 33889
vs, vt 0.2622243366627531 0.26576502138579433
Updating learning rate to 1.0438721218484657e-05
Updating learning rate to 1.0438721218484657e-05
train 33889
vs, vt 0.2549857848577879 0.2529703245752237
need align? ->  True 0.2529703245752237
2023-09-04 03:41:19,089 - epoch:1, training loss:11.0585 validation loss:0.2550
Updating learning rate to 2.802750441854847e-05
Updating learning rate to 2.802750441854847e-05
train 33889
vs, vt 0.25453604563054716 0.2532481748864732
need align? ->  True 0.2529703245752237
2023-09-04 03:44:22,911 - epoch:2, training loss:6.4373 validation loss:0.2545
Updating learning rate to 5.2047629950292354e-05
Updating learning rate to 5.2047629950292354e-05
train 33889
vs, vt 0.2519050756977363 0.2524407435716553
need align? ->  False 0.2524407435716553
2023-09-04 03:47:30,381 - epoch:3, training loss:2.6668 validation loss:0.2519
Updating learning rate to 7.605497731655361e-05
Updating learning rate to 7.605497731655361e-05
train 33889
vs, vt 0.25238294315270404 0.25030282050879166
need align? ->  True 0.25030282050879166
2023-09-04 03:50:38,748 - epoch:4, training loss:1.6179 validation loss:0.2524
Updating learning rate to 9.3608854147054e-05
Updating learning rate to 9.3608854147054e-05
train 33889
vs, vt 0.24854181067679415 0.25235449336469173
need align? ->  False 0.25030282050879166
2023-09-04 03:53:47,019 - epoch:5, training loss:1.4439 validation loss:0.2485
Updating learning rate to 9.99999938537861e-05
Updating learning rate to 9.99999938537861e-05
train 33889
vs, vt 0.2498086547817696 0.25106965183195745
need align? ->  False 0.25030282050879166
2023-09-04 03:56:54,787 - epoch:6, training loss:1.2595 validation loss:0.2498
Updating learning rate to 9.956900274488073e-05
Updating learning rate to 9.956900274488073e-05
train 33889
vs, vt 0.24733901989053597 0.2539057642729445
need align? ->  False 0.25030282050879166
2023-09-04 04:00:02,266 - epoch:7, training loss:1.1185 validation loss:0.2473
Updating learning rate to 9.828987567794199e-05
Updating learning rate to 9.828987567794199e-05
train 33889
vs, vt 0.24646477032960815 0.25040215791457077
need align? ->  False 0.25030282050879166
2023-09-04 04:03:18,515 - epoch:8, training loss:1.0404 validation loss:0.2465
Updating learning rate to 9.618449887172618e-05
Updating learning rate to 9.618449887172618e-05
train 33889
vs, vt 0.2482094097120518 0.2502981562760066
need align? ->  False 0.2502981562760066
2023-09-04 04:06:40,647 - epoch:9, training loss:0.9944 validation loss:0.2482
Updating learning rate to 9.328889590710838e-05
Updating learning rate to 9.328889590710838e-05
train 33889
vs, vt 0.24667560956864196 0.25241710829802533
need align? ->  False 0.2502981562760066
2023-09-04 04:09:49,634 - epoch:10, training loss:1.1574 validation loss:0.2467
Updating learning rate to 8.965261135362604e-05
Updating learning rate to 8.965261135362604e-05
train 33889
vs, vt 0.24565302237698977 0.25275905785912817
need align? ->  False 0.2502981562760066
2023-09-04 04:12:59,975 - epoch:11, training loss:1.0703 validation loss:0.2457
Updating learning rate to 8.533786304815776e-05
Updating learning rate to 8.533786304815776e-05
train 33889
vs, vt 0.24685268391939727 0.2524403990669684
need align? ->  False 0.2502981562760066
2023-09-04 04:16:12,223 - epoch:12, training loss:1.0151 validation loss:0.2469
Updating learning rate to 8.041847753048434e-05
Updating learning rate to 8.041847753048434e-05
train 33889
vs, vt 0.24608590339564465 0.25151174583218316
need align? ->  False 0.2502981562760066
2023-09-04 04:19:23,802 - epoch:13, training loss:0.9711 validation loss:0.2461
Updating learning rate to 7.497862685072454e-05
Updating learning rate to 7.497862685072454e-05
train 33889
vs, vt 0.24391505757177417 0.2529942236248065
need align? ->  False 0.2502981562760066
2023-09-04 04:22:32,286 - epoch:14, training loss:0.9435 validation loss:0.2439
Updating learning rate to 6.911138836222055e-05
Updating learning rate to 6.911138836222055e-05
train 33889
vs, vt 0.2461558733643456 0.24953176944770597
need align? ->  False 0.24953176944770597
2023-09-04 04:25:44,779 - epoch:15, training loss:0.9252 validation loss:0.2462
Updating learning rate to 6.291715214221653e-05
Updating learning rate to 6.291715214221653e-05
train 33889
vs, vt 0.24975463172251527 0.25155539031733165
need align? ->  True 0.24953176944770597
2023-09-04 04:28:57,901 - epoch:16, training loss:1.1467 validation loss:0.2498
Updating learning rate to 5.6501903289803477e-05
Updating learning rate to 5.6501903289803477e-05
train 33889
vs, vt 0.24658907687460835 0.2522528267634863
need align? ->  False 0.24953176944770597
2023-09-04 04:32:09,727 - epoch:17, training loss:1.1035 validation loss:0.2466
Updating learning rate to 4.997540849148917e-05
Updating learning rate to 4.997540849148917e-05
train 33889
vs, vt 0.24743528321216052 0.251217332795601
need align? ->  False 0.24953176944770597
2023-09-04 04:35:24,121 - epoch:18, training loss:1.0826 validation loss:0.2474
Updating learning rate to 4.344933788275899e-05
Updating learning rate to 4.344933788275899e-05
train 33889
vs, vt 0.2480671976015649 0.25095352809876204
need align? ->  False 0.24953176944770597
2023-09-04 04:38:37,361 - epoch:19, training loss:1.0634 validation loss:0.2481
Updating learning rate to 3.703535434109692e-05
Updating learning rate to 3.703535434109692e-05
train 33889
vs, vt 0.24695030616765673 0.2520067489875311
need align? ->  False 0.24953176944770597
2023-09-04 04:41:50,702 - epoch:20, training loss:1.0482 validation loss:0.2470
Updating learning rate to 3.084320290319299e-05
Updating learning rate to 3.084320290319299e-05
train 33889
vs, vt 0.24766708821566266 0.2513613356700675
need align? ->  False 0.24953176944770597
2023-09-04 04:45:04,679 - epoch:21, training loss:1.0362 validation loss:0.2477
Updating learning rate to 2.4978832996938436e-05
Updating learning rate to 2.4978832996938436e-05
train 33889
vs, vt 0.2472439769401469 0.2516478524035351
need align? ->  False 0.24953176944770597
2023-09-04 04:48:20,224 - epoch:22, training loss:1.0289 validation loss:0.2472
Updating learning rate to 1.954258561733979e-05
Updating learning rate to 1.954258561733979e-05
train 33889
vs, vt 0.24842838320711796 0.25111483376134525
need align? ->  False 0.24953176944770597
2023-09-04 04:51:33,485 - epoch:23, training loss:1.0216 validation loss:0.2484
Updating learning rate to 1.4627476464274535e-05
Updating learning rate to 1.4627476464274535e-05
train 33889
vs, vt 0.24759405914863403 0.2514966877854683
need align? ->  False 0.24953176944770597
2023-09-04 04:54:46,040 - epoch:24, training loss:1.0157 validation loss:0.2476
Updating learning rate to 1.0317604418077296e-05
Updating learning rate to 1.0317604418077296e-05
train 33889
vs, vt 0.24765091512182896 0.25119028287008405
need align? ->  False 0.24953176944770597
2023-09-04 04:57:57,019 - epoch:25, training loss:1.0145 validation loss:0.2477
check exp/ECL-PatchTST2023-09-04-03:35:16.027129/0/0.2439_epoch_14.pkl  &  0.24953176944770597
2023-09-04 04:58:28,639 - [*] loss:0.3777
2023-09-04 04:58:28,828 - [*] phase 0, testing
2023-09-04 04:58:32,514 - T:336	MAE	0.385353	RMSE	0.377524	MAPE	227.003312
2023-09-04 04:58:32,516 - 336	mae	0.3854	
2023-09-04 04:58:32,516 - 336	rmse	0.3775	
2023-09-04 04:58:32,516 - 336	mape	227.0033	
----*-----
2023-09-04 04:58:56,832 - [*] loss:0.3777
2023-09-04 04:58:56,873 - [*] phase 0, testing
2023-09-04 04:59:00,703 - T:336	MAE	0.385353	RMSE	0.377524	MAPE	227.003312
2023-09-04 04:59:26,415 - [*] loss:0.3872
2023-09-04 04:59:26,459 - [*] phase 0, testing
2023-09-04 04:59:29,685 - T:336	MAE	0.398575	RMSE	0.387010	MAPE	238.173985
2023-09-04 04:59:55,471 - [*] loss:0.4196
2023-09-04 04:59:55,513 - [*] phase 0, testing
2023-09-04 04:59:58,218 - T:336	MAE	0.418793	RMSE	0.419607	MAPE	264.563894
2023-09-04 05:00:25,052 - [*] loss:0.3842
2023-09-04 05:00:25,096 - [*] phase 0, testing
2023-09-04 05:00:27,348 - T:336	MAE	0.392234	RMSE	0.384033	MAPE	241.873479
2023-09-04 05:01:03,829 - [*] loss:0.4441
2023-09-04 05:01:03,886 - [*] phase 0, testing
2023-09-04 05:01:06,091 - T:336	MAE	0.444325	RMSE	0.443879	MAPE	239.810109
2023-09-04 05:01:39,235 - [*] loss:0.4257
2023-09-04 05:01:39,279 - [*] phase 0, testing
2023-09-04 05:01:41,397 - T:336	MAE	0.422714	RMSE	0.425452	MAPE	205.046177
2023-09-04 05:02:10,099 - [*] loss:0.3797
2023-09-04 05:02:10,143 - [*] phase 0, testing
2023-09-04 05:02:12,373 - T:336	MAE	0.389022	RMSE	0.379551	MAPE	230.622196
2023-09-04 05:02:37,701 - [*] loss:0.3838
2023-09-04 05:02:37,744 - [*] phase 0, testing
2023-09-04 05:02:39,424 - T:336	MAE	0.394434	RMSE	0.383469	MAPE	215.045261
----*-----
2023-09-04 05:02:57,065 - [*] loss:0.3750
2023-09-04 05:02:57,108 - [*] phase 0, testing
2023-09-04 05:02:58,730 - T:336	MAE	0.392573	RMSE	0.374671	MAPE	222.145629
2023-09-04 05:03:24,057 - [*] loss:0.4282
2023-09-04 05:03:24,118 - [*] phase 0, testing
2023-09-04 05:03:25,577 - T:336	MAE	0.430521	RMSE	0.428279	MAPE	236.073589
2023-09-04 05:03:42,914 - [*] loss:0.3940
2023-09-04 05:03:42,954 - [*] phase 0, testing
2023-09-04 05:03:44,323 - T:336	MAE	0.398027	RMSE	0.393908	MAPE	225.542569
2023-09-04 05:03:44,324 - 336	mae	0.3980	
2023-09-04 05:03:44,324 - 336	rmse	0.3939	
2023-09-04 05:03:44,324 - 336	mape	225.5426	
2023-09-04 05:03:46,730 - logger name:exp/ECL-PatchTST2023-09-04-05:03:46.730584/ECL-PatchTST.log
2023-09-04 05:03:46,731 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'abl_tmp_context': 1, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-04-05:03:46.730584', 'path': 'exp/ECL-PatchTST2023-09-04-05:03:46.730584', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 05:03:46,731 - [*] phase 0 start training
0 69680
train 33889
val 11185
test 11185
2023-09-04 05:03:47,601 - [*] phase 0 Dataset load!
2023-09-04 05:03:48,748 - [*] phase 0 Training start
train 33889
2023-09-04 05:05:28,138 - epoch:0, training loss:0.2079 validation loss:0.2602
train 33889
vs, vt 0.2602407373064621 0.26377567885951564
Updating learning rate to 1.0438721218484657e-05
Updating learning rate to 1.0438721218484657e-05
train 33889
vs, vt 0.25504728258502757 0.25309385545551777
need align? ->  True 0.25309385545551777
2023-09-04 05:09:28,330 - epoch:1, training loss:10.9151 validation loss:0.2550
Updating learning rate to 2.802750441854847e-05
Updating learning rate to 2.802750441854847e-05
train 33889
vs, vt 0.2541488856077194 0.2527263830839233
need align? ->  True 0.2527263830839233
2023-09-04 05:12:37,873 - epoch:2, training loss:6.0563 validation loss:0.2541
Updating learning rate to 5.2047629950292354e-05
Updating learning rate to 5.2047629950292354e-05
train 33889
vs, vt 0.25203961828215554 0.25200167048553174
need align? ->  True 0.25200167048553174
2023-09-04 05:15:56,452 - epoch:3, training loss:2.3118 validation loss:0.2520
Updating learning rate to 7.605497731655361e-05
Updating learning rate to 7.605497731655361e-05
train 33889
vs, vt 0.2524204134348441 0.25154951392588293
need align? ->  True 0.25154951392588293
2023-09-04 05:19:28,869 - epoch:4, training loss:1.5797 validation loss:0.2524
Updating learning rate to 9.3608854147054e-05
Updating learning rate to 9.3608854147054e-05
train 33889
vs, vt 0.24781369282440704 0.2512386834079569
need align? ->  False 0.2512386834079569
2023-09-04 05:22:49,059 - epoch:5, training loss:1.4046 validation loss:0.2478
Updating learning rate to 9.99999938537861e-05
Updating learning rate to 9.99999938537861e-05
train 33889
vs, vt 0.24675686132501473 0.24892179274254225
need align? ->  False 0.24892179274254225
2023-09-04 05:25:56,415 - epoch:6, training loss:1.2500 validation loss:0.2468
Updating learning rate to 9.956900274488073e-05
Updating learning rate to 9.956900274488073e-05
train 33889
vs, vt 0.24668541394004767 0.24898072683506392
need align? ->  False 0.24892179274254225
2023-09-04 05:29:05,867 - epoch:7, training loss:1.1207 validation loss:0.2467
Updating learning rate to 9.828987567794199e-05
Updating learning rate to 9.828987567794199e-05
train 33889
vs, vt 0.24533443982628259 0.2519315402870151
need align? ->  False 0.24892179274254225
2023-09-04 05:32:16,231 - epoch:8, training loss:1.0269 validation loss:0.2453
Updating learning rate to 9.618449887172618e-05
Updating learning rate to 9.618449887172618e-05
train 33889
vs, vt 0.24697842673313888 0.25140481014651334
need align? ->  False 0.24892179274254225
2023-09-04 05:35:22,024 - epoch:9, training loss:0.9766 validation loss:0.2470
Updating learning rate to 9.328889590710838e-05
Updating learning rate to 9.328889590710838e-05
train 33889
vs, vt 0.24496419143609025 0.25064112669364974
need align? ->  False 0.24892179274254225
2023-09-04 05:38:29,956 - epoch:10, training loss:0.9429 validation loss:0.2450
Updating learning rate to 8.965261135362604e-05
Updating learning rate to 8.965261135362604e-05
train 33889
vs, vt 0.245932398871942 0.2506979326294227
need align? ->  False 0.24892179274254225
2023-09-04 05:41:38,406 - epoch:11, training loss:0.9196 validation loss:0.2459
Updating learning rate to 8.533786304815776e-05
Updating learning rate to 8.533786304815776e-05
train 33889
vs, vt 0.24592167905277826 0.25140876695513725
need align? ->  False 0.24892179274254225
2023-09-04 05:44:45,975 - epoch:12, training loss:0.9006 validation loss:0.2459
Updating learning rate to 8.041847753048434e-05
Updating learning rate to 8.041847753048434e-05
train 33889
vs, vt 0.24459035559134049 0.2515507885678248
need align? ->  False 0.24892179274254225
2023-09-04 05:47:55,096 - epoch:13, training loss:0.8853 validation loss:0.2446
Updating learning rate to 7.497862685072454e-05
Updating learning rate to 7.497862685072454e-05
train 33889
vs, vt 0.24504214838485827 0.25307458952407946
need align? ->  False 0.24892179274254225
2023-09-04 05:51:03,051 - epoch:14, training loss:0.8718 validation loss:0.2450
Updating learning rate to 6.911138836222055e-05
Updating learning rate to 6.911138836222055e-05
train 33889
vs, vt 0.24518237538127738 0.2518638314425268
need align? ->  False 0.24892179274254225
2023-09-04 05:54:10,956 - epoch:15, training loss:0.8601 validation loss:0.2452
Updating learning rate to 6.291715214221653e-05
Updating learning rate to 6.291715214221653e-05
train 33889
vs, vt 0.24447486181320113 0.2528591926040297
need align? ->  False 0.24892179274254225
2023-09-04 05:57:19,068 - epoch:16, training loss:0.8510 validation loss:0.2445
Updating learning rate to 5.6501903289803477e-05
Updating learning rate to 5.6501903289803477e-05
train 33889
vs, vt 0.2435535856691951 0.251882103652778
need align? ->  False 0.24892179274254225
2023-09-04 06:00:27,932 - epoch:17, training loss:0.8434 validation loss:0.2436
Updating learning rate to 4.997540849148917e-05
Updating learning rate to 4.997540849148917e-05
train 33889
vs, vt 0.2451398547061465 0.25110571628267114
need align? ->  False 0.24892179274254225
2023-09-04 06:03:39,121 - epoch:18, training loss:0.8372 validation loss:0.2451
Updating learning rate to 4.344933788275899e-05
Updating learning rate to 4.344933788275899e-05
train 33889
vs, vt 0.24352182938971303 0.2518227358602665
need align? ->  False 0.24892179274254225
2023-09-04 06:06:48,434 - epoch:19, training loss:0.8315 validation loss:0.2435
Updating learning rate to 3.703535434109692e-05
Updating learning rate to 3.703535434109692e-05
train 33889
vs, vt 0.24315544391389599 0.25105751161886886
need align? ->  False 0.24892179274254225
2023-09-04 06:10:07,105 - epoch:20, training loss:0.8273 validation loss:0.2432
Updating learning rate to 3.084320290319299e-05
Updating learning rate to 3.084320290319299e-05
train 33889
vs, vt 0.2455838116055185 0.2508103282783519
need align? ->  False 0.24892179274254225
2023-09-04 06:13:34,770 - epoch:21, training loss:0.8240 validation loss:0.2456
Updating learning rate to 2.4978832996938436e-05
Updating learning rate to 2.4978832996938436e-05
train 33889
vs, vt 0.24363800362599167 0.2523211204785515
need align? ->  False 0.24892179274254225
2023-09-04 06:16:46,985 - epoch:22, training loss:0.8208 validation loss:0.2436
Updating learning rate to 1.954258561733979e-05
Updating learning rate to 1.954258561733979e-05
train 33889
vs, vt 0.24534030741249974 0.25226075993850827
need align? ->  False 0.24892179274254225
2023-09-04 06:19:59,296 - epoch:23, training loss:0.8187 validation loss:0.2453
Updating learning rate to 1.4627476464274535e-05
Updating learning rate to 1.4627476464274535e-05
train 33889
vs, vt 0.24433280087330125 0.2523648284206336
need align? ->  False 0.24892179274254225
2023-09-04 06:23:12,217 - epoch:24, training loss:0.8167 validation loss:0.2443
Updating learning rate to 1.0317604418077296e-05
Updating learning rate to 1.0317604418077296e-05
train 33889
vs, vt 0.24378519636494192 0.2519795121625066
need align? ->  False 0.24892179274254225
2023-09-04 06:26:23,022 - epoch:25, training loss:0.8157 validation loss:0.2438
Updating learning rate to 6.686712584380782e-06
Updating learning rate to 6.686712584380782e-06
train 33889
vs, vt 0.24304289277642965 0.25137207559733227
need align? ->  False 0.24892179274254225
2023-09-04 06:29:36,839 - epoch:26, training loss:0.8151 validation loss:0.2430
Updating learning rate to 3.7969265291329562e-06
Updating learning rate to 3.7969265291329562e-06
train 33889
vs, vt 0.24346171243285591 0.2511776056225327
need align? ->  False 0.24892179274254225
2023-09-04 06:32:51,623 - epoch:27, training loss:0.8146 validation loss:0.2435
Updating learning rate to 1.6976912929391648e-06
Updating learning rate to 1.6976912929391648e-06
train 33889
vs, vt 0.24358814408663998 0.2513928589072417
need align? ->  False 0.24892179274254225
2023-09-04 06:36:06,847 - epoch:28, training loss:0.8143 validation loss:0.2436
Updating learning rate to 4.2492537270863806e-07
Updating learning rate to 4.2492537270863806e-07
train 33889
vs, vt 0.2436417629942298 0.2513688460669734
need align? ->  False 0.24892179274254225
2023-09-04 06:39:21,257 - epoch:29, training loss:0.8139 validation loss:0.2436
Updating learning rate to 4.0614621390542476e-10
Updating learning rate to 4.0614621390542476e-10
check exp/ECL-PatchTST2023-09-04-05:03:46.730584/0/0.243_epoch_26.pkl  &  0.24892179274254225
2023-09-04 06:39:47,013 - [*] loss:0.3696
2023-09-04 06:39:47,048 - [*] phase 0, testing
2023-09-04 06:39:47,695 - T:336	MAE	0.382749	RMSE	0.369408	MAPE	227.559209
2023-09-04 06:39:47,696 - 336	mae	0.3827	
2023-09-04 06:39:47,696 - 336	rmse	0.3694	
2023-09-04 06:39:47,696 - 336	mape	227.5592	
----*-----
2023-09-04 06:40:14,345 - [*] loss:0.3696
2023-09-04 06:40:14,390 - [*] phase 0, testing
2023-09-04 06:40:15,012 - T:336	MAE	0.382749	RMSE	0.369408	MAPE	227.559209
2023-09-04 06:40:39,099 - [*] loss:0.3817
2023-09-04 06:40:39,135 - [*] phase 0, testing
2023-09-04 06:40:39,800 - T:336	MAE	0.398125	RMSE	0.381456	MAPE	243.400955
2023-09-04 06:41:07,162 - [*] loss:0.4115
2023-09-04 06:41:07,196 - [*] phase 0, testing
2023-09-04 06:41:08,011 - T:336	MAE	0.416618	RMSE	0.411308	MAPE	268.811822
2023-09-04 06:41:36,015 - [*] loss:0.3805
2023-09-04 06:41:36,051 - [*] phase 0, testing
2023-09-04 06:41:36,674 - T:336	MAE	0.394838	RMSE	0.380268	MAPE	250.151157
2023-09-04 06:42:14,250 - [*] loss:0.4338
2023-09-04 06:42:14,285 - [*] phase 0, testing
2023-09-04 06:42:14,914 - T:336	MAE	0.439529	RMSE	0.433557	MAPE	240.381885
2023-09-04 06:42:49,083 - [*] loss:0.4120
2023-09-04 06:42:49,118 - [*] phase 0, testing
2023-09-04 06:42:49,796 - T:336	MAE	0.416686	RMSE	0.411749	MAPE	205.734897
2023-09-04 06:43:17,838 - [*] loss:0.3734
2023-09-04 06:43:17,884 - [*] phase 0, testing
2023-09-04 06:43:18,466 - T:336	MAE	0.387907	RMSE	0.373221	MAPE	233.255029
2023-09-04 06:43:44,870 - [*] loss:0.3795
2023-09-04 06:43:44,904 - [*] phase 0, testing
2023-09-04 06:43:45,496 - T:336	MAE	0.394348	RMSE	0.379177	MAPE	215.213609
----*-----
2023-09-04 06:44:03,209 - [*] loss:0.3708
2023-09-04 06:44:03,243 - [*] phase 0, testing
2023-09-04 06:44:03,887 - T:336	MAE	0.392506	RMSE	0.370459	MAPE	223.255634
2023-09-04 06:44:28,056 - [*] loss:0.4316
2023-09-04 06:44:28,090 - [*] phase 0, testing
2023-09-04 06:44:28,708 - T:336	MAE	0.427663	RMSE	0.431654	MAPE	224.211645
2023-09-04 06:44:47,028 - [*] loss:0.3836
2023-09-04 06:44:47,061 - [*] phase 0, testing
2023-09-04 06:44:47,674 - T:336	MAE	0.398635	RMSE	0.383463	MAPE	223.168755
2023-09-04 06:44:47,675 - 336	mae	0.3986	
2023-09-04 06:44:47,675 - 336	rmse	0.3835	
2023-09-04 06:44:47,676 - 336	mape	223.1688	
2023-09-04 06:44:49,932 - logger name:exp/ECL-PatchTST2023-09-04-06:44:49.931578/ECL-PatchTST.log
2023-09-04 06:44:49,932 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'abl_tmp_context': 1, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 36, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-04-06:44:49.931578', 'path': 'exp/ECL-PatchTST2023-09-04-06:44:49.931578', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 06:44:49,932 - [*] phase 0 start training
0 69680
train 33889
val 11185
test 11185
2023-09-04 06:44:50,788 - [*] phase 0 Dataset load!
2023-09-04 06:44:51,883 - [*] phase 0 Training start
train 33889
2023-09-04 06:46:30,996 - epoch:0, training loss:0.2094 validation loss:0.2606
train 33889
vs, vt 0.2606062770567157 0.26400578081269155
Updating learning rate to 1.0438721218484657e-05
Updating learning rate to 1.0438721218484657e-05
train 33889
vs, vt 0.25285312571478163 0.2537594479508698
need align? ->  False 0.2537594479508698
2023-09-04 06:50:33,448 - epoch:1, training loss:10.9320 validation loss:0.2529
Updating learning rate to 2.802750441854847e-05
Updating learning rate to 2.802750441854847e-05
train 33889
vs, vt 0.25450564857402985 0.25229477404024114
need align? ->  True 0.25229477404024114
2023-09-04 06:53:42,900 - epoch:2, training loss:6.2674 validation loss:0.2545
Updating learning rate to 5.2047629950292354e-05
Updating learning rate to 5.2047629950292354e-05
train 33889
vs, vt 0.2517608216625046 0.25320261729542504
need align? ->  False 0.25229477404024114
2023-09-04 06:56:54,614 - epoch:3, training loss:2.4393 validation loss:0.2518
Updating learning rate to 7.605497731655361e-05
Updating learning rate to 7.605497731655361e-05
train 33889
vs, vt 0.25260175743394275 0.2515082702633332
need align? ->  True 0.2515082702633332
2023-09-04 07:00:14,143 - epoch:4, training loss:1.6467 validation loss:0.2526
Updating learning rate to 9.3608854147054e-05
Updating learning rate to 9.3608854147054e-05
train 33889
vs, vt 0.2520552490922538 0.2500219418751923
need align? ->  True 0.2500219418751923
2023-09-04 07:03:46,971 - epoch:5, training loss:1.4386 validation loss:0.2521
Updating learning rate to 9.99999938537861e-05
Updating learning rate to 9.99999938537861e-05
train 33889
vs, vt 0.24660881113430316 0.2519023768095808
need align? ->  False 0.2500219418751923
2023-09-04 07:07:07,381 - epoch:6, training loss:1.2872 validation loss:0.2466
Updating learning rate to 9.956900274488073e-05
Updating learning rate to 9.956900274488073e-05
train 33889
vs, vt 0.24640771111642773 0.2503049273297868
need align? ->  False 0.2500219418751923
2023-09-04 07:10:14,441 - epoch:7, training loss:1.0939 validation loss:0.2464
Updating learning rate to 9.828987567794199e-05
Updating learning rate to 9.828987567794199e-05
train 33889
vs, vt 0.24442674189975316 0.2518404405140741
need align? ->  False 0.2500219418751923
2023-09-04 07:13:24,701 - epoch:8, training loss:1.0055 validation loss:0.2444
Updating learning rate to 9.618449887172618e-05
Updating learning rate to 9.618449887172618e-05
train 33889
vs, vt 0.24770470069382677 0.2504910892298953
need align? ->  False 0.2500219418751923
2023-09-04 07:16:31,313 - epoch:9, training loss:0.9594 validation loss:0.2477
Updating learning rate to 9.328889590710838e-05
Updating learning rate to 9.328889590710838e-05
train 33889
vs, vt 0.2490900241854516 0.2533087166957557
need align? ->  False 0.2500219418751923
2023-09-04 07:19:39,527 - epoch:10, training loss:0.9286 validation loss:0.2491
Updating learning rate to 8.965261135362604e-05
Updating learning rate to 8.965261135362604e-05
train 33889
vs, vt 0.24588253336365928 0.25394050616093655
need align? ->  False 0.2500219418751923
2023-09-04 07:22:48,555 - epoch:11, training loss:0.9077 validation loss:0.2459
Updating learning rate to 8.533786304815776e-05
Updating learning rate to 8.533786304815776e-05
train 33889
vs, vt 0.24605250434780662 0.251292206262323
need align? ->  False 0.2500219418751923
2023-09-04 07:25:55,384 - epoch:12, training loss:0.8915 validation loss:0.2461
Updating learning rate to 8.041847753048434e-05
Updating learning rate to 8.041847753048434e-05
train 33889
vs, vt 0.2453518686392768 0.252369024917822
need align? ->  False 0.2500219418751923
2023-09-04 07:29:03,724 - epoch:13, training loss:0.8783 validation loss:0.2454
Updating learning rate to 7.497862685072454e-05
Updating learning rate to 7.497862685072454e-05
train 33889
vs, vt 0.24641896462575955 0.25207447027787566
need align? ->  False 0.2500219418751923
2023-09-04 07:32:11,699 - epoch:14, training loss:0.8669 validation loss:0.2464
Updating learning rate to 6.911138836222055e-05
Updating learning rate to 6.911138836222055e-05
train 33889
vs, vt 0.24553430850871585 0.2520883056673814
need align? ->  False 0.2500219418751923
2023-09-04 07:35:20,211 - epoch:15, training loss:0.8572 validation loss:0.2455
Updating learning rate to 6.291715214221653e-05
Updating learning rate to 6.291715214221653e-05
train 33889
vs, vt 0.2436217636885968 0.25282420040192927
need align? ->  False 0.2500219418751923
2023-09-04 07:38:30,830 - epoch:16, training loss:0.8488 validation loss:0.2436
Updating learning rate to 5.6501903289803477e-05
Updating learning rate to 5.6501903289803477e-05
train 33889
vs, vt 0.2464509364640848 0.25182057125493884
need align? ->  False 0.2500219418751923
2023-09-04 07:41:42,083 - epoch:17, training loss:0.8416 validation loss:0.2465
Updating learning rate to 4.997540849148917e-05
Updating learning rate to 4.997540849148917e-05
train 33889
vs, vt 0.24571356651457874 0.2530768578448756
need align? ->  False 0.2500219418751923
2023-09-04 07:44:56,690 - epoch:18, training loss:0.8352 validation loss:0.2457
Updating learning rate to 4.344933788275899e-05
Updating learning rate to 4.344933788275899e-05
train 33889
vs, vt 0.2447680184532973 0.25185552976009523
need align? ->  False 0.2500219418751923
2023-09-04 07:48:09,768 - epoch:19, training loss:0.8297 validation loss:0.2448
Updating learning rate to 3.703535434109692e-05
Updating learning rate to 3.703535434109692e-05
train 33889
vs, vt 0.24480484633452512 0.2517947943627157
need align? ->  False 0.2500219418751923
2023-09-04 07:51:23,860 - epoch:20, training loss:0.8254 validation loss:0.2448
Updating learning rate to 3.084320290319299e-05
Updating learning rate to 3.084320290319299e-05
train 33889
vs, vt 0.2443929107182405 0.2515313292599537
need align? ->  False 0.2500219418751923
2023-09-04 07:54:37,878 - epoch:21, training loss:0.8221 validation loss:0.2444
Updating learning rate to 2.4978832996938436e-05
Updating learning rate to 2.4978832996938436e-05
train 33889
vs, vt 0.24546264289793643 0.25053756264969707
need align? ->  False 0.2500219418751923
2023-09-04 07:57:51,069 - epoch:22, training loss:0.8194 validation loss:0.2455
Updating learning rate to 1.954258561733979e-05
Updating learning rate to 1.954258561733979e-05
train 33889
vs, vt 0.24523397496986118 0.25163779767568817
need align? ->  False 0.2500219418751923
2023-09-04 08:01:05,736 - epoch:23, training loss:0.8174 validation loss:0.2452
Updating learning rate to 1.4627476464274535e-05
Updating learning rate to 1.4627476464274535e-05
train 33889
vs, vt 0.24517262252894315 0.2513152238150889
need align? ->  False 0.2500219418751923
2023-09-04 08:04:18,917 - epoch:24, training loss:0.8154 validation loss:0.2452
Updating learning rate to 1.0317604418077296e-05
Updating learning rate to 1.0317604418077296e-05
train 33889
vs, vt 0.245242329610681 0.25121087801050057
need align? ->  False 0.2500219418751923
2023-09-04 08:07:33,907 - epoch:25, training loss:0.8145 validation loss:0.2452
Updating learning rate to 6.686712584380782e-06
Updating learning rate to 6.686712584380782e-06
train 33889
vs, vt 0.24508740109476176 0.25117374888875266
need align? ->  False 0.2500219418751923
2023-09-04 08:10:50,682 - epoch:26, training loss:0.8135 validation loss:0.2451
Updating learning rate to 3.7969265291329562e-06
Updating learning rate to 3.7969265291329562e-06
train 33889
vs, vt 0.24535798814825036 0.2510439591397616
need align? ->  False 0.2500219418751923
2023-09-04 08:14:04,864 - epoch:27, training loss:0.8129 validation loss:0.2454
check exp/ECL-PatchTST2023-09-04-06:44:49.931578/0/0.2436_epoch_16.pkl  &  0.2500219418751923
2023-09-04 08:14:33,713 - [*] loss:0.3773
2023-09-04 08:14:33,749 - [*] phase 0, testing
2023-09-04 08:14:34,507 - T:336	MAE	0.383853	RMSE	0.377197	MAPE	229.626155
2023-09-04 08:14:34,508 - 336	mae	0.3839	
2023-09-04 08:14:34,508 - 336	rmse	0.3772	
2023-09-04 08:14:34,508 - 336	mape	229.6262	
----*-----
2023-09-04 08:15:03,126 - [*] loss:0.3773
2023-09-04 08:15:03,159 - [*] phase 0, testing
2023-09-04 08:15:03,778 - T:336	MAE	0.383853	RMSE	0.377197	MAPE	229.626155
2023-09-04 08:15:29,888 - [*] loss:0.3873
2023-09-04 08:15:29,921 - [*] phase 0, testing
2023-09-04 08:15:30,556 - T:336	MAE	0.398145	RMSE	0.387131	MAPE	241.773129
2023-09-04 08:15:55,924 - [*] loss:0.4122
2023-09-04 08:15:55,960 - [*] phase 0, testing
2023-09-04 08:15:56,603 - T:336	MAE	0.413891	RMSE	0.412182	MAPE	266.012883
2023-09-04 08:16:23,751 - [*] loss:0.3839
2023-09-04 08:16:23,785 - [*] phase 0, testing
2023-09-04 08:16:24,615 - T:336	MAE	0.392157	RMSE	0.383831	MAPE	247.079182
2023-09-04 08:17:05,591 - [*] loss:0.4404
2023-09-04 08:17:05,630 - [*] phase 0, testing
2023-09-04 08:17:06,272 - T:336	MAE	0.442216	RMSE	0.440288	MAPE	243.799424
2023-09-04 08:17:43,976 - [*] loss:0.4165
2023-09-04 08:17:44,025 - [*] phase 0, testing
2023-09-04 08:17:45,104 - T:336	MAE	0.418190	RMSE	0.416258	MAPE	206.037521
2023-09-04 08:18:22,086 - [*] loss:0.3802
2023-09-04 08:18:22,125 - [*] phase 0, testing
2023-09-04 08:18:22,937 - T:336	MAE	0.388457	RMSE	0.380066	MAPE	233.899474
2023-09-04 08:18:54,727 - [*] loss:0.3822
2023-09-04 08:18:54,761 - [*] phase 0, testing
2023-09-04 08:18:55,401 - T:336	MAE	0.393574	RMSE	0.382009	MAPE	214.384413
----*-----
2023-09-04 08:19:16,820 - [*] loss:0.3722
2023-09-04 08:19:16,853 - [*] phase 0, testing
2023-09-04 08:19:17,498 - T:336	MAE	0.392218	RMSE	0.371904	MAPE	221.021867
2023-09-04 08:19:41,970 - [*] loss:0.4223
2023-09-04 08:19:42,006 - [*] phase 0, testing
2023-09-04 08:19:42,673 - T:336	MAE	0.421744	RMSE	0.422290	MAPE	223.683524
2023-09-04 08:20:00,392 - [*] loss:0.3799
2023-09-04 08:20:00,426 - [*] phase 0, testing
2023-09-04 08:20:01,033 - T:336	MAE	0.396951	RMSE	0.379697	MAPE	219.852471
2023-09-04 08:20:01,034 - 336	mae	0.3970	
2023-09-04 08:20:01,034 - 336	rmse	0.3797	
2023-09-04 08:20:01,034 - 336	mape	219.8525	
2023-09-04 08:20:03,422 - logger name:exp/ECL-PatchTST2023-09-04-08:20:03.421356/ECL-PatchTST.log
2023-09-04 08:20:03,422 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'abl_tmp_context': 1, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 36, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-04-08:20:03.421356', 'path': 'exp/ECL-PatchTST2023-09-04-08:20:03.421356', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 08:20:03,422 - [*] phase 0 start training
0 17420
train 7585
val 2161
test 2161
2023-09-04 08:20:03,669 - [*] phase 0 Dataset load!
2023-09-04 08:20:04,784 - [*] phase 0 Training start
train 7585
2023-09-04 08:20:25,591 - epoch:0, training loss:0.3272 validation loss:0.2922
train 7585
vs, vt 0.2921844326397952 0.2930576025563128
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.2690828793189105 0.2741312358309241
need align? ->  False 0.2741312358309241
2023-09-04 08:21:25,419 - epoch:1, training loss:12.9999 validation loss:0.2691
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.2626729576903231 0.2632766085512498
need align? ->  False 0.2632766085512498
2023-09-04 08:22:08,118 - epoch:2, training loss:12.1898 validation loss:0.2627
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.26635253736201453 0.26182731854565006
need align? ->  True 0.26182731854565006
2023-09-04 08:22:49,724 - epoch:3, training loss:10.9320 validation loss:0.2664
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.26372816325987086 0.2685206330874387
need align? ->  True 0.26182731854565006
2023-09-04 08:23:29,121 - epoch:4, training loss:8.3047 validation loss:0.2637
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.26306476444005966 0.27103598810294094
need align? ->  True 0.26182731854565006
2023-09-04 08:24:15,192 - epoch:5, training loss:5.6627 validation loss:0.2631
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.25899691485306797 0.2746464145534179
need align? ->  False 0.26182731854565006
2023-09-04 08:25:00,817 - epoch:6, training loss:4.2028 validation loss:0.2590
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.25816529331838384 0.2766106124309933
need align? ->  False 0.26182731854565006
2023-09-04 08:25:41,672 - epoch:7, training loss:3.6385 validation loss:0.2582
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.2597202917232233 0.27526412948089485
need align? ->  False 0.26182731854565006
2023-09-04 08:26:22,871 - epoch:8, training loss:3.3454 validation loss:0.2597
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.25776634540627985 0.27719470332650575
need align? ->  False 0.26182731854565006
2023-09-04 08:27:03,865 - epoch:9, training loss:3.1250 validation loss:0.2578
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.2582613041295725 0.2767962865969714
need align? ->  False 0.26182731854565006
2023-09-04 08:27:51,001 - epoch:10, training loss:2.9934 validation loss:0.2583
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.26123619824647903 0.28232986523824577
need align? ->  False 0.26182731854565006
2023-09-04 08:28:33,954 - epoch:11, training loss:2.9276 validation loss:0.2612
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.25884785634629864 0.2871926409356734
need align? ->  False 0.26182731854565006
2023-09-04 08:29:14,919 - epoch:12, training loss:2.8763 validation loss:0.2588
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.2583266553633353 0.2865588217973709
need align? ->  False 0.26182731854565006
2023-09-04 08:29:56,431 - epoch:13, training loss:2.8351 validation loss:0.2583
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.26104576622738557 0.28555051368825574
need align? ->  False 0.26182731854565006
2023-09-04 08:30:42,084 - epoch:14, training loss:2.8029 validation loss:0.2610
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.2603244185447693 0.2910468429327011
need align? ->  False 0.26182731854565006
2023-09-04 08:31:27,470 - epoch:15, training loss:2.7631 validation loss:0.2603
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.2587194438366329 0.2884339585023768
need align? ->  False 0.26182731854565006
2023-09-04 08:32:08,411 - epoch:16, training loss:2.7343 validation loss:0.2587
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.2596042844302514 0.2891270084416165
need align? ->  False 0.26182731854565006
2023-09-04 08:32:48,197 - epoch:17, training loss:2.6988 validation loss:0.2596
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.25949829685337406 0.28984565375482335
need align? ->  False 0.26182731854565006
2023-09-04 08:33:28,069 - epoch:18, training loss:2.6708 validation loss:0.2595
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.25936151690342846 0.29169898348696094
need align? ->  False 0.26182731854565006
2023-09-04 08:34:14,205 - epoch:19, training loss:2.6469 validation loss:0.2594
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.25820545063299294 0.29236730494919944
need align? ->  False 0.26182731854565006
2023-09-04 08:34:58,324 - epoch:20, training loss:2.6264 validation loss:0.2582
check exp/ECL-PatchTST2023-09-04-08:20:03.421356/0/0.2578_epoch_9.pkl  &  0.26182731854565006
2023-09-04 08:35:04,066 - [*] loss:0.3880
2023-09-04 08:35:04,081 - [*] phase 0, testing
2023-09-04 08:35:04,323 - T:720	MAE	0.421047	RMSE	0.386418	MAPE	190.041351
2023-09-04 08:35:04,323 - 720	mae	0.4210	
2023-09-04 08:35:04,323 - 720	rmse	0.3864	
2023-09-04 08:35:04,323 - 720	mape	190.0414	
----*-----
2023-09-04 08:35:09,325 - [*] loss:0.3880
2023-09-04 08:35:09,339 - [*] phase 0, testing
2023-09-04 08:35:09,584 - T:720	MAE	0.421047	RMSE	0.386418	MAPE	190.041351
2023-09-04 08:35:14,323 - [*] loss:0.4053
2023-09-04 08:35:14,338 - [*] phase 0, testing
2023-09-04 08:35:14,584 - T:720	MAE	0.443697	RMSE	0.403743	MAPE	219.255018
2023-09-04 08:35:19,782 - [*] loss:0.3953
2023-09-04 08:35:19,797 - [*] phase 0, testing
2023-09-04 08:35:20,033 - T:720	MAE	0.430041	RMSE	0.393758	MAPE	192.291510
2023-09-04 08:35:25,793 - [*] loss:0.3836
2023-09-04 08:35:25,813 - [*] phase 0, testing
2023-09-04 08:35:26,053 - T:720	MAE	0.416903	RMSE	0.381782	MAPE	181.258011
2023-09-04 08:35:32,142 - [*] loss:0.4541
2023-09-04 08:35:32,157 - [*] phase 0, testing
2023-09-04 08:35:32,427 - T:720	MAE	0.477835	RMSE	0.452620	MAPE	261.961317
2023-09-04 08:35:37,382 - [*] loss:0.4816
2023-09-04 08:35:37,408 - [*] phase 0, testing
2023-09-04 08:35:37,657 - T:720	MAE	0.486673	RMSE	0.479882	MAPE	174.066317
2023-09-04 08:35:42,261 - [*] loss:0.3935
2023-09-04 08:35:42,276 - [*] phase 0, testing
2023-09-04 08:35:42,557 - T:720	MAE	0.429036	RMSE	0.391891	MAPE	199.856603
2023-09-04 08:35:47,634 - [*] loss:0.4200
2023-09-04 08:35:47,652 - [*] phase 0, testing
2023-09-04 08:35:47,905 - T:720	MAE	0.450271	RMSE	0.417966	MAPE	178.720295
----*-----
2023-09-04 08:35:51,420 - [*] loss:0.4209
2023-09-04 08:35:51,439 - [*] phase 0, testing
2023-09-04 08:35:51,696 - T:720	MAE	0.451022	RMSE	0.418859	MAPE	179.494095
2023-09-04 08:35:57,358 - [*] loss:0.4458
2023-09-04 08:35:57,373 - [*] phase 0, testing
2023-09-04 08:35:57,618 - T:720	MAE	0.465955	RMSE	0.443594	MAPE	180.011070
2023-09-04 08:36:01,511 - [*] loss:0.4571
2023-09-04 08:36:01,526 - [*] phase 0, testing
2023-09-04 08:36:01,763 - T:720	MAE	0.472269	RMSE	0.455009	MAPE	177.940881
2023-09-04 08:36:01,763 - 720	mae	0.4723	
2023-09-04 08:36:01,764 - 720	rmse	0.4550	
2023-09-04 08:36:01,764 - 720	mape	177.9409	
2023-09-04 08:36:04,025 - logger name:exp/ECL-PatchTST2023-09-04-08:36:04.024541/ECL-PatchTST.log
2023-09-04 08:36:04,025 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'abl_tmp_context': 1, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-04-08:36:04.024541', 'path': 'exp/ECL-PatchTST2023-09-04-08:36:04.024541', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 08:36:04,025 - [*] phase 0 start training
0 17420
train 7585
val 2161
test 2161
2023-09-04 08:36:04,222 - [*] phase 0 Dataset load!
2023-09-04 08:36:05,276 - [*] phase 0 Training start
train 7585
2023-09-04 08:36:26,249 - epoch:0, training loss:0.3265 validation loss:0.2917
train 7585
vs, vt 0.2916712024632622 0.2926822401144925
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.26967728181796913 0.2738412609871696
need align? ->  False 0.2738412609871696
2023-09-04 08:37:22,873 - epoch:1, training loss:13.0128 validation loss:0.2697
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.26506394717623205 0.2625991478562355
need align? ->  True 0.2625991478562355
2023-09-04 08:38:08,513 - epoch:2, training loss:12.1791 validation loss:0.2651
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.2632518305498011 0.2628830614335397
need align? ->  True 0.2625991478562355
2023-09-04 08:38:56,169 - epoch:3, training loss:10.8928 validation loss:0.2633
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.2576935922398287 0.266902613289216
need align? ->  False 0.2625991478562355
2023-09-04 08:39:44,376 - epoch:4, training loss:8.9506 validation loss:0.2577
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.2607116401195526 0.2627155601101763
need align? ->  False 0.2625991478562355
2023-09-04 08:40:33,083 - epoch:5, training loss:6.5423 validation loss:0.2607
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.2608203839729814 0.26952708074275183
need align? ->  False 0.2625991478562355
2023-09-04 08:41:19,962 - epoch:6, training loss:5.3010 validation loss:0.2608
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.2582184387480511 0.2769526260740617
need align? ->  False 0.2625991478562355
2023-09-04 08:42:08,615 - epoch:7, training loss:4.7418 validation loss:0.2582
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.2593191936612129 0.27287039248382344
need align? ->  False 0.2625991478562355
2023-09-04 08:42:56,018 - epoch:8, training loss:4.3993 validation loss:0.2593
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.2587617140482454 0.27954428818295984
need align? ->  False 0.2625991478562355
2023-09-04 08:43:44,363 - epoch:9, training loss:4.1916 validation loss:0.2588
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.25952208261279497 0.28258809841731014
need align? ->  False 0.2625991478562355
2023-09-04 08:44:25,201 - epoch:10, training loss:4.0686 validation loss:0.2595
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.25930720830664916 0.2858675911146052
need align? ->  False 0.2625991478562355
2023-09-04 08:45:08,867 - epoch:11, training loss:3.9651 validation loss:0.2593
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.25961460304610867 0.28424912077539105
need align? ->  False 0.2625991478562355
2023-09-04 08:45:51,690 - epoch:12, training loss:3.8675 validation loss:0.2596
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.25961438215830746 0.28525101919384566
need align? ->  False 0.2625991478562355
2023-09-04 08:46:35,455 - epoch:13, training loss:3.7692 validation loss:0.2596
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.2611292797852965 0.287268290624899
need align? ->  False 0.2625991478562355
2023-09-04 08:47:20,059 - epoch:14, training loss:3.6647 validation loss:0.2611
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.2571324175771545 0.2904763383900418
need align? ->  False 0.2625991478562355
2023-09-04 08:48:04,057 - epoch:15, training loss:3.5789 validation loss:0.2571
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.25654855502002377 0.2883236232925864
need align? ->  False 0.2625991478562355
2023-09-04 08:48:47,664 - epoch:16, training loss:3.4961 validation loss:0.2565
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.2577028042253326 0.2894655387191212
need align? ->  False 0.2625991478562355
2023-09-04 08:49:30,698 - epoch:17, training loss:3.4193 validation loss:0.2577
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.25964157502440843 0.2897673490292886
need align? ->  False 0.2625991478562355
2023-09-04 08:50:14,309 - epoch:18, training loss:3.3539 validation loss:0.2596
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.25813936266828985 0.2927948083071148
need align? ->  False 0.2625991478562355
2023-09-04 08:50:56,677 - epoch:19, training loss:3.2938 validation loss:0.2581
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.25833447891123157 0.28933291049564586
need align? ->  False 0.2625991478562355
2023-09-04 08:51:40,375 - epoch:20, training loss:3.2548 validation loss:0.2583
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.25879721431171193 0.2910226828911725
need align? ->  False 0.2625991478562355
2023-09-04 08:52:22,916 - epoch:21, training loss:3.2168 validation loss:0.2588
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.2579900701256359 0.29074670023777904
need align? ->  False 0.2625991478562355
2023-09-04 08:53:05,508 - epoch:22, training loss:3.1911 validation loss:0.2580
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.25807292277322097 0.2910648111034842
need align? ->  False 0.2625991478562355
2023-09-04 08:53:48,337 - epoch:23, training loss:3.1659 validation loss:0.2581
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.25873684620156007 0.2922438128906138
need align? ->  False 0.2625991478562355
2023-09-04 08:54:30,194 - epoch:24, training loss:3.1525 validation loss:0.2587
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.2587742879986763 0.2918530838454471
need align? ->  False 0.2625991478562355
2023-09-04 08:55:12,027 - epoch:25, training loss:3.1428 validation loss:0.2588
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.2587942108511925 0.29319268070599613
need align? ->  False 0.2625991478562355
2023-09-04 08:55:53,849 - epoch:26, training loss:3.1338 validation loss:0.2588
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.258787095108453 0.29260355102665286
need align? ->  False 0.2625991478562355
2023-09-04 08:56:36,111 - epoch:27, training loss:3.1295 validation loss:0.2588
check exp/ECL-PatchTST2023-09-04-08:36:04.024541/0/0.2565_epoch_16.pkl  &  0.2625991478562355
2023-09-04 08:56:41,497 - [*] loss:0.3872
2023-09-04 08:56:41,513 - [*] phase 0, testing
2023-09-04 08:56:41,771 - T:720	MAE	0.420388	RMSE	0.385628	MAPE	189.545989
2023-09-04 08:56:41,772 - 720	mae	0.4204	
2023-09-04 08:56:41,772 - 720	rmse	0.3856	
2023-09-04 08:56:41,772 - 720	mape	189.5460	
----*-----
2023-09-04 08:56:48,498 - [*] loss:0.3872
2023-09-04 08:56:48,513 - [*] phase 0, testing
2023-09-04 08:56:48,771 - T:720	MAE	0.420388	RMSE	0.385628	MAPE	189.545989
2023-09-04 08:56:55,085 - [*] loss:0.4058
2023-09-04 08:56:55,100 - [*] phase 0, testing
2023-09-04 08:56:55,355 - T:720	MAE	0.445692	RMSE	0.404289	MAPE	223.023319
2023-09-04 08:57:01,144 - [*] loss:0.3999
2023-09-04 08:57:01,158 - [*] phase 0, testing
2023-09-04 08:57:01,405 - T:720	MAE	0.433742	RMSE	0.398491	MAPE	194.051147
2023-09-04 08:57:06,316 - [*] loss:0.3822
2023-09-04 08:57:06,332 - [*] phase 0, testing
2023-09-04 08:57:06,588 - T:720	MAE	0.415618	RMSE	0.380264	MAPE	182.545471
2023-09-04 08:57:12,869 - [*] loss:0.4461
2023-09-04 08:57:12,884 - [*] phase 0, testing
2023-09-04 08:57:13,134 - T:720	MAE	0.474054	RMSE	0.444722	MAPE	266.979098
2023-09-04 08:57:18,704 - [*] loss:0.4628
2023-09-04 08:57:18,718 - [*] phase 0, testing
2023-09-04 08:57:18,962 - T:720	MAE	0.475970	RMSE	0.461018	MAPE	174.779391
2023-09-04 08:57:24,395 - [*] loss:0.3931
2023-09-04 08:57:24,410 - [*] phase 0, testing
2023-09-04 08:57:24,650 - T:720	MAE	0.429456	RMSE	0.391549	MAPE	200.946903
2023-09-04 08:57:30,620 - [*] loss:0.4197
2023-09-04 08:57:30,634 - [*] phase 0, testing
2023-09-04 08:57:30,920 - T:720	MAE	0.449836	RMSE	0.417733	MAPE	181.128979
----*-----
2023-09-04 08:57:35,488 - [*] loss:0.4180
2023-09-04 08:57:35,504 - [*] phase 0, testing
2023-09-04 08:57:35,760 - T:720	MAE	0.448561	RMSE	0.416042	MAPE	178.798711
2023-09-04 08:57:42,538 - [*] loss:0.4693
2023-09-04 08:57:42,554 - [*] phase 0, testing
2023-09-04 08:57:42,819 - T:720	MAE	0.481222	RMSE	0.467259	MAPE	184.583044
2023-09-04 08:57:47,178 - [*] loss:0.4779
2023-09-04 08:57:47,197 - [*] phase 0, testing
2023-09-04 08:57:47,472 - T:720	MAE	0.485633	RMSE	0.475929	MAPE	181.538165
2023-09-04 08:57:47,473 - 720	mae	0.4856	
2023-09-04 08:57:47,473 - 720	rmse	0.4759	
2023-09-04 08:57:47,473 - 720	mape	181.5382	
2023-09-04 08:57:49,710 - logger name:exp/ECL-PatchTST2023-09-04-08:57:49.709984/ECL-PatchTST.log
2023-09-04 08:57:49,710 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'abl_tmp_context': 1, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 192, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 36, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-04-08:57:49.709984', 'path': 'exp/ECL-PatchTST2023-09-04-08:57:49.709984', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 08:57:49,710 - [*] phase 0 start training
0 17420
train 8113
val 2689
test 2689
2023-09-04 08:57:49,908 - [*] phase 0 Dataset load!
2023-09-04 08:57:51,081 - [*] phase 0 Training start
train 8113
2023-09-04 08:58:13,201 - epoch:0, training loss:0.2439 validation loss:0.1549
train 8113
vs, vt 0.15494578771970488 0.15768190371719273
Updating learning rate to 1.0464153247552845e-05
Updating learning rate to 1.0464153247552845e-05
train 8113
vs, vt 0.13289013115519827 0.13857951387763023
need align? ->  False 0.13857951387763023
2023-09-04 08:59:13,375 - epoch:1, training loss:12.7453 validation loss:0.1329
Updating learning rate to 2.8115559773217685e-05
Updating learning rate to 2.8115559773217685e-05
train 8113
vs, vt 0.1252853860231963 0.1281212419271469
need align? ->  False 0.1281212419271469
2023-09-04 09:00:00,341 - epoch:2, training loss:11.8660 validation loss:0.1253
Updating learning rate to 5.2199994709629883e-05
Updating learning rate to 5.2199994709629883e-05
train 8113
vs, vt 0.12433004176074808 0.12587054768069225
need align? ->  False 0.12587054768069225
2023-09-04 09:00:48,040 - epoch:3, training loss:10.4677 validation loss:0.1243
Updating learning rate to 7.623056312721927e-05
Updating learning rate to 7.623056312721927e-05
train 8113
vs, vt 0.12523963166908783 0.12747514806687832
need align? ->  False 0.12587054768069225
2023-09-04 09:01:34,303 - epoch:4, training loss:7.3929 validation loss:0.1252
Updating learning rate to 9.373487848943999e-05
Updating learning rate to 9.373487848943999e-05
train 8113
vs, vt 0.12780854004350575 0.12814607149498028
need align? ->  True 0.12587054768069225
2023-09-04 09:02:22,467 - epoch:5, training loss:4.8900 validation loss:0.1278
Updating learning rate to 9.999989207196297e-05
Updating learning rate to 9.999989207196297e-05
train 8113
vs, vt 0.12343973852694035 0.13270344585180283
need align? ->  False 0.12587054768069225
2023-09-04 09:03:13,409 - epoch:6, training loss:3.7272 validation loss:0.1234
Updating learning rate to 9.955857764964711e-05
Updating learning rate to 9.955857764964711e-05
train 8113
vs, vt 0.12485683218322018 0.12827021882615305
need align? ->  False 0.12587054768069225
2023-09-04 09:03:58,762 - epoch:7, training loss:3.2483 validation loss:0.1249
Updating learning rate to 9.826930564556767e-05
Updating learning rate to 9.826930564556767e-05
train 8113
vs, vt 0.12226676517589526 0.12794376706535165
need align? ->  False 0.12587054768069225
2023-09-04 09:04:45,457 - epoch:8, training loss:2.9716 validation loss:0.1223
Updating learning rate to 9.61541358611682e-05
Updating learning rate to 9.61541358611682e-05
train 8113
vs, vt 0.12533465739000926 0.12773649868640033
need align? ->  False 0.12587054768069225
2023-09-04 09:05:32,574 - epoch:9, training loss:2.7939 validation loss:0.1253
Updating learning rate to 9.324925943789559e-05
Updating learning rate to 9.324925943789559e-05
train 8113
vs, vt 0.12352953631092202 0.128493242981759
need align? ->  False 0.12587054768069225
2023-09-04 09:06:18,751 - epoch:10, training loss:2.7007 validation loss:0.1235
Updating learning rate to 8.960437961673599e-05
Updating learning rate to 8.960437961673599e-05
train 8113
vs, vt 0.1237063780426979 0.12701409089971671
need align? ->  False 0.12587054768069225
2023-09-04 09:07:04,673 - epoch:11, training loss:2.6452 validation loss:0.1237
Updating learning rate to 8.528186130198099e-05
Updating learning rate to 8.528186130198099e-05
train 8113
vs, vt 0.12516701898791574 0.12692659873176704
need align? ->  False 0.12587054768069225
2023-09-04 09:07:49,467 - epoch:12, training loss:2.6056 validation loss:0.1252
Updating learning rate to 8.035566398042457e-05
Updating learning rate to 8.035566398042457e-05
train 8113
vs, vt 0.12518962980671364 0.13124362955039198
need align? ->  False 0.12587054768069225
2023-09-04 09:08:35,862 - epoch:13, training loss:2.5570 validation loss:0.1252
Updating learning rate to 7.491007625403847e-05
Updating learning rate to 7.491007625403847e-05
train 8113
vs, vt 0.12349025617268952 0.1313511234792796
need align? ->  False 0.12587054768069225
2023-09-04 09:09:20,971 - epoch:14, training loss:2.5230 validation loss:0.1235
Updating learning rate to 6.903827363862332e-05
Updating learning rate to 6.903827363862332e-05
train 8113
vs, vt 0.1226227114146406 0.12742054276168346
need align? ->  False 0.12587054768069225
2023-09-04 09:10:07,296 - epoch:15, training loss:2.4814 validation loss:0.1226
Updating learning rate to 6.284072430490012e-05
Updating learning rate to 6.284072430490012e-05
train 8113
vs, vt 0.1227445648136464 0.1261031791906465
need align? ->  False 0.12587054768069225
2023-09-04 09:10:52,788 - epoch:16, training loss:2.4409 validation loss:0.1227
Updating learning rate to 5.642347004025414e-05
Updating learning rate to 5.642347004025414e-05
train 8113
vs, vt 0.12340693158859556 0.1273387684063478
need align? ->  False 0.12587054768069225
2023-09-04 09:11:38,323 - epoch:17, training loss:2.4034 validation loss:0.1234
Updating learning rate to 4.989631184435254e-05
Updating learning rate to 4.989631184435254e-05
train 8113
vs, vt 0.12427211264995011 0.1290907428007234
need align? ->  False 0.12587054768069225
2023-09-04 09:12:24,194 - epoch:18, training loss:2.3671 validation loss:0.1243
Updating learning rate to 4.337093120359729e-05
Updating learning rate to 4.337093120359729e-05
train 8113
vs, vt 0.12429705685512586 0.12729343636469406
need align? ->  False 0.12587054768069225
2023-09-04 09:13:09,854 - epoch:19, training loss:2.3439 validation loss:0.1243
check exp/ECL-PatchTST2023-09-04-08:57:49.709984/0/0.1223_epoch_8.pkl  &  0.12587054768069225
2023-09-04 09:13:16,147 - [*] loss:0.3603
2023-09-04 09:13:16,152 - [*] phase 0, testing
2023-09-04 09:13:16,241 - T:192	MAE	0.376429	RMSE	0.345595	MAPE	145.409834
2023-09-04 09:13:16,241 - 192	mae	0.3764	
2023-09-04 09:13:16,241 - 192	rmse	0.3456	
2023-09-04 09:13:16,241 - 192	mape	145.4098	
----*-----
2023-09-04 09:13:22,154 - [*] loss:0.3603
2023-09-04 09:13:22,159 - [*] phase 0, testing
2023-09-04 09:13:22,238 - T:192	MAE	0.376429	RMSE	0.345595	MAPE	145.409834
2023-09-04 09:13:30,669 - [*] loss:0.3753
2023-09-04 09:13:30,673 - [*] phase 0, testing
2023-09-04 09:13:30,762 - T:192	MAE	0.400317	RMSE	0.358371	MAPE	172.158492
2023-09-04 09:13:38,248 - [*] loss:0.3594
2023-09-04 09:13:38,252 - [*] phase 0, testing
2023-09-04 09:13:38,342 - T:192	MAE	0.380650	RMSE	0.344815	MAPE	145.586443
2023-09-04 09:13:45,745 - [*] loss:0.3631
2023-09-04 09:13:45,749 - [*] phase 0, testing
2023-09-04 09:13:45,833 - T:192	MAE	0.378686	RMSE	0.349519	MAPE	141.865730
2023-09-04 09:13:53,311 - [*] loss:0.4132
2023-09-04 09:13:53,315 - [*] phase 0, testing
2023-09-04 09:13:53,397 - T:192	MAE	0.424161	RMSE	0.398098	MAPE	186.305797
2023-09-04 09:13:59,839 - [*] loss:0.4907
2023-09-04 09:13:59,844 - [*] phase 0, testing
2023-09-04 09:13:59,936 - T:192	MAE	0.459665	RMSE	0.463035	MAPE	137.318277
2023-09-04 09:14:06,676 - [*] loss:0.3633
2023-09-04 09:14:06,680 - [*] phase 0, testing
2023-09-04 09:14:06,759 - T:192	MAE	0.384813	RMSE	0.348544	MAPE	155.782509
2023-09-04 09:14:12,761 - [*] loss:0.3998
2023-09-04 09:14:12,767 - [*] phase 0, testing
2023-09-04 09:14:12,860 - T:192	MAE	0.416257	RMSE	0.386889	MAPE	135.991251
----*-----
2023-09-04 09:14:18,181 - [*] loss:0.3947
2023-09-04 09:14:18,186 - [*] phase 0, testing
2023-09-04 09:14:18,281 - T:192	MAE	0.415603	RMSE	0.384425	MAPE	137.226737
2023-09-04 09:14:25,673 - [*] loss:0.3939
2023-09-04 09:14:25,678 - [*] phase 0, testing
2023-09-04 09:14:25,772 - T:192	MAE	0.417107	RMSE	0.381593	MAPE	139.836740
2023-09-04 09:14:30,824 - [*] loss:0.4066
2023-09-04 09:14:30,829 - [*] phase 0, testing
2023-09-04 09:14:30,923 - T:192	MAE	0.423052	RMSE	0.393022	MAPE	137.617075
2023-09-04 09:14:30,923 - 192	mae	0.4231	
2023-09-04 09:14:30,923 - 192	rmse	0.3930	
2023-09-04 09:14:30,923 - 192	mape	137.6171	
2023-09-04 09:14:33,239 - logger name:exp/ECL-PatchTST2023-09-04-09:14:33.239637/ECL-PatchTST.log
2023-09-04 09:14:33,240 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'abl_tmp_context': 1, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 192, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-04-09:14:33.239637', 'path': 'exp/ECL-PatchTST2023-09-04-09:14:33.239637', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 09:14:33,240 - [*] phase 0 start training
0 17420
train 8113
val 2689
test 2689
2023-09-04 09:14:33,441 - [*] phase 0 Dataset load!
2023-09-04 09:14:34,534 - [*] phase 0 Training start
train 8113
2023-09-04 09:14:56,511 - epoch:0, training loss:0.2429 validation loss:0.1549
train 8113
vs, vt 0.15489390289241617 0.15706340900876306
Updating learning rate to 1.0464153247552845e-05
Updating learning rate to 1.0464153247552845e-05
train 8113
vs, vt 0.13228726031428034 0.13805291784757917
need align? ->  False 0.13805291784757917
2023-09-04 09:15:55,134 - epoch:1, training loss:12.7972 validation loss:0.1323
Updating learning rate to 2.8115559773217685e-05
Updating learning rate to 2.8115559773217685e-05
train 8113
vs, vt 0.12499627708034082 0.1273608019744808
need align? ->  False 0.1273608019744808
2023-09-04 09:16:40,583 - epoch:2, training loss:11.9085 validation loss:0.1250
Updating learning rate to 5.2199994709629883e-05
Updating learning rate to 5.2199994709629883e-05
train 8113
vs, vt 0.1260410062968731 0.12536191601644864
need align? ->  True 0.12536191601644864
2023-09-04 09:17:34,335 - epoch:3, training loss:10.5121 validation loss:0.1260
Updating learning rate to 7.623056312721927e-05
Updating learning rate to 7.623056312721927e-05
train 8113
vs, vt 0.12276508980853991 0.1261790432035923
need align? ->  False 0.12536191601644864
2023-09-04 09:18:21,764 - epoch:4, training loss:7.3865 validation loss:0.1228
Updating learning rate to 9.373487848943999e-05
Updating learning rate to 9.373487848943999e-05
train 8113
vs, vt 0.12390886856751009 0.12577141364189712
need align? ->  False 0.12536191601644864
2023-09-04 09:19:08,435 - epoch:5, training loss:4.7792 validation loss:0.1239
Updating learning rate to 9.999989207196297e-05
Updating learning rate to 9.999989207196297e-05
train 8113
vs, vt 0.12740900452164086 0.12894880077378315
need align? ->  True 0.12536191601644864
2023-09-04 09:19:54,585 - epoch:6, training loss:3.7371 validation loss:0.1274
Updating learning rate to 9.955857764964711e-05
Updating learning rate to 9.955857764964711e-05
train 8113
vs, vt 0.12497529776936228 0.13065756010738286
need align? ->  False 0.12536191601644864
2023-09-04 09:20:40,498 - epoch:7, training loss:3.4054 validation loss:0.1250
Updating learning rate to 9.826930564556767e-05
Updating learning rate to 9.826930564556767e-05
train 8113
vs, vt 0.1242536741562865 0.12950156307355923
need align? ->  False 0.12536191601644864
2023-09-04 09:21:26,456 - epoch:8, training loss:3.1911 validation loss:0.1243
Updating learning rate to 9.61541358611682e-05
Updating learning rate to 9.61541358611682e-05
train 8113
vs, vt 0.1245717726309191 0.12894251028245146
need align? ->  False 0.12536191601644864
2023-09-04 09:22:15,823 - epoch:9, training loss:3.0285 validation loss:0.1246
Updating learning rate to 9.324925943789559e-05
Updating learning rate to 9.324925943789559e-05
train 8113
