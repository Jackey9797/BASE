2023-09-01 13:00:57,746 - logger name:exp/ECL-PatchTST2023-09-01-13:00:57.746378/ECL-PatchTST.log
2023-09-01 13:00:57,746 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.2, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-01-13:00:57.746378', 'path': 'exp/ECL-PatchTST2023-09-01-13:00:57.746378', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-01 13:00:57,746 - [*] phase 0 start training
0 69680
train 34129
val 11425
test 11425
2023-09-01 13:00:58,543 - [*] phase 0 Dataset load!
2023-09-01 13:00:59,554 - [*] phase 0 Training start
train 34129
2023-09-01 13:02:33,837 - epoch:0, training loss:0.1848 validation loss:0.1786
train 34129
vs, vt 0.17859705603784984 0.18382729362282488
Updating learning rate to 1.0438661460315324e-05
Updating learning rate to 1.0438661460315324e-05
train 34129
vs, vt 0.1682440927459134 0.1676121981193622
need align? ->  True 0.1676121981193622
2023-09-01 13:06:39,813 - epoch:1, training loss:9.5448 validation loss:0.1682
Updating learning rate to 2.8027297449571736e-05
Updating learning rate to 2.8027297449571736e-05
train 34129
vs, vt 0.1662293863379293 0.16768443257444435
need align? ->  False 0.1676121981193622
2023-09-01 13:10:05,985 - epoch:2, training loss:3.3055 validation loss:0.1662
Updating learning rate to 5.204727160595503e-05
Updating learning rate to 5.204727160595503e-05
train 34129
vs, vt 0.16161852843231625 0.16633412502706052
need align? ->  False 0.16633412502706052
2023-09-01 13:13:30,689 - epoch:3, training loss:2.5061 validation loss:0.1616
Updating learning rate to 7.605456385119542e-05
Updating learning rate to 7.605456385119542e-05
train 34129
vs, vt 0.16528974713550673 0.16441862798399395
need align? ->  True 0.16441862798399395
2023-09-01 13:16:52,445 - epoch:4, training loss:1.8940 validation loss:0.1653
Updating learning rate to 9.360855637921138e-05
Updating learning rate to 9.360855637921138e-05
train 34129
vs, vt 0.1603827445457379 0.1621944587263796
need align? ->  False 0.1621944587263796
2023-09-01 13:20:29,182 - epoch:5, training loss:1.5864 validation loss:0.1604
Updating learning rate to 9.99999939458629e-05
Updating learning rate to 9.99999939458629e-05
train 34129
vs, vt 0.15829142419000466 0.1601476573695739
need align? ->  False 0.1601476573695739
2023-09-01 13:23:48,196 - epoch:6, training loss:1.4261 validation loss:0.1583
Updating learning rate to 9.956902716655286e-05
Updating learning rate to 9.956902716655286e-05
train 34129
vs, vt 0.16003631158835357 0.15852494951751497
need align? ->  True 0.15852494951751497
2023-09-01 13:27:05,759 - epoch:7, training loss:1.3271 validation loss:0.1600
Updating learning rate to 9.828992401134782e-05
Updating learning rate to 9.828992401134782e-05
train 34129
vs, vt 0.15739783458411694 0.15792540384249554
need align? ->  False 0.15792540384249554
2023-09-01 13:30:23,373 - epoch:8, training loss:1.2499 validation loss:0.1574
Updating learning rate to 9.618457028986775e-05
Updating learning rate to 9.618457028986775e-05
train 34129
vs, vt 0.15719714309606286 0.1560786339557833
need align? ->  True 0.1560786339557833
2023-09-01 13:33:40,380 - epoch:9, training loss:1.1990 validation loss:0.1572
Updating learning rate to 9.32889891880015e-05
Updating learning rate to 9.32889891880015e-05
train 34129
vs, vt 0.15819965816206402 0.15799874766833252
need align? ->  True 0.1560786339557833
2023-09-01 13:36:57,993 - epoch:10, training loss:1.1422 validation loss:0.1582
Updating learning rate to 8.965272490120875e-05
Updating learning rate to 8.965272490120875e-05
train 34129
vs, vt 0.15880052037537098 0.15701708797779348
need align? ->  True 0.1560786339557833
2023-09-01 13:40:13,171 - epoch:11, training loss:1.1076 validation loss:0.1588
Updating learning rate to 8.533799491959945e-05
Updating learning rate to 8.533799491959945e-05
train 34129
vs, vt 0.1596336910708083 0.15654997299942705
need align? ->  True 0.1560786339557833
2023-09-01 13:43:29,208 - epoch:12, training loss:1.0780 validation loss:0.1596
Updating learning rate to 8.041862546942808e-05
Updating learning rate to 8.041862546942808e-05
train 34129
vs, vt 0.15749246900280317 0.15774176551236047
need align? ->  True 0.1560786339557833
2023-09-01 13:46:45,457 - epoch:13, training loss:1.0534 validation loss:0.1575
Updating learning rate to 7.497878832589398e-05
Updating learning rate to 7.497878832589398e-05
train 34129
vs, vt 0.156710067184435 0.15480918695943224
need align? ->  True 0.15480918695943224
2023-09-01 13:50:00,560 - epoch:14, training loss:1.0318 validation loss:0.1567
Updating learning rate to 6.911156061073078e-05
Updating learning rate to 6.911156061073078e-05
train 34129
vs, vt 0.1567384404440721 0.15509199272427293
need align? ->  True 0.15480918695943224
2023-09-01 13:53:14,558 - epoch:15, training loss:1.0689 validation loss:0.1567
Updating learning rate to 6.291733221684778e-05
Updating learning rate to 6.291733221684778e-05
train 34129
vs, vt 0.15694550619357162 0.15472304672002793
need align? ->  True 0.15472304672002793
2023-09-01 13:56:29,116 - epoch:16, training loss:1.0408 validation loss:0.1569
Updating learning rate to 5.650208810942889e-05
Updating learning rate to 5.650208810942889e-05
train 34129
vs, vt 0.1554672759026289 0.1546049808876382
need align? ->  True 0.1546049808876382
2023-09-01 13:59:42,009 - epoch:17, training loss:1.0590 validation loss:0.1555
Updating learning rate to 4.9975594893793686e-05
Updating learning rate to 4.9975594893793686e-05
train 34129
vs, vt 0.1551198480443822 0.15316855750150152
need align? ->  True 0.15316855750150152
2023-09-01 14:02:55,663 - epoch:18, training loss:1.0641 validation loss:0.1551
Updating learning rate to 4.3449522678347527e-05
Updating learning rate to 4.3449522678347527e-05
train 34129
vs, vt 0.15443157607482538 0.15396601855754852
need align? ->  True 0.15316855750150152
2023-09-01 14:06:08,992 - epoch:19, training loss:1.0610 validation loss:0.1544
Updating learning rate to 3.7035534368065714e-05
Updating learning rate to 3.7035534368065714e-05
train 34129
vs, vt 0.1546316758212116 0.15282622393634585
need align? ->  True 0.15282622393634585
2023-09-01 14:09:22,548 - epoch:20, training loss:1.0514 validation loss:0.1546
Updating learning rate to 3.084337508123067e-05
Updating learning rate to 3.084337508123067e-05
train 34129
vs, vt 0.15495486023525398 0.15331346893476117
need align? ->  True 0.15282622393634585
2023-09-01 14:12:32,580 - epoch:21, training loss:1.0662 validation loss:0.1550
Updating learning rate to 2.497899438003108e-05
Updating learning rate to 2.497899438003108e-05
train 34129
vs, vt 0.15502176591091685 0.15292415151165592
need align? ->  True 0.15282622393634585
2023-09-01 14:15:44,974 - epoch:22, training loss:1.0595 validation loss:0.1550
Updating learning rate to 1.9542733444177923e-05
Updating learning rate to 1.9542733444177923e-05
train 34129
vs, vt 0.15508396820061737 0.15239049067927732
need align? ->  True 0.15239049067927732
2023-09-01 14:18:59,173 - epoch:23, training loss:1.0534 validation loss:0.1551
Updating learning rate to 1.4627608205499963e-05
Updating learning rate to 1.4627608205499963e-05
train 34129
vs, vt 0.15499029950135285 0.1522434944493903
need align? ->  True 0.1522434944493903
2023-09-01 14:22:24,166 - epoch:24, training loss:1.0636 validation loss:0.1550
Updating learning rate to 1.0317717819561129e-05
Updating learning rate to 1.0317717819561129e-05
train 34129
vs, vt 0.1552984718233347 0.15275505603187614
need align? ->  True 0.1522434944493903
2023-09-01 14:25:41,633 - epoch:25, training loss:1.0631 validation loss:0.1553
Updating learning rate to 6.686805705792195e-06
Updating learning rate to 6.686805705792195e-06
train 34129
vs, vt 0.15468228724267746 0.15218397817677923
need align? ->  True 0.15218397817677923
2023-09-01 14:29:00,007 - epoch:26, training loss:1.0616 validation loss:0.1547
Updating learning rate to 3.79699777713878e-06
Updating learning rate to 3.79699777713878e-06
train 34129
vs, vt 0.1547330723454555 0.1521316834208038
need align? ->  True 0.1521316834208038
2023-09-01 14:32:13,983 - epoch:27, training loss:1.0629 validation loss:0.1547
Updating learning rate to 1.6977394484662593e-06
Updating learning rate to 1.6977394484662593e-06
train 34129
vs, vt 0.15471480389436085 0.1520420327782631
need align? ->  True 0.1520420327782631
2023-09-01 14:35:27,230 - epoch:28, training loss:1.0636 validation loss:0.1547
Updating learning rate to 4.2494961180259127e-07
Updating learning rate to 4.2494961180259127e-07
train 34129
vs, vt 0.1549459377096759 0.15217018276453018
need align? ->  True 0.1520420327782631
2023-09-01 14:38:58,139 - epoch:29, training loss:1.0653 validation loss:0.1549
Updating learning rate to 4.0605413710007e-10
Updating learning rate to 4.0605413710007e-10
check exp/ECL-PatchTST2023-09-01-13:00:57.746378/0/0.1544_epoch_19.pkl  &  0.1520420327782631
2023-09-01 14:39:29,967 - [*] loss:0.2952
2023-09-01 14:39:29,978 - [*] phase 0, testing
2023-09-01 14:39:30,278 - T:96	MAE	0.341696	RMSE	0.296667	MAPE	220.814633
2023-09-01 14:39:30,279 - 96	mae	0.3417	
2023-09-01 14:39:30,279 - 96	rmse	0.2967	
2023-09-01 14:39:30,279 - 96	mape	220.8146	
----*-----
2023-09-01 14:39:59,872 - [*] loss:0.2952
2023-09-01 14:39:59,882 - [*] phase 0, testing
2023-09-01 14:40:00,325 - T:96	MAE	0.341696	RMSE	0.296667	MAPE	220.814633
2023-09-01 14:40:30,738 - [*] loss:0.3137
2023-09-01 14:40:30,749 - [*] phase 0, testing
2023-09-01 14:40:31,163 - T:96	MAE	0.368533	RMSE	0.315199	MAPE	245.258522
2023-09-01 14:41:02,090 - [*] loss:0.3694
2023-09-01 14:41:02,100 - [*] phase 0, testing
2023-09-01 14:41:02,405 - T:96	MAE	0.401206	RMSE	0.371103	MAPE	269.869232
2023-09-01 14:41:36,740 - [*] loss:0.3039
2023-09-01 14:41:36,751 - [*] phase 0, testing
2023-09-01 14:41:37,074 - T:96	MAE	0.354300	RMSE	0.305483	MAPE	239.745712
2023-09-01 14:42:12,820 - [*] loss:0.3695
2023-09-01 14:42:12,831 - [*] phase 0, testing
2023-09-01 14:42:13,077 - T:96	MAE	0.410147	RMSE	0.371201	MAPE	240.511894
2023-09-01 14:42:44,148 - [*] loss:0.3349
2023-09-01 14:42:44,159 - [*] phase 0, testing
2023-09-01 14:42:44,509 - T:96	MAE	0.378920	RMSE	0.336416	MAPE	208.162045
2023-09-01 14:43:13,188 - [*] loss:0.3007
2023-09-01 14:43:13,198 - [*] phase 0, testing
2023-09-01 14:43:13,380 - T:96	MAE	0.350434	RMSE	0.302108	MAPE	228.976583
2023-09-01 14:43:37,405 - [*] loss:0.3057
2023-09-01 14:43:37,415 - [*] phase 0, testing
2023-09-01 14:43:37,640 - T:96	MAE	0.360171	RMSE	0.307093	MAPE	210.859680
----*-----
2023-09-01 14:43:56,438 - [*] loss:0.3026
2023-09-01 14:43:56,449 - [*] phase 0, testing
2023-09-01 14:43:56,633 - T:96	MAE	0.356224	RMSE	0.303627	MAPE	208.957887
2023-09-01 14:44:27,549 - [*] loss:0.3107
2023-09-01 14:44:27,560 - [*] phase 0, testing
2023-09-01 14:44:27,753 - T:96	MAE	0.356181	RMSE	0.312545	MAPE	200.758338
2023-09-01 14:44:48,697 - [*] loss:0.3002
2023-09-01 14:44:48,708 - [*] phase 0, testing
2023-09-01 14:44:48,887 - T:96	MAE	0.346788	RMSE	0.301243	MAPE	203.194594
2023-09-01 14:44:48,888 - 96	mae	0.3468	
2023-09-01 14:44:48,888 - 96	rmse	0.3012	
2023-09-01 14:44:48,888 - 96	mape	203.1946	
2023-09-01 14:44:51,210 - logger name:exp/ECL-PatchTST2023-09-01-14:44:51.209974/ECL-PatchTST.log
2023-09-01 14:44:51,210 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.2, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-01-14:44:51.209974', 'path': 'exp/ECL-PatchTST2023-09-01-14:44:51.209974', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-01 14:44:51,211 - [*] phase 0 start training
0 69680
train 33889
val 11185
test 11185
2023-09-01 14:44:52,076 - [*] phase 0 Dataset load!
2023-09-01 14:44:53,140 - [*] phase 0 Training start
train 33889
2023-09-01 14:46:22,745 - epoch:0, training loss:0.2070 validation loss:0.2601
train 33889
vs, vt 0.26012083583257417 0.2633179491809146
Updating learning rate to 1.0438721218484657e-05
Updating learning rate to 1.0438721218484657e-05
train 33889
vs, vt 0.2545993732796474 0.25409903144463897
need align? ->  True 0.25409903144463897
2023-09-01 14:50:25,452 - epoch:1, training loss:9.2577 validation loss:0.2546
Updating learning rate to 2.802750441854847e-05
Updating learning rate to 2.802750441854847e-05
train 33889
vs, vt 0.25080741400068457 0.25206348807974294
need align? ->  False 0.25206348807974294
2023-09-01 14:53:37,972 - epoch:2, training loss:3.2682 validation loss:0.2508
Updating learning rate to 5.2047629950292354e-05
Updating learning rate to 5.2047629950292354e-05
train 33889
vs, vt 0.2503808322362602 0.25442164844240656
need align? ->  False 0.25206348807974294
2023-09-01 14:56:50,852 - epoch:3, training loss:2.3126 validation loss:0.2504
Updating learning rate to 7.605497731655361e-05
Updating learning rate to 7.605497731655361e-05
train 33889
vs, vt 0.2520831416530365 0.25153218379074876
need align? ->  True 0.25153218379074876
2023-09-01 15:00:06,036 - epoch:4, training loss:1.9425 validation loss:0.2521
Updating learning rate to 9.3608854147054e-05
Updating learning rate to 9.3608854147054e-05
train 33889
vs, vt 0.24761416856199503 0.2508649505590173
need align? ->  False 0.2508649505590173
2023-09-01 15:03:22,591 - epoch:5, training loss:1.6324 validation loss:0.2476
Updating learning rate to 9.99999938537861e-05
Updating learning rate to 9.99999938537861e-05
train 33889
vs, vt 0.24724181745709342 0.24930117799985138
need align? ->  False 0.24930117799985138
2023-09-01 15:06:37,215 - epoch:6, training loss:1.4919 validation loss:0.2472
Updating learning rate to 9.956900274488073e-05
Updating learning rate to 9.956900274488073e-05
train 33889
vs, vt 0.24876498227769678 0.25063477926464245
need align? ->  False 0.24930117799985138
2023-09-01 15:09:50,866 - epoch:7, training loss:1.3436 validation loss:0.2488
Updating learning rate to 9.828987567794199e-05
Updating learning rate to 9.828987567794199e-05
train 33889
vs, vt 0.24798502688380805 0.25179290369322355
need align? ->  False 0.24930117799985138
2023-09-01 15:13:04,287 - epoch:8, training loss:1.2486 validation loss:0.2480
Updating learning rate to 9.618449887172618e-05
Updating learning rate to 9.618449887172618e-05
train 33889
vs, vt 0.24739334241233088 0.2507557629776949
need align? ->  False 0.24930117799985138
2023-09-01 15:16:17,998 - epoch:9, training loss:1.1955 validation loss:0.2474
Updating learning rate to 9.328889590710838e-05
Updating learning rate to 9.328889590710838e-05
train 33889
vs, vt 0.24569099701263689 0.2519043779677965
need align? ->  False 0.24930117799985138
2023-09-01 15:19:35,524 - epoch:10, training loss:1.1586 validation loss:0.2457
Updating learning rate to 8.965261135362604e-05
Updating learning rate to 8.965261135362604e-05
train 33889
vs, vt 0.24619926194745032 0.25101747749034653
need align? ->  False 0.24930117799985138
2023-09-01 15:23:01,942 - epoch:11, training loss:1.1296 validation loss:0.2462
Updating learning rate to 8.533786304815776e-05
Updating learning rate to 8.533786304815776e-05
train 33889
vs, vt 0.24614624044095929 0.2518410945565186
need align? ->  False 0.24930117799985138
2023-09-01 15:26:17,318 - epoch:12, training loss:1.1055 validation loss:0.2461
Updating learning rate to 8.041847753048434e-05
Updating learning rate to 8.041847753048434e-05
train 33889
vs, vt 0.24446460934863848 0.25098896509205754
need align? ->  False 0.24930117799985138
2023-09-01 15:29:32,595 - epoch:13, training loss:1.0866 validation loss:0.2445
Updating learning rate to 7.497862685072454e-05
Updating learning rate to 7.497862685072454e-05
train 33889
vs, vt 0.24274766957387328 0.2496730947274376
need align? ->  False 0.24930117799985138
2023-09-01 15:32:47,644 - epoch:14, training loss:1.0720 validation loss:0.2427
Updating learning rate to 6.911138836222055e-05
Updating learning rate to 6.911138836222055e-05
train 33889
vs, vt 0.24492566918276928 0.24989931082183664
need align? ->  False 0.24930117799985138
2023-09-01 15:36:03,553 - epoch:15, training loss:1.0578 validation loss:0.2449
Updating learning rate to 6.291715214221653e-05
Updating learning rate to 6.291715214221653e-05
train 33889
vs, vt 0.24323669579726728 0.25022853385995736
need align? ->  False 0.24930117799985138
2023-09-01 15:39:19,731 - epoch:16, training loss:1.0458 validation loss:0.2432
Updating learning rate to 5.6501903289803477e-05
Updating learning rate to 5.6501903289803477e-05
train 33889
vs, vt 0.24458020798523317 0.2499747585759244
need align? ->  False 0.24930117799985138
2023-09-01 15:42:43,224 - epoch:17, training loss:1.0371 validation loss:0.2446
Updating learning rate to 4.997540849148917e-05
Updating learning rate to 4.997540849148917e-05
train 33889
vs, vt 0.24521430806172165 0.24895387011664835
need align? ->  False 0.24895387011664835
2023-09-01 15:45:58,111 - epoch:18, training loss:1.0276 validation loss:0.2452
Updating learning rate to 4.344933788275899e-05
Updating learning rate to 4.344933788275899e-05
train 33889
vs, vt 0.25291578556326305 0.2510302505371245
need align? ->  True 0.24895387011664835
2023-09-01 15:49:15,952 - epoch:19, training loss:1.7667 validation loss:0.2529
Updating learning rate to 3.703535434109692e-05
Updating learning rate to 3.703535434109692e-05
train 33889
vs, vt 0.25052225788716564 0.25125344669107685
need align? ->  True 0.24895387011664835
2023-09-01 15:52:31,987 - epoch:20, training loss:1.5268 validation loss:0.2505
Updating learning rate to 3.084320290319299e-05
Updating learning rate to 3.084320290319299e-05
train 33889
vs, vt 0.24742827107283202 0.2508754985068332
need align? ->  False 0.24895387011664835
2023-09-01 15:55:45,005 - epoch:21, training loss:1.4332 validation loss:0.2474
Updating learning rate to 2.4978832996938436e-05
Updating learning rate to 2.4978832996938436e-05
train 33889
vs, vt 0.24708782221105965 0.2494859339838678
need align? ->  False 0.24895387011664835
2023-09-01 15:58:58,466 - epoch:22, training loss:1.3519 validation loss:0.2471
Updating learning rate to 1.954258561733979e-05
Updating learning rate to 1.954258561733979e-05
train 33889
vs, vt 0.24691125475378198 0.24989209531552412
need align? ->  False 0.24895387011664835
2023-09-01 16:02:14,360 - epoch:23, training loss:1.3243 validation loss:0.2469
Updating learning rate to 1.4627476464274535e-05
Updating learning rate to 1.4627476464274535e-05
train 33889
vs, vt 0.24730028872462836 0.2498071675019508
need align? ->  False 0.24895387011664835
2023-09-01 16:05:30,000 - epoch:24, training loss:1.2978 validation loss:0.2473
Updating learning rate to 1.0317604418077296e-05
Updating learning rate to 1.0317604418077296e-05
train 33889
vs, vt 0.24645557796413248 0.25001502879471943
need align? ->  False 0.24895387011664835
2023-09-01 16:09:03,081 - epoch:25, training loss:1.2807 validation loss:0.2465
check exp/ECL-PatchTST2023-09-01-14:44:51.209974/0/0.2427_epoch_14.pkl  &  0.24895387011664835
2023-09-01 16:09:33,708 - [*] loss:0.3795
2023-09-01 16:09:33,750 - [*] phase 0, testing
2023-09-01 16:09:34,883 - T:336	MAE	0.388886	RMSE	0.379253	MAPE	235.649705
2023-09-01 16:09:34,884 - 336	mae	0.3889	
2023-09-01 16:09:34,884 - 336	rmse	0.3793	
2023-09-01 16:09:34,884 - 336	mape	235.6497	
----*-----
2023-09-01 16:10:04,576 - [*] loss:0.3795
2023-09-01 16:10:04,638 - [*] phase 0, testing
2023-09-01 16:10:06,111 - T:336	MAE	0.388886	RMSE	0.379253	MAPE	235.649705
2023-09-01 16:10:36,918 - [*] loss:0.3878
2023-09-01 16:10:36,970 - [*] phase 0, testing
2023-09-01 16:10:38,482 - T:336	MAE	0.400228	RMSE	0.387488	MAPE	244.211340
2023-09-01 16:11:08,165 - [*] loss:0.4077
2023-09-01 16:11:08,208 - [*] phase 0, testing
2023-09-01 16:11:09,422 - T:336	MAE	0.413372	RMSE	0.407564	MAPE	263.803315
2023-09-01 16:11:39,713 - [*] loss:0.3855
2023-09-01 16:11:39,755 - [*] phase 0, testing
2023-09-01 16:11:40,369 - T:336	MAE	0.396447	RMSE	0.385304	MAPE	252.839184
2023-09-01 16:12:07,012 - [*] loss:0.4401
2023-09-01 16:12:07,053 - [*] phase 0, testing
2023-09-01 16:12:07,701 - T:336	MAE	0.443268	RMSE	0.439721	MAPE	245.146513
2023-09-01 16:12:36,751 - [*] loss:0.4170
2023-09-01 16:12:36,795 - [*] phase 0, testing
2023-09-01 16:12:37,438 - T:336	MAE	0.422169	RMSE	0.416522	MAPE	211.285019
2023-09-01 16:13:07,894 - [*] loss:0.3818
2023-09-01 16:13:07,938 - [*] phase 0, testing
2023-09-01 16:13:08,637 - T:336	MAE	0.392305	RMSE	0.381497	MAPE	238.173842
2023-09-01 16:13:35,210 - [*] loss:0.3791
2023-09-01 16:13:35,254 - [*] phase 0, testing
2023-09-01 16:13:35,872 - T:336	MAE	0.394157	RMSE	0.378673	MAPE	218.288946
----*-----
2023-09-01 16:13:53,860 - [*] loss:0.3710
2023-09-01 16:13:53,908 - [*] phase 0, testing
2023-09-01 16:13:54,505 - T:336	MAE	0.391473	RMSE	0.370592	MAPE	219.552827
2023-09-01 16:14:21,417 - [*] loss:0.4397
2023-09-01 16:14:21,461 - [*] phase 0, testing
2023-09-01 16:14:22,121 - T:336	MAE	0.430696	RMSE	0.439804	MAPE	248.332453
2023-09-01 16:14:38,652 - [*] loss:0.3987
2023-09-01 16:14:38,693 - [*] phase 0, testing
2023-09-01 16:14:39,298 - T:336	MAE	0.401032	RMSE	0.398768	MAPE	230.602169
2023-09-01 16:14:39,299 - 336	mae	0.4010	
2023-09-01 16:14:39,299 - 336	rmse	0.3988	
2023-09-01 16:14:39,299 - 336	mape	230.6022	
2023-09-01 16:14:41,452 - logger name:exp/ECL-PatchTST2023-09-01-16:14:41.449238/ECL-PatchTST.log
2023-09-01 16:14:41,452 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-01-16:14:41.449238', 'path': 'exp/ECL-PatchTST2023-09-01-16:14:41.449238', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-01 16:14:41,452 - [*] phase 0 start training
0 69680
train 34129
val 11425
test 11425
2023-09-01 16:14:42,249 - [*] phase 0 Dataset load!
2023-09-01 16:14:43,255 - [*] phase 0 Training start
train 34129
2023-09-01 16:16:08,048 - epoch:0, training loss:0.1869 validation loss:0.1809
train 34129
vs, vt 0.18094490004910363 0.1855959395981497
Updating learning rate to 1.0438661460315324e-05
Updating learning rate to 1.0438661460315324e-05
train 34129
vs, vt 0.167312702909112 0.16886687564353148
need align? ->  False 0.16886687564353148
2023-09-01 16:20:08,480 - epoch:1, training loss:9.7058 validation loss:0.1673
Updating learning rate to 2.8027297449571736e-05
Updating learning rate to 2.8027297449571736e-05
train 34129
vs, vt 0.16599126015272406 0.16614827178418637
need align? ->  False 0.16614827178418637
2023-09-01 16:23:14,392 - epoch:2, training loss:3.3590 validation loss:0.1660
Updating learning rate to 5.204727160595503e-05
Updating learning rate to 5.204727160595503e-05
train 34129
vs, vt 0.16284682398868933 0.16580587406125333
need align? ->  False 0.16580587406125333
2023-09-01 16:26:14,428 - epoch:3, training loss:2.3170 validation loss:0.1628
Updating learning rate to 7.605456385119542e-05
Updating learning rate to 7.605456385119542e-05
train 34129
vs, vt 0.16388445600039428 0.16426396614147556
need align? ->  False 0.16426396614147556
2023-09-01 16:29:15,211 - epoch:4, training loss:1.9029 validation loss:0.1639
Updating learning rate to 9.360855637921138e-05
Updating learning rate to 9.360855637921138e-05
train 34129
vs, vt 0.16303311329748896 0.16225265846070316
need align? ->  True 0.16225265846070316
2023-09-01 16:32:16,859 - epoch:5, training loss:1.6226 validation loss:0.1630
Updating learning rate to 9.99999939458629e-05
Updating learning rate to 9.99999939458629e-05
train 34129
vs, vt 0.1610766870694028 0.16043645431184106
need align? ->  True 0.16043645431184106
2023-09-01 16:35:18,008 - epoch:6, training loss:1.4357 validation loss:0.1611
Updating learning rate to 9.956902716655286e-05
Updating learning rate to 9.956902716655286e-05
train 34129
vs, vt 0.15930325955980354 0.15939795362452666
need align? ->  False 0.15939795362452666
2023-09-01 16:38:18,906 - epoch:7, training loss:1.2987 validation loss:0.1593
Updating learning rate to 9.828992401134782e-05
Updating learning rate to 9.828992401134782e-05
train 34129
vs, vt 0.15799903753730987 0.16018407675955032
need align? ->  False 0.15939795362452666
2023-09-01 16:41:21,369 - epoch:8, training loss:1.2236 validation loss:0.1580
Updating learning rate to 9.618457028986775e-05
Updating learning rate to 9.618457028986775e-05
train 34129
vs, vt 0.15780009172028966 0.15864246139923732
need align? ->  False 0.15864246139923732
2023-09-01 16:44:22,618 - epoch:9, training loss:1.1598 validation loss:0.1578
Updating learning rate to 9.32889891880015e-05
Updating learning rate to 9.32889891880015e-05
train 34129
vs, vt 0.15674339189297623 0.1585098698321316
need align? ->  False 0.1585098698321316
2023-09-01 16:47:23,837 - epoch:10, training loss:1.1452 validation loss:0.1567
Updating learning rate to 8.965272490120875e-05
Updating learning rate to 8.965272490120875e-05
train 34129
vs, vt 0.15714118373062874 0.15684077553451062
need align? ->  True 0.15684077553451062
2023-09-01 16:50:26,509 - epoch:11, training loss:1.1155 validation loss:0.1571
Updating learning rate to 8.533799491959945e-05
Updating learning rate to 8.533799491959945e-05
train 34129
vs, vt 0.1573519464996126 0.15598202223579088
need align? ->  True 0.15598202223579088
2023-09-01 16:53:30,028 - epoch:12, training loss:1.0909 validation loss:0.1574
Updating learning rate to 8.041862546942808e-05
Updating learning rate to 8.041862546942808e-05
train 34129
vs, vt 0.15963411981032954 0.1581433042469952
need align? ->  True 0.15598202223579088
2023-09-01 16:56:35,490 - epoch:13, training loss:1.0750 validation loss:0.1596
Updating learning rate to 7.497878832589398e-05
Updating learning rate to 7.497878832589398e-05
train 34129
vs, vt 0.15598708238038753 0.15579210685359107
need align? ->  True 0.15579210685359107
2023-09-01 16:59:40,161 - epoch:14, training loss:1.0512 validation loss:0.1560
Updating learning rate to 6.911156061073078e-05
Updating learning rate to 6.911156061073078e-05
train 34129
vs, vt 0.1561683963984251 0.15533968773153092
need align? ->  True 0.15533968773153092
2023-09-01 17:02:47,200 - epoch:15, training loss:1.0488 validation loss:0.1562
Updating learning rate to 6.291733221684778e-05
Updating learning rate to 6.291733221684778e-05
train 34129
vs, vt 0.15619419088794126 0.15487157781091002
need align? ->  True 0.15487157781091002
2023-09-01 17:05:55,194 - epoch:16, training loss:1.0390 validation loss:0.1562
Updating learning rate to 5.650208810942889e-05
Updating learning rate to 5.650208810942889e-05
train 34129
vs, vt 0.15607582852244378 0.15545103578931754
need align? ->  True 0.15487157781091002
2023-09-01 17:09:01,821 - epoch:17, training loss:1.0449 validation loss:0.1561
Updating learning rate to 4.9975594893793686e-05
Updating learning rate to 4.9975594893793686e-05
train 34129
vs, vt 0.15438337160481347 0.155432897226678
need align? ->  False 0.15487157781091002
2023-09-01 17:12:11,128 - epoch:18, training loss:1.0339 validation loss:0.1544
Updating learning rate to 4.3449522678347527e-05
Updating learning rate to 4.3449522678347527e-05
train 34129
vs, vt 0.15667373169627455 0.1535181663930416
need align? ->  True 0.1535181663930416
2023-09-01 17:15:19,156 - epoch:19, training loss:1.0250 validation loss:0.1567
Updating learning rate to 3.7035534368065714e-05
Updating learning rate to 3.7035534368065714e-05
train 34129
vs, vt 0.15428694296214315 0.15447804753979047
need align? ->  True 0.1535181663930416
2023-09-01 17:18:29,014 - epoch:20, training loss:1.0366 validation loss:0.1543
Updating learning rate to 3.084337508123067e-05
Updating learning rate to 3.084337508123067e-05
train 34129
vs, vt 0.1539058718416426 0.15404492807057169
need align? ->  True 0.1535181663930416
2023-09-01 17:21:35,842 - epoch:21, training loss:1.0299 validation loss:0.1539
Updating learning rate to 2.497899438003108e-05
Updating learning rate to 2.497899438003108e-05
train 34129
vs, vt 0.15543234766357475 0.15322866228719553
need align? ->  True 0.15322866228719553
2023-09-01 17:24:41,227 - epoch:22, training loss:1.0244 validation loss:0.1554
Updating learning rate to 1.9542733444177923e-05
Updating learning rate to 1.9542733444177923e-05
train 34129
vs, vt 0.15418409647213088 0.15373574044141505
need align? ->  True 0.15322866228719553
2023-09-01 17:27:47,415 - epoch:23, training loss:1.0383 validation loss:0.1542
Updating learning rate to 1.4627608205499963e-05
Updating learning rate to 1.4627608205499963e-05
train 34129
vs, vt 0.15429525723059972 0.15316486640108956
need align? ->  True 0.15316486640108956
2023-09-01 17:30:51,618 - epoch:24, training loss:1.0362 validation loss:0.1543
Updating learning rate to 1.0317717819561129e-05
Updating learning rate to 1.0317717819561129e-05
train 34129
vs, vt 0.15403890949156548 0.15310251952873336
need align? ->  True 0.15310251952873336
2023-09-01 17:33:54,860 - epoch:25, training loss:1.0368 validation loss:0.1540
Updating learning rate to 6.686805705792195e-06
Updating learning rate to 6.686805705792195e-06
train 34129
vs, vt 0.15439074428545105 0.15317837273081145
need align? ->  True 0.15310251952873336
2023-09-01 17:37:11,096 - epoch:26, training loss:1.0368 validation loss:0.1544
Updating learning rate to 3.79699777713878e-06
Updating learning rate to 3.79699777713878e-06
train 34129
vs, vt 0.1542190742989381 0.15328162250419458
need align? ->  True 0.15310251952873336
2023-09-01 17:40:16,750 - epoch:27, training loss:1.0353 validation loss:0.1542
Updating learning rate to 1.6977394484662593e-06
Updating learning rate to 1.6977394484662593e-06
train 34129
vs, vt 0.1541724021236102 0.1531923770904541
need align? ->  True 0.15310251952873336
2023-09-01 17:43:18,664 - epoch:28, training loss:1.0352 validation loss:0.1542
Updating learning rate to 4.2494961180259127e-07
Updating learning rate to 4.2494961180259127e-07
train 34129
vs, vt 0.15421437463826604 0.15317086051735612
need align? ->  True 0.15310251952873336
2023-09-01 17:46:21,860 - epoch:29, training loss:1.0348 validation loss:0.1542
Updating learning rate to 4.0605413710007e-10
Updating learning rate to 4.0605413710007e-10
check exp/ECL-PatchTST2023-09-01-16:14:41.449238/0/0.1539_epoch_21.pkl  &  0.15310251952873336
2023-09-01 17:46:44,917 - [*] loss:0.2910
2023-09-01 17:46:44,927 - [*] phase 0, testing
2023-09-01 17:46:45,109 - T:96	MAE	0.336800	RMSE	0.292653	MAPE	213.264179
2023-09-01 17:46:45,110 - 96	mae	0.3368	
2023-09-01 17:46:45,110 - 96	rmse	0.2927	
2023-09-01 17:46:45,110 - 96	mape	213.2642	
----*-----
2023-09-01 17:47:15,515 - [*] loss:0.2910
2023-09-01 17:47:15,525 - [*] phase 0, testing
2023-09-01 17:47:15,709 - T:96	MAE	0.336800	RMSE	0.292653	MAPE	213.264179
2023-09-01 17:47:45,002 - [*] loss:0.3101
2023-09-01 17:47:45,012 - [*] phase 0, testing
2023-09-01 17:47:45,198 - T:96	MAE	0.362913	RMSE	0.311831	MAPE	235.414743
2023-09-01 17:48:15,090 - [*] loss:0.3518
2023-09-01 17:48:15,099 - [*] phase 0, testing
2023-09-01 17:48:15,277 - T:96	MAE	0.386592	RMSE	0.353742	MAPE	246.847582
2023-09-01 17:48:47,987 - [*] loss:0.2953
2023-09-01 17:48:47,997 - [*] phase 0, testing
2023-09-01 17:48:48,180 - T:96	MAE	0.342837	RMSE	0.297110	MAPE	224.734640
2023-09-01 17:49:22,262 - [*] loss:0.3616
2023-09-01 17:49:22,276 - [*] phase 0, testing
2023-09-01 17:49:22,481 - T:96	MAE	0.403007	RMSE	0.363432	MAPE	229.287720
2023-09-01 17:49:53,553 - [*] loss:0.3382
2023-09-01 17:49:53,563 - [*] phase 0, testing
2023-09-01 17:49:53,760 - T:96	MAE	0.378778	RMSE	0.339786	MAPE	201.038504
2023-09-01 17:50:24,916 - [*] loss:0.2968
2023-09-01 17:50:24,926 - [*] phase 0, testing
2023-09-01 17:50:25,106 - T:96	MAE	0.345450	RMSE	0.298456	MAPE	220.820880
2023-09-01 17:50:55,762 - [*] loss:0.3028
2023-09-01 17:50:55,773 - [*] phase 0, testing
2023-09-01 17:50:55,957 - T:96	MAE	0.354707	RMSE	0.304328	MAPE	201.991296
----*-----
2023-09-01 17:51:15,862 - [*] loss:0.2999
2023-09-01 17:51:15,872 - [*] phase 0, testing
2023-09-01 17:51:16,052 - T:96	MAE	0.352580	RMSE	0.300630	MAPE	204.647923
2023-09-01 17:51:46,932 - [*] loss:0.3088
2023-09-01 17:51:46,942 - [*] phase 0, testing
2023-09-01 17:51:47,124 - T:96	MAE	0.351676	RMSE	0.310759	MAPE	193.218970
2023-09-01 17:52:08,438 - [*] loss:0.3022
2023-09-01 17:52:08,449 - [*] phase 0, testing
2023-09-01 17:52:08,630 - T:96	MAE	0.346733	RMSE	0.303248	MAPE	201.748323
2023-09-01 17:52:08,633 - 96	mae	0.3467	
2023-09-01 17:52:08,633 - 96	rmse	0.3032	
2023-09-01 17:52:08,633 - 96	mape	201.7483	
2023-09-01 17:52:10,898 - logger name:exp/ECL-PatchTST2023-09-01-17:52:10.897968/ECL-PatchTST.log
2023-09-01 17:52:10,898 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-01-17:52:10.897968', 'path': 'exp/ECL-PatchTST2023-09-01-17:52:10.897968', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-01 17:52:10,899 - [*] phase 0 start training
0 69680
train 33889
val 11185
test 11185
2023-09-01 17:52:11,897 - [*] phase 0 Dataset load!
2023-09-01 17:52:12,953 - [*] phase 0 Training start
train 33889
2023-09-01 17:53:53,976 - epoch:0, training loss:0.2094 validation loss:0.2613
train 33889
vs, vt 0.26132756704464555 0.2648998451080512
Updating learning rate to 1.0438721218484657e-05
Updating learning rate to 1.0438721218484657e-05
train 33889
vs, vt 0.2542323484932157 0.2540789655494419
need align? ->  True 0.2540789655494419
2023-09-01 17:57:48,383 - epoch:1, training loss:9.4943 validation loss:0.2542
Updating learning rate to 2.802750441854847e-05
Updating learning rate to 2.802750441854847e-05
train 33889
vs, vt 0.24878204918720506 0.25184583393010224
need align? ->  False 0.25184583393010224
2023-09-01 18:00:51,313 - epoch:2, training loss:3.2693 validation loss:0.2488
Updating learning rate to 5.2047629950292354e-05
Updating learning rate to 5.2047629950292354e-05
train 33889
vs, vt 0.25275727251375263 0.25180746733464976
need align? ->  True 0.25180746733464976
2023-09-01 18:03:53,413 - epoch:3, training loss:2.3071 validation loss:0.2528
Updating learning rate to 7.605497731655361e-05
Updating learning rate to 7.605497731655361e-05
train 33889
vs, vt 0.2561431018296968 0.2523823362267153
need align? ->  True 0.25180746733464976
2023-09-01 18:06:54,719 - epoch:4, training loss:1.8867 validation loss:0.2561
Updating learning rate to 9.3608854147054e-05
Updating learning rate to 9.3608854147054e-05
train 33889
vs, vt 0.251533269924535 0.25585141367363656
need align? ->  False 0.25180746733464976
2023-09-01 18:09:56,708 - epoch:5, training loss:1.6430 validation loss:0.2515
Updating learning rate to 9.99999938537861e-05
Updating learning rate to 9.99999938537861e-05
train 33889
vs, vt 0.24847302945669403 0.2528290970843624
need align? ->  False 0.25180746733464976
2023-09-01 18:13:02,896 - epoch:6, training loss:1.4996 validation loss:0.2485
Updating learning rate to 9.956900274488073e-05
Updating learning rate to 9.956900274488073e-05
train 33889
vs, vt 0.2524622995406389 0.25287520373240113
need align? ->  True 0.25180746733464976
2023-09-01 18:16:08,014 - epoch:7, training loss:1.3943 validation loss:0.2525
Updating learning rate to 9.828987567794199e-05
Updating learning rate to 9.828987567794199e-05
train 33889
vs, vt 0.2532026040452448 0.2548730712045323
need align? ->  True 0.25180746733464976
2023-09-01 18:19:13,299 - epoch:8, training loss:1.3183 validation loss:0.2532
Updating learning rate to 9.618449887172618e-05
Updating learning rate to 9.618449887172618e-05
train 33889
vs, vt 0.25128075590526516 0.25501428078860044
need align? ->  False 0.25180746733464976
2023-09-01 18:22:20,922 - epoch:9, training loss:1.2662 validation loss:0.2513
Updating learning rate to 9.328889590710838e-05
Updating learning rate to 9.328889590710838e-05
train 33889
vs, vt 0.25216475438157265 0.25446813947267155
need align? ->  True 0.25180746733464976
2023-09-01 18:25:29,686 - epoch:10, training loss:1.2289 validation loss:0.2522
Updating learning rate to 8.965261135362604e-05
Updating learning rate to 8.965261135362604e-05
train 33889
vs, vt 0.24962191127071326 0.25659413330934266
need align? ->  False 0.25180746733464976
2023-09-01 18:28:37,754 - epoch:11, training loss:1.1996 validation loss:0.2496
Updating learning rate to 8.533786304815776e-05
Updating learning rate to 8.533786304815776e-05
train 33889
vs, vt 0.2494559349830855 0.2547677517021922
need align? ->  False 0.25180746733464976
2023-09-01 18:31:46,777 - epoch:12, training loss:1.1759 validation loss:0.2495
Updating learning rate to 8.041847753048434e-05
Updating learning rate to 8.041847753048434e-05
train 33889
vs, vt 0.24808789167383855 0.25377240125089884
need align? ->  False 0.25180746733464976
2023-09-01 18:34:57,204 - epoch:13, training loss:1.1570 validation loss:0.2481
Updating learning rate to 7.497862685072454e-05
Updating learning rate to 7.497862685072454e-05
train 33889
vs, vt 0.2492501920258457 0.2540654077787291
need align? ->  False 0.25180746733464976
2023-09-01 18:38:08,434 - epoch:14, training loss:1.1394 validation loss:0.2493
Updating learning rate to 6.911138836222055e-05
Updating learning rate to 6.911138836222055e-05
train 33889
vs, vt 0.2492562331767245 0.25408616137098183
need align? ->  False 0.25180746733464976
2023-09-01 18:41:17,017 - epoch:15, training loss:1.1253 validation loss:0.2493
Updating learning rate to 6.291715214221653e-05
Updating learning rate to 6.291715214221653e-05
train 33889
vs, vt 0.2477879145775329 0.25586617399345746
need align? ->  False 0.25180746733464976
2023-09-01 18:44:24,701 - epoch:16, training loss:1.1130 validation loss:0.2478
Updating learning rate to 5.6501903289803477e-05
Updating learning rate to 5.6501903289803477e-05
train 33889
vs, vt 0.24607684962790122 0.2545500818893991
need align? ->  False 0.25180746733464976
2023-09-01 18:47:30,260 - epoch:17, training loss:1.1039 validation loss:0.2461
Updating learning rate to 4.997540849148917e-05
Updating learning rate to 4.997540849148917e-05
train 33889
vs, vt 0.24846405193040316 0.25412206377156754
need align? ->  False 0.25180746733464976
2023-09-01 18:50:36,338 - epoch:18, training loss:1.0974 validation loss:0.2485
Updating learning rate to 4.344933788275899e-05
Updating learning rate to 4.344933788275899e-05
train 33889
vs, vt 0.24790055956691504 0.2541600683365356
need align? ->  False 0.25180746733464976
2023-09-01 18:53:43,619 - epoch:19, training loss:1.0901 validation loss:0.2479
Updating learning rate to 3.703535434109692e-05
Updating learning rate to 3.703535434109692e-05
train 33889
vs, vt 0.24754717941819268 0.254286350089718
need align? ->  False 0.25180746733464976
2023-09-01 18:56:48,183 - epoch:20, training loss:1.0840 validation loss:0.2475
Updating learning rate to 3.084320290319299e-05
Updating learning rate to 3.084320290319299e-05
train 33889
vs, vt 0.24734204067763957 0.25444765702228656
need align? ->  False 0.25180746733464976
2023-09-01 18:59:50,478 - epoch:21, training loss:1.0796 validation loss:0.2473
Updating learning rate to 2.4978832996938436e-05
Updating learning rate to 2.4978832996938436e-05
train 33889
vs, vt 0.24689113153991374 0.2543152899227359
need align? ->  False 0.25180746733464976
2023-09-01 19:02:54,447 - epoch:22, training loss:1.0763 validation loss:0.2469
Updating learning rate to 1.954258561733979e-05
Updating learning rate to 1.954258561733979e-05
train 33889
vs, vt 0.24747142318466847 0.25421586564996024
need align? ->  False 0.25180746733464976
2023-09-01 19:05:58,554 - epoch:23, training loss:1.0731 validation loss:0.2475
Updating learning rate to 1.4627476464274535e-05
Updating learning rate to 1.4627476464274535e-05
train 33889
vs, vt 0.24743043140254237 0.2541381488130851
need align? ->  False 0.25180746733464976
2023-09-01 19:09:02,637 - epoch:24, training loss:1.0709 validation loss:0.2474
Updating learning rate to 1.0317604418077296e-05
Updating learning rate to 1.0317604418077296e-05
train 33889
vs, vt 0.2468639062717557 0.253968403716995
need align? ->  False 0.25180746733464976
2023-09-01 19:12:04,757 - epoch:25, training loss:1.0690 validation loss:0.2469
Updating learning rate to 6.686712584380782e-06
Updating learning rate to 6.686712584380782e-06
train 33889
vs, vt 0.2468670380979099 0.25358851584182546
need align? ->  False 0.25180746733464976
2023-09-01 19:15:09,090 - epoch:26, training loss:1.0680 validation loss:0.2469
Updating learning rate to 3.7969265291329562e-06
Updating learning rate to 3.7969265291329562e-06
train 33889
vs, vt 0.24619113802063194 0.2536677014167336
need align? ->  False 0.25180746733464976
2023-09-01 19:18:32,154 - epoch:27, training loss:1.0670 validation loss:0.2462
Updating learning rate to 1.6976912929391648e-06
Updating learning rate to 1.6976912929391648e-06
train 33889
vs, vt 0.24634985718876123 0.2534683517773043
need align? ->  False 0.25180746733464976
2023-09-01 19:21:58,071 - epoch:28, training loss:1.0667 validation loss:0.2463
check exp/ECL-PatchTST2023-09-01-17:52:10.897968/0/0.2461_epoch_17.pkl  &  0.25180746733464976
2023-09-01 19:22:20,658 - [*] loss:0.3850
2023-09-01 19:22:20,693 - [*] phase 0, testing
2023-09-01 19:22:21,265 - T:336	MAE	0.391699	RMSE	0.384769	MAPE	240.769219
2023-09-01 19:22:21,266 - 336	mae	0.3917	
2023-09-01 19:22:21,266 - 336	rmse	0.3848	
2023-09-01 19:22:21,266 - 336	mape	240.7692	
----*-----
2023-09-01 19:22:46,484 - [*] loss:0.3850
2023-09-01 19:22:46,520 - [*] phase 0, testing
2023-09-01 19:22:47,152 - T:336	MAE	0.391699	RMSE	0.384769	MAPE	240.769219
2023-09-01 19:23:16,691 - [*] loss:0.3947
2023-09-01 19:23:16,725 - [*] phase 0, testing
2023-09-01 19:23:17,338 - T:336	MAE	0.403057	RMSE	0.394358	MAPE	248.432541
2023-09-01 19:23:46,959 - [*] loss:0.4071
2023-09-01 19:23:46,994 - [*] phase 0, testing
2023-09-01 19:23:47,591 - T:336	MAE	0.410839	RMSE	0.406888	MAPE	265.838623
2023-09-01 19:24:13,428 - [*] loss:0.3908
2023-09-01 19:24:13,463 - [*] phase 0, testing
2023-09-01 19:24:14,099 - T:336	MAE	0.398667	RMSE	0.390598	MAPE	257.999444
2023-09-01 19:24:43,747 - [*] loss:0.4558
2023-09-01 19:24:43,783 - [*] phase 0, testing
2023-09-01 19:24:44,416 - T:336	MAE	0.454731	RMSE	0.455335	MAPE	249.101448
2023-09-01 19:25:07,991 - [*] loss:0.4238
2023-09-01 19:25:08,035 - [*] phase 0, testing
2023-09-01 19:25:08,623 - T:336	MAE	0.428251	RMSE	0.423300	MAPE	215.828848
2023-09-01 19:25:31,283 - [*] loss:0.3872
2023-09-01 19:25:31,319 - [*] phase 0, testing
2023-09-01 19:25:31,898 - T:336	MAE	0.394941	RMSE	0.386863	MAPE	242.739749
2023-09-01 19:25:53,993 - [*] loss:0.3821
2023-09-01 19:25:54,029 - [*] phase 0, testing
2023-09-01 19:25:54,642 - T:336	MAE	0.395557	RMSE	0.381631	MAPE	218.587160
----*-----
2023-09-01 19:26:10,525 - [*] loss:0.3728
2023-09-01 19:26:10,559 - [*] phase 0, testing
2023-09-01 19:26:11,150 - T:336	MAE	0.391544	RMSE	0.372311	MAPE	221.666050
2023-09-01 19:26:34,557 - [*] loss:0.4137
2023-09-01 19:26:34,595 - [*] phase 0, testing
2023-09-01 19:26:35,217 - T:336	MAE	0.417004	RMSE	0.413439	MAPE	220.698071
2023-09-01 19:26:54,058 - [*] loss:0.3838
2023-09-01 19:26:54,106 - [*] phase 0, testing
2023-09-01 19:26:54,696 - T:336	MAE	0.398733	RMSE	0.383689	MAPE	223.377204
2023-09-01 19:26:54,697 - 336	mae	0.3987	
2023-09-01 19:26:54,698 - 336	rmse	0.3837	
2023-09-01 19:26:54,698 - 336	mape	223.3772	
2023-09-01 19:26:56,967 - logger name:exp/ECL-PatchTST2023-09-01-19:26:56.967051/ECL-PatchTST.log
2023-09-01 19:26:56,968 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-01-19:26:56.967051', 'path': 'exp/ECL-PatchTST2023-09-01-19:26:56.967051', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-01 19:26:56,968 - [*] phase 0 start training
0 69680
train 34129
val 11425
test 11425
2023-09-01 19:26:57,889 - [*] phase 0 Dataset load!
2023-09-01 19:26:59,028 - [*] phase 0 Training start
train 34129
2023-09-01 19:28:30,918 - epoch:0, training loss:0.1831 validation loss:0.1788
train 34129
vs, vt 0.17875255420804023 0.18399998342825308
Updating learning rate to 1.0438661460315324e-05
Updating learning rate to 1.0438661460315324e-05
train 34129
vs, vt 0.16786289173695776 0.16891605162786114
need align? ->  False 0.16891605162786114
2023-09-01 19:32:33,396 - epoch:1, training loss:9.2597 validation loss:0.1679
Updating learning rate to 2.8027297449571736e-05
Updating learning rate to 2.8027297449571736e-05
train 34129
vs, vt 0.16727419267925953 0.16666474868026043
need align? ->  True 0.16666474868026043
2023-09-01 19:35:48,387 - epoch:2, training loss:3.1051 validation loss:0.1673
Updating learning rate to 5.204727160595503e-05
Updating learning rate to 5.204727160595503e-05
train 34129
vs, vt 0.1658199754026201 0.16718198561833963
need align? ->  False 0.16666474868026043
2023-09-01 19:39:01,661 - epoch:3, training loss:2.1311 validation loss:0.1658
Updating learning rate to 7.605456385119542e-05
Updating learning rate to 7.605456385119542e-05
train 34129
vs, vt 0.16540867073668375 0.16487470443050067
need align? ->  True 0.16487470443050067
2023-09-01 19:42:17,224 - epoch:4, training loss:1.7385 validation loss:0.1654
Updating learning rate to 9.360855637921138e-05
Updating learning rate to 9.360855637921138e-05
train 34129
vs, vt 0.16421012584533956 0.16525541452897918
need align? ->  False 0.16487470443050067
2023-09-01 19:45:34,200 - epoch:5, training loss:1.3881 validation loss:0.1642
Updating learning rate to 9.99999939458629e-05
Updating learning rate to 9.99999939458629e-05
train 34129
vs, vt 0.16420223340392112 0.16214806501650147
need align? ->  True 0.16214806501650147
2023-09-01 19:48:50,535 - epoch:6, training loss:1.2109 validation loss:0.1642
Updating learning rate to 9.956902716655286e-05
Updating learning rate to 9.956902716655286e-05
train 34129
vs, vt 0.16334855573044882 0.16641229656007556
need align? ->  True 0.16214806501650147
2023-09-01 19:52:11,359 - epoch:7, training loss:1.1055 validation loss:0.1633
Updating learning rate to 9.828992401134782e-05
Updating learning rate to 9.828992401134782e-05
train 34129
vs, vt 0.15747192824880282 0.16105920068091817
need align? ->  False 0.16105920068091817
2023-09-01 19:55:31,847 - epoch:8, training loss:1.0140 validation loss:0.1575
Updating learning rate to 9.618457028986775e-05
Updating learning rate to 9.618457028986775e-05
train 34129
vs, vt 0.15994668478767077 0.15864923217644294
need align? ->  True 0.15864923217644294
2023-09-01 19:58:51,041 - epoch:9, training loss:1.0038 validation loss:0.1599
Updating learning rate to 9.32889891880015e-05
Updating learning rate to 9.32889891880015e-05
train 34129
vs, vt 0.15755778147528568 0.16122475131932232
need align? ->  False 0.15864923217644294
2023-09-01 20:02:09,525 - epoch:10, training loss:0.9661 validation loss:0.1576
Updating learning rate to 8.965272490120875e-05
Updating learning rate to 8.965272490120875e-05
train 34129
vs, vt 0.1576801550057199 0.1555116678070691
need align? ->  True 0.1555116678070691
2023-09-01 20:05:27,964 - epoch:11, training loss:0.9258 validation loss:0.1577
Updating learning rate to 8.533799491959945e-05
Updating learning rate to 8.533799491959945e-05
train 34129
vs, vt 0.15733326230612066 0.15564772805405988
need align? ->  True 0.1555116678070691
2023-09-01 20:08:47,622 - epoch:12, training loss:0.9497 validation loss:0.1573
Updating learning rate to 8.041862546942808e-05
Updating learning rate to 8.041862546942808e-05
train 34129
vs, vt 0.1566484241435925 0.1554794286688169
need align? ->  True 0.1554794286688169
2023-09-01 20:12:08,652 - epoch:13, training loss:0.9129 validation loss:0.1566
Updating learning rate to 7.497878832589398e-05
Updating learning rate to 7.497878832589398e-05
train 34129
vs, vt 0.1558178264233801 0.1560388320022159
need align? ->  True 0.1554794286688169
2023-09-01 20:15:41,395 - epoch:14, training loss:0.9333 validation loss:0.1558
Updating learning rate to 6.911156061073078e-05
Updating learning rate to 6.911156061073078e-05
train 34129
vs, vt 0.15737335131400162 0.15386734253002537
need align? ->  True 0.15386734253002537
2023-09-01 20:19:05,104 - epoch:15, training loss:0.9102 validation loss:0.1574
Updating learning rate to 6.291733221684778e-05
Updating learning rate to 6.291733221684778e-05
train 34129
vs, vt 0.15554690187176068 0.15584498420357704
need align? ->  True 0.15386734253002537
2023-09-01 20:22:26,028 - epoch:16, training loss:0.9159 validation loss:0.1555
Updating learning rate to 5.650208810942889e-05
Updating learning rate to 5.650208810942889e-05
train 34129
vs, vt 0.15456586960289215 0.1540828066981501
need align? ->  True 0.15386734253002537
2023-09-01 20:25:46,594 - epoch:17, training loss:0.9008 validation loss:0.1546
Updating learning rate to 4.9975594893793686e-05
Updating learning rate to 4.9975594893793686e-05
train 34129
vs, vt 0.1555272128019068 0.15488175989853012
need align? ->  True 0.15386734253002537
2023-09-01 20:29:06,233 - epoch:18, training loss:0.8905 validation loss:0.1555
Updating learning rate to 4.3449522678347527e-05
Updating learning rate to 4.3449522678347527e-05
train 34129
vs, vt 0.15512241700457202 0.15401709833078914
need align? ->  True 0.15386734253002537
2023-09-01 20:32:28,442 - epoch:19, training loss:0.8826 validation loss:0.1551
Updating learning rate to 3.7035534368065714e-05
Updating learning rate to 3.7035534368065714e-05
train 34129
vs, vt 0.15418264468510945 0.15407636053860188
need align? ->  True 0.15386734253002537
2023-09-01 20:35:49,559 - epoch:20, training loss:0.8759 validation loss:0.1542
Updating learning rate to 3.084337508123067e-05
Updating learning rate to 3.084337508123067e-05
train 34129
vs, vt 0.15465664925674596 0.1535412362880177
need align? ->  True 0.1535412362880177
2023-09-01 20:39:12,831 - epoch:21, training loss:0.8709 validation loss:0.1547
Updating learning rate to 2.497899438003108e-05
Updating learning rate to 2.497899438003108e-05
train 34129
vs, vt 0.15420860081083246 0.1529001594417625
need align? ->  True 0.1529001594417625
2023-09-01 20:42:34,241 - epoch:22, training loss:0.9355 validation loss:0.1542
Updating learning rate to 1.9542733444177923e-05
Updating learning rate to 1.9542733444177923e-05
train 34129
vs, vt 0.15394240108629068 0.1531591809458203
need align? ->  True 0.1529001594417625
2023-09-01 20:45:53,849 - epoch:23, training loss:0.9327 validation loss:0.1539
Updating learning rate to 1.4627608205499963e-05
Updating learning rate to 1.4627608205499963e-05
train 34129
vs, vt 0.15411252967185443 0.15366470701992513
need align? ->  True 0.1529001594417625
2023-09-01 20:49:15,674 - epoch:24, training loss:0.9285 validation loss:0.1541
Updating learning rate to 1.0317717819561129e-05
Updating learning rate to 1.0317717819561129e-05
train 34129
vs, vt 0.15365862945715586 0.15291261999971337
need align? ->  True 0.1529001594417625
2023-09-01 20:52:35,155 - epoch:25, training loss:0.9253 validation loss:0.1537
Updating learning rate to 6.686805705792195e-06
Updating learning rate to 6.686805705792195e-06
train 34129
vs, vt 0.15369292178915606 0.1530768796801567
need align? ->  True 0.1529001594417625
2023-09-01 20:55:55,672 - epoch:26, training loss:0.9236 validation loss:0.1537
Updating learning rate to 3.79699777713878e-06
Updating learning rate to 3.79699777713878e-06
train 34129
vs, vt 0.15361206146577994 0.15316164038247532
need align? ->  True 0.1529001594417625
2023-09-01 20:59:19,711 - epoch:27, training loss:0.9225 validation loss:0.1536
Updating learning rate to 1.6977394484662593e-06
Updating learning rate to 1.6977394484662593e-06
train 34129
vs, vt 0.15350112741192182 0.15300097602109114
need align? ->  True 0.1529001594417625
2023-09-01 21:03:01,539 - epoch:28, training loss:0.9225 validation loss:0.1535
Updating learning rate to 4.2494961180259127e-07
Updating learning rate to 4.2494961180259127e-07
train 34129
vs, vt 0.15353857580986288 0.15296660802430576
need align? ->  True 0.1529001594417625
2023-09-01 21:06:22,071 - epoch:29, training loss:0.9217 validation loss:0.1535
Updating learning rate to 4.0605413710007e-10
Updating learning rate to 4.0605413710007e-10
check exp/ECL-PatchTST2023-09-01-19:26:56.967051/0/0.1535_epoch_28.pkl  &  0.1529001594417625
2023-09-01 21:06:45,122 - [*] loss:0.2887
2023-09-01 21:06:45,147 - [*] phase 0, testing
2023-09-01 21:06:45,428 - T:96	MAE	0.336155	RMSE	0.290269	MAPE	213.640451
2023-09-01 21:06:45,428 - 96	mae	0.3362	
2023-09-01 21:06:45,428 - 96	rmse	0.2903	
2023-09-01 21:06:45,429 - 96	mape	213.6405	
----*-----
2023-09-01 21:07:08,320 - [*] loss:0.2887
2023-09-01 21:07:08,330 - [*] phase 0, testing
2023-09-01 21:07:08,640 - T:96	MAE	0.336155	RMSE	0.290269	MAPE	213.640451
2023-09-01 21:07:32,611 - [*] loss:0.3094
2023-09-01 21:07:32,621 - [*] phase 0, testing
2023-09-01 21:07:32,945 - T:96	MAE	0.363922	RMSE	0.311102	MAPE	238.698244
2023-09-01 21:07:55,711 - [*] loss:0.3832
2023-09-01 21:07:55,722 - [*] phase 0, testing
2023-09-01 21:07:56,012 - T:96	MAE	0.408304	RMSE	0.385361	MAPE	273.230743
2023-09-01 21:08:19,822 - [*] loss:0.2948
2023-09-01 21:08:19,832 - [*] phase 0, testing
2023-09-01 21:08:20,025 - T:96	MAE	0.345124	RMSE	0.296517	MAPE	229.606128
2023-09-01 21:08:53,203 - [*] loss:0.3674
2023-09-01 21:08:53,214 - [*] phase 0, testing
2023-09-01 21:08:53,395 - T:96	MAE	0.408699	RMSE	0.369244	MAPE	236.201715
2023-09-01 21:09:24,319 - [*] loss:0.3363
2023-09-01 21:09:24,329 - [*] phase 0, testing
2023-09-01 21:09:24,559 - T:96	MAE	0.378783	RMSE	0.337800	MAPE	202.222228
2023-09-01 21:09:52,217 - [*] loss:0.2942
2023-09-01 21:09:52,227 - [*] phase 0, testing
2023-09-01 21:09:52,403 - T:96	MAE	0.344888	RMSE	0.295814	MAPE	222.323871
2023-09-01 21:10:17,288 - [*] loss:0.3008
2023-09-01 21:10:17,297 - [*] phase 0, testing
2023-09-01 21:10:17,476 - T:96	MAE	0.355274	RMSE	0.302277	MAPE	202.850556
----*-----
2023-09-01 21:10:33,021 - [*] loss:0.2978
2023-09-01 21:10:33,031 - [*] phase 0, testing
2023-09-01 21:10:33,227 - T:96	MAE	0.352832	RMSE	0.298285	MAPE	204.908705
2023-09-01 21:10:56,932 - [*] loss:0.3029
2023-09-01 21:10:56,942 - [*] phase 0, testing
2023-09-01 21:10:57,123 - T:96	MAE	0.351979	RMSE	0.304644	MAPE	199.386990
2023-09-01 21:11:14,397 - [*] loss:0.3005
2023-09-01 21:11:14,407 - [*] phase 0, testing
2023-09-01 21:11:14,597 - T:96	MAE	0.346662	RMSE	0.301617	MAPE	201.269031
2023-09-01 21:11:14,598 - 96	mae	0.3467	
2023-09-01 21:11:14,598 - 96	rmse	0.3016	
2023-09-01 21:11:14,598 - 96	mape	201.2690	
2023-09-01 21:11:16,887 - logger name:exp/ECL-PatchTST2023-09-01-21:11:16.887291/ECL-PatchTST.log
2023-09-01 21:11:16,887 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-01-21:11:16.887291', 'path': 'exp/ECL-PatchTST2023-09-01-21:11:16.887291', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-01 21:11:16,887 - [*] phase 0 start training
0 69680
train 33889
val 11185
test 11185
2023-09-01 21:11:17,713 - [*] phase 0 Dataset load!
2023-09-01 21:11:18,748 - [*] phase 0 Training start
train 33889
2023-09-01 21:12:55,241 - epoch:0, training loss:0.2074 validation loss:0.2604
train 33889
vs, vt 0.26037318153645506 0.2635187496515838
Updating learning rate to 1.0438721218484657e-05
Updating learning rate to 1.0438721218484657e-05
train 33889
vs, vt 0.2552970733324235 0.255028753795407
need align? ->  True 0.255028753795407
2023-09-01 21:17:03,921 - epoch:1, training loss:8.9611 validation loss:0.2553
Updating learning rate to 2.802750441854847e-05
Updating learning rate to 2.802750441854847e-05
train 33889
vs, vt 0.2517539606090974 0.2532173475165936
need align? ->  False 0.2532173475165936
2023-09-01 21:20:19,908 - epoch:2, training loss:3.0743 validation loss:0.2518
Updating learning rate to 5.2047629950292354e-05
Updating learning rate to 5.2047629950292354e-05
train 33889
vs, vt 0.252129579809579 0.2527900416488675
need align? ->  False 0.2527900416488675
2023-09-01 21:23:35,045 - epoch:3, training loss:2.1339 validation loss:0.2521
Updating learning rate to 7.605497731655361e-05
Updating learning rate to 7.605497731655361e-05
train 33889
vs, vt 0.2524407195689326 0.2526665948416022
need align? ->  False 0.2526665948416022
2023-09-01 21:26:49,606 - epoch:4, training loss:1.6827 validation loss:0.2524
Updating learning rate to 9.3608854147054e-05
Updating learning rate to 9.3608854147054e-05
train 33889
vs, vt 0.24870479669929904 0.2529312626628036
need align? ->  False 0.2526665948416022
2023-09-01 21:30:04,091 - epoch:5, training loss:1.3893 validation loss:0.2487
Updating learning rate to 9.99999938537861e-05
Updating learning rate to 9.99999938537861e-05
train 33889
vs, vt 0.2491042418828742 0.2496945486319336
need align? ->  False 0.2496945486319336
2023-09-01 21:33:18,731 - epoch:6, training loss:1.2234 validation loss:0.2491
Updating learning rate to 9.956900274488073e-05
Updating learning rate to 9.956900274488073e-05
train 33889
vs, vt 0.2478008068759333 0.25121227596801793
need align? ->  False 0.2496945486319336
2023-09-01 21:36:33,954 - epoch:7, training loss:1.1798 validation loss:0.2478
Updating learning rate to 9.828987567794199e-05
Updating learning rate to 9.828987567794199e-05
train 33889
vs, vt 0.24575939122587442 0.25032055492258887
need align? ->  False 0.2496945486319336
2023-09-01 21:39:48,292 - epoch:8, training loss:1.0759 validation loss:0.2458
Updating learning rate to 9.618449887172618e-05
Updating learning rate to 9.618449887172618e-05
train 33889
vs, vt 0.24845703763209961 0.2505583510818807
need align? ->  False 0.2496945486319336
2023-09-01 21:43:04,017 - epoch:9, training loss:1.0209 validation loss:0.2485
Updating learning rate to 9.328889590710838e-05
Updating learning rate to 9.328889590710838e-05
train 33889
vs, vt 0.2490529352799058 0.25369724529710685
need align? ->  False 0.2496945486319336
2023-09-01 21:46:18,777 - epoch:10, training loss:0.9845 validation loss:0.2491
Updating learning rate to 8.965261135362604e-05
Updating learning rate to 8.965261135362604e-05
train 33889
vs, vt 0.2458940980193967 0.2548006877557121
need align? ->  False 0.2496945486319336
2023-09-01 21:49:34,774 - epoch:11, training loss:0.9582 validation loss:0.2459
Updating learning rate to 8.533786304815776e-05
Updating learning rate to 8.533786304815776e-05
train 33889
vs, vt 0.24676837226037274 0.2530603652227331
need align? ->  False 0.2496945486319336
2023-09-01 21:52:49,599 - epoch:12, training loss:0.9391 validation loss:0.2468
Updating learning rate to 8.041847753048434e-05
Updating learning rate to 8.041847753048434e-05
train 33889
vs, vt 0.24927303250032393 0.2541783235713162
need align? ->  False 0.2496945486319336
2023-09-01 21:56:04,806 - epoch:13, training loss:0.9222 validation loss:0.2493
Updating learning rate to 7.497862685072454e-05
Updating learning rate to 7.497862685072454e-05
train 33889
vs, vt 0.24874006148258393 0.2550908230583776
need align? ->  False 0.2496945486319336
2023-09-01 21:59:20,448 - epoch:14, training loss:0.9090 validation loss:0.2487
Updating learning rate to 6.911138836222055e-05
Updating learning rate to 6.911138836222055e-05
train 33889
vs, vt 0.2485954946076328 0.25472980813885276
need align? ->  False 0.2496945486319336
2023-09-01 22:02:36,275 - epoch:15, training loss:0.8978 validation loss:0.2486
Updating learning rate to 6.291715214221653e-05
Updating learning rate to 6.291715214221653e-05
train 33889
vs, vt 0.24785319190811028 0.25600229521197354
need align? ->  False 0.2496945486319336
2023-09-01 22:05:52,842 - epoch:16, training loss:0.8891 validation loss:0.2479
Updating learning rate to 5.6501903289803477e-05
Updating learning rate to 5.6501903289803477e-05
train 33889
vs, vt 0.2470130124129355 0.2550759131766178
need align? ->  False 0.2496945486319336
2023-09-01 22:09:09,374 - epoch:17, training loss:0.8826 validation loss:0.2470
Updating learning rate to 4.997540849148917e-05
Updating learning rate to 4.997540849148917e-05
train 33889
vs, vt 0.24640726323493503 0.2537518525919454
need align? ->  False 0.2496945486319336
2023-09-01 22:12:26,509 - epoch:18, training loss:0.8757 validation loss:0.2464
Updating learning rate to 4.344933788275899e-05
Updating learning rate to 4.344933788275899e-05
train 33889
vs, vt 0.24860479504886 0.2547564074312421
need align? ->  False 0.2496945486319336
2023-09-01 22:15:40,963 - epoch:19, training loss:0.8715 validation loss:0.2486
check exp/ECL-PatchTST2023-09-01-21:11:16.887291/0/0.2458_epoch_8.pkl  &  0.2496945486319336
2023-09-01 22:16:10,786 - [*] loss:0.3799
2023-09-01 22:16:10,823 - [*] phase 0, testing
2023-09-01 22:16:15,071 - T:336	MAE	0.390853	RMSE	0.379720	MAPE	238.780022
2023-09-01 22:16:15,072 - 336	mae	0.3909	
2023-09-01 22:16:15,072 - 336	rmse	0.3797	
2023-09-01 22:16:15,072 - 336	mape	238.7800	
----*-----
2023-09-01 22:16:45,191 - [*] loss:0.3799
2023-09-01 22:16:45,226 - [*] phase 0, testing
2023-09-01 22:16:48,209 - T:336	MAE	0.390853	RMSE	0.379720	MAPE	238.780022
2023-09-01 22:17:12,140 - [*] loss:0.3897
2023-09-01 22:17:12,173 - [*] phase 0, testing
2023-09-01 22:17:14,144 - T:336	MAE	0.403666	RMSE	0.389487	MAPE	249.664116
2023-09-01 22:17:37,519 - [*] loss:0.4088
2023-09-01 22:17:37,555 - [*] phase 0, testing
2023-09-01 22:17:39,545 - T:336	MAE	0.415673	RMSE	0.408679	MAPE	266.298342
2023-09-01 22:18:03,624 - [*] loss:0.3868
2023-09-01 22:18:03,659 - [*] phase 0, testing
2023-09-01 22:18:05,412 - T:336	MAE	0.399789	RMSE	0.386622	MAPE	257.532930
2023-09-01 22:18:33,121 - [*] loss:0.4373
2023-09-01 22:18:33,161 - [*] phase 0, testing
2023-09-01 22:18:33,962 - T:336	MAE	0.441542	RMSE	0.437063	MAPE	249.801064
2023-09-01 22:18:58,248 - [*] loss:0.4163
2023-09-01 22:18:58,283 - [*] phase 0, testing
2023-09-01 22:18:59,008 - T:336	MAE	0.423713	RMSE	0.415859	MAPE	214.877725
2023-09-01 22:19:30,562 - [*] loss:0.3826
2023-09-01 22:19:30,597 - [*] phase 0, testing
2023-09-01 22:19:31,600 - T:336	MAE	0.394946	RMSE	0.382371	MAPE	242.382455
2023-09-01 22:20:04,453 - [*] loss:0.3803
2023-09-01 22:20:04,489 - [*] phase 0, testing
2023-09-01 22:20:05,205 - T:336	MAE	0.397536	RMSE	0.379954	MAPE	221.033359
----*-----
2023-09-01 22:20:26,046 - [*] loss:0.3751
2023-09-01 22:20:26,090 - [*] phase 0, testing
2023-09-01 22:20:27,569 - T:336	MAE	0.395072	RMSE	0.374786	MAPE	223.214316
2023-09-01 22:20:52,626 - [*] loss:0.3894
2023-09-01 22:20:52,660 - [*] phase 0, testing
2023-09-01 22:20:54,134 - T:336	MAE	0.400937	RMSE	0.389148	MAPE	222.345924
2023-09-01 22:21:10,695 - [*] loss:0.3863
2023-09-01 22:21:10,730 - [*] phase 0, testing
2023-09-01 22:21:12,018 - T:336	MAE	0.398780	RMSE	0.386107	MAPE	224.876118
2023-09-01 22:21:12,019 - 336	mae	0.3988	
2023-09-01 22:21:12,019 - 336	rmse	0.3861	
2023-09-01 22:21:12,020 - 336	mape	224.8761	
