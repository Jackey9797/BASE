2023-09-04 22:41:14,677 - logger name:exp/ECL-Informer2023-09-04-22:41:14.677139/ECL-Informer.log
2023-09-04 22:41:14,677 - params : {'loss': 'mse', 'conf': 'ECL-Informer', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 0, 'abl_tmp_context': 0, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'type3', 'dropout': 0.05, 'fc_dropout': 0.05, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 512, 'd_model': 128, 'n_heads': 8, 'seq_len': 96, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 64, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 2, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'informer', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 3, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 0, 'pct_start': 0.3, 'indie': 0, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': False, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-Informer', 'time': '2023-09-04-22:41:14.677139', 'path': 'exp/ECL-Informer2023-09-04-22:41:14.677139', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 22:41:14,677 - [*] phase 0 start training
train 8449
val 2785
test 2785
2023-09-04 22:41:14,755 - [*] phase 0 Dataset load!
Correction_Module(
  (Aligner): Linear(in_features=128, out_features=128, bias=True)
  (Refiner): Refiner(
    (ref): ModuleList(
      (0-1): 2 x Ref_block(
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (attn_fn): ReLUSquared()
        (to_hidden): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): SiLU()
        )
        (to_qk): Sequential(
          (0): Linear(in_features=128, out_features=32, bias=True)
          (1): SiLU()
        )
        (offsetscale): OffsetScale()
        (to_out): Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): Linear(in_features=64, out_features=128, bias=True)
        )
        (shrinkage): Shrinkage(
          (gap): AdaptiveAvgPool1d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=128, out_features=128, bias=True)
            (3): Sigmoid()
          )
        )
        (norm_attn): Sequential(
          (0): Transpose()
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Transpose()
        )
        (norm_ffn): Sequential(
          (0): Transpose()
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Transpose()
        )
        (self_attn): _MultiheadAttention(
          (W_Q): Linear(in_features=128, out_features=128, bias=True)
          (W_K): Linear(in_features=128, out_features=128, bias=True)
          (W_V): Linear(in_features=128, out_features=128, bias=True)
          (sdp_attn): _ScaledDotProductAttention(
            (attn_dropout): Dropout(p=0, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Dropout(p=0.2, inplace=False)
          )
        )
        (dropout_attn): Dropout(p=0.0, inplace=False)
      )
    )
    (rec): Sequential(
      (0): Rec_block(
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (attn_fn): ReLUSquared()
        (to_hidden): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): SiLU()
        )
        (to_qk): Sequential(
          (0): Linear(in_features=128, out_features=32, bias=True)
          (1): SiLU()
        )
        (offsetscale): OffsetScale()
        (to_out): Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): ReLU()
          (2): Linear(in_features=64, out_features=32, bias=True)
          (3): ReLU()
          (4): Linear(in_features=32, out_features=64, bias=True)
          (5): ReLU()
          (6): Linear(in_features=64, out_features=128, bias=True)
        )
        (shrinkage): Shrinkage(
          (gap): AdaptiveAvgPool1d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=128, out_features=128, bias=True)
            (3): Sigmoid()
          )
        )
        (self_attn): _MultiheadAttention(
          (W_Q): Linear(in_features=128, out_features=128, bias=True)
          (W_K): Linear(in_features=128, out_features=128, bias=True)
          (W_V): Linear(in_features=128, out_features=128, bias=True)
          (sdp_attn): _ScaledDotProductAttention(
            (attn_dropout): Dropout(p=0, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (rec_head): ReconHead(
      (dropout): Dropout(p=0.1, inplace=False)
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
dropout 0.05
dropout 0.05
True
1 True None
2023-09-04 22:41:15,277 - [*] phase 0 Training start
train 8449
2023-09-04 22:41:49,010 - epoch:0, training loss:1.1889 validation loss:1.6454
(8449, 1)
(8449, 1) True
train 8449
vs, vt 1.645379426804456 1.8869369768283584
Updating learning rate to 0.0001
Updating learning rate to 0.0001
(8449, 1)
(8449, 1) True
train 8449
vs, vt 0.6338459104299545 1.1066097867759792
need align? ->  False 1.1066097867759792
2023-09-04 22:42:54,027 - epoch:1, training loss:16.5320 validation loss:0.6338
Updating learning rate to 0.0001
Updating learning rate to 0.0001
(8449, 1)
(8449, 1) True
train 8449
vs, vt 0.7496824305165898 0.7535666379738938
need align? ->  False 0.7535666379738938
2023-09-04 22:43:45,986 - epoch:2, training loss:14.9596 validation loss:0.7497
Updating learning rate to 0.0001
Updating learning rate to 0.0001
(8449, 1)
(8449, 1) True
train 8449
vs, vt 0.7519521455873143 0.6603886783123016
need align? ->  True 0.6603886783123016
2023-09-04 22:44:35,533 - epoch:3, training loss:14.2324 validation loss:0.7520
Updating learning rate to 9e-05
Updating learning rate to 9e-05
(8449, 1)
(8449, 1) True
train 8449
vs, vt 0.7879852489991621 0.6964846178889275
need align? ->  True 0.6603886783123016
2023-09-04 22:45:27,487 - epoch:4, training loss:13.9046 validation loss:0.7880
Updating learning rate to 8.1e-05
Updating learning rate to 8.1e-05
(8449, 1)
(8449, 1) True
train 8449
vs, vt 0.9005636210468683 0.7259970818730918
need align? ->  True 0.6603886783123016
2023-09-04 22:46:17,411 - epoch:5, training loss:13.7323 validation loss:0.9006
Updating learning rate to 7.290000000000001e-05
Updating learning rate to 7.290000000000001e-05
(8449, 1)
(8449, 1) True
train 8449
vs, vt 1.054226186803796 0.7602760551328008
need align? ->  True 0.6603886783123016
2023-09-04 22:47:06,726 - epoch:6, training loss:13.5798 validation loss:1.0542
Updating learning rate to 6.561e-05
Updating learning rate to 6.561e-05
(8449, 1)
(8449, 1) True
train 8449
vs, vt 1.0950359173796393 0.8686531226743351
need align? ->  True 0.6603886783123016
2023-09-04 22:48:00,004 - epoch:7, training loss:13.4753 validation loss:1.0950
Updating learning rate to 5.904900000000001e-05
Updating learning rate to 5.904900000000001e-05
(8449, 1)
(8449, 1) True
train 8449
vs, vt 1.1017326976765285 0.8574960438365286
need align? ->  True 0.6603886783123016
2023-09-04 22:48:47,182 - epoch:8, training loss:13.3795 validation loss:1.1017
Updating learning rate to 5.3144100000000005e-05
Updating learning rate to 5.3144100000000005e-05
(8449, 1)
(8449, 1) True
train 8449
vs, vt 1.2276653159748425 0.8695006577128713
need align? ->  True 0.6603886783123016
2023-09-04 22:49:36,587 - epoch:9, training loss:13.2792 validation loss:1.2277
Updating learning rate to 4.782969000000001e-05
Updating learning rate to 4.782969000000001e-05
(8449, 1)
(8449, 1) True
train 8449
vs, vt 1.253660245036537 0.881840508769859
need align? ->  True 0.6603886783123016
2023-09-04 22:50:26,921 - epoch:10, training loss:13.2195 validation loss:1.2537
Updating learning rate to 4.304672100000001e-05
Updating learning rate to 4.304672100000001e-05
(8449, 1)
(8449, 1) True
train 8449
vs, vt 1.2425301918251948 0.9214061979543079
need align? ->  True 0.6603886783123016
2023-09-04 22:51:14,107 - epoch:11, training loss:13.1330 validation loss:1.2425
Updating learning rate to 3.874204890000001e-05
Updating learning rate to 3.874204890000001e-05
(8449, 1)
(8449, 1) True
train 8449
vs, vt 1.2710043323988265 0.9003045579249208
need align? ->  True 0.6603886783123016
2023-09-04 22:52:05,327 - epoch:12, training loss:13.0754 validation loss:1.2710
Correction_Module(
  (Aligner): Linear(in_features=128, out_features=128, bias=True)
  (Refiner): Refiner(
    (ref): ModuleList(
      (0-1): 2 x Ref_block(
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (attn_fn): ReLUSquared()
        (to_hidden): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): SiLU()
        )
        (to_qk): Sequential(
          (0): Linear(in_features=128, out_features=32, bias=True)
          (1): SiLU()
        )
        (offsetscale): OffsetScale()
        (to_out): Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): Linear(in_features=64, out_features=128, bias=True)
        )
        (shrinkage): Shrinkage(
          (gap): AdaptiveAvgPool1d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=128, out_features=128, bias=True)
            (3): Sigmoid()
          )
        )
        (norm_attn): Sequential(
          (0): Transpose()
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Transpose()
        )
        (norm_ffn): Sequential(
          (0): Transpose()
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Transpose()
        )
        (self_attn): _MultiheadAttention(
          (W_Q): Linear(in_features=128, out_features=128, bias=True)
          (W_K): Linear(in_features=128, out_features=128, bias=True)
          (W_V): Linear(in_features=128, out_features=128, bias=True)
          (sdp_attn): _ScaledDotProductAttention(
            (attn_dropout): Dropout(p=0, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Dropout(p=0.2, inplace=False)
          )
        )
        (dropout_attn): Dropout(p=0.0, inplace=False)
      )
    )
    (rec): Sequential(
      (0): Rec_block(
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (attn_fn): ReLUSquared()
        (to_hidden): Sequential(
          (0): Linear(in_features=128, out_features=512, bias=True)
          (1): SiLU()
        )
        (to_qk): Sequential(
          (0): Linear(in_features=128, out_features=32, bias=True)
          (1): SiLU()
        )
        (offsetscale): OffsetScale()
        (to_out): Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): ReLU()
          (2): Linear(in_features=64, out_features=32, bias=True)
          (3): ReLU()
          (4): Linear(in_features=32, out_features=64, bias=True)
          (5): ReLU()
          (6): Linear(in_features=64, out_features=128, bias=True)
        )
        (shrinkage): Shrinkage(
          (gap): AdaptiveAvgPool1d(output_size=1)
          (fc): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): ReLU(inplace=True)
            (2): Linear(in_features=128, out_features=128, bias=True)
            (3): Sigmoid()
          )
        )
        (self_attn): _MultiheadAttention(
          (W_Q): Linear(in_features=128, out_features=128, bias=True)
          (W_K): Linear(in_features=128, out_features=128, bias=True)
          (W_V): Linear(in_features=128, out_features=128, bias=True)
          (sdp_attn): _ScaledDotProductAttention(
            (attn_dropout): Dropout(p=0, inplace=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): Dropout(p=0, inplace=False)
          )
        )
      )
    )
    (rec_head): ReconHead(
      (dropout): Dropout(p=0.1, inplace=False)
      (linear): Linear(in_features=128, out_features=8, bias=True)
    )
  )
)
dropout 0.05
dropout 0.05
True
check exp/ECL-Informer2023-09-04-22:41:14.677139/0/0.6338_epoch_1.pkl  &  0.6603886783123016
2023-09-04 22:52:12,648 - [*] loss:1.4861
2023-09-04 22:52:12,651 - [*] phase 0, testing
2023-09-04 22:52:12,698 - T:96	MAE	0.970700	RMSE	1.495194	MAPE	502.249765
2023-09-04 22:52:12,698 - 96	mae	0.9707	
2023-09-04 22:52:12,698 - 96	rmse	1.4952	
2023-09-04 22:52:12,698 - 96	mape	502.2498	
2023-09-04 22:52:19,881 - [*] loss:1.4869
2023-09-04 22:52:19,884 - [*] phase 0, testing
2023-09-04 22:52:19,920 - T:96	MAE	0.971274	RMSE	1.496070	MAPE	503.784513
