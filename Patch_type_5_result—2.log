2023-08-03 21:30:21,318 - logger name:exp/ECL-PatchTST2023-08-03-21:30:21.318400/ECL-PatchTST.log
2023-08-03 21:30:21,319 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-21:30:21.318400', 'path': 'exp/ECL-PatchTST2023-08-03-21:30:21.318400', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 21:30:21,319 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 21:30:21,595 - [*] phase 0 Dataset load!
2023-08-03 21:30:22,750 - [*] phase 0 Training start
train 7969
2023-08-03 21:30:54,273 - epoch:0, training loss:0.7789 validation loss:0.4190
train 7969
vs, vt 0.41895488537847997 0.4132383428514004
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.3884594913572073 0.3885953791439533
2023-08-03 21:31:47,147 - epoch:1, training loss:0.7827 validation loss:0.3885
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.37636158131062986 0.38732904344797137
2023-08-03 21:32:26,602 - epoch:2, training loss:0.7247 validation loss:0.3764
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.3692531656473875 0.37150229774415494
2023-08-03 21:33:05,861 - epoch:3, training loss:0.6853 validation loss:0.3693
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.37501056008040906 0.37064487636089327
2023-08-03 21:33:45,127 - epoch:4, training loss:0.6570 validation loss:0.3750
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.38222029991447926 0.3795190904289484
2023-08-03 21:34:27,172 - epoch:5, training loss:0.6274 validation loss:0.3822
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.384800049290061 0.3743063505738974
2023-08-03 21:35:08,263 - epoch:6, training loss:0.6018 validation loss:0.3848
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.39033303037285805 0.3742960184812546
2023-08-03 21:35:48,097 - epoch:7, training loss:0.5840 validation loss:0.3903
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.4042418375611305 0.3683088950812817
2023-08-03 21:36:28,096 - epoch:8, training loss:0.5665 validation loss:0.4042
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.39587954208254816 0.36832850240170956
2023-08-03 21:37:08,868 - epoch:9, training loss:0.5552 validation loss:0.3959
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.40352830700576303 0.3688770061358809
2023-08-03 21:37:49,363 - epoch:10, training loss:0.5437 validation loss:0.4035
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.40291605815291404 0.37274851333349945
2023-08-03 21:38:26,351 - epoch:11, training loss:0.5367 validation loss:0.4029
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.40562997534871104 0.37250636890530586
2023-08-03 21:39:06,793 - epoch:12, training loss:0.5288 validation loss:0.4056
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.4082068149000406 0.3741697113960981
2023-08-03 21:39:48,008 - epoch:13, training loss:0.5233 validation loss:0.4082
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.40224927887320516 0.3716515524312854
2023-08-03 21:40:28,388 - epoch:14, training loss:0.5162 validation loss:0.4022
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.40842701122164726 0.3760084010660648
2023-08-03 21:41:07,593 - epoch:15, training loss:0.5125 validation loss:0.4084
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.4020721226930618 0.37496607322245834
2023-08-03 21:41:49,955 - epoch:16, training loss:0.5067 validation loss:0.4021
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.4106238901615143 0.37596512362360957
2023-08-03 21:42:31,412 - epoch:17, training loss:0.5031 validation loss:0.4106
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.41638905704021456 0.37749884389340876
2023-08-03 21:43:12,431 - epoch:18, training loss:0.5001 validation loss:0.4164
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.41248416937887666 0.3823992099612951
2023-08-03 21:43:52,365 - epoch:19, training loss:0.4973 validation loss:0.4125
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.41345476284623145 0.38106851149350407
2023-08-03 21:44:33,176 - epoch:20, training loss:0.4938 validation loss:0.4135
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.41314056776463987 0.37940026093274354
2023-08-03 21:45:14,131 - epoch:21, training loss:0.4908 validation loss:0.4131
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.4144771359860897 0.38237633034586904
2023-08-03 21:45:55,944 - epoch:22, training loss:0.4901 validation loss:0.4145
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.4146151874214411 0.3790848825126886
2023-08-03 21:46:39,479 - epoch:23, training loss:0.4890 validation loss:0.4146
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.4166548874229193 0.3805908065289259
2023-08-03 21:47:20,495 - epoch:24, training loss:0.4869 validation loss:0.4167
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.4161531891673803 0.3811688283458352
2023-08-03 21:48:06,998 - epoch:25, training loss:0.4857 validation loss:0.4162
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.4165823206305504 0.3809694429859519
2023-08-03 21:48:50,780 - epoch:26, training loss:0.4853 validation loss:0.4166
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.41638774834573267 0.379313001409173
2023-08-03 21:49:29,722 - epoch:27, training loss:0.4847 validation loss:0.4164
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.4166882984340191 0.38182540629059075
2023-08-03 21:50:08,480 - epoch:28, training loss:0.4850 validation loss:0.4167
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.4169592048972845 0.3810563111677766
2023-08-03 21:50:48,990 - epoch:29, training loss:0.4859 validation loss:0.4170
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-21:30:21.318400/0/0.3693_epoch_3.pkl  &  0.3683088950812817
2023-08-03 21:50:53,842 - [*] loss:0.3693
2023-08-03 21:50:53,936 - [*] phase 0, testing
2023-08-03 21:50:54,444 - T:336	MAE	0.399641	RMSE	0.364573	MAPE	164.044392
2023-08-03 21:50:54,447 - 336	mae	0.3996	
2023-08-03 21:50:54,447 - 336	rmse	0.3646	
2023-08-03 21:50:54,447 - 336	mape	164.0444	
2023-08-03 21:50:58,956 - [*] loss:0.3683
2023-08-03 21:50:59,112 - [*] phase 0, testing
2023-08-03 21:50:59,896 - T:336	MAE	0.396014	RMSE	0.364173	MAPE	156.963718
2023-08-03 21:50:59,898 - 336	mae	0.3960	
2023-08-03 21:50:59,898 - 336	rmse	0.3642	
2023-08-03 21:50:59,898 - 336	mape	156.9637	
2023-08-03 21:51:02,728 - logger name:exp/ECL-PatchTST2023-08-03-21:51:02.728185/ECL-PatchTST.log
2023-08-03 21:51:02,729 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-21:51:02.728185', 'path': 'exp/ECL-PatchTST2023-08-03-21:51:02.728185', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 21:51:02,729 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 21:51:03,003 - [*] phase 0 Dataset load!
2023-08-03 21:51:04,397 - [*] phase 0 Training start
train 7969
2023-08-03 21:51:35,620 - epoch:0, training loss:0.7267 validation loss:0.4004
train 7969
vs, vt 0.40042925663292406 0.40652801282703876
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.3816366598010063 0.3826275959610939
2023-08-03 21:52:37,699 - epoch:1, training loss:8.3284 validation loss:0.3816
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.37305177189409733 0.3610491886734962
2023-08-03 21:53:23,897 - epoch:2, training loss:6.3797 validation loss:0.3731
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.3640374310314655 0.3610488183796406
2023-08-03 21:54:09,655 - epoch:3, training loss:4.1391 validation loss:0.3640
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.36620635651051997 0.3621365040540695
2023-08-03 21:54:54,714 - epoch:4, training loss:2.9334 validation loss:0.3662
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.36682358346879484 0.370387627184391
2023-08-03 21:55:38,556 - epoch:5, training loss:2.5617 validation loss:0.3668
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.3698970653116703 0.36420471854507924
2023-08-03 21:56:22,943 - epoch:6, training loss:2.4560 validation loss:0.3699
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.3711994614452124 0.36416260376572607
2023-08-03 21:57:06,970 - epoch:7, training loss:2.4110 validation loss:0.3712
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.37026478350162506 0.37342050541192295
2023-08-03 21:57:52,230 - epoch:8, training loss:2.2594 validation loss:0.3703
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.3713286768645048 0.3616839209571481
2023-08-03 21:58:38,554 - epoch:9, training loss:2.1561 validation loss:0.3713
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.373111554607749 0.3722861336544156
2023-08-03 21:59:27,433 - epoch:10, training loss:2.0369 validation loss:0.3731
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.37227215357124804 0.3650020046159625
2023-08-03 22:00:16,667 - epoch:11, training loss:2.0209 validation loss:0.3723
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.3763911139219999 0.3665220756083727
2023-08-03 22:01:05,658 - epoch:12, training loss:1.9428 validation loss:0.3764
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.3784597560763359 0.3660734945908189
2023-08-03 22:01:52,267 - epoch:13, training loss:1.9334 validation loss:0.3785
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.3730879358947277 0.36897804867476225
2023-08-03 22:02:41,484 - epoch:14, training loss:1.9304 validation loss:0.3731
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.3764876343309879 0.3667407877743244
2023-08-03 22:03:27,009 - epoch:15, training loss:2.0210 validation loss:0.3765
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.38290984332561495 0.37337258383631705
2023-08-03 22:04:12,847 - epoch:16, training loss:1.9753 validation loss:0.3829
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.3771873190999031 0.36857874058187007
2023-08-03 22:04:57,423 - epoch:17, training loss:1.9598 validation loss:0.3772
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.37565880455076694 0.3713231775909662
2023-08-03 22:05:41,095 - epoch:18, training loss:2.0021 validation loss:0.3757
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.38184466026723385 0.3708651702851057
2023-08-03 22:06:25,860 - epoch:19, training loss:1.9798 validation loss:0.3818
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.3815013751387596 0.3734862249344587
2023-08-03 22:07:11,151 - epoch:20, training loss:1.9470 validation loss:0.3815
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.38231273777782915 0.3746135890483856
2023-08-03 22:07:56,592 - epoch:21, training loss:1.9649 validation loss:0.3823
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.3789444074034691 0.37573868632316587
2023-08-03 22:08:42,101 - epoch:22, training loss:2.0034 validation loss:0.3789
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.38121625781059265 0.3739040095359087
2023-08-03 22:09:26,339 - epoch:23, training loss:1.9978 validation loss:0.3812
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.37973535172641276 0.37348177619278433
2023-08-03 22:10:12,302 - epoch:24, training loss:1.9744 validation loss:0.3797
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.38069336526095865 0.3752741239964962
2023-08-03 22:10:56,990 - epoch:25, training loss:1.9979 validation loss:0.3807
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.38204416297376154 0.3756151456385851
2023-08-03 22:11:44,129 - epoch:26, training loss:2.0084 validation loss:0.3820
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.37983407489955423 0.3740034233778715
2023-08-03 22:12:32,448 - epoch:27, training loss:1.9668 validation loss:0.3798
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.3800667427480221 0.37501735277473924
2023-08-03 22:13:22,510 - epoch:28, training loss:1.9619 validation loss:0.3801
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.38065649904310705 0.374090214446187
2023-08-03 22:14:13,064 - epoch:29, training loss:1.9969 validation loss:0.3807
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-21:51:02.728185/0/0.364_epoch_3.pkl  &  0.3610488183796406
2023-08-03 22:14:17,340 - [*] loss:0.3640
2023-08-03 22:14:17,348 - [*] phase 0, testing
2023-08-03 22:14:17,583 - T:336	MAE	0.397080	RMSE	0.359495	MAPE	164.095914
2023-08-03 22:14:17,584 - 336	mae	0.3971	
2023-08-03 22:14:17,584 - 336	rmse	0.3595	
2023-08-03 22:14:17,584 - 336	mape	164.0959	
2023-08-03 22:14:22,126 - [*] loss:0.3610
2023-08-03 22:14:22,134 - [*] phase 0, testing
2023-08-03 22:14:22,266 - T:336	MAE	0.395137	RMSE	0.356418	MAPE	159.679580
2023-08-03 22:14:22,266 - 336	mae	0.3951	
2023-08-03 22:14:22,266 - 336	rmse	0.3564	
2023-08-03 22:14:22,266 - 336	mape	159.6796	
2023-08-03 22:14:24,519 - logger name:exp/ECL-PatchTST2023-08-03-22:14:24.513709/ECL-PatchTST.log
2023-08-03 22:14:24,520 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-22:14:24.513709', 'path': 'exp/ECL-PatchTST2023-08-03-22:14:24.513709', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 22:14:24,520 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 22:14:24,744 - [*] phase 0 Dataset load!
2023-08-03 22:14:25,786 - [*] phase 0 Training start
train 7969
2023-08-03 22:15:00,657 - epoch:0, training loss:0.2540 validation loss:0.1794
train 7969
vs, vt 0.1794192695990205 0.18175389114767312
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.17217409312725068 0.17094285879284143
2023-08-03 22:16:01,356 - epoch:1, training loss:0.6959 validation loss:0.1722
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.16650329399853944 0.16220603473484516
2023-08-03 22:16:49,012 - epoch:2, training loss:0.6381 validation loss:0.1665
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.16430572103708982 0.1603609399870038
2023-08-03 22:17:36,429 - epoch:3, training loss:0.5797 validation loss:0.1643
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16869430020451545 0.1609202429652214
2023-08-03 22:18:23,180 - epoch:4, training loss:0.5277 validation loss:0.1687
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.16881193779408932 0.164902575686574
2023-08-03 22:19:11,531 - epoch:5, training loss:0.5022 validation loss:0.1688
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.17251111883670092 0.1653353282250464
2023-08-03 22:19:59,894 - epoch:6, training loss:0.4890 validation loss:0.1725
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.16884073968976737 0.16554973274469376
2023-08-03 22:20:47,896 - epoch:7, training loss:0.4839 validation loss:0.1688
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.17276970576494932 0.1668158445507288
2023-08-03 22:21:36,684 - epoch:8, training loss:0.4771 validation loss:0.1728
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.16921264994889498 0.1628488373942673
2023-08-03 22:22:24,489 - epoch:9, training loss:0.4711 validation loss:0.1692
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.16969355773180722 0.16738409753888844
2023-08-03 22:23:13,748 - epoch:10, training loss:0.4663 validation loss:0.1697
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.1700291933491826 0.16558846263214946
2023-08-03 22:24:02,968 - epoch:11, training loss:0.4571 validation loss:0.1700
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.17005993612110615 0.16660289475694298
2023-08-03 22:24:52,855 - epoch:12, training loss:0.4558 validation loss:0.1701
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.1712366057559848 0.16449009403586387
2023-08-03 22:25:42,422 - epoch:13, training loss:0.4476 validation loss:0.1712
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.16990295015275478 0.16612161723896862
2023-08-03 22:26:32,366 - epoch:14, training loss:0.4442 validation loss:0.1699
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.17209025006741285 0.1658206786029041
2023-08-03 22:27:22,367 - epoch:15, training loss:0.4378 validation loss:0.1721
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.17118200678378342 0.16724109454080463
2023-08-03 22:28:12,826 - epoch:16, training loss:0.4348 validation loss:0.1712
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.17153495270758867 0.16652809288352727
2023-08-03 22:29:02,908 - epoch:17, training loss:0.4312 validation loss:0.1715
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.1717606561258435 0.16810203474014998
2023-08-03 22:29:51,810 - epoch:18, training loss:0.4289 validation loss:0.1718
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.17185930404812097 0.16743953563272954
2023-08-03 22:30:41,960 - epoch:19, training loss:0.4303 validation loss:0.1719
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.17115238010883332 0.16784234261140227
2023-08-03 22:31:31,499 - epoch:20, training loss:0.4239 validation loss:0.1712
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.171933857165277 0.16742579685524106
2023-08-03 22:32:20,396 - epoch:21, training loss:0.4226 validation loss:0.1719
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.17181428167968987 0.1678890973329544
2023-08-03 22:33:09,240 - epoch:22, training loss:0.4236 validation loss:0.1718
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.17222508881241083 0.16732622729614377
2023-08-03 22:33:56,521 - epoch:23, training loss:0.4232 validation loss:0.1722
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.17248486820608377 0.16768888821825384
2023-08-03 22:34:43,688 - epoch:24, training loss:0.4210 validation loss:0.1725
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.17238318771123887 0.16788382371887564
2023-08-03 22:35:31,647 - epoch:25, training loss:0.4195 validation loss:0.1724
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.17236408218741417 0.16800260776653886
2023-08-03 22:36:18,079 - epoch:26, training loss:0.4202 validation loss:0.1724
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.17217494323849677 0.16767997480928898
2023-08-03 22:37:05,716 - epoch:27, training loss:0.4210 validation loss:0.1722
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.17240522671490907 0.16797813065350056
2023-08-03 22:37:52,179 - epoch:28, training loss:0.4184 validation loss:0.1724
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.17262211851775647 0.16779802413657308
2023-08-03 22:38:39,746 - epoch:29, training loss:0.4203 validation loss:0.1726
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-22:14:24.513709/0/0.1643_epoch_3.pkl  &  0.1603609399870038
2023-08-03 22:38:45,714 - [*] loss:0.3691
2023-08-03 22:38:45,724 - [*] phase 0, testing
2023-08-03 22:38:45,910 - T:336	MAE	0.398266	RMSE	0.364442	MAPE	162.213242
2023-08-03 22:38:45,910 - 336	mae	0.3983	
2023-08-03 22:38:45,910 - 336	rmse	0.3644	
2023-08-03 22:38:45,911 - 336	mape	162.2132	
2023-08-03 22:38:51,956 - [*] loss:0.3590
2023-08-03 22:38:51,965 - [*] phase 0, testing
2023-08-03 22:38:52,119 - T:336	MAE	0.393894	RMSE	0.354430	MAPE	159.204376
2023-08-03 22:38:52,120 - 336	mae	0.3939	
2023-08-03 22:38:52,120 - 336	rmse	0.3544	
2023-08-03 22:38:52,120 - 336	mape	159.2044	
2023-08-03 22:38:55,032 - logger name:exp/ECL-PatchTST2023-08-03-22:38:55.032042/ECL-PatchTST.log
2023-08-03 22:38:55,044 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-22:38:55.032042', 'path': 'exp/ECL-PatchTST2023-08-03-22:38:55.032042', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 22:38:55,044 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 22:38:55,270 - [*] phase 0 Dataset load!
2023-08-03 22:38:56,402 - [*] phase 0 Training start
train 7969
2023-08-03 22:39:28,278 - epoch:0, training loss:0.2705 validation loss:0.1867
train 7969
vs, vt 0.1866797858849168 0.18489858955144883
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.17678104415535928 0.17481709364801645
2023-08-03 22:40:28,682 - epoch:1, training loss:1.4124 validation loss:0.1768
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.16952955555170773 0.17689544036984445
2023-08-03 22:41:13,453 - epoch:2, training loss:1.1377 validation loss:0.1695
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.1644246056675911 0.16581294257193804
2023-08-03 22:41:58,346 - epoch:3, training loss:0.8792 validation loss:0.1644
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16372620156034828 0.16539810718968512
2023-08-03 22:42:42,716 - epoch:4, training loss:0.7474 validation loss:0.1637
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.1632874934002757 0.16635963264852763
2023-08-03 22:43:28,091 - epoch:5, training loss:0.6861 validation loss:0.1633
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.16148472214117646 0.16544905388727785
2023-08-03 22:44:12,853 - epoch:6, training loss:0.6598 validation loss:0.1615
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.1622985566034913 0.16407940313220024
2023-08-03 22:44:58,005 - epoch:7, training loss:0.6434 validation loss:0.1623
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.16188363786786794 0.16041282834485174
2023-08-03 22:45:42,394 - epoch:8, training loss:0.6374 validation loss:0.1619
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.16193614676594734 0.16076799649745227
2023-08-03 22:46:27,240 - epoch:9, training loss:0.6321 validation loss:0.1619
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.16221563573926687 0.1636753600090742
2023-08-03 22:47:11,948 - epoch:10, training loss:0.6264 validation loss:0.1622
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.16264732275158167 0.16302890479564666
2023-08-03 22:47:56,540 - epoch:11, training loss:0.6214 validation loss:0.1626
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.16355696013197302 0.16167438495904207
2023-08-03 22:48:41,502 - epoch:12, training loss:0.6229 validation loss:0.1636
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.16250956617295742 0.16466804621741177
2023-08-03 22:49:24,349 - epoch:13, training loss:0.6194 validation loss:0.1625
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.1618585217744112 0.1627862320281565
2023-08-03 22:50:07,457 - epoch:14, training loss:0.6178 validation loss:0.1619
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.16316726254299282 0.16416067853569985
2023-08-03 22:50:50,931 - epoch:15, training loss:0.6201 validation loss:0.1632
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.1643098789267242 0.16468064542859792
2023-08-03 22:51:33,723 - epoch:16, training loss:0.6128 validation loss:0.1643
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.16434050016105176 0.16435703029856086
2023-08-03 22:52:15,743 - epoch:17, training loss:0.6119 validation loss:0.1643
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.16379408026114106 0.1648788282647729
2023-08-03 22:52:57,386 - epoch:18, training loss:0.6091 validation loss:0.1638
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.16418391605839133 0.16572296293452382
2023-08-03 22:53:39,898 - epoch:19, training loss:0.6116 validation loss:0.1642
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.16321089528501034 0.16581326387822629
2023-08-03 22:54:21,418 - epoch:20, training loss:0.6126 validation loss:0.1632
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.16385364858433604 0.16567146992310883
2023-08-03 22:55:02,465 - epoch:21, training loss:0.6106 validation loss:0.1639
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.16389599265530705 0.1655285419896245
2023-08-03 22:55:44,697 - epoch:22, training loss:0.6107 validation loss:0.1639
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.16441094120964408 0.16507648406550288
2023-08-03 22:56:26,037 - epoch:23, training loss:0.6100 validation loss:0.1644
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.1643658610060811 0.16567727392539383
2023-08-03 22:57:06,969 - epoch:24, training loss:0.6079 validation loss:0.1644
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.16417109863832594 0.16550177475437522
2023-08-03 22:57:47,632 - epoch:25, training loss:0.6086 validation loss:0.1642
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.16413126466795802 0.16529113203287124
2023-08-03 22:58:27,757 - epoch:26, training loss:0.6090 validation loss:0.1641
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.16426505902782082 0.16530020479112864
2023-08-03 22:59:11,436 - epoch:27, training loss:0.6082 validation loss:0.1643
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.1642352889291942 0.16559553537517785
2023-08-03 22:59:54,788 - epoch:28, training loss:0.6082 validation loss:0.1642
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.16450080815702678 0.16569589553400874
2023-08-03 23:00:39,632 - epoch:29, training loss:0.6094 validation loss:0.1645
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-22:38:55.032042/0/0.1615_epoch_6.pkl  &  0.16041282834485174
2023-08-03 23:00:45,169 - [*] loss:0.3635
2023-08-03 23:00:45,179 - [*] phase 0, testing
2023-08-03 23:00:45,466 - T:336	MAE	0.392836	RMSE	0.358821	MAPE	159.532630
2023-08-03 23:00:45,466 - 336	mae	0.3928	
2023-08-03 23:00:45,467 - 336	rmse	0.3588	
2023-08-03 23:00:45,467 - 336	mape	159.5326	
2023-08-03 23:00:49,797 - [*] loss:0.3622
2023-08-03 23:00:49,805 - [*] phase 0, testing
2023-08-03 23:00:50,059 - T:336	MAE	0.390547	RMSE	0.357590	MAPE	155.586922
2023-08-03 23:00:50,060 - 336	mae	0.3905	
2023-08-03 23:00:50,060 - 336	rmse	0.3576	
2023-08-03 23:00:50,060 - 336	mape	155.5869	
2023-08-03 23:00:52,297 - logger name:exp/ECL-PatchTST2023-08-03-23:00:52.296374/ECL-PatchTST.log
2023-08-03 23:00:52,297 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-23:00:52.296374', 'path': 'exp/ECL-PatchTST2023-08-03-23:00:52.296374', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 23:00:52,297 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 23:00:52,504 - [*] phase 0 Dataset load!
2023-08-03 23:00:53,558 - [*] phase 0 Training start
train 7969
2023-08-03 23:01:27,762 - epoch:0, training loss:0.2540 validation loss:0.1794
train 7969
vs, vt 0.1794192695990205 0.18175389114767312
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.17261679824441672 0.1709385732188821
2023-08-03 23:02:30,897 - epoch:1, training loss:3.0712 validation loss:0.1726
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.16882177852094174 0.1620703911408782
2023-08-03 23:03:18,661 - epoch:2, training loss:2.4401 validation loss:0.1688
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.16464273184537886 0.16133025772869586
2023-08-03 23:04:07,631 - epoch:3, training loss:1.6254 validation loss:0.1646
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16526520289480687 0.1616130167618394
2023-08-03 23:04:56,236 - epoch:4, training loss:1.1422 validation loss:0.1653
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.16411533132195472 0.165074320230633
2023-08-03 23:05:44,767 - epoch:5, training loss:0.9824 validation loss:0.1641
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.16312153227627277 0.16284376652911306
2023-08-03 23:06:34,476 - epoch:6, training loss:0.9241 validation loss:0.1631
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.16291046077385546 0.16074128597974777
2023-08-03 23:07:24,465 - epoch:7, training loss:0.8928 validation loss:0.1629
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.16276012491434813 0.16690604090690614
2023-08-03 23:08:14,154 - epoch:8, training loss:0.8505 validation loss:0.1628
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.16243139626458286 0.16129348082467915
2023-08-03 23:09:04,444 - epoch:9, training loss:0.8069 validation loss:0.1624
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.16799763906747103 0.16460526678711176
2023-08-03 23:09:54,426 - epoch:10, training loss:0.7530 validation loss:0.1680
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.16290594963356853 0.1603987687267363
2023-08-03 23:10:44,501 - epoch:11, training loss:0.7328 validation loss:0.1629
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.16665880791842938 0.16178737860172987
2023-08-03 23:11:34,499 - epoch:12, training loss:0.7217 validation loss:0.1667
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.1625614701770246 0.16205160170793534
2023-08-03 23:12:24,416 - epoch:13, training loss:0.7036 validation loss:0.1626
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.16362679954618214 0.161334244068712
2023-08-03 23:13:15,170 - epoch:14, training loss:0.7097 validation loss:0.1636
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.16434729862958192 0.15986889163032175
2023-08-03 23:14:05,889 - epoch:15, training loss:0.7228 validation loss:0.1643
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.16539883716031908 0.16103685582056643
2023-08-03 23:14:57,153 - epoch:16, training loss:0.7211 validation loss:0.1654
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.16428546328097582 0.1604896103963256
2023-08-03 23:15:48,266 - epoch:17, training loss:0.7183 validation loss:0.1643
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.16505316868424416 0.16148802721872926
2023-08-03 23:16:38,619 - epoch:18, training loss:0.7238 validation loss:0.1651
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.16513916663825512 0.16130796680226922
2023-08-03 23:17:28,363 - epoch:19, training loss:0.7194 validation loss:0.1651
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.16616503708064556 0.16199479037895798
2023-08-03 23:18:18,001 - epoch:20, training loss:0.7111 validation loss:0.1662
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.16586103960871695 0.1619984045624733
2023-08-03 23:19:08,137 - epoch:21, training loss:0.7145 validation loss:0.1659
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.16572155393660068 0.16212384747341274
2023-08-03 23:19:58,071 - epoch:22, training loss:0.7185 validation loss:0.1657
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.16600294690579176 0.16201309291645885
2023-08-03 23:20:47,791 - epoch:23, training loss:0.7216 validation loss:0.1660
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.1657286850735545 0.16211864994838834
2023-08-03 23:21:36,098 - epoch:24, training loss:0.7143 validation loss:0.1657
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.16598034501075745 0.16223622718825936
2023-08-03 23:22:24,404 - epoch:25, training loss:0.7216 validation loss:0.1660
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.1664630478248 0.16259187292307614
2023-08-03 23:23:12,861 - epoch:26, training loss:0.7197 validation loss:0.1665
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.16577929127961397 0.1619225800037384
2023-08-03 23:23:59,633 - epoch:27, training loss:0.7162 validation loss:0.1658
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.16594817936420442 0.16227922942489387
2023-08-03 23:24:48,061 - epoch:28, training loss:0.7105 validation loss:0.1659
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.166019750200212 0.16204812992364168
2023-08-03 23:25:33,390 - epoch:29, training loss:0.7229 validation loss:0.1660
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-23:00:52.296374/0/0.1624_epoch_9.pkl  &  0.15986889163032175
2023-08-03 23:25:39,505 - [*] loss:0.3647
2023-08-03 23:25:39,521 - [*] phase 0, testing
2023-08-03 23:25:39,877 - T:336	MAE	0.394722	RMSE	0.360237	MAPE	161.228144
2023-08-03 23:25:39,878 - 336	mae	0.3947	
2023-08-03 23:25:39,878 - 336	rmse	0.3602	
2023-08-03 23:25:39,878 - 336	mape	161.2281	
2023-08-03 23:25:45,840 - [*] loss:0.3610
2023-08-03 23:25:45,849 - [*] phase 0, testing
2023-08-03 23:25:46,169 - T:336	MAE	0.390177	RMSE	0.356656	MAPE	155.943906
2023-08-03 23:25:46,170 - 336	mae	0.3902	
2023-08-03 23:25:46,170 - 336	rmse	0.3567	
2023-08-03 23:25:46,170 - 336	mape	155.9439	
2023-08-03 23:25:49,187 - logger name:exp/ECL-PatchTST2023-08-03-23:25:49.187130/ECL-PatchTST.log
2023-08-03 23:25:49,188 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-23:25:49.187130', 'path': 'exp/ECL-PatchTST2023-08-03-23:25:49.187130', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 23:25:49,188 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 23:25:49,468 - [*] phase 0 Dataset load!
2023-08-03 23:25:50,817 - [*] phase 0 Training start
train 7969
2023-08-03 23:26:21,576 - epoch:0, training loss:0.7817 validation loss:0.4167
train 7969
vs, vt 0.4167296431958675 0.4123847097158432
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.3876314040273428 0.3871305737644434
2023-08-03 23:27:23,627 - epoch:1, training loss:0.7850 validation loss:0.3876
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.37420373409986496 0.38321346528828143
2023-08-03 23:28:07,892 - epoch:2, training loss:0.7256 validation loss:0.3742
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.383265845105052 0.37648963592946527
2023-08-03 23:28:52,433 - epoch:3, training loss:0.6805 validation loss:0.3833
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.3827069777995348 0.37858289033174514
2023-08-03 23:29:37,282 - epoch:4, training loss:0.6548 validation loss:0.3827
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.3871807973831892 0.37936418838799
2023-08-03 23:30:19,959 - epoch:5, training loss:0.6296 validation loss:0.3872
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.39621306136250495 0.3698203571140766
2023-08-03 23:31:05,138 - epoch:6, training loss:0.6046 validation loss:0.3962
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.3946853868663311 0.3632135380059481
2023-08-03 23:31:47,844 - epoch:7, training loss:0.5824 validation loss:0.3947
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.39806302525103093 0.3680301671847701
2023-08-03 23:32:31,328 - epoch:8, training loss:0.5665 validation loss:0.3981
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.3968516692519188 0.36661980152130125
2023-08-03 23:33:13,675 - epoch:9, training loss:0.5552 validation loss:0.3969
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.39830633625388145 0.3673460818827152
2023-08-03 23:33:56,282 - epoch:10, training loss:0.5446 validation loss:0.3983
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.4062670383602381 0.36994571927934883
2023-08-03 23:34:38,482 - epoch:11, training loss:0.5369 validation loss:0.4063
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.4073169704526663 0.3746752716600895
2023-08-03 23:35:21,836 - epoch:12, training loss:0.5289 validation loss:0.4073
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.40702265202999116 0.37205852307379245
2023-08-03 23:36:05,097 - epoch:13, training loss:0.5229 validation loss:0.4070
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.40798113904893396 0.3718140233308077
2023-08-03 23:36:46,772 - epoch:14, training loss:0.5173 validation loss:0.4080
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.4137137483805418 0.3720414288341999
2023-08-03 23:37:27,775 - epoch:15, training loss:0.5121 validation loss:0.4137
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.4082370851188898 0.37868351098150016
2023-08-03 23:38:09,071 - epoch:16, training loss:0.5073 validation loss:0.4082
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.4129898238927126 0.3762074578553438
2023-08-03 23:38:51,077 - epoch:17, training loss:0.5045 validation loss:0.4130
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.4131840441375971 0.379788401350379
2023-08-03 23:39:33,008 - epoch:18, training loss:0.5002 validation loss:0.4132
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.412611224129796 0.3769471328705549
2023-08-03 23:40:14,007 - epoch:19, training loss:0.4970 validation loss:0.4126
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.4163555298000574 0.3786321684718132
2023-08-03 23:40:55,650 - epoch:20, training loss:0.4935 validation loss:0.4164
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.4164810419082642 0.3801243081688881
2023-08-03 23:41:37,859 - epoch:21, training loss:0.4929 validation loss:0.4165
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.4154406826943159 0.3767362147569656
2023-08-03 23:42:19,420 - epoch:22, training loss:0.4908 validation loss:0.4154
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.4173882454633713 0.3798141680657864
2023-08-03 23:43:01,388 - epoch:23, training loss:0.4896 validation loss:0.4174
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.4151085015386343 0.37885707803070545
2023-08-03 23:43:43,193 - epoch:24, training loss:0.4890 validation loss:0.4151
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.41749700978398324 0.3788635164499283
2023-08-03 23:44:23,099 - epoch:25, training loss:0.4875 validation loss:0.4175
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.41619018092751503 0.378339447453618
2023-08-03 23:45:06,267 - epoch:26, training loss:0.4869 validation loss:0.4162
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.4186307225376368 0.3795242987573147
2023-08-03 23:45:49,002 - epoch:27, training loss:0.4876 validation loss:0.4186
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.4185335271060467 0.3785880718380213
2023-08-03 23:46:34,323 - epoch:28, training loss:0.4858 validation loss:0.4185
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.41656267531216146 0.3783268123865128
2023-08-03 23:47:17,668 - epoch:29, training loss:0.4858 validation loss:0.4166
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-23:25:49.187130/0/0.3742_epoch_2.pkl  &  0.3632135380059481
2023-08-03 23:47:23,729 - [*] loss:0.3742
2023-08-03 23:47:23,738 - [*] phase 0, testing
2023-08-03 23:47:24,155 - T:336	MAE	0.405891	RMSE	0.369685	MAPE	169.481814
2023-08-03 23:47:24,156 - 336	mae	0.4059	
2023-08-03 23:47:24,156 - 336	rmse	0.3697	
2023-08-03 23:47:24,156 - 336	mape	169.4818	
2023-08-03 23:47:28,627 - [*] loss:0.3632
2023-08-03 23:47:28,636 - [*] phase 0, testing
2023-08-03 23:47:28,925 - T:336	MAE	0.393712	RMSE	0.359039	MAPE	158.533692
2023-08-03 23:47:28,926 - 336	mae	0.3937	
2023-08-03 23:47:28,926 - 336	rmse	0.3590	
2023-08-03 23:47:28,926 - 336	mape	158.5337	
2023-08-03 23:47:31,249 - logger name:exp/ECL-PatchTST2023-08-03-23:47:31.249370/ECL-PatchTST.log
2023-08-03 23:47:31,249 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-03-23:47:31.249370', 'path': 'exp/ECL-PatchTST2023-08-03-23:47:31.249370', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-03 23:47:31,250 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-03 23:47:31,475 - [*] phase 0 Dataset load!
2023-08-03 23:47:32,539 - [*] phase 0 Training start
train 7969
2023-08-03 23:48:06,986 - epoch:0, training loss:0.7274 validation loss:0.4006
train 7969
vs, vt 0.4006344098597765 0.40311049185693265
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.3826471149921417 0.37841304801404474
2023-08-03 23:49:10,617 - epoch:1, training loss:8.0023 validation loss:0.3826
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.3720865834504366 0.3586102470755577
2023-08-03 23:50:00,234 - epoch:2, training loss:6.0607 validation loss:0.3721
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.3739914637058973 0.3620452854782343
2023-08-03 23:50:50,321 - epoch:3, training loss:3.8579 validation loss:0.3740
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.3651581685990095 0.36686019971966743
2023-08-03 23:51:39,045 - epoch:4, training loss:2.7806 validation loss:0.3652
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.36715830601751803 0.3679791260510683
2023-08-03 23:52:28,660 - epoch:5, training loss:2.4826 validation loss:0.3672
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.3689790707081556 0.36671629138290884
2023-08-03 23:53:18,264 - epoch:6, training loss:2.3910 validation loss:0.3690
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.3671717677265406 0.37243923880159857
2023-08-03 23:54:08,438 - epoch:7, training loss:2.3324 validation loss:0.3672
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.3712715171277523 0.37072234861552716
2023-08-03 23:54:56,946 - epoch:8, training loss:2.2338 validation loss:0.3713
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.3656015045940876 0.36900957226753234
2023-08-03 23:55:47,390 - epoch:9, training loss:2.1512 validation loss:0.3656
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.3707634065300226 0.36793573424220083
2023-08-03 23:56:37,868 - epoch:10, training loss:2.0377 validation loss:0.3708
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.3717238076031208 0.36866439916193483
2023-08-03 23:57:28,722 - epoch:11, training loss:2.0144 validation loss:0.3717
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.3726979736238718 0.36921371780335904
2023-08-03 23:58:19,530 - epoch:12, training loss:2.0182 validation loss:0.3727
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.375280587375164 0.3740428064018488
2023-08-03 23:59:08,452 - epoch:13, training loss:2.0021 validation loss:0.3753
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.3738173760473728 0.37330408915877344
2023-08-03 23:59:59,101 - epoch:14, training loss:1.9666 validation loss:0.3738
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.376391863077879 0.37846346385776997
2023-08-04 00:00:49,775 - epoch:15, training loss:1.9053 validation loss:0.3764
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.37813144475221633 0.3799860145896673
2023-08-04 00:01:38,002 - epoch:16, training loss:1.8855 validation loss:0.3781
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.3791930049657822 0.3799901753664017
2023-08-04 00:02:26,822 - epoch:17, training loss:1.9046 validation loss:0.3792
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.3771681431680918 0.3811184836551547
2023-08-04 00:03:16,622 - epoch:18, training loss:1.8703 validation loss:0.3772
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.379359819740057 0.3801983932033181
2023-08-04 00:04:04,586 - epoch:19, training loss:1.8767 validation loss:0.3794
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.3789852850139141 0.38339968509972094
2023-08-04 00:04:53,383 - epoch:20, training loss:1.8592 validation loss:0.3790
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.3801446437835693 0.38584033362567427
2023-08-04 00:05:43,219 - epoch:21, training loss:1.8551 validation loss:0.3801
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.38159756995737554 0.38281832356005907
2023-08-04 00:06:32,266 - epoch:22, training loss:1.8534 validation loss:0.3816
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.3813325148075819 0.383545177988708
2023-08-04 00:07:20,580 - epoch:23, training loss:1.8474 validation loss:0.3813
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.38272066973149776 0.3855605725198984
2023-08-04 00:08:07,069 - epoch:24, training loss:1.8235 validation loss:0.3827
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.38191348761320115 0.38505185060203073
2023-08-04 00:08:55,353 - epoch:25, training loss:1.8287 validation loss:0.3819
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.3811727620661259 0.3832943711429834
2023-08-04 00:09:44,385 - epoch:26, training loss:1.8583 validation loss:0.3812
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.3810183621942997 0.3831610487774014
2023-08-04 00:10:39,449 - epoch:27, training loss:1.8492 validation loss:0.3810
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.38248599134385586 0.385032607242465
2023-08-04 00:11:26,024 - epoch:28, training loss:1.8373 validation loss:0.3825
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.3818032737821341 0.38386278729885814
2023-08-04 00:12:13,797 - epoch:29, training loss:1.8073 validation loss:0.3818
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-03-23:47:31.249370/0/0.3652_epoch_4.pkl  &  0.3586102470755577
2023-08-04 00:12:19,990 - [*] loss:0.3652
2023-08-04 00:12:20,011 - [*] phase 0, testing
2023-08-04 00:12:20,671 - T:336	MAE	0.397539	RMSE	0.360606	MAPE	163.938057
2023-08-04 00:12:20,672 - 336	mae	0.3975	
2023-08-04 00:12:20,672 - 336	rmse	0.3606	
2023-08-04 00:12:20,672 - 336	mape	163.9381	
2023-08-04 00:12:27,523 - [*] loss:0.3586
2023-08-04 00:12:27,534 - [*] phase 0, testing
2023-08-04 00:12:27,881 - T:336	MAE	0.400994	RMSE	0.354227	MAPE	166.033494
2023-08-04 00:12:27,882 - 336	mae	0.4010	
2023-08-04 00:12:27,882 - 336	rmse	0.3542	
2023-08-04 00:12:27,883 - 336	mape	166.0335	
2023-08-04 00:12:30,530 - logger name:exp/ECL-PatchTST2023-08-04-00:12:30.529530/ECL-PatchTST.log
2023-08-04 00:12:30,530 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-00:12:30.529530', 'path': 'exp/ECL-PatchTST2023-08-04-00:12:30.529530', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 00:12:30,531 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-04 00:12:30,829 - [*] phase 0 Dataset load!
2023-08-04 00:12:32,011 - [*] phase 0 Training start
train 7969
2023-08-04 00:13:02,925 - epoch:0, training loss:0.2543 validation loss:0.1796
train 7969
vs, vt 0.17958344966173173 0.1804242931306362
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.17189027797430753 0.16928006689995528
2023-08-04 00:14:00,672 - epoch:1, training loss:0.7111 validation loss:0.1719
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.16639502365142106 0.16090102829039096
2023-08-04 00:14:44,475 - epoch:2, training loss:0.6525 validation loss:0.1664
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.16766139715909958 0.16170224715024234
2023-08-04 00:15:29,781 - epoch:3, training loss:0.5892 validation loss:0.1677
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16907214801758527 0.16375024085864426
2023-08-04 00:16:14,851 - epoch:4, training loss:0.5345 validation loss:0.1691
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.16844581523910165 0.16286407494917512
2023-08-04 00:16:58,282 - epoch:5, training loss:0.5086 validation loss:0.1684
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.16943605281412602 0.16293353792279958
2023-08-04 00:17:45,625 - epoch:6, training loss:0.4954 validation loss:0.1694
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.16701161153614522 0.16299490816891193
2023-08-04 00:18:31,877 - epoch:7, training loss:0.4884 validation loss:0.1670
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.1684619899839163 0.1620491218753159
2023-08-04 00:19:18,093 - epoch:8, training loss:0.4836 validation loss:0.1685
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.16915800236165524 0.1614160265773535
2023-08-04 00:20:03,890 - epoch:9, training loss:0.4771 validation loss:0.1692
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.1705720791593194 0.16149365017190576
2023-08-04 00:20:50,791 - epoch:10, training loss:0.4630 validation loss:0.1706
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.17228556219488383 0.16143804332241415
2023-08-04 00:21:37,043 - epoch:11, training loss:0.4614 validation loss:0.1723
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.17058978993445634 0.16135862078517677
2023-08-04 00:22:22,729 - epoch:12, training loss:0.4560 validation loss:0.1706
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.1717741223052144 0.15976885464042426
2023-08-04 00:23:07,243 - epoch:13, training loss:0.4518 validation loss:0.1718
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.17221312765032054 0.16041568061336875
2023-08-04 00:23:52,080 - epoch:14, training loss:0.4504 validation loss:0.1722
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.173364326544106 0.16138312984257935
2023-08-04 00:24:36,407 - epoch:15, training loss:0.4507 validation loss:0.1734
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.1734619364142418 0.16041035987436772
2023-08-04 00:25:22,284 - epoch:16, training loss:0.4421 validation loss:0.1735
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.173386375233531 0.16044738730415703
2023-08-04 00:26:08,155 - epoch:17, training loss:0.4423 validation loss:0.1734
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.17512741554528474 0.16109427427873016
2023-08-04 00:26:52,378 - epoch:18, training loss:0.4415 validation loss:0.1751
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.1736587041988969 0.16189422132447362
2023-08-04 00:27:38,991 - epoch:19, training loss:0.4392 validation loss:0.1737
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.1748612118884921 0.16150796953588725
2023-08-04 00:28:25,287 - epoch:20, training loss:0.4347 validation loss:0.1749
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.17490988671779634 0.16234173020347953
2023-08-04 00:29:12,153 - epoch:21, training loss:0.4355 validation loss:0.1749
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.17415226586163043 0.1625318180769682
2023-08-04 00:29:58,237 - epoch:22, training loss:0.4355 validation loss:0.1742
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.17469416111707686 0.1616488772444427
2023-08-04 00:30:45,421 - epoch:23, training loss:0.4353 validation loss:0.1747
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.17399241887032985 0.1624011284671724
2023-08-04 00:31:31,288 - epoch:24, training loss:0.4335 validation loss:0.1740
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.1742621475830674 0.16203878885135053
2023-08-04 00:32:19,149 - epoch:25, training loss:0.4346 validation loss:0.1743
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.17440836373716592 0.1618776903487742
2023-08-04 00:33:05,645 - epoch:26, training loss:0.4333 validation loss:0.1744
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.17480353116989136 0.1618539411574602
2023-08-04 00:33:51,453 - epoch:27, training loss:0.4357 validation loss:0.1748
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.17460078299045562 0.16224832609295844
2023-08-04 00:34:37,553 - epoch:28, training loss:0.4320 validation loss:0.1746
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.1744152557104826 0.16196628361940385
2023-08-04 00:35:26,698 - epoch:29, training loss:0.4347 validation loss:0.1744
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-04-00:12:30.529530/0/0.1664_epoch_2.pkl  &  0.15976885464042426
2023-08-04 00:35:32,050 - [*] loss:0.3733
2023-08-04 00:35:32,058 - [*] phase 0, testing
2023-08-04 00:35:32,208 - T:336	MAE	0.401488	RMSE	0.368695	MAPE	164.227974
2023-08-04 00:35:32,208 - 336	mae	0.4015	
2023-08-04 00:35:32,208 - 336	rmse	0.3687	
2023-08-04 00:35:32,209 - 336	mape	164.2280	
2023-08-04 00:35:37,707 - [*] loss:0.3592
2023-08-04 00:35:37,716 - [*] phase 0, testing
2023-08-04 00:35:37,872 - T:336	MAE	0.390310	RMSE	0.354744	MAPE	157.168305
2023-08-04 00:35:37,873 - 336	mae	0.3903	
2023-08-04 00:35:37,873 - 336	rmse	0.3547	
2023-08-04 00:35:37,873 - 336	mape	157.1683	
2023-08-04 00:35:40,587 - logger name:exp/ECL-PatchTST2023-08-04-00:35:40.587065/ECL-PatchTST.log
2023-08-04 00:35:40,588 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-00:35:40.587065', 'path': 'exp/ECL-PatchTST2023-08-04-00:35:40.587065', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 00:35:40,588 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-04 00:35:40,843 - [*] phase 0 Dataset load!
2023-08-04 00:35:42,015 - [*] phase 0 Training start
train 7969
2023-08-04 00:36:15,160 - epoch:0, training loss:0.2713 validation loss:0.1861
train 7969
vs, vt 0.18606640696525573 0.18458142075687647
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.1762928307056427 0.17420527450740336
2023-08-04 00:37:12,003 - epoch:1, training loss:1.3929 validation loss:0.1763
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.16739275064319373 0.17534079179167747
2023-08-04 00:37:54,328 - epoch:2, training loss:1.1043 validation loss:0.1674
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.1687181556597352 0.1679232569411397
2023-08-04 00:38:37,154 - epoch:3, training loss:0.8374 validation loss:0.1687
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16563968267291784 0.1687107871286571
2023-08-04 00:39:19,163 - epoch:4, training loss:0.7209 validation loss:0.1656
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.16499768001958728 0.1676021119579673
2023-08-04 00:40:01,904 - epoch:5, training loss:0.6704 validation loss:0.1650
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.1634133467450738 0.16348740756511687
2023-08-04 00:40:43,972 - epoch:6, training loss:0.6463 validation loss:0.1634
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.1618995669297874 0.16268926532939076
2023-08-04 00:41:25,613 - epoch:7, training loss:0.6359 validation loss:0.1619
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.1665754488669336 0.16075872350484133
2023-08-04 00:42:08,740 - epoch:8, training loss:0.6257 validation loss:0.1666
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.16380771351978182 0.16415417324751616
2023-08-04 00:42:51,649 - epoch:9, training loss:0.6190 validation loss:0.1638
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.16333988131955265 0.162934457603842
2023-08-04 00:43:36,455 - epoch:10, training loss:0.6163 validation loss:0.1633
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.16276204260066152 0.16289323307573794
2023-08-04 00:44:19,218 - epoch:11, training loss:0.6173 validation loss:0.1628
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.16266918675974013 0.16166053730994462
2023-08-04 00:45:02,788 - epoch:12, training loss:0.6186 validation loss:0.1627
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.16246092915534974 0.16291156262159348
2023-08-04 00:45:46,392 - epoch:13, training loss:0.6172 validation loss:0.1625
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.16395515538752078 0.1627190263941884
2023-08-04 00:46:30,408 - epoch:14, training loss:0.6198 validation loss:0.1640
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.1636816463433206 0.16326403841376305
2023-08-04 00:47:13,066 - epoch:15, training loss:0.6167 validation loss:0.1637
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.1644517251290381 0.16392281800508499
2023-08-04 00:47:56,615 - epoch:16, training loss:0.6166 validation loss:0.1645
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.16465001637116075 0.1638239251449704
2023-08-04 00:48:40,190 - epoch:17, training loss:0.6162 validation loss:0.1647
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.16389794703572988 0.16426971983164548
2023-08-04 00:49:25,536 - epoch:18, training loss:0.6148 validation loss:0.1639
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.1648701958358288 0.1638320659287274
2023-08-04 00:50:08,450 - epoch:19, training loss:0.6150 validation loss:0.1649
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.16562306974083185 0.1644819712266326
2023-08-04 00:50:50,809 - epoch:20, training loss:0.6157 validation loss:0.1656
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.16556838443502783 0.16569376904517413
2023-08-04 00:51:33,829 - epoch:21, training loss:0.6131 validation loss:0.1656
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.1656686750240624 0.1646505633369088
2023-08-04 00:52:17,407 - epoch:22, training loss:0.6125 validation loss:0.1657
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.16569384541362525 0.165397072955966
2023-08-04 00:53:02,044 - epoch:23, training loss:0.6113 validation loss:0.1657
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.16632671589031817 0.16545774145051836
2023-08-04 00:53:45,616 - epoch:24, training loss:0.6097 validation loss:0.1663
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.1657762685790658 0.16562071396037936
2023-08-04 00:54:29,109 - epoch:25, training loss:0.6102 validation loss:0.1658
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.16596592366695403 0.1651960125193
2023-08-04 00:55:13,686 - epoch:26, training loss:0.6094 validation loss:0.1660
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.1661278628744185 0.16541545931249857
2023-08-04 00:55:59,461 - epoch:27, training loss:0.6096 validation loss:0.1661
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.16606737915426492 0.16522494796663523
2023-08-04 00:56:43,976 - epoch:28, training loss:0.6096 validation loss:0.1661
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.16592862978577613 0.16515134163200856
2023-08-04 00:57:32,234 - epoch:29, training loss:0.6095 validation loss:0.1659
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-04-00:35:40.587065/0/0.1619_epoch_7.pkl  &  0.16075872350484133
2023-08-04 00:57:37,531 - [*] loss:0.3639
2023-08-04 00:57:37,543 - [*] phase 0, testing
2023-08-04 00:57:37,709 - T:336	MAE	0.393510	RMSE	0.359204	MAPE	161.981463
2023-08-04 00:57:37,710 - 336	mae	0.3935	
2023-08-04 00:57:37,710 - 336	rmse	0.3592	
2023-08-04 00:57:37,710 - 336	mape	161.9815	
2023-08-04 00:57:41,862 - [*] loss:0.3623
2023-08-04 00:57:41,872 - [*] phase 0, testing
2023-08-04 00:57:42,027 - T:336	MAE	0.391202	RMSE	0.357824	MAPE	156.752467
2023-08-04 00:57:42,027 - 336	mae	0.3912	
2023-08-04 00:57:42,027 - 336	rmse	0.3578	
2023-08-04 00:57:42,028 - 336	mape	156.7525	
2023-08-04 00:57:44,570 - logger name:exp/ECL-PatchTST2023-08-04-00:57:44.569740/ECL-PatchTST.log
2023-08-04 00:57:44,570 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-00:57:44.569740', 'path': 'exp/ECL-PatchTST2023-08-04-00:57:44.569740', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 00:57:44,570 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-04 00:57:44,789 - [*] phase 0 Dataset load!
2023-08-04 00:57:45,979 - [*] phase 0 Training start
train 7969
2023-08-04 00:58:21,018 - epoch:0, training loss:0.2543 validation loss:0.1796
train 7969
vs, vt 0.17958344966173173 0.1804242931306362
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.17300092410296203 0.16927948240190743
2023-08-04 00:59:26,816 - epoch:1, training loss:2.9709 validation loss:0.1730
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.1681740904226899 0.16109119728207588
2023-08-04 01:00:17,963 - epoch:2, training loss:2.3318 validation loss:0.1682
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.16926850210875272 0.16211287640035152
2023-08-04 01:01:07,250 - epoch:3, training loss:1.5175 validation loss:0.1693
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16439930591732263 0.1648804650641978
2023-08-04 01:01:59,864 - epoch:4, training loss:1.0904 validation loss:0.1644
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.1650696420110762 0.16409355765208603
2023-08-04 01:02:49,376 - epoch:5, training loss:0.9519 validation loss:0.1651
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.16367417173460125 0.16370020024478435
2023-08-04 01:03:41,197 - epoch:6, training loss:0.8871 validation loss:0.1637
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.1626770962961018 0.16347268521785735
2023-08-04 01:04:30,982 - epoch:7, training loss:0.8469 validation loss:0.1627
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.1633264455012977 0.16373300645500422
2023-08-04 01:05:22,329 - epoch:8, training loss:0.8070 validation loss:0.1633
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.1633166430518031 0.16277112048119308
2023-08-04 01:06:13,161 - epoch:9, training loss:0.7634 validation loss:0.1633
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.16198948016390205 0.16178516633808612
2023-08-04 01:07:03,429 - epoch:10, training loss:0.7207 validation loss:0.1620
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.1666073958389461 0.1628150511533022
2023-08-04 01:07:54,368 - epoch:11, training loss:0.7210 validation loss:0.1666
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.1633522043004632 0.16234213039278983
2023-08-04 01:08:45,084 - epoch:12, training loss:0.6858 validation loss:0.1634
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.1642948062159121 0.1628803128376603
2023-08-04 01:09:36,696 - epoch:13, training loss:0.7060 validation loss:0.1643
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.16414753003045918 0.1639602399431169
2023-08-04 01:10:27,544 - epoch:14, training loss:0.6828 validation loss:0.1641
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.16467016637325288 0.16395037649199368
2023-08-04 01:11:18,455 - epoch:15, training loss:0.6742 validation loss:0.1647
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.1636320000514388 0.16373626776039601
2023-08-04 01:12:09,387 - epoch:16, training loss:0.6698 validation loss:0.1636
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.16372418869286776 0.16301295598968862
2023-08-04 01:12:59,920 - epoch:17, training loss:0.6770 validation loss:0.1637
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.1639744618907571 0.16435125656425953
2023-08-04 01:13:51,154 - epoch:18, training loss:0.6673 validation loss:0.1640
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.16365325571969153 0.16448715394362806
2023-08-04 01:14:41,095 - epoch:19, training loss:0.6663 validation loss:0.1637
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.1645732458680868 0.1656133433803916
2023-08-04 01:15:32,058 - epoch:20, training loss:0.6632 validation loss:0.1646
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.16426534419879318 0.16637770971283317
2023-08-04 01:16:21,383 - epoch:21, training loss:0.6609 validation loss:0.1643
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.16425866018980742 0.16605923203751444
2023-08-04 01:17:10,081 - epoch:22, training loss:0.6638 validation loss:0.1643
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.16438643326982855 0.16547312075272202
2023-08-04 01:17:59,093 - epoch:23, training loss:0.6629 validation loss:0.1644
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.16453670086339117 0.1660275266505778
2023-08-04 01:18:48,475 - epoch:24, training loss:0.6571 validation loss:0.1645
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.16456337217241526 0.1658435443416238
2023-08-04 01:19:38,395 - epoch:25, training loss:0.6592 validation loss:0.1646
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.1643999544903636 0.16556967506185175
2023-08-04 01:20:30,656 - epoch:26, training loss:0.6614 validation loss:0.1644
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.16412230469286443 0.16546886414289474
2023-08-04 01:21:23,715 - epoch:27, training loss:0.6637 validation loss:0.1641
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.16470138337463142 0.16599164986982942
2023-08-04 01:22:19,427 - epoch:28, training loss:0.6636 validation loss:0.1647
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.16449064239859582 0.1656883295625448
2023-08-04 01:23:18,183 - epoch:29, training loss:0.6534 validation loss:0.1645
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-04-00:57:44.569740/0/0.162_epoch_10.pkl  &  0.16109119728207588
2023-08-04 01:23:26,577 - [*] loss:0.3638
2023-08-04 01:23:26,740 - [*] phase 0, testing
2023-08-04 01:23:27,841 - T:336	MAE	0.394520	RMSE	0.359493	MAPE	160.703003
2023-08-04 01:23:27,845 - 336	mae	0.3945	
2023-08-04 01:23:27,845 - 336	rmse	0.3595	
2023-08-04 01:23:27,845 - 336	mape	160.7030	
2023-08-04 01:23:35,098 - [*] loss:0.3577
2023-08-04 01:23:35,366 - [*] phase 0, testing
2023-08-04 01:23:37,025 - T:336	MAE	0.400358	RMSE	0.353325	MAPE	166.059184
2023-08-04 01:23:37,033 - 336	mae	0.4004	
2023-08-04 01:23:37,033 - 336	rmse	0.3533	
2023-08-04 01:23:37,033 - 336	mape	166.0592	
2023-08-04 01:23:40,903 - logger name:exp/ECL-PatchTST2023-08-04-01:23:40.897838/ECL-PatchTST.log
2023-08-04 01:23:40,903 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-01:23:40.897838', 'path': 'exp/ECL-PatchTST2023-08-04-01:23:40.897838', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 01:23:40,903 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-04 01:23:41,252 - [*] phase 0 Dataset load!
2023-08-04 01:23:44,929 - [*] phase 0 Training start
train 7969
2023-08-04 01:24:26,813 - epoch:0, training loss:0.7805 validation loss:0.4173
train 7969
vs, vt 0.4172559309750795 0.41229391992092135
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.38917614556849 0.38716056309640406
2023-08-04 01:25:43,318 - epoch:1, training loss:0.7839 validation loss:0.3892
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.3803667314350605 0.38423193134367467
2023-08-04 01:26:39,637 - epoch:2, training loss:0.7261 validation loss:0.3804
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.3722642567008734 0.377781293168664
2023-08-04 01:27:34,802 - epoch:3, training loss:0.6820 validation loss:0.3723
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.38287182301282885 0.37348058968782427
2023-08-04 01:28:30,533 - epoch:4, training loss:0.6527 validation loss:0.3829
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.3829612024128437 0.37500788550823927
2023-08-04 01:29:26,020 - epoch:5, training loss:0.6204 validation loss:0.3830
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.3854151040315628 0.37295592948794365
2023-08-04 01:30:21,495 - epoch:6, training loss:0.5989 validation loss:0.3854
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.3870586175471544 0.3672395724803209
2023-08-04 01:31:16,585 - epoch:7, training loss:0.5824 validation loss:0.3871
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.40181018859148027 0.36394464075565336
2023-08-04 01:32:11,506 - epoch:8, training loss:0.5680 validation loss:0.4018
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.39775267615914345 0.3693282788619399
2023-08-04 01:33:05,767 - epoch:9, training loss:0.5551 validation loss:0.3978
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.4018056817352772 0.36097380686551334
2023-08-04 01:33:58,739 - epoch:10, training loss:0.5443 validation loss:0.4018
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.4130539521574974 0.36427293829619883
2023-08-04 01:34:49,361 - epoch:11, training loss:0.5378 validation loss:0.4131
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.40830092169344423 0.3694477269425988
2023-08-04 01:35:42,898 - epoch:12, training loss:0.5284 validation loss:0.4083
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.41110352501273156 0.36893864534795284
2023-08-04 01:36:34,164 - epoch:13, training loss:0.5207 validation loss:0.4111
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.41804004870355127 0.37255688905715945
2023-08-04 01:37:26,887 - epoch:14, training loss:0.5145 validation loss:0.4180
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.4172793820500374 0.37109893318265674
2023-08-04 01:38:20,050 - epoch:15, training loss:0.5101 validation loss:0.4173
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.4202317051589489 0.3736016919836402
2023-08-04 01:39:12,771 - epoch:16, training loss:0.5060 validation loss:0.4202
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.4224001131951809 0.37213861756026745
2023-08-04 01:40:05,035 - epoch:17, training loss:0.5026 validation loss:0.4224
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.42374623827636243 0.37284641098231075
2023-08-04 01:40:58,320 - epoch:18, training loss:0.5005 validation loss:0.4237
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.421118275821209 0.37549552638083694
2023-08-04 01:41:51,052 - epoch:19, training loss:0.4953 validation loss:0.4211
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.42665219455957415 0.3763864504173398
2023-08-04 01:42:43,708 - epoch:20, training loss:0.4934 validation loss:0.4267
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.42312016487121584 0.3764932433143258
2023-08-04 01:43:36,114 - epoch:21, training loss:0.4901 validation loss:0.4231
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.4251875415444374 0.3755166307091713
2023-08-04 01:44:27,791 - epoch:22, training loss:0.4883 validation loss:0.4252
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.42617694698274133 0.3752763407304883
2023-08-04 01:45:19,589 - epoch:23, training loss:0.4893 validation loss:0.4262
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.42525718063116075 0.3754185041412711
2023-08-04 01:46:12,053 - epoch:24, training loss:0.4874 validation loss:0.4253
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.426384674012661 0.3747838204726577
2023-08-04 01:47:04,571 - epoch:25, training loss:0.4860 validation loss:0.4264
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.42705991566181184 0.3754644563421607
2023-08-04 01:47:58,624 - epoch:26, training loss:0.4856 validation loss:0.4271
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.4267853945493698 0.37713833134621383
2023-08-04 01:48:51,931 - epoch:27, training loss:0.4850 validation loss:0.4268
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.4266431175172329 0.3760843396186829
2023-08-04 01:49:45,207 - epoch:28, training loss:0.4853 validation loss:0.4266
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.4256879899650812 0.3754329835996032
2023-08-04 01:50:37,714 - epoch:29, training loss:0.4842 validation loss:0.4257
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-04-01:23:40.897838/0/0.3723_epoch_3.pkl  &  0.36097380686551334
2023-08-04 01:50:45,558 - [*] loss:0.3723
2023-08-04 01:50:45,853 - [*] phase 0, testing
2023-08-04 01:50:47,303 - T:336	MAE	0.400734	RMSE	0.367559	MAPE	164.076805
2023-08-04 01:50:47,312 - 336	mae	0.4007	
2023-08-04 01:50:47,312 - 336	rmse	0.3676	
2023-08-04 01:50:47,312 - 336	mape	164.0768	
2023-08-04 01:50:55,430 - [*] loss:0.3610
2023-08-04 01:50:55,792 - [*] phase 0, testing
2023-08-04 01:50:57,150 - T:336	MAE	0.392455	RMSE	0.356934	MAPE	158.236301
2023-08-04 01:50:57,156 - 336	mae	0.3925	
2023-08-04 01:50:57,156 - 336	rmse	0.3569	
2023-08-04 01:50:57,157 - 336	mape	158.2363	
2023-08-04 01:51:03,563 - logger name:exp/ECL-PatchTST2023-08-04-01:51:03.563014/ECL-PatchTST.log
2023-08-04 01:51:03,563 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-01:51:03.563014', 'path': 'exp/ECL-PatchTST2023-08-04-01:51:03.563014', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 01:51:03,563 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-04 01:51:03,840 - [*] phase 0 Dataset load!
2023-08-04 01:51:15,215 - [*] phase 0 Training start
train 7969
2023-08-04 01:52:04,361 - epoch:0, training loss:0.7280 validation loss:0.3987
train 7969
vs, vt 0.3987369067966938 0.4035313956439495
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.38272864036262033 0.37880159355700016
2023-08-04 01:53:24,668 - epoch:1, training loss:8.2038 validation loss:0.3827
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.3753746826201677 0.3636090587824583
2023-08-04 01:54:25,889 - epoch:2, training loss:6.1191 validation loss:0.3754
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.36688816621899606 0.3584977451711893
2023-08-04 01:55:26,763 - epoch:3, training loss:3.9184 validation loss:0.3669
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.3695821654051542 0.3579050827771425
2023-08-04 01:56:26,454 - epoch:4, training loss:2.8338 validation loss:0.3696
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.36591515727341173 0.3610041841864586
2023-08-04 01:57:23,849 - epoch:5, training loss:2.4899 validation loss:0.3659
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.3681838158518076 0.35915065221488474
2023-08-04 01:58:24,404 - epoch:6, training loss:2.4001 validation loss:0.3682
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.3677385028451681 0.36264687590301037
2023-08-04 01:59:23,848 - epoch:7, training loss:2.3724 validation loss:0.3677
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.3715163443237543 0.36456137150526047
2023-08-04 02:00:23,530 - epoch:8, training loss:2.2955 validation loss:0.3715
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.3654849104583263 0.3561123989522457
2023-08-04 02:01:22,898 - epoch:9, training loss:2.2412 validation loss:0.3655
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.3679446492344141 0.3554547481238842
2023-08-04 02:02:13,615 - epoch:10, training loss:2.1887 validation loss:0.3679
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.37443547174334524 0.35824964568018913
2023-08-04 02:03:07,373 - epoch:11, training loss:2.2268 validation loss:0.3744
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.3699956238269806 0.3649141300469637
2023-08-04 02:03:56,565 - epoch:12, training loss:2.1980 validation loss:0.3700
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.36811951361596584 0.36322676166892054
2023-08-04 02:04:46,214 - epoch:13, training loss:2.1675 validation loss:0.3681
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.3721947893500328 0.35709808245301244
2023-08-04 02:05:32,872 - epoch:14, training loss:2.1752 validation loss:0.3722
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.37271213084459304 0.3624414522200823
2023-08-04 02:06:17,811 - epoch:15, training loss:2.1335 validation loss:0.3727
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.37478704787790773 0.36412840001285074
2023-08-04 02:07:04,290 - epoch:16, training loss:2.1657 validation loss:0.3748
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.3761887960135937 0.3610356546938419
2023-08-04 02:07:51,467 - epoch:17, training loss:2.1782 validation loss:0.3762
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.3739587012678385 0.36506158784031867
2023-08-04 02:08:36,850 - epoch:18, training loss:2.1194 validation loss:0.3740
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.3751072399318218 0.3631440166383982
2023-08-04 02:09:23,771 - epoch:19, training loss:2.1041 validation loss:0.3751
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.37752575874328614 0.36606682762503623
2023-08-04 02:10:08,549 - epoch:20, training loss:2.1355 validation loss:0.3775
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.3763015065342188 0.36470472887158395
2023-08-04 02:10:54,052 - epoch:21, training loss:2.1153 validation loss:0.3763
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.3788152664899826 0.36566018536686895
2023-08-04 02:11:40,257 - epoch:22, training loss:2.1367 validation loss:0.3788
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.37615845426917077 0.36366423442959783
2023-08-04 02:12:26,358 - epoch:23, training loss:2.1239 validation loss:0.3762
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.37621600925922394 0.3627175997942686
2023-08-04 02:13:12,434 - epoch:24, training loss:2.0986 validation loss:0.3762
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.37781749069690707 0.3642420884221792
2023-08-04 02:14:00,531 - epoch:25, training loss:2.1104 validation loss:0.3778
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.37704777605831624 0.36489684209227563
2023-08-04 02:14:47,553 - epoch:26, training loss:2.1102 validation loss:0.3770
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.377308090403676 0.3662776991724968
2023-08-04 02:15:33,470 - epoch:27, training loss:2.1224 validation loss:0.3773
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.377675698697567 0.3655958939343691
2023-08-04 02:16:19,737 - epoch:28, training loss:2.1023 validation loss:0.3777
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.3767393875867128 0.3644752286374569
2023-08-04 02:17:04,908 - epoch:29, training loss:2.1092 validation loss:0.3767
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-04-01:51:03.563014/0/0.3655_epoch_9.pkl  &  0.3554547481238842
2023-08-04 02:17:10,841 - [*] loss:0.3655
2023-08-04 02:17:10,850 - [*] phase 0, testing
2023-08-04 02:17:11,029 - T:336	MAE	0.398547	RMSE	0.361183	MAPE	162.861037
2023-08-04 02:17:11,030 - 336	mae	0.3985	
2023-08-04 02:17:11,030 - 336	rmse	0.3612	
2023-08-04 02:17:11,030 - 336	mape	162.8610	
2023-08-04 02:17:17,211 - [*] loss:0.3555
2023-08-04 02:17:17,221 - [*] phase 0, testing
2023-08-04 02:17:17,417 - T:336	MAE	0.389264	RMSE	0.351102	MAPE	157.129598
2023-08-04 02:17:17,418 - 336	mae	0.3893	
2023-08-04 02:17:17,418 - 336	rmse	0.3511	
2023-08-04 02:17:17,419 - 336	mape	157.1296	
2023-08-04 02:17:22,352 - logger name:exp/ECL-PatchTST2023-08-04-02:17:22.351723/ECL-PatchTST.log
2023-08-04 02:17:22,352 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-02:17:22.351723', 'path': 'exp/ECL-PatchTST2023-08-04-02:17:22.351723', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 02:17:22,352 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-04 02:17:22,732 - [*] phase 0 Dataset load!
2023-08-04 02:17:30,311 - [*] phase 0 Training start
train 7969
2023-08-04 02:18:05,226 - epoch:0, training loss:0.2547 validation loss:0.1787
train 7969
vs, vt 0.1787222580984235 0.1805749973282218
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.1730119626969099 0.16946218386292458
2023-08-04 02:19:11,935 - epoch:1, training loss:0.7008 validation loss:0.1730
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.16802798323333262 0.1620864247903228
2023-08-04 02:20:02,117 - epoch:2, training loss:0.6421 validation loss:0.1680
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.16425363216549158 0.16029210463166238
2023-08-04 02:20:52,053 - epoch:3, training loss:0.5828 validation loss:0.1643
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.1677730968222022 0.16053685136139392
2023-08-04 02:21:41,339 - epoch:4, training loss:0.5304 validation loss:0.1678
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.168538929335773 0.16154381819069386
2023-08-04 02:22:28,759 - epoch:5, training loss:0.5059 validation loss:0.1685
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.16729757096618414 0.16475353632122278
2023-08-04 02:23:18,194 - epoch:6, training loss:0.4953 validation loss:0.1673
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.16691887862980365 0.16383101297542452
2023-08-04 02:24:07,091 - epoch:7, training loss:0.4917 validation loss:0.1669
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.17070511337369682 0.16401429064571857
2023-08-04 02:25:00,022 - epoch:8, training loss:0.4793 validation loss:0.1707
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.17114358972758054 0.16083669364452363
2023-08-04 02:25:49,045 - epoch:9, training loss:0.4750 validation loss:0.1711
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.17273574620485305 0.16347271511331202
2023-08-04 02:26:39,836 - epoch:10, training loss:0.4665 validation loss:0.1727
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.17378182858228683 0.16388275921344758
2023-08-04 02:27:28,037 - epoch:11, training loss:0.4611 validation loss:0.1738
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.1744656503200531 0.16466960292309524
2023-08-04 02:28:15,095 - epoch:12, training loss:0.4532 validation loss:0.1745
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.17389819417148827 0.16404124088585376
2023-08-04 02:29:00,347 - epoch:13, training loss:0.4479 validation loss:0.1739
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.17461467515677215 0.16297024060040713
2023-08-04 02:29:46,362 - epoch:14, training loss:0.4461 validation loss:0.1746
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.1741093138232827 0.1649999225512147
2023-08-04 02:30:29,672 - epoch:15, training loss:0.4408 validation loss:0.1741
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.175829042121768 0.1646198660135269
2023-08-04 02:31:13,902 - epoch:16, training loss:0.4395 validation loss:0.1758
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.17581240236759185 0.16569143999367952
2023-08-04 02:31:57,434 - epoch:17, training loss:0.4368 validation loss:0.1758
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.17599089220166206 0.1667096395045519
2023-08-04 02:32:41,446 - epoch:18, training loss:0.4360 validation loss:0.1760
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.17575061805546283 0.16624824404716493
2023-08-04 02:33:25,571 - epoch:19, training loss:0.4345 validation loss:0.1758
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.17636619005352258 0.16632383447140456
2023-08-04 02:34:10,924 - epoch:20, training loss:0.4336 validation loss:0.1764
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.17529946714639663 0.165764388628304
2023-08-04 02:34:56,477 - epoch:21, training loss:0.4290 validation loss:0.1753
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.1759585766121745 0.16586359385401012
2023-08-04 02:35:42,016 - epoch:22, training loss:0.4302 validation loss:0.1760
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.1765747271478176 0.16566098015755415
2023-08-04 02:36:28,316 - epoch:23, training loss:0.4322 validation loss:0.1766
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.1760285733267665 0.16546019520610572
2023-08-04 02:37:14,060 - epoch:24, training loss:0.4295 validation loss:0.1760
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.17619360685348512 0.16612899973988532
2023-08-04 02:38:00,531 - epoch:25, training loss:0.4287 validation loss:0.1762
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.17624473851174116 0.16593472249805927
2023-08-04 02:38:47,613 - epoch:26, training loss:0.4285 validation loss:0.1762
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.17647594194859267 0.16653712168335916
2023-08-04 02:39:33,926 - epoch:27, training loss:0.4285 validation loss:0.1765
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.17647208273410797 0.16640714444220067
2023-08-04 02:40:22,818 - epoch:28, training loss:0.4279 validation loss:0.1765
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.1762324946001172 0.16611333135515452
2023-08-04 02:41:08,996 - epoch:29, training loss:0.4269 validation loss:0.1762
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-04-02:17:22.351723/0/0.1643_epoch_3.pkl  &  0.16029210463166238
2023-08-04 02:41:13,227 - [*] loss:0.3688
2023-08-04 02:41:13,236 - [*] phase 0, testing
2023-08-04 02:41:13,649 - T:336	MAE	0.398035	RMSE	0.364163	MAPE	162.343144
2023-08-04 02:41:13,649 - 336	mae	0.3980	
2023-08-04 02:41:13,649 - 336	rmse	0.3642	
2023-08-04 02:41:13,649 - 336	mape	162.3431	
2023-08-04 02:41:18,283 - [*] loss:0.3577
2023-08-04 02:41:18,292 - [*] phase 0, testing
2023-08-04 02:41:18,566 - T:336	MAE	0.396410	RMSE	0.353247	MAPE	161.029613
2023-08-04 02:41:18,572 - 336	mae	0.3964	
2023-08-04 02:41:18,572 - 336	rmse	0.3532	
2023-08-04 02:41:18,572 - 336	mape	161.0296	
2023-08-04 02:41:21,151 - logger name:exp/ECL-PatchTST2023-08-04-02:41:21.150881/ECL-PatchTST.log
2023-08-04 02:41:21,151 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-02:41:21.150881', 'path': 'exp/ECL-PatchTST2023-08-04-02:41:21.150881', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 02:41:21,152 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-04 02:41:21,393 - [*] phase 0 Dataset load!
2023-08-04 02:41:22,595 - [*] phase 0 Training start
train 7969
2023-08-04 02:41:57,811 - epoch:0, training loss:0.2710 validation loss:0.1861
train 7969
vs, vt 0.18605846632272005 0.18454548604786397
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.17688768710941077 0.17427780106663704
2023-08-04 02:42:55,790 - epoch:1, training loss:1.3975 validation loss:0.1769
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.17194924522191285 0.17435979880392552
2023-08-04 02:43:39,705 - epoch:2, training loss:1.1183 validation loss:0.1719
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.16503252349793912 0.1702175797894597
2023-08-04 02:44:23,666 - epoch:3, training loss:0.8545 validation loss:0.1650
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16707939319312573 0.1643549669533968
2023-08-04 02:45:07,462 - epoch:4, training loss:0.7257 validation loss:0.1671
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.16220704158768057 0.1686804009601474
2023-08-04 02:45:50,407 - epoch:5, training loss:0.6756 validation loss:0.1622
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.162848265375942 0.16468003932386638
2023-08-04 02:46:33,757 - epoch:6, training loss:0.6504 validation loss:0.1628
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.16298832399770619 0.16216658540070056
2023-08-04 02:47:17,255 - epoch:7, training loss:0.6383 validation loss:0.1630
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.16300293039530517 0.16289858324453235
2023-08-04 02:48:00,465 - epoch:8, training loss:0.6319 validation loss:0.1630
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.1612774054519832 0.16273256167769432
2023-08-04 02:48:45,202 - epoch:9, training loss:0.6295 validation loss:0.1613
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.16239668894559145 0.16223184252157807
2023-08-04 02:49:29,161 - epoch:10, training loss:0.6304 validation loss:0.1624
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.16252904217690228 0.16383077269420027
2023-08-04 02:50:14,613 - epoch:11, training loss:0.6174 validation loss:0.1625
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.16244338024407626 0.1622148615308106
2023-08-04 02:50:59,611 - epoch:12, training loss:0.6167 validation loss:0.1624
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.1608428828418255 0.16226842096075417
2023-08-04 02:51:44,945 - epoch:13, training loss:0.6140 validation loss:0.1608
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.16280612554401158 0.16256899842992426
2023-08-04 02:52:30,348 - epoch:14, training loss:0.6095 validation loss:0.1628
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.16336882822215557 0.1641359436325729
2023-08-04 02:53:15,487 - epoch:15, training loss:0.6144 validation loss:0.1634
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.16212287349626422 0.16462653866037727
2023-08-04 02:54:01,129 - epoch:16, training loss:0.6097 validation loss:0.1621
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.16310923034325242 0.16308294301852583
2023-08-04 02:54:46,912 - epoch:17, training loss:0.6114 validation loss:0.1631
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.16143835978582502 0.16420977925881744
2023-08-04 02:55:32,800 - epoch:18, training loss:0.6088 validation loss:0.1614
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.16211921647191047 0.16373780267313123
2023-08-04 02:56:17,052 - epoch:19, training loss:0.6106 validation loss:0.1621
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.16237533278763294 0.16427757600322365
2023-08-04 02:57:00,336 - epoch:20, training loss:0.6073 validation loss:0.1624
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.16221587331965565 0.16422054236754774
2023-08-04 02:57:43,701 - epoch:21, training loss:0.6094 validation loss:0.1622
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.16269083647057414 0.16399339521303774
2023-08-04 02:58:27,801 - epoch:22, training loss:0.6073 validation loss:0.1627
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.1624288296326995 0.1639121918939054
2023-08-04 02:59:10,590 - epoch:23, training loss:0.6061 validation loss:0.1624
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.16221911823377014 0.16389756398275496
2023-08-04 02:59:52,821 - epoch:24, training loss:0.6056 validation loss:0.1622
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.16255047712475063 0.16379508785903454
2023-08-04 03:00:34,802 - epoch:25, training loss:0.6062 validation loss:0.1626
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.16251574084162712 0.16432893937453627
2023-08-04 03:01:17,181 - epoch:26, training loss:0.6063 validation loss:0.1625
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.16241018818691372 0.16438646810129284
2023-08-04 03:01:59,562 - epoch:27, training loss:0.6065 validation loss:0.1624
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.16274578869342804 0.16412308998405933
2023-08-04 03:02:40,479 - epoch:28, training loss:0.6069 validation loss:0.1627
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.16262675495818257 0.1641228304244578
2023-08-04 03:03:21,743 - epoch:29, training loss:0.6061 validation loss:0.1626
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-04-02:41:21.150881/0/0.1608_epoch_13.pkl  &  0.16216658540070056
2023-08-04 03:03:27,774 - [*] loss:0.3607
2023-08-04 03:03:27,782 - [*] phase 0, testing
2023-08-04 03:03:28,688 - T:336	MAE	0.393470	RMSE	0.356230	MAPE	161.754656
2023-08-04 03:03:28,688 - 336	mae	0.3935	
2023-08-04 03:03:28,688 - 336	rmse	0.3562	
2023-08-04 03:03:28,688 - 336	mape	161.7547	
2023-08-04 03:03:34,684 - [*] loss:0.3675
2023-08-04 03:03:34,694 - [*] phase 0, testing
2023-08-04 03:03:35,465 - T:336	MAE	0.392331	RMSE	0.362802	MAPE	155.423713
2023-08-04 03:03:35,466 - 336	mae	0.3923	
2023-08-04 03:03:35,466 - 336	rmse	0.3628	
2023-08-04 03:03:35,466 - 336	mape	155.4237	
2023-08-04 03:03:38,122 - logger name:exp/ECL-PatchTST2023-08-04-03:03:38.121298/ECL-PatchTST.log
2023-08-04 03:03:38,122 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-03:03:38.121298', 'path': 'exp/ECL-PatchTST2023-08-04-03:03:38.121298', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 03:03:38,122 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-04 03:03:38,371 - [*] phase 0 Dataset load!
2023-08-04 03:03:39,578 - [*] phase 0 Training start
train 7969
2023-08-04 03:04:11,675 - epoch:0, training loss:0.2547 validation loss:0.1787
train 7969
vs, vt 0.1787222580984235 0.1805749973282218
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.17320293039083481 0.1694608772173524
2023-08-04 03:05:18,471 - epoch:1, training loss:3.0330 validation loss:0.1732
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.17006298657506705 0.1618526453152299
2023-08-04 03:06:07,721 - epoch:2, training loss:2.3538 validation loss:0.1701
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.16533850952982904 0.1597613465040922
2023-08-04 03:06:56,143 - epoch:3, training loss:1.5426 validation loss:0.1653
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16774868741631507 0.1601833574473858
2023-08-04 03:07:44,124 - epoch:4, training loss:1.1001 validation loss:0.1677
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.1629048427566886 0.16134786615148186
2023-08-04 03:08:31,351 - epoch:5, training loss:0.9519 validation loss:0.1629
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.1635277220979333 0.16229350101202727
2023-08-04 03:09:18,547 - epoch:6, training loss:0.8966 validation loss:0.1635
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.16344607779756187 0.15890109222382306
2023-08-04 03:10:04,767 - epoch:7, training loss:0.8542 validation loss:0.1634
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.16489955885335802 0.15820310525596143
2023-08-04 03:10:52,002 - epoch:8, training loss:0.8031 validation loss:0.1649
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.16331422794610262 0.15810546707361936
2023-08-04 03:11:38,620 - epoch:9, training loss:0.7642 validation loss:0.1633
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.16392445350065826 0.15813450925052167
2023-08-04 03:12:23,353 - epoch:10, training loss:0.7489 validation loss:0.1639
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.16352083627134562 0.15970306899398565
2023-08-04 03:13:09,403 - epoch:11, training loss:0.7217 validation loss:0.1635
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.16303407717496157 0.16001045219600202
2023-08-04 03:13:55,643 - epoch:12, training loss:0.7275 validation loss:0.1630
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.1608029531314969 0.16122663477435709
2023-08-04 03:14:41,097 - epoch:13, training loss:0.7221 validation loss:0.1608
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.1621714223176241 0.15909995641559363
2023-08-04 03:15:26,560 - epoch:14, training loss:0.7058 validation loss:0.1622
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.1624149013310671 0.16097603179514408
2023-08-04 03:16:10,640 - epoch:15, training loss:0.6965 validation loss:0.1624
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.1633071329444647 0.16088090920820833
2023-08-04 03:16:55,593 - epoch:16, training loss:0.7052 validation loss:0.1633
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.16193203236907722 0.16128976931795477
2023-08-04 03:17:38,320 - epoch:17, training loss:0.7023 validation loss:0.1619
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.16246138662099838 0.16209054961800576
2023-08-04 03:18:22,487 - epoch:18, training loss:0.7036 validation loss:0.1625
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.16235232446342707 0.1615827997215092
2023-08-04 03:19:07,127 - epoch:19, training loss:0.7057 validation loss:0.1624
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.16290467455983162 0.16192779466509818
2023-08-04 03:19:52,153 - epoch:20, training loss:0.7047 validation loss:0.1629
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.16277817133814096 0.16215709019452335
2023-08-04 03:20:38,927 - epoch:21, training loss:0.6976 validation loss:0.1628
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.163789271004498 0.16207633838057517
2023-08-04 03:21:24,441 - epoch:22, training loss:0.7027 validation loss:0.1638
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.16328721586614847 0.16201697746291757
2023-08-04 03:22:08,718 - epoch:23, training loss:0.7012 validation loss:0.1633
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.16307978425174952 0.16163726849481463
2023-08-04 03:22:53,573 - epoch:24, training loss:0.6965 validation loss:0.1631
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.16330660749226808 0.1621079313568771
2023-08-04 03:23:40,593 - epoch:25, training loss:0.6993 validation loss:0.1633
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.16345712542533875 0.16223712135106325
2023-08-04 03:24:25,358 - epoch:26, training loss:0.6969 validation loss:0.1635
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.16343882828950881 0.1627735724672675
2023-08-04 03:25:12,016 - epoch:27, training loss:0.6980 validation loss:0.1634
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.16353451274335384 0.16248075235635043
2023-08-04 03:25:57,645 - epoch:28, training loss:0.6960 validation loss:0.1635
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.16330475825816393 0.16230536652728916
2023-08-04 03:26:44,279 - epoch:29, training loss:0.7012 validation loss:0.1633
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-04-03:03:38.121298/0/0.1608_epoch_13.pkl  &  0.15810546707361936
2023-08-04 03:26:48,233 - [*] loss:0.3607
2023-08-04 03:26:48,242 - [*] phase 0, testing
2023-08-04 03:26:48,660 - T:336	MAE	0.394269	RMSE	0.356387	MAPE	159.520853
2023-08-04 03:26:48,661 - 336	mae	0.3943	
2023-08-04 03:26:48,661 - 336	rmse	0.3564	
2023-08-04 03:26:48,661 - 336	mape	159.5209	
2023-08-04 03:26:53,393 - [*] loss:0.3557
2023-08-04 03:26:53,402 - [*] phase 0, testing
2023-08-04 03:26:53,805 - T:336	MAE	0.388466	RMSE	0.351199	MAPE	155.033231
2023-08-04 03:26:53,806 - 336	mae	0.3885	
2023-08-04 03:26:53,806 - 336	rmse	0.3512	
2023-08-04 03:26:53,806 - 336	mape	155.0332	
