2023-08-28 11:57:12,384 - logger name:exp/ECL-PatchTST2023-08-28-11:57:12.384302/ECL-PatchTST.log
2023-08-28 11:57:12,384 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'exchange_rate', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.3, 'fc_dropout': 0.3, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 128, 'd_model': 16, 'n_heads': 4, 'seq_len': 336, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-11:57:12.384302', 'path': 'exp/ECL-PatchTST2023-08-28-11:57:12.384302', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 11:57:12,384 - [*] phase 0 start training
0 7588
5311 760 1517 0.7 0.2 7588
train 4880
5311 760 1517 0.7 0.2 7588
val 665
5311 760 1517 0.7 0.2 7588
test 1422
2023-08-28 11:57:12,456 - [*] phase 0 Dataset load!
2023-08-28 11:57:13,444 - [*] phase 0 Training start
5311 760 1517 0.7 0.2 7588
train 4880
2023-08-28 11:57:27,280 - epoch:0, training loss:0.4350 validation loss:0.3239
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.32391637936234474 0.3252384848892689
Updating learning rate to 1.048624961282033e-05
Updating learning rate to 1.048624961282033e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.28766067201892537 0.2931889084478219
need align? ->  False 0.2931889084478219
2023-08-28 11:57:43,516 - epoch:1, training loss:0.4485 validation loss:0.2877
Updating learning rate to 2.8192022032955813e-05
Updating learning rate to 2.8192022032955813e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.2223082296550274 0.23269792646169662
need align? ->  False 0.23269792646169662
2023-08-28 11:57:54,696 - epoch:2, training loss:0.3732 validation loss:0.2223
Updating learning rate to 5.2332148114373596e-05
Updating learning rate to 5.2332148114373596e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.1777580175548792 0.18230414328475794
need align? ->  False 0.18230414328475794
2023-08-28 11:58:10,498 - epoch:3, training loss:0.2767 validation loss:0.1778
Updating learning rate to 7.638250771336411e-05
Updating learning rate to 7.638250771336411e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.16228429228067398 0.16532539886732897
need align? ->  False 0.16532539886732897
2023-08-28 11:58:23,828 - epoch:4, training loss:0.2239 validation loss:0.1623
Updating learning rate to 9.384324101171307e-05
Updating learning rate to 9.384324101171307e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.14982641115784645 0.15579700966676077
need align? ->  False 0.15579700966676077
2023-08-28 11:58:35,690 - epoch:5, training loss:0.1980 validation loss:0.1498
Updating learning rate to 9.999970334756959e-05
Updating learning rate to 9.999970334756959e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.14225059685607752 0.14802926095823446
need align? ->  False 0.14802926095823446
2023-08-28 11:58:49,587 - epoch:6, training loss:0.1823 validation loss:0.1423
Updating learning rate to 9.95494694329882e-05
Updating learning rate to 9.95494694329882e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.13658389511207739 0.14258728300531706
need align? ->  False 0.14258728300531706
2023-08-28 11:59:01,117 - epoch:7, training loss:0.1725 validation loss:0.1366
Updating learning rate to 9.825143378075556e-05
Updating learning rate to 9.825143378075556e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.13357355942328772 0.13821392940978208
need align? ->  False 0.13821392940978208
2023-08-28 11:59:18,174 - epoch:8, training loss:0.1662 validation loss:0.1336
Updating learning rate to 9.612780614076482e-05
Updating learning rate to 9.612780614076482e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.13125679766138396 0.1358809433877468
need align? ->  False 0.1358809433877468
2023-08-28 11:59:27,058 - epoch:9, training loss:0.1616 validation loss:0.1313
Updating learning rate to 9.321492237071706e-05
Updating learning rate to 9.321492237071706e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12918667246898016 0.13340137091775736
need align? ->  False 0.13340137091775736
2023-08-28 11:59:41,926 - epoch:10, training loss:0.1580 validation loss:0.1292
Updating learning rate to 8.956262271952172e-05
Updating learning rate to 8.956262271952172e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.127766452729702 0.1312559669216474
need align? ->  False 0.1312559669216474
2023-08-28 11:59:55,606 - epoch:11, training loss:0.1556 validation loss:0.1278
Updating learning rate to 8.523339904681954e-05
Updating learning rate to 8.523339904681954e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12654206218818823 0.1295104194432497
need align? ->  False 0.1295104194432497
2023-08-28 12:00:09,106 - epoch:12, training loss:0.1539 validation loss:0.1265
Updating learning rate to 8.030132556993802e-05
Updating learning rate to 8.030132556993802e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12573381202916303 0.1279108915477991
need align? ->  False 0.1279108915477991
2023-08-28 12:00:24,166 - epoch:13, training loss:0.1516 validation loss:0.1257
Updating learning rate to 7.48507914334957e-05
Updating learning rate to 7.48507914334957e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12530654047926268 0.1264016404747963
need align? ->  False 0.1264016404747963
2023-08-28 12:00:37,730 - epoch:14, training loss:0.1504 validation loss:0.1253
Updating learning rate to 6.897505678774068e-05
Updating learning rate to 6.897505678774068e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12498024788995583 0.12563489315410456
need align? ->  False 0.12563489315410456
2023-08-28 12:00:49,050 - epoch:15, training loss:0.1491 validation loss:0.1250
Updating learning rate to 6.277465708152322e-05
Updating learning rate to 6.277465708152322e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.124695660546422 0.1253658595184485
need align? ->  False 0.1253658595184485
2023-08-28 12:01:05,190 - epoch:16, training loss:0.1476 validation loss:0.1247
Updating learning rate to 5.635568287289227e-05
Updating learning rate to 5.635568287289227e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12448732741177082 0.12460346954564254
need align? ->  False 0.12460346954564254
2023-08-28 12:01:16,860 - epoch:17, training loss:0.1474 validation loss:0.1245
Updating learning rate to 4.98279645902334e-05
Updating learning rate to 4.98279645902334e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12441142710546653 0.12353179107109706
need align? ->  True 0.12353179107109706
2023-08-28 12:01:30,463 - epoch:18, training loss:0.1461 validation loss:0.1244
Updating learning rate to 4.330319330318832e-05
Updating learning rate to 4.330319330318832e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12402130849659443 0.12349099852144718
need align? ->  True 0.12349099852144718
2023-08-28 12:01:45,021 - epoch:19, training loss:0.1456 validation loss:0.1240
Updating learning rate to 3.689300965748672e-05
Updating learning rate to 3.689300965748672e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12381932387749355 0.12291545358796914
need align? ->  True 0.12291545358796914
2023-08-28 12:01:57,726 - epoch:20, training loss:0.1452 validation loss:0.1238
Updating learning rate to 3.0707093672545214e-05
Updating learning rate to 3.0707093672545214e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12389111084242661 0.122642965366443
need align? ->  True 0.122642965366443
2023-08-28 12:02:15,211 - epoch:21, training loss:0.1446 validation loss:0.1239
Updating learning rate to 2.4851288085926123e-05
Updating learning rate to 2.4851288085926123e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12375625781714916 0.12256365393598874
need align? ->  True 0.12256365393598874
2023-08-28 12:02:29,801 - epoch:22, training loss:0.1441 validation loss:0.1238
Updating learning rate to 1.9425787354752483e-05
Updating learning rate to 1.9425787354752483e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.123789481818676 0.12228905347486337
need align? ->  True 0.12228905347486337
2023-08-28 12:02:46,034 - epoch:23, training loss:0.1432 validation loss:0.1238
Updating learning rate to 1.4523423300767656e-05
Updating learning rate to 1.4523423300767656e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12387764764328797 0.12214545470972855
need align? ->  True 0.12214545470972855
2023-08-28 12:02:57,943 - epoch:24, training loss:0.1435 validation loss:0.1239
Updating learning rate to 1.0228076732127463e-05
Updating learning rate to 1.0228076732127463e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12384623102843761 0.12207379378378391
need align? ->  True 0.12207379378378391
2023-08-28 12:03:12,625 - epoch:25, training loss:0.1431 validation loss:0.1238
Updating learning rate to 6.613242219516368e-06
Updating learning rate to 6.613242219516368e-06
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12387137735883395 0.12199281342327595
need align? ->  True 0.12199281342327595
2023-08-28 12:03:27,825 - epoch:26, training loss:0.1437 validation loss:0.1239
Updating learning rate to 3.7407705836666047e-06
Updating learning rate to 3.7407705836666047e-06
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.12384646634260814 0.12197077771027882
need align? ->  True 0.12197077771027882
2023-08-28 12:03:39,956 - epoch:27, training loss:0.1434 validation loss:0.1238
Updating learning rate to 1.6598106106671826e-06
Updating learning rate to 1.6598106106671826e-06
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.123832148189346 0.12197194558878739
need align? ->  True 0.12197077771027882
2023-08-28 12:03:55,222 - epoch:28, training loss:0.1438 validation loss:0.1238
Updating learning rate to 4.059681026072072e-07
Updating learning rate to 4.059681026072072e-07
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.1238745463391145 0.12194502415756385
need align? ->  True 0.12194502415756385
2023-08-28 12:04:08,602 - epoch:29, training loss:0.1438 validation loss:0.1239
Updating learning rate to 6.966524304123304e-10
Updating learning rate to 6.966524304123304e-10
check exp/ECL-PatchTST2023-08-28-11:57:12.384302/0/0.1238_epoch_22.pkl  &  0.12194502415756385
2023-08-28 12:04:11,724 - [*] loss:0.0966
2023-08-28 12:04:11,727 - [*] phase 0, testing
2023-08-28 12:04:11,749 - T:96	MAE	0.213162	RMSE	0.092358	MAPE	128.777182
2023-08-28 12:04:11,749 - 96	mae	0.2132	
2023-08-28 12:04:11,750 - 96	rmse	0.0924	
2023-08-28 12:04:11,750 - 96	mape	128.7772	
2023-08-28 12:04:14,681 - [*] loss:0.0966
2023-08-28 12:04:14,683 - [*] phase 0, testing
2023-08-28 12:04:14,705 - T:96	MAE	0.213162	RMSE	0.092358	MAPE	128.777182
2023-08-28 12:04:16,919 - [*] loss:0.1001
2023-08-28 12:04:16,920 - [*] phase 0, testing
2023-08-28 12:04:16,940 - T:96	MAE	0.219454	RMSE	0.097067	MAPE	137.484241
2023-08-28 12:04:18,582 - [*] loss:0.0948
2023-08-28 12:04:18,584 - [*] phase 0, testing
2023-08-28 12:04:18,604 - T:96	MAE	0.210543	RMSE	0.091025	MAPE	128.234518
2023-08-28 12:04:18,604 - 96	mae	0.2105	
2023-08-28 12:04:18,604 - 96	rmse	0.0910	
2023-08-28 12:04:18,604 - 96	mape	128.2345	
2023-08-28 12:04:20,878 - logger name:exp/ECL-PatchTST2023-08-28-12:04:20.878272/ECL-PatchTST.log
2023-08-28 12:04:20,878 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'exchange_rate', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.3, 'fc_dropout': 0.3, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 128, 'd_model': 16, 'n_heads': 4, 'seq_len': 336, 'pred_len': 192, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-12:04:20.878272', 'path': 'exp/ECL-PatchTST2023-08-28-12:04:20.878272', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 12:04:20,878 - [*] phase 0 start training
0 7588
5311 760 1517 0.7 0.2 7588
train 4784
5311 760 1517 0.7 0.2 7588
val 569
5311 760 1517 0.7 0.2 7588
test 1326
2023-08-28 12:04:20,950 - [*] phase 0 Dataset load!
2023-08-28 12:04:21,924 - [*] phase 0 Training start
5311 760 1517 0.7 0.2 7588
train 4784
2023-08-28 12:04:35,159 - epoch:0, training loss:0.5495 validation loss:0.4464
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.4463923811912537 0.4487708404660225
Updating learning rate to 1.0487758639348156e-05
Updating learning rate to 1.0487758639348156e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.4099069356918335 0.4196535676717758
need align? ->  False 0.4196535676717758
2023-08-28 12:04:52,439 - epoch:1, training loss:0.5852 validation loss:0.4099
Updating learning rate to 2.8197242383957768e-05
Updating learning rate to 2.8197242383957768e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.341704335808754 0.35811918824911115
need align? ->  False 0.35811918824911115
2023-08-28 12:05:05,730 - epoch:2, training loss:0.5154 validation loss:0.3417
Updating learning rate to 5.2341165560391886e-05
Updating learning rate to 5.2341165560391886e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.2823714628815651 0.2921636328101158
need align? ->  False 0.2921636328101158
2023-08-28 12:05:20,411 - epoch:3, training loss:0.4204 validation loss:0.2824
Updating learning rate to 7.63928637446643e-05
Updating learning rate to 7.63928637446643e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.25633629858493806 0.26769452095031737
need align? ->  False 0.26769452095031737
2023-08-28 12:05:31,884 - epoch:4, training loss:0.3593 validation loss:0.2563
Updating learning rate to 9.385060307485807e-05
Updating learning rate to 9.385060307485807e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.24048131853342056 0.2514526039361954
need align? ->  False 0.2514526039361954
2023-08-28 12:05:47,151 - epoch:5, training loss:0.3328 validation loss:0.2405
Updating learning rate to 9.999968709562724e-05
Updating learning rate to 9.999968709562724e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.23175896257162093 0.23992083072662354
need align? ->  False 0.23992083072662354
2023-08-28 12:05:56,165 - epoch:6, training loss:0.3164 validation loss:0.2318
Updating learning rate to 9.954884572171476e-05
Updating learning rate to 9.954884572171476e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.226511050760746 0.23220333755016326
need align? ->  False 0.23220333755016326
2023-08-28 12:06:10,702 - epoch:7, training loss:0.3067 validation loss:0.2265
Updating learning rate to 9.825021328202383e-05
Updating learning rate to 9.825021328202383e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.2216674193739891 0.2268415570259094
need align? ->  False 0.2268415570259094
2023-08-28 12:06:23,464 - epoch:8, training loss:0.3021 validation loss:0.2217
Updating learning rate to 9.612600973764647e-05
Updating learning rate to 9.612600973764647e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.21896738708019256 0.22271282523870467
need align? ->  False 0.22271282523870467
2023-08-28 12:06:35,427 - epoch:9, training loss:0.2974 validation loss:0.2190
Updating learning rate to 9.321258080016751e-05
Updating learning rate to 9.321258080016751e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.21605631113052368 0.2201770916581154
need align? ->  False 0.2201770916581154
2023-08-28 12:06:50,916 - epoch:10, training loss:0.2947 validation loss:0.2161
Updating learning rate to 8.95597760464623e-05
Updating learning rate to 8.95597760464623e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.21511492878198624 0.21778467744588853
need align? ->  False 0.21778467744588853
2023-08-28 12:07:01,201 - epoch:11, training loss:0.2907 validation loss:0.2151
Updating learning rate to 8.523009597861551e-05
Updating learning rate to 8.523009597861551e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.21301752775907518 0.21586845219135284
need align? ->  False 0.21586845219135284
2023-08-28 12:07:15,900 - epoch:12, training loss:0.2891 validation loss:0.2130
Updating learning rate to 8.029762262300215e-05
Updating learning rate to 8.029762262300215e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.2118866503238678 0.21396236419677733
need align? ->  False 0.21396236419677733
2023-08-28 12:07:28,094 - epoch:13, training loss:0.2878 validation loss:0.2119
Updating learning rate to 7.484675196627673e-05
Updating learning rate to 7.484675196627673e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.21100154370069504 0.21255166232585906
need align? ->  False 0.21255166232585906
2023-08-28 12:07:40,687 - epoch:14, training loss:0.2849 validation loss:0.2110
Updating learning rate to 6.897074991664266e-05
Updating learning rate to 6.897074991664266e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.2099926307797432 0.21153268963098526
need align? ->  False 0.21153268963098526
2023-08-28 12:07:57,346 - epoch:15, training loss:0.2847 validation loss:0.2100
Updating learning rate to 6.277015649830474e-05
Updating learning rate to 6.277015649830474e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.2094098523259163 0.21030276864767075
need align? ->  False 0.21030276864767075
2023-08-28 12:08:08,086 - epoch:16, training loss:0.2834 validation loss:0.2094
Updating learning rate to 5.6351065583779994e-05
Updating learning rate to 5.6351065583779994e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.20919063687324524 0.2096243530511856
need align? ->  False 0.2096243530511856
2023-08-28 12:08:21,161 - epoch:17, training loss:0.2816 validation loss:0.2092
Updating learning rate to 4.982330959832417e-05
Updating learning rate to 4.982330959832417e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.20857586413621904 0.20908102840185167
need align? ->  False 0.20908102840185167
2023-08-28 12:08:36,559 - epoch:18, training loss:0.2812 validation loss:0.2086
Updating learning rate to 4.329858025668433e-05
Updating learning rate to 4.329858025668433e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.2083404019474983 0.20841088742017747
need align? ->  False 0.20841088742017747
2023-08-28 12:08:48,412 - epoch:19, training loss:0.2810 validation loss:0.2083
Updating learning rate to 3.6888517486892635e-05
Updating learning rate to 3.6888517486892635e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.20814652293920516 0.20776803642511368
need align? ->  True 0.20776803642511368
2023-08-28 12:09:04,929 - epoch:20, training loss:0.2799 validation loss:0.2081
Updating learning rate to 3.070279924014536e-05
Updating learning rate to 3.070279924014536e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.20795017331838608 0.20759784132242204
need align? ->  True 0.20759784132242204
2023-08-28 12:09:16,718 - epoch:21, training loss:0.2791 validation loss:0.2080
Updating learning rate to 2.4847264870649493e-05
Updating learning rate to 2.4847264870649493e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.20781877785921096 0.20737619549036027
need align? ->  True 0.20737619549036027
2023-08-28 12:09:30,311 - epoch:22, training loss:0.2796 validation loss:0.2078
Updating learning rate to 1.942210419492792e-05
Updating learning rate to 1.942210419492792e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.2077833592891693 0.207058784365654
need align? ->  True 0.207058784365654
2023-08-28 12:09:45,977 - epoch:23, training loss:0.2784 validation loss:0.2078
Updating learning rate to 1.452014321628092e-05
Updating learning rate to 1.452014321628092e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.20763676166534423 0.20681876093149185
need align? ->  True 0.20681876093149185
2023-08-28 12:09:57,659 - epoch:24, training loss:0.2791 validation loss:0.2076
Updating learning rate to 1.0225255846133529e-05
Updating learning rate to 1.0225255846133529e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.20766595005989075 0.20678143501281737
need align? ->  True 0.20678143501281737
2023-08-28 12:10:11,113 - epoch:25, training loss:0.2797 validation loss:0.2077
Updating learning rate to 6.610928798156691e-06
Updating learning rate to 6.610928798156691e-06
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.20762839764356614 0.20680468231439592
need align? ->  True 0.20678143501281737
2023-08-28 12:10:25,526 - epoch:26, training loss:0.2781 validation loss:0.2076
Updating learning rate to 3.73900421022205e-06
Updating learning rate to 3.73900421022205e-06
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.20764571130275727 0.20670367628335953
need align? ->  True 0.20670367628335953
2023-08-28 12:10:38,411 - epoch:27, training loss:0.2786 validation loss:0.2076
Updating learning rate to 1.658621508277118e-06
Updating learning rate to 1.658621508277118e-06
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.20760901421308517 0.206676621735096
need align? ->  True 0.206676621735096
2023-08-28 12:10:52,698 - epoch:28, training loss:0.2782 validation loss:0.2076
Updating learning rate to 4.053766171432101e-07
Updating learning rate to 4.053766171432101e-07
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.20759694874286652 0.20667965561151505
need align? ->  True 0.206676621735096
2023-08-28 12:11:07,038 - epoch:29, training loss:0.2783 validation loss:0.2076
Updating learning rate to 7.129043727608552e-10
Updating learning rate to 7.129043727608552e-10
check exp/ECL-PatchTST2023-08-28-12:04:20.878272/0/0.2076_epoch_29.pkl  &  0.206676621735096
2023-08-28 12:11:10,038 - [*] loss:0.1951
2023-08-28 12:11:10,042 - [*] phase 0, testing
2023-08-28 12:11:10,083 - T:192	MAE	0.315331	RMSE	0.195002	MAPE	205.100346
2023-08-28 12:11:10,084 - 192	mae	0.3153	
2023-08-28 12:11:10,084 - 192	rmse	0.1950	
2023-08-28 12:11:10,085 - 192	mape	205.1003	
2023-08-28 12:11:12,839 - [*] loss:0.1951
2023-08-28 12:11:12,843 - [*] phase 0, testing
2023-08-28 12:11:12,884 - T:192	MAE	0.315331	RMSE	0.195002	MAPE	205.100346
2023-08-28 12:11:14,989 - [*] loss:0.2020
2023-08-28 12:11:14,993 - [*] phase 0, testing
2023-08-28 12:11:15,041 - T:192	MAE	0.323682	RMSE	0.202083	MAPE	215.140224
2023-08-28 12:11:16,648 - [*] loss:0.1888
2023-08-28 12:11:16,655 - [*] phase 0, testing
2023-08-28 12:11:16,697 - T:192	MAE	0.307407	RMSE	0.188354	MAPE	201.275206
2023-08-28 12:11:16,698 - 192	mae	0.3074	
2023-08-28 12:11:16,698 - 192	rmse	0.1884	
2023-08-28 12:11:16,698 - 192	mape	201.2752	
2023-08-28 12:11:18,903 - logger name:exp/ECL-PatchTST2023-08-28-12:11:18.903538/ECL-PatchTST.log
2023-08-28 12:11:18,904 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'exchange_rate', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.3, 'fc_dropout': 0.3, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 128, 'd_model': 16, 'n_heads': 4, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-12:11:18.903538', 'path': 'exp/ECL-PatchTST2023-08-28-12:11:18.903538', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 12:11:18,904 - [*] phase 0 start training
0 7588
5311 760 1517 0.7 0.2 7588
train 4640
5311 760 1517 0.7 0.2 7588
val 425
5311 760 1517 0.7 0.2 7588
test 1182
2023-08-28 12:11:18,976 - [*] phase 0 Dataset load!
2023-08-28 12:11:19,954 - [*] phase 0 Training start
5311 760 1517 0.7 0.2 7588
train 4640
2023-08-28 12:11:33,678 - epoch:0, training loss:0.7099 validation loss:0.6526
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.6525945290923119 0.6559979990124702
Updating learning rate to 1.0489352067289656e-05
Updating learning rate to 1.0489352067289656e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.6089643053710461 0.6241983883082867
need align? ->  False 0.6241983883082867
2023-08-28 12:11:50,476 - epoch:1, training loss:0.7806 validation loss:0.6090
Updating learning rate to 2.820275450860712e-05
Updating learning rate to 2.820275450860712e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.5259527862071991 0.551245491951704
need align? ->  False 0.551245491951704
2023-08-28 12:12:03,339 - epoch:2, training loss:0.7152 validation loss:0.5260
Updating learning rate to 5.235068629264831e-05
Updating learning rate to 5.235068629264831e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.4591403491795063 0.4665367789566517
need align? ->  False 0.4665367789566517
2023-08-28 12:12:17,575 - epoch:3, training loss:0.6281 validation loss:0.4591
Updating learning rate to 7.640379612593253e-05
Updating learning rate to 7.640379612593253e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.43088778853416443 0.43897291645407677
need align? ->  False 0.43897291645407677
2023-08-28 12:12:29,511 - epoch:4, training loss:0.5719 validation loss:0.4309
Updating learning rate to 9.385837159090278e-05
Updating learning rate to 9.385837159090278e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.4161204919219017 0.4227420911192894
need align? ->  False 0.4227420911192894
2023-08-28 12:12:45,023 - epoch:5, training loss:0.5428 validation loss:0.4161
Updating learning rate to 9.999966947063185e-05
Updating learning rate to 9.999966947063185e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.40873007476329803 0.4161897525191307
need align? ->  False 0.4161897525191307
2023-08-28 12:12:55,775 - epoch:6, training loss:0.5266 validation loss:0.4087
Updating learning rate to 9.95481868938872e-05
Updating learning rate to 9.95481868938872e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.4031652435660362 0.4054795168340206
need align? ->  False 0.4054795168340206
2023-08-28 12:13:08,387 - epoch:7, training loss:0.5171 validation loss:0.4032
Updating learning rate to 9.82489245240909e-05
Updating learning rate to 9.82489245240909e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.40022994950413704 0.4011530727148056
need align? ->  False 0.4011530727148056
2023-08-28 12:13:22,432 - epoch:8, training loss:0.5104 validation loss:0.4002
Updating learning rate to 9.612411310061373e-05
Updating learning rate to 9.612411310061373e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.398164551705122 0.39871344342827797
need align? ->  False 0.39871344342827797
2023-08-28 12:13:38,181 - epoch:9, training loss:0.5059 validation loss:0.3982
Updating learning rate to 9.321010873602042e-05
Updating learning rate to 9.321010873602042e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.3987935557961464 0.3965400233864784
need align? ->  True 0.3965400233864784
2023-08-28 12:13:51,591 - epoch:10, training loss:0.5019 validation loss:0.3988
Updating learning rate to 8.955677085290379e-05
Updating learning rate to 8.955677085290379e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.3926902264356613 0.3968472294509411
need align? ->  False 0.3965400233864784
2023-08-28 12:14:06,155 - epoch:11, training loss:0.4985 validation loss:0.3927
Updating learning rate to 8.52266090753406e-05
Updating learning rate to 8.52266090753406e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.3923742212355137 0.3951333984732628
need align? ->  False 0.3951333984732628
2023-08-28 12:14:20,059 - epoch:12, training loss:0.4967 validation loss:0.3924
Updating learning rate to 8.029371367189262e-05
Updating learning rate to 8.029371367189262e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.3953738734126091 0.39168255031108856
need align? ->  True 0.39168255031108856
2023-08-28 12:14:31,813 - epoch:13, training loss:0.4949 validation loss:0.3954
Updating learning rate to 7.484248785056981e-05
Updating learning rate to 7.484248785056981e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.3895856700837612 0.39413943514227867
need align? ->  False 0.39168255031108856
2023-08-28 12:14:46,967 - epoch:14, training loss:0.4921 validation loss:0.3896
Updating learning rate to 6.896620359654033e-05
Updating learning rate to 6.896620359654033e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.3932592011988163 0.3893610090017319
need align? ->  True 0.3893610090017319
2023-08-28 12:14:59,354 - epoch:15, training loss:0.4923 validation loss:0.3933
Updating learning rate to 6.276540576260444e-05
Updating learning rate to 6.276540576260444e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.39340413734316826 0.39201899245381355
need align? ->  True 0.3893610090017319
2023-08-28 12:15:11,662 - epoch:16, training loss:0.4906 validation loss:0.3934
Updating learning rate to 5.63461917188867e-05
Updating learning rate to 5.63461917188867e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.39127327501773834 0.39187247678637505
need align? ->  True 0.3893610090017319
2023-08-28 12:15:26,349 - epoch:17, training loss:0.4892 validation loss:0.3913
Updating learning rate to 4.9818395997417525e-05
Updating learning rate to 4.9818395997417525e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.39192506670951843 0.3890617601573467
need align? ->  True 0.3890617601573467
2023-08-28 12:15:37,317 - epoch:18, training loss:0.4901 validation loss:0.3919
Updating learning rate to 4.3293710992838125e-05
Updating learning rate to 4.3293710992838125e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.3917514756321907 0.38939546048641205
need align? ->  True 0.3890617601573467
2023-08-28 12:15:51,356 - epoch:19, training loss:0.4885 validation loss:0.3918
Updating learning rate to 3.688377587456131e-05
Updating learning rate to 3.688377587456131e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.3918788433074951 0.38792015612125397
need align? ->  True 0.38792015612125397
2023-08-28 12:16:02,859 - epoch:20, training loss:0.4870 validation loss:0.3919
Updating learning rate to 3.069826640963051e-05
Updating learning rate to 3.069826640963051e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.39218658208847046 0.391376793384552
need align? ->  True 0.38792015612125397
2023-08-28 12:16:15,245 - epoch:21, training loss:0.4870 validation loss:0.3922
Updating learning rate to 2.4843018379937972e-05
Updating learning rate to 2.4843018379937972e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.3921472318470478 0.38938357681035995
need align? ->  True 0.38792015612125397
2023-08-28 12:16:29,041 - epoch:22, training loss:0.4851 validation loss:0.3921
Updating learning rate to 1.9418216702653153e-05
Updating learning rate to 1.9418216702653153e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.3914055675268173 0.38939765840768814
need align? ->  True 0.38792015612125397
2023-08-28 12:16:39,758 - epoch:23, training loss:0.4863 validation loss:0.3914
Updating learning rate to 1.4516681238513547e-05
Updating learning rate to 1.4516681238513547e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.39168286696076393 0.3885185793042183
need align? ->  True 0.38792015612125397
2023-08-28 12:16:54,477 - epoch:24, training loss:0.4859 validation loss:0.3917
Updating learning rate to 1.0222278618272986e-05
Updating learning rate to 1.0222278618272986e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.39225515723228455 0.3889930993318558
need align? ->  True 0.38792015612125397
2023-08-28 12:17:05,724 - epoch:25, training loss:0.4862 validation loss:0.3923
Updating learning rate to 6.608487261397122e-06
Updating learning rate to 6.608487261397122e-06
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.39241259172558784 0.38885847106575966
need align? ->  True 0.38792015612125397
2023-08-28 12:17:17,817 - epoch:26, training loss:0.4855 validation loss:0.3924
Updating learning rate to 3.7371401399343318e-06
Updating learning rate to 3.7371401399343318e-06
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.3922518864274025 0.38902825862169266
need align? ->  True 0.38792015612125397
2023-08-28 12:17:30,660 - epoch:27, training loss:0.4848 validation loss:0.3923
Updating learning rate to 1.6573667992206914e-06
Updating learning rate to 1.6573667992206914e-06
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.39223646745085716 0.3892154432833195
need align? ->  True 0.38792015612125397
2023-08-28 12:17:44,032 - epoch:28, training loss:0.4844 validation loss:0.3922
Updating learning rate to 4.047527377379024e-07
Updating learning rate to 4.047527377379024e-07
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.3922571763396263 0.389134056866169
need align? ->  True 0.38792015612125397
2023-08-28 12:17:58,311 - epoch:29, training loss:0.4858 validation loss:0.3923
Updating learning rate to 7.305293681617861e-10
Updating learning rate to 7.305293681617861e-10
check exp/ECL-PatchTST2023-08-28-12:11:18.903538/0/0.3896_epoch_14.pkl  &  0.38792015612125397
2023-08-28 12:17:59,942 - [*] loss:0.3512
2023-08-28 12:17:59,946 - [*] phase 0, testing
2023-08-28 12:18:00,027 - T:336	MAE	0.438538	RMSE	0.358023	MAPE	320.874190
2023-08-28 12:18:00,029 - 336	mae	0.4385	
2023-08-28 12:18:00,029 - 336	rmse	0.3580	
2023-08-28 12:18:00,029 - 336	mape	320.8742	
2023-08-28 12:18:01,936 - [*] loss:0.3512
2023-08-28 12:18:01,941 - [*] phase 0, testing
2023-08-28 12:18:02,021 - T:336	MAE	0.438538	RMSE	0.358023	MAPE	320.874190
2023-08-28 12:18:03,507 - [*] loss:0.3483
2023-08-28 12:18:03,511 - [*] phase 0, testing
2023-08-28 12:18:03,589 - T:336	MAE	0.434442	RMSE	0.353094	MAPE	315.707397
2023-08-28 12:18:04,593 - [*] loss:0.3506
2023-08-28 12:18:04,598 - [*] phase 0, testing
2023-08-28 12:18:04,677 - T:336	MAE	0.432876	RMSE	0.356481	MAPE	306.769156
2023-08-28 12:18:04,679 - 336	mae	0.4329	
2023-08-28 12:18:04,679 - 336	rmse	0.3565	
2023-08-28 12:18:04,679 - 336	mape	306.7692	
2023-08-28 12:18:06,855 - logger name:exp/ECL-PatchTST2023-08-28-12:18:06.855696/ECL-PatchTST.log
2023-08-28 12:18:06,856 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'exchange_rate', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.3, 'fc_dropout': 0.3, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 128, 'd_model': 16, 'n_heads': 4, 'seq_len': 336, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-12:18:06.855696', 'path': 'exp/ECL-PatchTST2023-08-28-12:18:06.855696', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 12:18:06,856 - [*] phase 0 start training
0 7588
5311 760 1517 0.7 0.2 7588
train 4256
5311 760 1517 0.7 0.2 7588
val 41
5311 760 1517 0.7 0.2 7588
test 798
2023-08-28 12:18:06,935 - [*] phase 0 Dataset load!
2023-08-28 12:18:08,139 - [*] phase 0 Training start
5311 760 1517 0.7 0.2 7588
train 4256
2023-08-28 12:18:19,471 - epoch:0, training loss:1.0743 validation loss:1.1196
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.1195685863494873 1.116964340209961
Updating learning rate to 1.0494716053729492e-05
Updating learning rate to 1.0494716053729492e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.1383213996887207 1.1250860691070557
need align? ->  True 1.116964340209961
2023-08-28 12:18:35,184 - epoch:1, training loss:1.2404 validation loss:1.1383
Updating learning rate to 2.8221308522477503e-05
Updating learning rate to 2.8221308522477503e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.1979005336761475 1.163671851158142
need align? ->  True 1.116964340209961
2023-08-28 12:18:46,664 - epoch:2, training loss:1.1809 validation loss:1.1979
Updating learning rate to 5.238272804438159e-05
Updating learning rate to 5.238272804438159e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.3261609077453613 1.2894233465194702
need align? ->  True 1.116964340209961
2023-08-28 12:18:59,955 - epoch:3, training loss:1.0886 validation loss:1.3262
Updating learning rate to 7.644057631736663e-05
Updating learning rate to 7.644057631736663e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4299496412277222 1.396544098854065
need align? ->  True 1.116964340209961
2023-08-28 12:19:09,891 - epoch:4, training loss:1.0227 validation loss:1.4299
Updating learning rate to 9.38844827832349e-05
Updating learning rate to 9.38844827832349e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4691969156265259 1.440832495689392
need align? ->  True 1.116964340209961
2023-08-28 12:19:22,972 - epoch:5, training loss:0.9946 validation loss:1.4692
Updating learning rate to 9.99996066428178e-05
Updating learning rate to 9.99996066428178e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4870853424072266 1.4542020559310913
need align? ->  True 1.116964340209961
2023-08-28 12:19:32,850 - epoch:6, training loss:0.9796 validation loss:1.4871
Updating learning rate to 9.95459673249453e-05
Updating learning rate to 9.95459673249453e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4866588115692139 1.4672249555587769
need align? ->  True 1.116964340209961
2023-08-28 12:19:44,910 - epoch:7, training loss:0.9703 validation loss:1.4867
Updating learning rate to 9.824458619146117e-05
Updating learning rate to 9.824458619146117e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4852044582366943 1.4828991889953613
need align? ->  True 1.116964340209961
2023-08-28 12:19:55,017 - epoch:8, training loss:0.9645 validation loss:1.4852
Updating learning rate to 9.611773023437026e-05
Updating learning rate to 9.611773023437026e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4580395221710205 1.492132544517517
need align? ->  True 1.116964340209961
2023-08-28 12:20:06,329 - epoch:9, training loss:0.9600 validation loss:1.4580
Updating learning rate to 9.320179054877429e-05
Updating learning rate to 9.320179054877429e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4534776210784912 1.4964169263839722
need align? ->  True 1.116964340209961
2023-08-28 12:20:17,646 - epoch:10, training loss:0.9562 validation loss:1.4535
Updating learning rate to 8.954665967114503e-05
Updating learning rate to 8.954665967114503e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4243665933609009 1.48432195186615
need align? ->  True 1.116964340209961
2023-08-28 12:20:29,449 - epoch:11, training loss:0.9534 validation loss:1.4244
Updating learning rate to 8.521487790419246e-05
Updating learning rate to 8.521487790419246e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4320944547653198 1.4724538326263428
need align? ->  True 1.116964340209961
2023-08-28 12:20:42,058 - epoch:12, training loss:0.9518 validation loss:1.4321
Updating learning rate to 8.02805632349459e-05
Updating learning rate to 8.02805632349459e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4190707206726074 1.4794836044311523
need align? ->  True 1.116964340209961
2023-08-28 12:20:52,883 - epoch:13, training loss:0.9491 validation loss:1.4191
Updating learning rate to 7.48281431554467e-05
Updating learning rate to 7.48281431554467e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4055132865905762 1.4713706970214844
need align? ->  True 1.116964340209961
2023-08-28 12:21:06,463 - epoch:14, training loss:0.9480 validation loss:1.4055
Updating learning rate to 6.895091008495147e-05
Updating learning rate to 6.895091008495147e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.3891732692718506 1.4544414281845093
need align? ->  True 1.116964340209961
2023-08-28 12:21:15,457 - epoch:15, training loss:0.9474 validation loss:1.3892
Updating learning rate to 6.274942511077328e-05
Updating learning rate to 6.274942511077328e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.3965965509414673 1.4548102617263794
need align? ->  True 1.116964340209961
2023-08-28 12:21:28,770 - epoch:16, training loss:0.9460 validation loss:1.3966
Updating learning rate to 5.632979736019675e-05
Updating learning rate to 5.632979736019675e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.3755087852478027 1.4469642639160156
need align? ->  True 1.116964340209961
2023-08-28 12:21:38,273 - epoch:17, training loss:0.9443 validation loss:1.3755
Updating learning rate to 4.980186844389133e-05
Updating learning rate to 4.980186844389133e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.3682538270950317 1.427916169166565
need align? ->  True 1.116964340209961
2023-08-28 12:21:50,862 - epoch:18, training loss:0.9424 validation loss:1.3683
Updating learning rate to 4.3277333035498836e-05
Updating learning rate to 4.3277333035498836e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.3707014322280884 1.4276436567306519
need align? ->  True 1.116964340209961
2023-08-28 12:22:00,164 - epoch:19, training loss:0.9437 validation loss:1.3707
Updating learning rate to 3.686782774479982e-05
Updating learning rate to 3.686782774479982e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.3639659881591797 1.430046796798706
need align? ->  True 1.116964340209961
2023-08-28 12:22:11,977 - epoch:20, training loss:0.9415 validation loss:1.3640
Updating learning rate to 3.0683020984368707e-05
Updating learning rate to 3.0683020984368707e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.36149263381958 1.4276982545852661
need align? ->  True 1.116964340209961
2023-08-28 12:22:23,332 - epoch:21, training loss:0.9407 validation loss:1.3615
Updating learning rate to 2.48287365126289e-05
Updating learning rate to 2.48287365126289e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.3559330701828003 1.4213553667068481
need align? ->  True 1.116964340209961
2023-08-28 12:22:34,493 - epoch:22, training loss:0.9410 validation loss:1.3559
Updating learning rate to 1.940514276000616e-05
Updating learning rate to 1.940514276000616e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.351387619972229 1.4194172620773315
need align? ->  True 1.116964340209961
2023-08-28 12:22:46,653 - epoch:23, training loss:0.9406 validation loss:1.3514
Updating learning rate to 1.4505038919312107e-05
Updating learning rate to 1.4505038919312107e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.353275179862976 1.415045142173767
need align? ->  True 1.116964340209961
2023-08-28 12:22:56,270 - epoch:24, training loss:0.9401 validation loss:1.3533
Updating learning rate to 1.0212267125826495e-05
Updating learning rate to 1.0212267125826495e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.3542464971542358 1.4159328937530518
need align? ->  True 1.116964340209961
2023-08-28 12:23:08,713 - epoch:25, training loss:0.9402 validation loss:1.3542
Updating learning rate to 6.600277895117048e-06
Updating learning rate to 6.600277895117048e-06
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.3522614240646362 1.4146422147750854
need align? ->  True 1.116964340209961
2023-08-28 12:23:18,599 - epoch:26, training loss:0.9401 validation loss:1.3523
Updating learning rate to 3.730873364353798e-06
Updating learning rate to 3.730873364353798e-06
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.351286768913269 1.4147026538848877
need align? ->  True 1.116964340209961
2023-08-28 12:23:31,087 - epoch:27, training loss:0.9395 validation loss:1.3513
Updating learning rate to 1.6531498406073574e-06
Updating learning rate to 1.6531498406073574e-06
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.3524106740951538 1.4147968292236328
need align? ->  True 1.116964340209961
2023-08-28 12:23:41,870 - epoch:28, training loss:0.9393 validation loss:1.3524
Updating learning rate to 4.026577494227991e-07
Updating learning rate to 4.026577494227991e-07
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.3521698713302612 1.414194107055664
need align? ->  True 1.116964340209961
2023-08-28 12:23:55,988 - epoch:29, training loss:0.9401 validation loss:1.3522
Updating learning rate to 7.933571822109223e-10
Updating learning rate to 7.933571822109223e-10
check exp/ECL-PatchTST2023-08-28-12:18:06.855696/0/1.1196_epoch_0.pkl  &  1.116964340209961
2023-08-28 12:23:57,467 - [*] loss:1.1605
2023-08-28 12:23:57,474 - [*] phase 0, testing
2023-08-28 12:23:57,581 - T:720	MAE	0.812587	RMSE	1.139874	MAPE	706.403971
2023-08-28 12:23:57,582 - 720	mae	0.8126	
2023-08-28 12:23:57,582 - 720	rmse	1.1399	
2023-08-28 12:23:57,582 - 720	mape	706.4040	
2023-08-28 12:23:58,361 - [*] loss:1.1605
2023-08-28 12:23:58,368 - [*] phase 0, testing
2023-08-28 12:23:58,473 - T:720	MAE	0.812587	RMSE	1.139874	MAPE	706.403971
2023-08-28 12:24:00,126 - [*] loss:1.1674
2023-08-28 12:24:00,133 - [*] phase 0, testing
2023-08-28 12:24:00,235 - T:720	MAE	0.813934	RMSE	1.146066	MAPE	708.241701
2023-08-28 12:24:01,251 - [*] loss:1.1686
2023-08-28 12:24:01,258 - [*] phase 0, testing
2023-08-28 12:24:01,369 - T:720	MAE	0.814296	RMSE	1.147322	MAPE	708.343935
2023-08-28 12:24:01,370 - 720	mae	0.8143	
2023-08-28 12:24:01,370 - 720	rmse	1.1473	
2023-08-28 12:24:01,370 - 720	mape	708.3439	
