2023-08-28 11:54:06,605 - logger name:exp/ECL-PatchTST2023-08-28-11:54:06.605210/ECL-PatchTST.log
2023-08-28 11:54:06,605 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'exchange_rate', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-11:54:06.605210', 'path': 'exp/ECL-PatchTST2023-08-28-11:54:06.605210', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 11:54:06,605 - [*] phase 0 start training
0 7588
5311 760 1517 0.7 0.2 7588
train 4880
5311 760 1517 0.7 0.2 7588
val 665
5311 760 1517 0.7 0.2 7588
test 1422
2023-08-28 11:54:06,679 - [*] phase 0 Dataset load!
2023-08-28 11:54:07,556 - [*] phase 0 Training start
5311 760 1517 0.7 0.2 7588
train 4880
2023-08-28 11:54:13,601 - epoch:0, training loss:0.3789 validation loss:0.2382
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.23818993692596754 0.24066764985521635
Updating learning rate to 1.048624961282033e-05
Updating learning rate to 1.048624961282033e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.17254423474272093 0.1804321991900603
need align? ->  False 0.1804321991900603
2023-08-28 11:54:28,696 - epoch:1, training loss:0.2770 validation loss:0.1725
Updating learning rate to 2.8192022032955813e-05
Updating learning rate to 2.8192022032955813e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.14651324103275934 0.15402142455180487
need align? ->  False 0.15402142455180487
2023-08-28 11:54:39,372 - epoch:2, training loss:0.2040 validation loss:0.1465
Updating learning rate to 5.2332148114373596e-05
Updating learning rate to 5.2332148114373596e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.13565668215354285 0.14071471616625786
need align? ->  False 0.14071471616625786
2023-08-28 11:54:50,150 - epoch:3, training loss:0.1637 validation loss:0.1357
Updating learning rate to 7.638250771336411e-05
Updating learning rate to 7.638250771336411e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.13318811481197676 0.13156570183734098
need align? ->  True 0.13156570183734098
2023-08-28 11:55:01,053 - epoch:4, training loss:0.1463 validation loss:0.1332
Updating learning rate to 9.384324101171307e-05
Updating learning rate to 9.384324101171307e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.13923530094325542 0.12681794911623
need align? ->  True 0.12681794911623
2023-08-28 11:55:12,130 - epoch:5, training loss:0.1383 validation loss:0.1392
Updating learning rate to 9.999970334756959e-05
Updating learning rate to 9.999970334756959e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.13997177282969156 0.12621407893796763
need align? ->  True 0.12621407893796763
2023-08-28 11:55:23,351 - epoch:6, training loss:0.1341 validation loss:0.1400
Updating learning rate to 9.95494694329882e-05
Updating learning rate to 9.95494694329882e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.1488190113256375 0.12530903021494547
need align? ->  True 0.12530903021494547
2023-08-28 11:55:34,393 - epoch:7, training loss:0.1315 validation loss:0.1488
Updating learning rate to 9.825143378075556e-05
Updating learning rate to 9.825143378075556e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.14469271960357824 0.1281147220482429
need align? ->  True 0.12530903021494547
2023-08-28 11:55:45,198 - epoch:8, training loss:0.1297 validation loss:0.1447
Updating learning rate to 9.612780614076482e-05
Updating learning rate to 9.612780614076482e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.1481441929936409 0.12599869507054487
need align? ->  True 0.12530903021494547
2023-08-28 11:55:56,241 - epoch:9, training loss:0.1290 validation loss:0.1481
Updating learning rate to 9.321492237071706e-05
Updating learning rate to 9.321492237071706e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.15135052427649498 0.13377919855217138
need align? ->  True 0.12530903021494547
2023-08-28 11:56:08,018 - epoch:10, training loss:0.1278 validation loss:0.1514
Updating learning rate to 8.956262271952172e-05
Updating learning rate to 8.956262271952172e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.1590599063783884 0.13927938789129257
need align? ->  True 0.12530903021494547
2023-08-28 11:56:19,620 - epoch:11, training loss:0.1262 validation loss:0.1591
Updating learning rate to 8.523339904681954e-05
Updating learning rate to 8.523339904681954e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.16329491138458252 0.1385687980800867
need align? ->  True 0.12530903021494547
2023-08-28 11:56:30,955 - epoch:12, training loss:0.1253 validation loss:0.1633
Updating learning rate to 8.030132556993802e-05
Updating learning rate to 8.030132556993802e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.15465343929827213 0.1374068713436524
need align? ->  True 0.12530903021494547
2023-08-28 11:56:42,716 - epoch:13, training loss:0.1242 validation loss:0.1547
Updating learning rate to 7.48507914334957e-05
Updating learning rate to 7.48507914334957e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.16092011208335558 0.13961498563488325
need align? ->  True 0.12530903021494547
2023-08-28 11:56:54,891 - epoch:14, training loss:0.1235 validation loss:0.1609
Updating learning rate to 6.897505678774068e-05
Updating learning rate to 6.897505678774068e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.1583226571480433 0.14011267013847828
need align? ->  True 0.12530903021494547
2023-08-28 11:57:06,247 - epoch:15, training loss:0.1227 validation loss:0.1583
Updating learning rate to 6.277465708152322e-05
Updating learning rate to 6.277465708152322e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.16415735706686974 0.1370757439484199
need align? ->  True 0.12530903021494547
2023-08-28 11:57:27,912 - epoch:16, training loss:0.1224 validation loss:0.1642
Updating learning rate to 5.635568287289227e-05
Updating learning rate to 5.635568287289227e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.16219421848654747 0.13848338772853216
need align? ->  True 0.12530903021494547
2023-08-28 11:57:49,131 - epoch:17, training loss:0.1209 validation loss:0.1622
Updating learning rate to 4.98279645902334e-05
Updating learning rate to 4.98279645902334e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.16557459657390913 0.1394714576502641
need align? ->  True 0.12530903021494547
2023-08-28 11:58:11,593 - epoch:18, training loss:0.1205 validation loss:0.1656
Updating learning rate to 4.330319330318832e-05
Updating learning rate to 4.330319330318832e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.16743881503740946 0.14363663891951242
need align? ->  True 0.12530903021494547
2023-08-28 11:58:32,878 - epoch:19, training loss:0.1205 validation loss:0.1674
Updating learning rate to 3.689300965748672e-05
Updating learning rate to 3.689300965748672e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.16445387403170267 0.14105313085019588
need align? ->  True 0.12530903021494547
2023-08-28 11:58:54,173 - epoch:20, training loss:0.1202 validation loss:0.1645
Updating learning rate to 3.0707093672545214e-05
Updating learning rate to 3.0707093672545214e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.16726000979542732 0.13384649716317654
need align? ->  True 0.12530903021494547
2023-08-28 11:59:17,480 - epoch:21, training loss:0.1195 validation loss:0.1673
Updating learning rate to 2.4851288085926123e-05
Updating learning rate to 2.4851288085926123e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.16539921859900156 0.13546322224040827
need align? ->  True 0.12530903021494547
2023-08-28 11:59:38,713 - epoch:22, training loss:0.1191 validation loss:0.1654
Updating learning rate to 1.9425787354752483e-05
Updating learning rate to 1.9425787354752483e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.16945322478810945 0.13652518267432848
need align? ->  True 0.12530903021494547
2023-08-28 11:59:59,707 - epoch:23, training loss:0.1186 validation loss:0.1695
Updating learning rate to 1.4523423300767656e-05
Updating learning rate to 1.4523423300767656e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.17077734197179475 0.13717113869885603
need align? ->  True 0.12530903021494547
2023-08-28 12:00:21,387 - epoch:24, training loss:0.1185 validation loss:0.1708
Updating learning rate to 1.0228076732127463e-05
Updating learning rate to 1.0228076732127463e-05
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.16963960975408554 0.13714923150837421
need align? ->  True 0.12530903021494547
2023-08-28 12:00:43,169 - epoch:25, training loss:0.1183 validation loss:0.1696
Updating learning rate to 6.613242219516368e-06
Updating learning rate to 6.613242219516368e-06
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.17010276267925897 0.13672827618817487
need align? ->  True 0.12530903021494547
2023-08-28 12:01:05,764 - epoch:26, training loss:0.1180 validation loss:0.1701
Updating learning rate to 3.7407705836666047e-06
Updating learning rate to 3.7407705836666047e-06
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.1688192424674829 0.13691336723665395
need align? ->  True 0.12530903021494547
2023-08-28 12:01:27,202 - epoch:27, training loss:0.1183 validation loss:0.1688
Updating learning rate to 1.6598106106671826e-06
Updating learning rate to 1.6598106106671826e-06
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.17016777644554773 0.1372844483703375
need align? ->  True 0.12530903021494547
2023-08-28 12:01:48,957 - epoch:28, training loss:0.1181 validation loss:0.1702
Updating learning rate to 4.059681026072072e-07
Updating learning rate to 4.059681026072072e-07
5311 760 1517 0.7 0.2 7588
train 4880
vs, vt 0.1697639524936676 0.13683594204485416
need align? ->  True 0.12530903021494547
2023-08-28 12:02:11,965 - epoch:29, training loss:0.1181 validation loss:0.1698
Updating learning rate to 6.966524304123304e-10
Updating learning rate to 6.966524304123304e-10
check exp/ECL-PatchTST2023-08-28-11:54:06.605210/0/0.1332_epoch_4.pkl  &  0.12530903021494547
2023-08-28 12:02:15,174 - [*] loss:0.0943
2023-08-28 12:02:15,176 - [*] phase 0, testing
2023-08-28 12:02:15,208 - T:96	MAE	0.211745	RMSE	0.090333	MAPE	125.401032
2023-08-28 12:02:15,208 - 96	mae	0.2117	
2023-08-28 12:02:15,208 - 96	rmse	0.0903	
2023-08-28 12:02:15,208 - 96	mape	125.4010	
2023-08-28 12:02:17,892 - [*] loss:0.0943
2023-08-28 12:02:17,894 - [*] phase 0, testing
2023-08-28 12:02:17,919 - T:96	MAE	0.211745	RMSE	0.090333	MAPE	125.401032
2023-08-28 12:02:20,482 - [*] loss:0.0923
2023-08-28 12:02:20,484 - [*] phase 0, testing
2023-08-28 12:02:20,505 - T:96	MAE	0.210046	RMSE	0.089209	MAPE	123.972976
2023-08-28 12:02:23,169 - [*] loss:0.0964
2023-08-28 12:02:23,171 - [*] phase 0, testing
2023-08-28 12:02:23,194 - T:96	MAE	0.213805	RMSE	0.093226	MAPE	128.199661
2023-08-28 12:02:23,194 - 96	mae	0.2138	
2023-08-28 12:02:23,194 - 96	rmse	0.0932	
2023-08-28 12:02:23,195 - 96	mape	128.1997	
2023-08-28 12:02:25,479 - logger name:exp/ECL-PatchTST2023-08-28-12:02:25.478364/ECL-PatchTST.log
2023-08-28 12:02:25,479 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'exchange_rate', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 192, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-12:02:25.478364', 'path': 'exp/ECL-PatchTST2023-08-28-12:02:25.478364', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 12:02:25,479 - [*] phase 0 start training
0 7588
5311 760 1517 0.7 0.2 7588
train 4784
5311 760 1517 0.7 0.2 7588
val 569
5311 760 1517 0.7 0.2 7588
test 1326
2023-08-28 12:02:25,572 - [*] phase 0 Dataset load!
2023-08-28 12:02:26,541 - [*] phase 0 Training start
5311 760 1517 0.7 0.2 7588
train 4784
2023-08-28 12:02:42,067 - epoch:0, training loss:0.4944 validation loss:0.3584
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.35836274474859237 0.3645239293575287
Updating learning rate to 1.0487758639348156e-05
Updating learning rate to 1.0487758639348156e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.2765442281961441 0.2867792874574661
need align? ->  False 0.2867792874574661
2023-08-28 12:03:10,065 - epoch:1, training loss:0.4192 validation loss:0.2765
Updating learning rate to 2.8197242383957768e-05
Updating learning rate to 2.8197242383957768e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.24169162809848785 0.25478695183992384
need align? ->  False 0.25478695183992384
2023-08-28 12:03:32,405 - epoch:2, training loss:0.3426 validation loss:0.2417
Updating learning rate to 5.2341165560391886e-05
Updating learning rate to 5.2341165560391886e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.22517314106225966 0.24171412438154222
need align? ->  False 0.24171412438154222
2023-08-28 12:03:54,096 - epoch:3, training loss:0.3023 validation loss:0.2252
Updating learning rate to 7.63928637446643e-05
Updating learning rate to 7.63928637446643e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.21670569628477096 0.22505921572446824
need align? ->  False 0.22505921572446824
2023-08-28 12:04:16,459 - epoch:4, training loss:0.2816 validation loss:0.2167
Updating learning rate to 9.385060307485807e-05
Updating learning rate to 9.385060307485807e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.21809644103050232 0.21430935114622116
need align? ->  True 0.21430935114622116
2023-08-28 12:04:39,688 - epoch:5, training loss:0.2700 validation loss:0.2181
Updating learning rate to 9.999968709562724e-05
Updating learning rate to 9.999968709562724e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.2357673704624176 0.21016207188367844
need align? ->  True 0.21016207188367844
2023-08-28 12:05:00,785 - epoch:6, training loss:0.2637 validation loss:0.2358
Updating learning rate to 9.954884572171476e-05
Updating learning rate to 9.954884572171476e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.244521626830101 0.22036297917366027
need align? ->  True 0.21016207188367844
2023-08-28 12:05:24,116 - epoch:7, training loss:0.2580 validation loss:0.2445
Updating learning rate to 9.825021328202383e-05
Updating learning rate to 9.825021328202383e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.29057746231555937 0.2293786257505417
need align? ->  True 0.21016207188367844
2023-08-28 12:05:46,196 - epoch:8, training loss:0.2547 validation loss:0.2906
Updating learning rate to 9.612600973764647e-05
Updating learning rate to 9.612600973764647e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.29236151576042174 0.25853710770606997
need align? ->  True 0.21016207188367844
2023-08-28 12:06:07,043 - epoch:9, training loss:0.2491 validation loss:0.2924
Updating learning rate to 9.321258080016751e-05
Updating learning rate to 9.321258080016751e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.27961458563804625 0.2558472156524658
need align? ->  True 0.21016207188367844
2023-08-28 12:06:27,968 - epoch:10, training loss:0.2449 validation loss:0.2796
Updating learning rate to 8.95597760464623e-05
Updating learning rate to 8.95597760464623e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.3152415961027145 0.2427461177110672
need align? ->  True 0.21016207188367844
2023-08-28 12:06:51,011 - epoch:11, training loss:0.2437 validation loss:0.3152
Updating learning rate to 8.523009597861551e-05
Updating learning rate to 8.523009597861551e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.31268549263477324 0.2531834304332733
need align? ->  True 0.21016207188367844
2023-08-28 12:07:12,381 - epoch:12, training loss:0.2400 validation loss:0.3127
Updating learning rate to 8.029762262300215e-05
Updating learning rate to 8.029762262300215e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.3258897364139557 0.258138382434845
need align? ->  True 0.21016207188367844
2023-08-28 12:07:34,186 - epoch:13, training loss:0.2375 validation loss:0.3259
Updating learning rate to 7.484675196627673e-05
Updating learning rate to 7.484675196627673e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.31788005232810973 0.25878091156482697
need align? ->  True 0.21016207188367844
2023-08-28 12:07:56,711 - epoch:14, training loss:0.2354 validation loss:0.3179
Updating learning rate to 6.897074991664266e-05
Updating learning rate to 6.897074991664266e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.33572627007961275 0.2559076637029648
need align? ->  True 0.21016207188367844
2023-08-28 12:08:17,572 - epoch:15, training loss:0.2330 validation loss:0.3357
Updating learning rate to 6.277015649830474e-05
Updating learning rate to 6.277015649830474e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.3227526158094406 0.27023138999938967
need align? ->  True 0.21016207188367844
2023-08-28 12:08:40,575 - epoch:16, training loss:0.2311 validation loss:0.3228
Updating learning rate to 5.6351065583779994e-05
Updating learning rate to 5.6351065583779994e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.3344480782747269 0.25311699509620667
need align? ->  True 0.21016207188367844
2023-08-28 12:09:02,988 - epoch:17, training loss:0.2297 validation loss:0.3344
Updating learning rate to 4.982330959832417e-05
Updating learning rate to 4.982330959832417e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.35916733294725417 0.2625975027680397
need align? ->  True 0.21016207188367844
2023-08-28 12:09:24,993 - epoch:18, training loss:0.2270 validation loss:0.3592
Updating learning rate to 4.329858025668433e-05
Updating learning rate to 4.329858025668433e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.3307194888591766 0.26736470609903334
need align? ->  True 0.21016207188367844
2023-08-28 12:09:47,512 - epoch:19, training loss:0.2260 validation loss:0.3307
Updating learning rate to 3.6888517486892635e-05
Updating learning rate to 3.6888517486892635e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.34539618492126467 0.2514626905322075
need align? ->  True 0.21016207188367844
2023-08-28 12:10:07,152 - epoch:20, training loss:0.2237 validation loss:0.3454
Updating learning rate to 3.070279924014536e-05
Updating learning rate to 3.070279924014536e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.3555404782295227 0.2589281365275383
need align? ->  True 0.21016207188367844
2023-08-28 12:10:29,004 - epoch:21, training loss:0.2227 validation loss:0.3555
Updating learning rate to 2.4847264870649493e-05
Updating learning rate to 2.4847264870649493e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.32690717875957487 0.26468079090118407
need align? ->  True 0.21016207188367844
2023-08-28 12:10:50,846 - epoch:22, training loss:0.2217 validation loss:0.3269
Updating learning rate to 1.942210419492792e-05
Updating learning rate to 1.942210419492792e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.3409592598676682 0.25485271513462066
need align? ->  True 0.21016207188367844
2023-08-28 12:11:14,116 - epoch:23, training loss:0.2210 validation loss:0.3410
Updating learning rate to 1.452014321628092e-05
Updating learning rate to 1.452014321628092e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.33987943530082704 0.2577421456575394
need align? ->  True 0.21016207188367844
2023-08-28 12:11:37,272 - epoch:24, training loss:0.2206 validation loss:0.3399
Updating learning rate to 1.0225255846133529e-05
Updating learning rate to 1.0225255846133529e-05
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.33992691338062286 0.2533378541469574
need align? ->  True 0.21016207188367844
2023-08-28 12:11:58,550 - epoch:25, training loss:0.2190 validation loss:0.3399
Updating learning rate to 6.610928798156691e-06
Updating learning rate to 6.610928798156691e-06
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.34134261906147 0.2560351550579071
need align? ->  True 0.21016207188367844
2023-08-28 12:12:21,221 - epoch:26, training loss:0.2200 validation loss:0.3413
Updating learning rate to 3.73900421022205e-06
Updating learning rate to 3.73900421022205e-06
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.34425667524337766 0.25959412157535555
need align? ->  True 0.21016207188367844
2023-08-28 12:12:43,942 - epoch:27, training loss:0.2192 validation loss:0.3443
Updating learning rate to 1.658621508277118e-06
Updating learning rate to 1.658621508277118e-06
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.343057444691658 0.2591343462467194
need align? ->  True 0.21016207188367844
2023-08-28 12:13:05,210 - epoch:28, training loss:0.2199 validation loss:0.3431
Updating learning rate to 4.053766171432101e-07
Updating learning rate to 4.053766171432101e-07
5311 760 1517 0.7 0.2 7588
train 4784
vs, vt 0.34423180520534513 0.25908163487911223
need align? ->  True 0.21016207188367844
2023-08-28 12:13:26,348 - epoch:29, training loss:0.2192 validation loss:0.3442
Updating learning rate to 7.129043727608552e-10
Updating learning rate to 7.129043727608552e-10
check exp/ECL-PatchTST2023-08-28-12:02:25.478364/0/0.2167_epoch_4.pkl  &  0.21016207188367844
2023-08-28 12:13:29,132 - [*] loss:0.1935
2023-08-28 12:13:29,136 - [*] phase 0, testing
2023-08-28 12:13:29,176 - T:192	MAE	0.315081	RMSE	0.193336	MAPE	199.707270
2023-08-28 12:13:29,178 - 192	mae	0.3151	
2023-08-28 12:13:29,178 - 192	rmse	0.1933	
2023-08-28 12:13:29,179 - 192	mape	199.7073	
2023-08-28 12:13:31,654 - [*] loss:0.1935
2023-08-28 12:13:31,657 - [*] phase 0, testing
2023-08-28 12:13:31,700 - T:192	MAE	0.315081	RMSE	0.193336	MAPE	199.707270
2023-08-28 12:13:34,557 - [*] loss:0.1814
2023-08-28 12:13:34,561 - [*] phase 0, testing
2023-08-28 12:13:34,603 - T:192	MAE	0.303410	RMSE	0.181248	MAPE	184.490418
2023-08-28 12:13:37,460 - [*] loss:0.1881
2023-08-28 12:13:37,464 - [*] phase 0, testing
2023-08-28 12:13:37,506 - T:192	MAE	0.308789	RMSE	0.188004	MAPE	193.449008
2023-08-28 12:13:37,508 - 192	mae	0.3088	
2023-08-28 12:13:37,508 - 192	rmse	0.1880	
2023-08-28 12:13:37,509 - 192	mape	193.4490	
2023-08-28 12:13:39,831 - logger name:exp/ECL-PatchTST2023-08-28-12:13:39.830891/ECL-PatchTST.log
2023-08-28 12:13:39,831 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'exchange_rate', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-12:13:39.830891', 'path': 'exp/ECL-PatchTST2023-08-28-12:13:39.830891', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 12:13:39,831 - [*] phase 0 start training
0 7588
5311 760 1517 0.7 0.2 7588
train 4640
5311 760 1517 0.7 0.2 7588
val 425
5311 760 1517 0.7 0.2 7588
test 1182
2023-08-28 12:13:39,916 - [*] phase 0 Dataset load!
2023-08-28 12:13:40,868 - [*] phase 0 Training start
5311 760 1517 0.7 0.2 7588
train 4640
2023-08-28 12:13:56,139 - epoch:0, training loss:0.6643 validation loss:0.5505
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.550472330302 0.5571636706590652
Updating learning rate to 1.0489352067289656e-05
Updating learning rate to 1.0489352067289656e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.4514439105987549 0.46522141993045807
need align? ->  False 0.46522141993045807
2023-08-28 12:14:24,163 - epoch:1, training loss:0.6249 validation loss:0.4514
Updating learning rate to 2.820275450860712e-05
Updating learning rate to 2.820275450860712e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.4196271449327469 0.43790990486741066
need align? ->  False 0.43790990486741066
2023-08-28 12:14:46,167 - epoch:2, training loss:0.5543 validation loss:0.4196
Updating learning rate to 5.235068629264831e-05
Updating learning rate to 5.235068629264831e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.4180586449801922 0.4295698441565037
need align? ->  False 0.4295698441565037
2023-08-28 12:15:07,350 - epoch:3, training loss:0.5131 validation loss:0.4181
Updating learning rate to 7.640379612593253e-05
Updating learning rate to 7.640379612593253e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.4238005317747593 0.4426809251308441
need align? ->  False 0.4295698441565037
2023-08-28 12:15:29,092 - epoch:4, training loss:0.4904 validation loss:0.4238
Updating learning rate to 9.385837159090278e-05
Updating learning rate to 9.385837159090278e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.4157038852572441 0.43662548810243607
need align? ->  False 0.4295698441565037
2023-08-28 12:15:50,220 - epoch:5, training loss:0.4767 validation loss:0.4157
Updating learning rate to 9.999966947063185e-05
Updating learning rate to 9.999966947063185e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.47189489379525185 0.46626685559749603
need align? ->  True 0.4295698441565037
2023-08-28 12:16:11,140 - epoch:6, training loss:0.4618 validation loss:0.4719
Updating learning rate to 9.95481868938872e-05
Updating learning rate to 9.95481868938872e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.4929291792213917 0.50531305372715
need align? ->  True 0.4295698441565037
2023-08-28 12:16:32,161 - epoch:7, training loss:0.4478 validation loss:0.4929
Updating learning rate to 9.82489245240909e-05
Updating learning rate to 9.82489245240909e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.44514279812574387 0.5399837531149387
need align? ->  True 0.4295698441565037
2023-08-28 12:16:53,559 - epoch:8, training loss:0.4392 validation loss:0.4451
Updating learning rate to 9.612411310061373e-05
Updating learning rate to 9.612411310061373e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.5210364870727062 0.4835565835237503
need align? ->  True 0.4295698441565037
2023-08-28 12:17:14,558 - epoch:9, training loss:0.4325 validation loss:0.5210
Updating learning rate to 9.321010873602042e-05
Updating learning rate to 9.321010873602042e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.4735807627439499 0.49794895201921463
need align? ->  True 0.4295698441565037
2023-08-28 12:17:34,804 - epoch:10, training loss:0.4236 validation loss:0.4736
Updating learning rate to 8.955677085290379e-05
Updating learning rate to 8.955677085290379e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.4863242991268635 0.4719812236726284
need align? ->  True 0.4295698441565037
2023-08-28 12:17:56,324 - epoch:11, training loss:0.4171 validation loss:0.4863
Updating learning rate to 8.52266090753406e-05
Updating learning rate to 8.52266090753406e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.5104101449251175 0.45080217719078064
need align? ->  True 0.4295698441565037
2023-08-28 12:18:17,810 - epoch:12, training loss:0.4112 validation loss:0.5104
Updating learning rate to 8.029371367189262e-05
Updating learning rate to 8.029371367189262e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.5613160096108913 0.4710363820195198
need align? ->  True 0.4295698441565037
2023-08-28 12:18:39,443 - epoch:13, training loss:0.4050 validation loss:0.5613
Updating learning rate to 7.484248785056981e-05
Updating learning rate to 7.484248785056981e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.5481304749846458 0.47193094342947006
need align? ->  True 0.4295698441565037
2023-08-28 12:19:00,889 - epoch:14, training loss:0.3989 validation loss:0.5481
Updating learning rate to 6.896620359654033e-05
Updating learning rate to 6.896620359654033e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.5466805025935173 0.4567894972860813
need align? ->  True 0.4295698441565037
2023-08-28 12:19:21,309 - epoch:15, training loss:0.3941 validation loss:0.5467
Updating learning rate to 6.276540576260444e-05
Updating learning rate to 6.276540576260444e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.5277898237109184 0.4507093094289303
need align? ->  True 0.4295698441565037
2023-08-28 12:19:42,307 - epoch:16, training loss:0.3887 validation loss:0.5278
Updating learning rate to 5.63461917188867e-05
Updating learning rate to 5.63461917188867e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.569000169634819 0.42978334054350853
need align? ->  True 0.4295698441565037
2023-08-28 12:20:02,477 - epoch:17, training loss:0.3851 validation loss:0.5690
Updating learning rate to 4.9818395997417525e-05
Updating learning rate to 4.9818395997417525e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.5691056326031685 0.49850843846797943
need align? ->  True 0.4295698441565037
2023-08-28 12:20:23,806 - epoch:18, training loss:0.3804 validation loss:0.5691
Updating learning rate to 4.3293710992838125e-05
Updating learning rate to 4.3293710992838125e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.5798378437757492 0.4424401633441448
need align? ->  True 0.4295698441565037
2023-08-28 12:20:44,978 - epoch:19, training loss:0.3792 validation loss:0.5798
Updating learning rate to 3.688377587456131e-05
Updating learning rate to 3.688377587456131e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.6090043112635612 0.4705267436802387
need align? ->  True 0.4295698441565037
2023-08-28 12:21:06,725 - epoch:20, training loss:0.3732 validation loss:0.6090
Updating learning rate to 3.069826640963051e-05
Updating learning rate to 3.069826640963051e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.609680600464344 0.5029857344925404
need align? ->  True 0.4295698441565037
2023-08-28 12:21:28,330 - epoch:21, training loss:0.3710 validation loss:0.6097
Updating learning rate to 2.4843018379937972e-05
Updating learning rate to 2.4843018379937972e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.6111975833773613 0.5124275200068951
need align? ->  True 0.4295698441565037
2023-08-28 12:21:49,512 - epoch:22, training loss:0.3706 validation loss:0.6112
Updating learning rate to 1.9418216702653153e-05
Updating learning rate to 1.9418216702653153e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.6070003062486649 0.4926469624042511
need align? ->  True 0.4295698441565037
2023-08-28 12:22:09,678 - epoch:23, training loss:0.3702 validation loss:0.6070
Updating learning rate to 1.4516681238513547e-05
Updating learning rate to 1.4516681238513547e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.6345000490546227 0.5057325586676598
need align? ->  True 0.4295698441565037
2023-08-28 12:22:30,347 - epoch:24, training loss:0.3669 validation loss:0.6345
Updating learning rate to 1.0222278618272986e-05
Updating learning rate to 1.0222278618272986e-05
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.5984317511320114 0.5094102099537849
need align? ->  True 0.4295698441565037
2023-08-28 12:22:51,144 - epoch:25, training loss:0.3663 validation loss:0.5984
Updating learning rate to 6.608487261397122e-06
Updating learning rate to 6.608487261397122e-06
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.6147380024194717 0.4853218086063862
need align? ->  True 0.4295698441565037
2023-08-28 12:23:12,265 - epoch:26, training loss:0.3667 validation loss:0.6147
Updating learning rate to 3.7371401399343318e-06
Updating learning rate to 3.7371401399343318e-06
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.6116451323032379 0.49621808901429176
need align? ->  True 0.4295698441565037
2023-08-28 12:23:33,730 - epoch:27, training loss:0.3662 validation loss:0.6116
Updating learning rate to 1.6573667992206914e-06
Updating learning rate to 1.6573667992206914e-06
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.6123258620500565 0.49676791578531265
need align? ->  True 0.4295698441565037
2023-08-28 12:23:55,873 - epoch:28, training loss:0.3650 validation loss:0.6123
Updating learning rate to 4.047527377379024e-07
Updating learning rate to 4.047527377379024e-07
5311 760 1517 0.7 0.2 7588
train 4640
vs, vt 0.6149193942546844 0.4959607869386673
need align? ->  True 0.4295698441565037
2023-08-28 12:24:08,145 - epoch:29, training loss:0.3652 validation loss:0.6149
Updating learning rate to 7.305293681617861e-10
Updating learning rate to 7.305293681617861e-10
check exp/ECL-PatchTST2023-08-28-12:13:39.830891/0/0.4157_epoch_5.pkl  &  0.4295698441565037
2023-08-28 12:24:08,823 - [*] loss:0.3570
2023-08-28 12:24:08,828 - [*] phase 0, testing
2023-08-28 12:24:08,910 - T:336	MAE	0.441852	RMSE	0.364298	MAPE	308.436656
2023-08-28 12:24:08,910 - 336	mae	0.4419	
2023-08-28 12:24:08,911 - 336	rmse	0.3643	
2023-08-28 12:24:08,911 - 336	mape	308.4367	
2023-08-28 12:24:09,435 - [*] loss:0.3570
2023-08-28 12:24:09,439 - [*] phase 0, testing
2023-08-28 12:24:09,514 - T:336	MAE	0.441852	RMSE	0.364298	MAPE	308.436656
2023-08-28 12:24:10,119 - [*] loss:0.3516
2023-08-28 12:24:10,122 - [*] phase 0, testing
2023-08-28 12:24:10,198 - T:336	MAE	0.437933	RMSE	0.355636	MAPE	310.567307
2023-08-28 12:24:10,713 - [*] loss:0.3362
2023-08-28 12:24:10,716 - [*] phase 0, testing
2023-08-28 12:24:10,787 - T:336	MAE	0.421122	RMSE	0.337333	MAPE	295.761538
2023-08-28 12:24:10,787 - 336	mae	0.4211	
2023-08-28 12:24:10,787 - 336	rmse	0.3373	
2023-08-28 12:24:10,787 - 336	mape	295.7615	
2023-08-28 12:24:12,765 - logger name:exp/ECL-PatchTST2023-08-28-12:24:12.764843/ECL-PatchTST.log
2023-08-28 12:24:12,765 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'exchange_rate', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-12:24:12.764843', 'path': 'exp/ECL-PatchTST2023-08-28-12:24:12.764843', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 12:24:12,765 - [*] phase 0 start training
0 7588
5311 760 1517 0.7 0.2 7588
train 4256
5311 760 1517 0.7 0.2 7588
val 41
5311 760 1517 0.7 0.2 7588
test 798
2023-08-28 12:24:12,837 - [*] phase 0 Dataset load!
2023-08-28 12:24:13,738 - [*] phase 0 Training start
5311 760 1517 0.7 0.2 7588
train 4256
2023-08-28 12:24:19,608 - epoch:0, training loss:1.0393 validation loss:1.1643
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.1642677783966064 1.1533349752426147
Updating learning rate to 1.0494716053729492e-05
Updating learning rate to 1.0494716053729492e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4126524925231934 1.2898759841918945
need align? ->  True 1.1533349752426147
2023-08-28 12:24:33,877 - epoch:1, training loss:1.0848 validation loss:1.4127
Updating learning rate to 2.8221308522477503e-05
Updating learning rate to 2.8221308522477503e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.5370688438415527 1.4692686796188354
need align? ->  True 1.1533349752426147
2023-08-28 12:24:44,549 - epoch:2, training loss:1.0056 validation loss:1.5371
Updating learning rate to 5.238272804438159e-05
Updating learning rate to 5.238272804438159e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.5598225593566895 1.665909767150879
need align? ->  True 1.1533349752426147
2023-08-28 12:24:55,207 - epoch:3, training loss:0.9660 validation loss:1.5598
Updating learning rate to 7.644057631736663e-05
Updating learning rate to 7.644057631736663e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4549552202224731 1.6353511810302734
need align? ->  True 1.1533349752426147
2023-08-28 12:25:05,754 - epoch:4, training loss:0.9461 validation loss:1.4550
Updating learning rate to 9.38844827832349e-05
Updating learning rate to 9.38844827832349e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4178614616394043 1.4380242824554443
need align? ->  True 1.1533349752426147
2023-08-28 12:25:16,611 - epoch:5, training loss:0.9311 validation loss:1.4179
Updating learning rate to 9.99996066428178e-05
Updating learning rate to 9.99996066428178e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.3313406705856323 1.3489848375320435
need align? ->  True 1.1533349752426147
2023-08-28 12:25:27,015 - epoch:6, training loss:0.9087 validation loss:1.3313
Updating learning rate to 9.95459673249453e-05
Updating learning rate to 9.95459673249453e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.5019670724868774 1.4369187355041504
need align? ->  True 1.1533349752426147
2023-08-28 12:25:37,680 - epoch:7, training loss:0.8843 validation loss:1.5020
Updating learning rate to 9.824458619146117e-05
Updating learning rate to 9.824458619146117e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.4340864419937134 1.4305343627929688
need align? ->  True 1.1533349752426147
2023-08-28 12:25:48,303 - epoch:8, training loss:0.8578 validation loss:1.4341
Updating learning rate to 9.611773023437026e-05
Updating learning rate to 9.611773023437026e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.5868803262710571 1.4186830520629883
need align? ->  True 1.1533349752426147
2023-08-28 12:25:58,783 - epoch:9, training loss:0.8443 validation loss:1.5869
Updating learning rate to 9.320179054877429e-05
Updating learning rate to 9.320179054877429e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.620174765586853 1.5006349086761475
need align? ->  True 1.1533349752426147
2023-08-28 12:26:09,459 - epoch:10, training loss:0.8231 validation loss:1.6202
Updating learning rate to 8.954665967114503e-05
Updating learning rate to 8.954665967114503e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.7399801015853882 1.5310221910476685
need align? ->  True 1.1533349752426147
2023-08-28 12:26:19,916 - epoch:11, training loss:0.7967 validation loss:1.7400
Updating learning rate to 8.521487790419246e-05
Updating learning rate to 8.521487790419246e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.5980627536773682 1.6146363019943237
need align? ->  True 1.1533349752426147
2023-08-28 12:26:30,496 - epoch:12, training loss:0.7836 validation loss:1.5981
Updating learning rate to 8.02805632349459e-05
Updating learning rate to 8.02805632349459e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.6912039518356323 1.5467071533203125
need align? ->  True 1.1533349752426147
2023-08-28 12:26:40,752 - epoch:13, training loss:0.7695 validation loss:1.6912
Updating learning rate to 7.48281431554467e-05
Updating learning rate to 7.48281431554467e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.7064260244369507 1.711097002029419
need align? ->  True 1.1533349752426147
2023-08-28 12:26:51,296 - epoch:14, training loss:0.7599 validation loss:1.7064
Updating learning rate to 6.895091008495147e-05
Updating learning rate to 6.895091008495147e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.8057531118392944 1.6173361539840698
need align? ->  True 1.1533349752426147
2023-08-28 12:27:02,075 - epoch:15, training loss:0.7480 validation loss:1.8058
Updating learning rate to 6.274942511077328e-05
Updating learning rate to 6.274942511077328e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.7100329399108887 1.7364487648010254
need align? ->  True 1.1533349752426147
2023-08-28 12:27:12,717 - epoch:16, training loss:0.7398 validation loss:1.7100
Updating learning rate to 5.632979736019675e-05
Updating learning rate to 5.632979736019675e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.6834274530410767 1.6254140138626099
need align? ->  True 1.1533349752426147
2023-08-28 12:27:23,288 - epoch:17, training loss:0.7320 validation loss:1.6834
Updating learning rate to 4.980186844389133e-05
Updating learning rate to 4.980186844389133e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.8658636808395386 1.6378389596939087
need align? ->  True 1.1533349752426147
2023-08-28 12:27:33,786 - epoch:18, training loss:0.7240 validation loss:1.8659
Updating learning rate to 4.3277333035498836e-05
Updating learning rate to 4.3277333035498836e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.7479193210601807 1.6992298364639282
need align? ->  True 1.1533349752426147
2023-08-28 12:27:44,245 - epoch:19, training loss:0.7202 validation loss:1.7479
Updating learning rate to 3.686782774479982e-05
Updating learning rate to 3.686782774479982e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.7761534452438354 1.5702296495437622
need align? ->  True 1.1533349752426147
2023-08-28 12:27:55,240 - epoch:20, training loss:0.7109 validation loss:1.7762
Updating learning rate to 3.0683020984368707e-05
Updating learning rate to 3.0683020984368707e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.7781778573989868 1.6492185592651367
need align? ->  True 1.1533349752426147
2023-08-28 12:28:05,914 - epoch:21, training loss:0.7104 validation loss:1.7782
Updating learning rate to 2.48287365126289e-05
Updating learning rate to 2.48287365126289e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.7682420015335083 1.6421903371810913
need align? ->  True 1.1533349752426147
2023-08-28 12:28:16,360 - epoch:22, training loss:0.7058 validation loss:1.7682
Updating learning rate to 1.940514276000616e-05
Updating learning rate to 1.940514276000616e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.7844865322113037 1.6461259126663208
need align? ->  True 1.1533349752426147
2023-08-28 12:28:27,155 - epoch:23, training loss:0.7027 validation loss:1.7845
Updating learning rate to 1.4505038919312107e-05
Updating learning rate to 1.4505038919312107e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.7982417345046997 1.6362268924713135
need align? ->  True 1.1533349752426147
2023-08-28 12:28:37,499 - epoch:24, training loss:0.7019 validation loss:1.7982
Updating learning rate to 1.0212267125826495e-05
Updating learning rate to 1.0212267125826495e-05
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.7952455282211304 1.6495941877365112
need align? ->  True 1.1533349752426147
2023-08-28 12:28:48,005 - epoch:25, training loss:0.7013 validation loss:1.7952
Updating learning rate to 6.600277895117048e-06
Updating learning rate to 6.600277895117048e-06
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.7737261056900024 1.6543009281158447
need align? ->  True 1.1533349752426147
2023-08-28 12:28:58,649 - epoch:26, training loss:0.6990 validation loss:1.7737
Updating learning rate to 3.730873364353798e-06
Updating learning rate to 3.730873364353798e-06
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.771188735961914 1.6217573881149292
need align? ->  True 1.1533349752426147
2023-08-28 12:29:09,105 - epoch:27, training loss:0.6990 validation loss:1.7712
Updating learning rate to 1.6531498406073574e-06
Updating learning rate to 1.6531498406073574e-06
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.7798922061920166 1.622327446937561
need align? ->  True 1.1533349752426147
2023-08-28 12:29:19,715 - epoch:28, training loss:0.6971 validation loss:1.7799
Updating learning rate to 4.026577494227991e-07
Updating learning rate to 4.026577494227991e-07
5311 760 1517 0.7 0.2 7588
train 4256
vs, vt 1.769441843032837 1.6279511451721191
need align? ->  True 1.1533349752426147
2023-08-28 12:29:30,268 - epoch:29, training loss:0.6996 validation loss:1.7694
Updating learning rate to 7.933571822109223e-10
Updating learning rate to 7.933571822109223e-10
check exp/ECL-PatchTST2023-08-28-12:24:12.764843/0/1.1643_epoch_0.pkl  &  1.1533349752426147
2023-08-28 12:29:30,852 - [*] loss:1.0675
2023-08-28 12:29:30,859 - [*] phase 0, testing
2023-08-28 12:29:30,967 - T:720	MAE	0.785491	RMSE	1.060021	MAPE	683.614254
2023-08-28 12:29:30,968 - 720	mae	0.7855	
2023-08-28 12:29:30,968 - 720	rmse	1.0600	
2023-08-28 12:29:30,968 - 720	mape	683.6143	
2023-08-28 12:29:31,538 - [*] loss:1.0675
2023-08-28 12:29:31,545 - [*] phase 0, testing
2023-08-28 12:29:31,643 - T:720	MAE	0.785491	RMSE	1.060021	MAPE	683.614254
2023-08-28 12:29:32,077 - [*] loss:1.0883
2023-08-28 12:29:32,083 - [*] phase 0, testing
2023-08-28 12:29:32,185 - T:720	MAE	0.789810	RMSE	1.077452	MAPE	689.099264
2023-08-28 12:29:32,624 - [*] loss:1.0866
2023-08-28 12:29:32,630 - [*] phase 0, testing
2023-08-28 12:29:32,730 - T:720	MAE	0.789457	RMSE	1.076207	MAPE	688.283062
2023-08-28 12:29:32,730 - 720	mae	0.7895	
2023-08-28 12:29:32,730 - 720	rmse	1.0762	
2023-08-28 12:29:32,730 - 720	mape	688.2831	
