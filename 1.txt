python main.py --conf ECL-PatchTST --noise_rate 0.5 --idx -1 --data_name ETTh2 --device "cuda:1" --seed 34 --same_init   --aligner 1 --loss huber --refiner 1 --enhance 1 --enhance_type 5 --add_noise 1 --mid_dim 128 --feature_jittering 1 --rec_intra_feature 1 --rec_ori 1 ; python main.py --conf ECL-PatchTST --noise_rate 0.5 --idx -1 --data_name ETTh2 --device "cuda:1" --seed 35 --same_init   --aligner 1 --loss huber --refiner 1 --enhance 1 --enhance_type 5 --add_noise 1 --mid_dim 128 --feature_jittering 1 --rec_intra_feature 1 --rec_ori 1 
2023-08-08 14:48:53,456 - logger name:exp/ECL-PatchTST2023-08-08-14:48:53.456044/ECL-PatchTST.log
2023-08-08 14:48:53,456 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-08-10:33:05.890873/0/0.1629_epoch_11.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 1, 'rec_ori': 1, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-14:48:53.456044', 'path': 'exp/ECL-PatchTST2023-08-08-14:48:53.456044', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 14:48:53,456 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 14:48:53,661 - [*] phase 0 Dataset load!
2023-08-08 14:48:55,254 - [*] phase 0 Training start
train 8209
2023-08-08 14:49:56,467 - epoch:0, training loss:0.2127 validation loss:0.1650
train 8209
vs, vt 0.16504068012264642 0.16559637168591673
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.14047758958556436 0.1360680346631191
need align? ->  True 0.1360680346631191
2023-08-08 14:51:47,112 - epoch:1, training loss:1.9675 validation loss:0.1405
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12987968901341612 0.12632461154664104
need align? ->  True 0.12632461154664104
2023-08-08 14:53:28,167 - epoch:2, training loss:1.6610 validation loss:0.1299
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12582805583422835 0.12314669897949154
need align? ->  True 0.12314669897949154
2023-08-08 14:55:08,847 - epoch:3, training loss:1.3534 validation loss:0.1258
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12419940810650587 0.12221794058992104
need align? ->  True 0.12221794058992104
2023-08-08 14:56:57,672 - epoch:4, training loss:1.1168 validation loss:0.1242
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12529233868487857 0.12175020567056807
need align? ->  True 0.12175020567056807
2023-08-08 14:58:46,239 - epoch:5, training loss:0.9958 validation loss:0.1253
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12476992928846316 0.12258723496713421
need align? ->  True 0.12175020567056807
2023-08-08 15:00:34,705 - epoch:6, training loss:0.9407 validation loss:0.1248
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12302749510854483 0.1215014314617623
need align? ->  True 0.1215014314617623
2023-08-08 15:02:14,875 - epoch:7, training loss:0.9179 validation loss:0.1230
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12541518326510082 0.12222704833204096
need align? ->  True 0.1215014314617623
2023-08-08 15:03:55,003 - epoch:8, training loss:0.8929 validation loss:0.1254
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.1238622237843546 0.12306749016385186
need align? ->  True 0.1215014314617623
2023-08-08 15:05:35,084 - epoch:9, training loss:0.8752 validation loss:0.1239
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12333123445172202 0.12178439257497137
need align? ->  True 0.1215014314617623
2023-08-08 15:07:18,128 - epoch:10, training loss:0.8638 validation loss:0.1233
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.1229822129349817 0.12155193547633561
need align? ->  True 0.1215014314617623
2023-08-08 15:09:01,326 - epoch:11, training loss:0.8527 validation loss:0.1230
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12552727479487658 0.12114956890317527
need align? ->  True 0.12114956890317527
2023-08-08 15:10:44,341 - epoch:12, training loss:0.8437 validation loss:0.1255
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12509017962623725 0.1229645570909435
need align? ->  True 0.12114956890317527
2023-08-08 15:12:27,008 - epoch:13, training loss:0.8510 validation loss:0.1251
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12548234601589767 0.12033281653103503
need align? ->  True 0.12033281653103503
2023-08-08 15:14:07,618 - epoch:14, training loss:0.8331 validation loss:0.1255
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12432093046266925 0.12218882135030898
need align? ->  True 0.12033281653103503
2023-08-08 15:15:47,979 - epoch:15, training loss:0.8520 validation loss:0.1243
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12382431302896955 0.12154315936971795
need align? ->  True 0.12033281653103503
2023-08-08 15:17:29,214 - epoch:16, training loss:0.8393 validation loss:0.1238
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12438157759606838 0.12138544861227274
need align? ->  True 0.12033281653103503
2023-08-08 15:19:10,103 - epoch:17, training loss:0.8322 validation loss:0.1244
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12486979475414212 0.12122054457325827
need align? ->  True 0.12033281653103503
2023-08-08 15:20:50,862 - epoch:18, training loss:0.8284 validation loss:0.1249
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12418953329324722 0.12160524040121924
need align? ->  True 0.12033281653103503
2023-08-08 15:22:31,414 - epoch:19, training loss:0.8237 validation loss:0.1242
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12428349248048934 0.12125276613303206
need align? ->  True 0.12033281653103503
2023-08-08 15:24:12,470 - epoch:20, training loss:0.8202 validation loss:0.1243
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12325799913907592 0.12175965715538371
need align? ->  True 0.12033281653103503
2023-08-08 15:25:52,789 - epoch:21, training loss:0.8176 validation loss:0.1233
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12322446720843966 0.12168048559264703
need align? ->  True 0.12033281653103503
2023-08-08 15:27:34,659 - epoch:22, training loss:0.8163 validation loss:0.1232
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12386953898451546 0.12147418079389767
need align? ->  True 0.12033281653103503
2023-08-08 15:29:17,521 - epoch:23, training loss:0.8141 validation loss:0.1239
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12445183851840821 0.12152880548753521
need align? ->  True 0.12033281653103503
2023-08-08 15:31:01,136 - epoch:24, training loss:0.8130 validation loss:0.1245
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12353987145152959 0.12166813583197919
need align? ->  True 0.12033281653103503
2023-08-08 15:32:43,580 - epoch:25, training loss:0.8114 validation loss:0.1235
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.123536323197186 0.12150776369327848
need align? ->  True 0.12033281653103503
2023-08-08 15:34:25,307 - epoch:26, training loss:0.8114 validation loss:0.1235
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.1236810487779704 0.12151529851623556
need align? ->  True 0.12033281653103503
2023-08-08 15:36:07,364 - epoch:27, training loss:0.8107 validation loss:0.1237
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12373566754500974 0.12150049438192086
need align? ->  True 0.12033281653103503
2023-08-08 15:37:49,478 - epoch:28, training loss:0.8118 validation loss:0.1237
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12360203537074002 0.12150366655127569
need align? ->  True 0.12033281653103503
2023-08-08 15:39:30,520 - epoch:29, training loss:0.8095 validation loss:0.1236
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-14:48:53.456044/0/0.123_epoch_11.pkl  &  0.12033281653103503
2023-08-08 15:39:36,125 - [*] loss:0.2743
2023-08-08 15:39:36,128 - [*] phase 0, testing
2023-08-08 15:39:36,177 - T:96  MAE     0.332322        RMSE    0.274480        MAPE    129.982746
2023-08-08 15:39:36,177 - 96    mae     0.3323
2023-08-08 15:39:36,177 - 96    rmse    0.2745
2023-08-08 15:39:36,177 - 96    mape    129.9827
2023-08-08 15:39:40,347 - [*] loss:0.2947
2023-08-08 15:39:40,350 - [*] phase 0, testing
2023-08-08 15:39:40,387 - T:96  MAE     0.353949        RMSE    0.294081        MAPE    135.044873
2023-08-08 15:39:45,855 - [*] loss:0.3029
2023-08-08 15:39:45,859 - [*] phase 0, testing
2023-08-08 15:39:45,908 - T:96  MAE     0.357812        RMSE    0.301968        MAPE    133.711779
2023-08-08 15:39:50,072 - [*] loss:0.2689
2023-08-08 15:39:50,075 - [*] phase 0, testing
2023-08-08 15:39:50,114 - T:96  MAE     0.328369        RMSE    0.269066        MAPE    129.051745
2023-08-08 15:39:50,115 - 96    mae     0.3284
2023-08-08 15:39:50,115 - 96    rmse    0.2691
2023-08-08 15:39:50,115 - 96    mape    129.0517
2023-08-08 15:39:52,112 - logger name:exp/ECL-PatchTST2023-08-08-15:39:52.112561/ECL-PatchTST.log
2023-08-08 15:39:52,113 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-08-10:33:05.890873/0/0.1629_epoch_11.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 1, 'rec_ori': 1, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-15:39:52.112561', 'path': 'exp/ECL-PatchTST2023-08-08-15:39:52.112561', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 15:39:52,113 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 15:39:52,320 - [*] phase 0 Dataset load!
2023-08-08 15:39:53,287 - [*] phase 0 Training start
train 8209
2023-08-08 15:40:48,513 - epoch:0, training loss:0.2133 validation loss:0.1653
train 8209
vs, vt 0.16531540216370064 0.16537542929026214
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.14046313224191015 0.13501011571762236
need align? ->  True 0.13501011571762236
2023-08-08 15:42:41,499 - epoch:1, training loss:1.9670 validation loss:0.1405
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.13028268643062224 0.12707252106205982
need align? ->  True 0.12707252106205982
2023-08-08 15:44:23,367 - epoch:2, training loss:1.6553 validation loss:0.1303
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12702209138396112 0.12372381206263196
need align? ->  True 0.12372381206263196
2023-08-08 15:46:04,167 - epoch:3, training loss:1.3478 validation loss:0.1270
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.1249740630896254 0.12322421194138852
need align? ->  True 0.12322421194138852
2023-08-08 15:47:45,564 - epoch:4, training loss:1.1005 validation loss:0.1250
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12444723549891602 0.12297446212985298
need align? ->  True 0.12297446212985298
2023-08-08 15:49:26,519 - epoch:5, training loss:0.9817 validation loss:0.1244
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.1255802812732079 0.12251868056641384
need align? ->  True 0.12251868056641384
2023-08-08 15:51:08,299 - epoch:6, training loss:0.9402 validation loss:0.1256
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12492691175165502 0.12117903865873814
need align? ->  True 0.12117903865873814
2023-08-08 15:52:48,329 - epoch:7, training loss:0.9054 validation loss:0.1249
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12413031379268928 0.12198947480117733
need align? ->  True 0.12117903865873814
2023-08-08 15:54:29,305 - epoch:8, training loss:0.8927 validation loss:0.1241
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12532614264637232 0.12216848829253153
need align? ->  True 0.12117903865873814
2023-08-08 15:56:10,431 - epoch:9, training loss:0.8748 validation loss:0.1253
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12434724735265429 0.12211609267714349
need align? ->  True 0.12117903865873814
2023-08-08 15:57:52,045 - epoch:10, training loss:0.8633 validation loss:0.1243
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12711417217823592 0.12151203787123616
need align? ->  True 0.12117903865873814
2023-08-08 15:59:33,128 - epoch:11, training loss:0.8520 validation loss:0.1271
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12471735308116133 0.12105536579408428
need align? ->  True 0.12105536579408428
2023-08-08 16:01:14,382 - epoch:12, training loss:0.8452 validation loss:0.1247
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12391163611953909 0.12105438968336041
need align? ->  True 0.12105438968336041
2023-08-08 16:02:55,748 - epoch:13, training loss:0.8487 validation loss:0.1239
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12604438327252865 0.12163975758647377
need align? ->  True 0.12105438968336041
2023-08-08 16:04:37,307 - epoch:14, training loss:0.8525 validation loss:0.1260
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12662248584357175 0.12135383622212843
need align? ->  True 0.12105438968336041
2023-08-08 16:06:21,939 - epoch:15, training loss:0.8374 validation loss:0.1266
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12492768220942128 0.12116979426619681
need align? ->  True 0.12105438968336041
2023-08-08 16:08:07,576 - epoch:16, training loss:0.8325 validation loss:0.1249
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12554662530733782 0.12126981509341435
need align? ->  True 0.12105438968336041
2023-08-08 16:09:54,957 - epoch:17, training loss:0.8275 validation loss:0.1255
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.1254531443965706 0.1215624554421414
need align? ->  True 0.12105438968336041
2023-08-08 16:11:45,821 - epoch:18, training loss:0.8205 validation loss:0.1255
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12509418092668056 0.12104292899708856
need align? ->  True 0.12104292899708856
2023-08-08 16:13:32,213 - epoch:19, training loss:0.8181 validation loss:0.1251
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12451765974136916 0.12113801559264009
need align? ->  True 0.12104292899708856
2023-08-08 16:15:18,365 - epoch:20, training loss:0.8539 validation loss:0.1245
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12558031497015196 0.12081077415496111
need align? ->  True 0.12081077415496111
2023-08-08 16:17:03,018 - epoch:21, training loss:0.8385 validation loss:0.1256
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.1246093763885173 0.1208611282265999
need align? ->  True 0.12081077415496111
2023-08-08 16:18:47,131 - epoch:22, training loss:0.8612 validation loss:0.1246
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.1264626785096797 0.12114622884175995
need align? ->  True 0.12081077415496111
2023-08-08 16:20:28,724 - epoch:23, training loss:0.8503 validation loss:0.1265
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12486510198902 0.12059733635661277
need align? ->  True 0.12059733635661277
2023-08-08 16:22:09,985 - epoch:24, training loss:0.8474 validation loss:0.1249
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.1246555180881511 0.12070542133667252
need align? ->  True 0.12059733635661277
2023-08-08 16:23:51,511 - epoch:25, training loss:0.8777 validation loss:0.1247
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12516186547211625 0.12072548515756022
need align? ->  True 0.12059733635661277
2023-08-08 16:25:27,794 - epoch:26, training loss:0.8676 validation loss:0.1252
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.1251345593482256 0.12078429343686863
need align? ->  True 0.12059733635661277
2023-08-08 16:26:51,253 - epoch:27, training loss:0.8649 validation loss:0.1251
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12503868189047684 0.12070446419106289
need align? ->  True 0.12059733635661277
2023-08-08 16:28:14,532 - epoch:28, training loss:0.8642 validation loss:0.1250
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.1250682007521391 0.12070429951629856
need align? ->  True 0.12059733635661277
2023-08-08 16:29:37,873 - epoch:29, training loss:0.8627 validation loss:0.1251
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-15:39:52.112561/0/0.1239_epoch_13.pkl  &  0.12059733635661277
2023-08-08 16:29:40,107 - [*] loss:0.2757
2023-08-08 16:29:40,110 - [*] phase 0, testing
2023-08-08 16:29:40,147 - T:96  MAE     0.334411        RMSE    0.275444        MAPE    132.520330
2023-08-08 16:29:40,148 - 96    mae     0.3344
2023-08-08 16:29:40,148 - 96    rmse    0.2754
2023-08-08 16:29:40,148 - 96    mape    132.5203
2023-08-08 16:29:41,126 - [*] loss:0.3465
2023-08-08 16:29:41,129 - [*] phase 0, testing
2023-08-08 16:29:41,166 - T:96  MAE     0.393673        RMSE    0.344646        MAPE    152.928066
2023-08-08 16:29:43,284 - [*] loss:0.3608
2023-08-08 16:29:43,287 - [*] phase 0, testing
2023-08-08 16:29:43,323 - T:96  MAE     0.398211        RMSE    0.357549        MAPE    151.787329
2023-08-08 16:29:44,304 - [*] loss:0.2680
2023-08-08 16:29:44,307 - [*] phase 0, testing
2023-08-08 16:29:44,343 - T:96  MAE     0.328490        RMSE    0.268247        MAPE    130.307472
2023-08-08 16:29:44,344 - 96    mae     0.3285
2023-08-08 16:29:44,344 - 96    rmse    0.2682
2023-08-08 16:29:44,344 - 96    mape    130.3075



















python main.py --conf ECL-PatchTST --noise_rate 0.5 --idx -1 --pred_len 24 --data_name ETTh2 --device "cuda:0" --seed 35 --same_init   --aligner 1 --loss huber --refiner 1 --enhance 1 --enhance_type 5 --add_noise 1 --mid_dim 128 --feature_jittering 1 --rec_intra_feature 1 --rec_ori 1 ; python main.py --conf ECL-PatchTST --noise_rate 0.5 --idx -1 --pred_len 336 --data_name ETTh2 --device "cuda:0" --seed 36 --same_init   --aligner 1 --loss huber --refiner 1 --enhance 1 --enhance_type 5 --add_noise 1 --mid_dim 128 --feature_jittering 1 --rec_intra_feature 1 --rec_ori 1 
2023-08-08 14:48:53,820 - logger name:exp/ECL-PatchTST2023-08-08-14:48:53.819737/ECL-PatchTST.log
2023-08-08 14:48:53,820 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-08-10:33:05.890873/0/0.1629_epoch_11.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 1, 'rec_ori': 1, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-14:48:53.819737', 'path': 'exp/ECL-PatchTST2023-08-08-14:48:53.819737', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 14:48:53,820 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-08 14:48:54,028 - [*] phase 0 Dataset load!
2023-08-08 14:48:55,254 - [*] phase 0 Training start
train 8281
2023-08-08 14:49:49,180 - epoch:0, training loss:0.1897 validation loss:0.1400
train 8281
vs, vt 0.1400123769822328 0.13970039023653322
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10605927111338014 0.09734115571431491
need align? ->  True 0.09734115571431491
2023-08-08 14:51:33,560 - epoch:1, training loss:1.8981 validation loss:0.1061
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.0870508388818606 0.08075355438758498
need align? ->  True 0.08075355438758498
2023-08-08 14:53:05,887 - epoch:2, training loss:1.5949 validation loss:0.0871
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.08000549827904804 0.07820478130293929
need align? ->  True 0.07820478130293929
2023-08-08 14:54:39,474 - epoch:3, training loss:1.3298 validation loss:0.0800
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.0787667910689893 0.0773077556134566
need align? ->  True 0.0773077556134566
2023-08-08 14:56:16,992 - epoch:4, training loss:1.0822 validation loss:0.0788
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07843566021841505 0.07743170217651388
need align? ->  True 0.0773077556134566
2023-08-08 14:57:54,070 - epoch:5, training loss:0.9325 validation loss:0.0784
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07806278813792311 0.07731020361509013
need align? ->  True 0.0773077556134566
2023-08-08 14:59:31,293 - epoch:6, training loss:0.9031 validation loss:0.0781
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07926434545737246 0.07708173180403917
need align? ->  True 0.07708173180403917
2023-08-08 15:01:07,328 - epoch:7, training loss:0.8828 validation loss:0.0793
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07694669804819253 0.07716782909372578
need align? ->  False 0.07708173180403917
2023-08-08 15:02:40,453 - epoch:8, training loss:0.8021 validation loss:0.0769
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07647464497257835 0.07662722533163817
need align? ->  False 0.07662722533163817
2023-08-08 15:04:13,806 - epoch:9, training loss:0.7807 validation loss:0.0765
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07686873897910118 0.07555588723524757
need align? ->  True 0.07555588723524757
2023-08-08 15:05:47,527 - epoch:10, training loss:0.7781 validation loss:0.0769
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07619757440103136 0.0751775171931671
need align? ->  True 0.0751775171931671
2023-08-08 15:07:23,687 - epoch:11, training loss:0.7781 validation loss:0.0762
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07519642190764779 0.07532487429030564
need align? ->  True 0.0751775171931671
2023-08-08 15:09:00,959 - epoch:12, training loss:0.7796 validation loss:0.0752
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07573425146224706 0.07473059282030749
need align? ->  True 0.07473059282030749
2023-08-08 15:10:37,254 - epoch:13, training loss:0.7659 validation loss:0.0757
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.0756132827018914 0.07537768562526806
need align? ->  True 0.07473059282030749
2023-08-08 15:12:10,832 - epoch:14, training loss:0.7824 validation loss:0.0756
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07535999859480755 0.07485037305108878
need align? ->  True 0.07473059282030749
2023-08-08 15:13:43,598 - epoch:15, training loss:0.7684 validation loss:0.0754
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07533660676816235 0.0746002363283997
need align? ->  True 0.0746002363283997
2023-08-08 15:15:18,091 - epoch:16, training loss:0.7620 validation loss:0.0753
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07494219036205955 0.07487542449456194
need align? ->  True 0.0746002363283997
2023-08-08 15:16:51,346 - epoch:17, training loss:0.7854 validation loss:0.0749
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07570570723518082 0.0743485383041527
need align? ->  True 0.0743485383041527
2023-08-08 15:18:24,769 - epoch:18, training loss:0.7699 validation loss:0.0757
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07525247618879961 0.07465041453099769
need align? ->  True 0.0743485383041527
2023-08-08 15:19:58,348 - epoch:19, training loss:0.7992 validation loss:0.0753
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07474220730364323 0.07423104021860205
need align? ->  True 0.07423104021860205
2023-08-08 15:21:32,032 - epoch:20, training loss:0.7846 validation loss:0.0747
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07473576328028804 0.07452087298683498
need align? ->  True 0.07423104021860205
2023-08-08 15:23:05,335 - epoch:21, training loss:0.8136 validation loss:0.0747
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07474097496141559 0.0739838374859613
need align? ->  True 0.0739838374859613
2023-08-08 15:24:39,211 - epoch:22, training loss:0.8004 validation loss:0.0747
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.0750559417773848 0.07481649874345116
need align? ->  True 0.0739838374859613
2023-08-08 15:26:12,571 - epoch:23, training loss:0.8248 validation loss:0.0751
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07473077535953211 0.07450087951577228
need align? ->  True 0.0739838374859613
2023-08-08 15:27:46,134 - epoch:24, training loss:0.8149 validation loss:0.0747
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07478407179207905 0.07432665627287782
need align? ->  True 0.0739838374859613
2023-08-08 15:29:22,563 - epoch:25, training loss:0.8118 validation loss:0.0748
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07472908764105776 0.07439936357347862
need align? ->  True 0.0739838374859613
2023-08-08 15:31:00,080 - epoch:26, training loss:0.8098 validation loss:0.0747
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07460247449900793 0.07426628259860951
need align? ->  True 0.0739838374859613
2023-08-08 15:32:36,317 - epoch:27, training loss:0.8098 validation loss:0.0746
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07460030265476393 0.07428057159742583
need align? ->  True 0.0739838374859613
2023-08-08 15:34:11,230 - epoch:28, training loss:0.8073 validation loss:0.0746
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07460285512649495 0.07431229730339153
need align? ->  True 0.0739838374859613
2023-08-08 15:35:44,697 - epoch:29, training loss:0.8095 validation loss:0.0746
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-14:48:53.819737/0/0.0746_epoch_28.pkl  &  0.0739838374859613
2023-08-08 15:35:49,002 - [*] loss:0.1585
2023-08-08 15:35:49,003 - [*] phase 0, testing
2023-08-08 15:35:49,013 - T:24  MAE     0.254831        RMSE    0.160705        MAPE    115.195489
2023-08-08 15:35:49,013 - 24    mae     0.2548
2023-08-08 15:35:49,013 - 24    rmse    0.1607
2023-08-08 15:35:49,013 - 24    mape    115.1955
2023-08-08 15:35:52,235 - [*] loss:0.1604
2023-08-08 15:35:52,237 - [*] phase 0, testing
2023-08-08 15:35:52,246 - T:24  MAE     0.257750        RMSE    0.162519        MAPE    113.954604
2023-08-08 15:35:57,035 - [*] loss:0.1632
2023-08-08 15:35:57,036 - [*] phase 0, testing
2023-08-08 15:35:57,046 - T:24  MAE     0.260862        RMSE    0.165359        MAPE    115.569234
2023-08-08 15:36:00,765 - [*] loss:0.1570
2023-08-08 15:36:00,766 - [*] phase 0, testing
2023-08-08 15:36:00,777 - T:24  MAE     0.252631        RMSE    0.159391        MAPE    113.267088
2023-08-08 15:36:00,777 - 24    mae     0.2526
2023-08-08 15:36:00,778 - 24    rmse    0.1594
2023-08-08 15:36:00,778 - 24    mape    113.2671
2023-08-08 15:36:03,288 - logger name:exp/ECL-PatchTST2023-08-08-15:36:03.288583/ECL-PatchTST.log
2023-08-08 15:36:03,289 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-08-10:33:05.890873/0/0.1629_epoch_11.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 36, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 1, 'rec_ori': 1, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-15:36:03.288583', 'path': 'exp/ECL-PatchTST2023-08-08-15:36:03.288583', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 15:36:03,289 - [*] phase 0 start training
0 26304
train 7969
val 2545
test 2545
2023-08-08 15:36:03,575 - [*] phase 0 Dataset load!
2023-08-08 15:36:04,701 - [*] phase 0 Training start
train 7969
2023-08-08 15:36:58,160 - epoch:0, training loss:0.2615 validation loss:0.1826
train 7969
vs, vt 0.18261710777878762 0.1857525074854493
Updating learning rate to 1.0464693625140384e-05
Updating learning rate to 1.0464693625140384e-05
train 7969
vs, vt 0.1776725735515356 0.17406122032552956
need align? ->  True 0.17406122032552956
2023-08-08 15:38:48,062 - epoch:1, training loss:2.0925 validation loss:0.1777
Updating learning rate to 2.811743018110611e-05
Updating learning rate to 2.811743018110611e-05
train 7969
vs, vt 0.17275347504764796 0.17002388034015894
need align? ->  True 0.17002388034015894
2023-08-08 15:40:26,587 - epoch:2, training loss:1.7515 validation loss:0.1728
Updating learning rate to 5.220322910624572e-05
Updating learning rate to 5.220322910624572e-05
train 7969
vs, vt 0.16585452482104301 0.16354930941015483
need align? ->  True 0.16354930941015483
2023-08-08 15:42:03,524 - epoch:3, training loss:1.3946 validation loss:0.1659
Updating learning rate to 7.623428578523578e-05
Updating learning rate to 7.623428578523578e-05
train 7969
vs, vt 0.16595770232379436 0.16236013788729906
need align? ->  True 0.16236013788729906
2023-08-08 15:43:41,192 - epoch:4, training loss:1.1838 validation loss:0.1660
Updating learning rate to 9.373754109649067e-05
Updating learning rate to 9.373754109649067e-05
train 7969
vs, vt 0.16658958634361626 0.16258031576871873
need align? ->  True 0.16236013788729906
2023-08-08 15:45:17,759 - epoch:5, training loss:1.0852 validation loss:0.1666
Updating learning rate to 9.999988856233768e-05
Updating learning rate to 9.999988856233768e-05
train 7969
vs, vt 0.1661813735961914 0.16221251832321287
need align? ->  True 0.16221251832321287
2023-08-08 15:46:57,253 - epoch:6, training loss:1.0544 validation loss:0.1662
Updating learning rate to 9.955835545866997e-05
Updating learning rate to 9.955835545866997e-05
train 7969
vs, vt 0.16424740441143512 0.1622962336987257
need align? ->  True 0.16221251832321287
2023-08-08 15:48:34,617 - epoch:7, training loss:1.0255 validation loss:0.1642
Updating learning rate to 9.826886857498791e-05
Updating learning rate to 9.826886857498791e-05
train 7969
vs, vt 0.1643451165407896 0.16051376182585955
need align? ->  True 0.16051376182585955
2023-08-08 15:50:11,928 - epoch:8, training loss:1.0002 validation loss:0.1643
Updating learning rate to 9.61534913893846e-05
Updating learning rate to 9.61534913893846e-05
train 7969
vs, vt 0.16330065354704856 0.16026425939053296
need align? ->  True 0.16026425939053296
2023-08-08 15:51:51,083 - epoch:9, training loss:1.0123 validation loss:0.1633
Updating learning rate to 9.324841859199907e-05
Updating learning rate to 9.324841859199907e-05
train 7969
vs, vt 0.16352012138813735 0.15951644722372293
need align? ->  True 0.15951644722372293
2023-08-08 15:53:29,107 - epoch:10, training loss:1.0122 validation loss:0.1635
Updating learning rate to 8.960335678383294e-05
Updating learning rate to 8.960335678383294e-05
train 7969
vs, vt 0.16447176579385997 0.16252865362912416
need align? ->  True 0.15951644722372293
2023-08-08 15:55:07,015 - epoch:11, training loss:1.0166 validation loss:0.1645
Updating learning rate to 8.528067398302599e-05
Updating learning rate to 8.528067398302599e-05
train 7969
vs, vt 0.16417335998266935 0.16135149132460355
need align? ->  True 0.15951644722372293
2023-08-08 15:56:44,738 - epoch:12, training loss:1.0007 validation loss:0.1642
Updating learning rate to 8.035433249077411e-05
Updating learning rate to 8.035433249077411e-05
train 7969
vs, vt 0.16442132890224456 0.16145472303032876
need align? ->  True 0.15951644722372293
2023-08-08 15:58:23,199 - epoch:13, training loss:0.9894 validation loss:0.1644
Updating learning rate to 7.490862337584965e-05
Updating learning rate to 7.490862337584965e-05
train 7969
vs, vt 0.1666232917457819 0.1636911952868104
need align? ->  True 0.15951644722372293
2023-08-08 15:59:59,840 - epoch:14, training loss:0.9808 validation loss:0.1666
Updating learning rate to 6.903672423104474e-05
Updating learning rate to 6.903672423104474e-05
train 7969
vs, vt 0.16696232352405788 0.16397981941699982
need align? ->  True 0.15951644722372293
2023-08-08 16:01:37,211 - epoch:15, training loss:0.9750 validation loss:0.1670
Updating learning rate to 6.283910487872503e-05
Updating learning rate to 6.283910487872503e-05
train 7969
vs, vt 0.16322108451277018 0.16332354359328746
need align? ->  True 0.15951644722372293
2023-08-08 16:03:15,721 - epoch:16, training loss:0.9680 validation loss:0.1632
Updating learning rate to 5.642180830431342e-05
Updating learning rate to 5.642180830431342e-05
train 7969
vs, vt 0.16514680963009595 0.16405914295464755
need align? ->  True 0.15951644722372293
2023-08-08 16:04:54,514 - epoch:17, training loss:0.9613 validation loss:0.1651
Updating learning rate to 4.9894636231408805e-05
Updating learning rate to 4.9894636231408805e-05
train 7969
vs, vt 0.1646609028801322 0.1659071747213602
need align? ->  True 0.15951644722372293
2023-08-08 16:06:34,195 - epoch:18, training loss:0.9576 validation loss:0.1647
Updating learning rate to 4.336927038385261e-05
Updating learning rate to 4.336927038385261e-05
train 7969
vs, vt 0.16265133880078791 0.16325953099876642
need align? ->  True 0.15951644722372293
2023-08-08 16:08:15,324 - epoch:19, training loss:0.9542 validation loss:0.1627
Updating learning rate to 3.6957361580469706e-05
Updating learning rate to 3.6957361580469706e-05
train 7969
vs, vt 0.16456504836678504 0.16434232145547867
need align? ->  True 0.15951644722372293
2023-08-08 16:09:58,941 - epoch:20, training loss:0.9499 validation loss:0.1646
Updating learning rate to 3.076861935860294e-05
Updating learning rate to 3.076861935860294e-05
train 7969
vs, vt 0.164434983022511 0.16507905274629592
need align? ->  True 0.15951644722372293
2023-08-08 16:11:45,873 - epoch:21, training loss:0.9481 validation loss:0.1644
Updating learning rate to 2.490893481351197e-05
Updating learning rate to 2.490893481351197e-05
train 7969
vs, vt 0.16485101375728844 0.1644548261538148
need align? ->  True 0.15951644722372293
2023-08-08 16:13:29,502 - epoch:22, training loss:0.9451 validation loss:0.1649
Updating learning rate to 1.947856877237479e-05
Updating learning rate to 1.947856877237479e-05
train 7969
vs, vt 0.1649458533152938 0.16468272637575865
need align? ->  True 0.15951644722372293
2023-08-08 16:15:12,274 - epoch:23, training loss:0.9432 validation loss:0.1649
Updating learning rate to 1.4570436303737125e-05
Updating learning rate to 1.4570436303737125e-05
train 7969
vs, vt 0.16429537404328584 0.1642177475616336
need align? ->  True 0.15951644722372293
2023-08-08 16:16:53,080 - epoch:24, training loss:0.9415 validation loss:0.1643
Updating learning rate to 1.0268516914928822e-05
Updating learning rate to 1.0268516914928822e-05
train 7969
vs, vt 0.16510365065187216 0.16459144800901412
need align? ->  True 0.15951644722372293
2023-08-08 16:18:33,434 - epoch:25, training loss:0.9413 validation loss:0.1651
Updating learning rate to 6.646417639409745e-06
Updating learning rate to 6.646417639409745e-06
train 7969
vs, vt 0.16475874092429876 0.16511298455297946
need align? ->  True 0.15951644722372293
2023-08-08 16:20:11,675 - epoch:26, training loss:0.9399 validation loss:0.1648
Updating learning rate to 3.766113600019663e-06
Updating learning rate to 3.766113600019663e-06
train 7969
vs, vt 0.16490332800894975 0.16463741958141326
need align? ->  True 0.15951644722372293
2023-08-08 16:21:48,332 - epoch:27, training loss:0.9400 validation loss:0.1649
Updating learning rate to 1.6768875974437087e-06
Updating learning rate to 1.6768875974437087e-06
train 7969
vs, vt 0.1647658135741949 0.16483048535883427
need align? ->  True 0.15951644722372293
2023-08-08 16:23:26,003 - epoch:28, training loss:0.9384 validation loss:0.1648
Updating learning rate to 4.144868678288239e-07
Updating learning rate to 4.144868678288239e-07
train 7969
vs, vt 0.16499831434339285 0.1649943720549345
need align? ->  True 0.15951644722372293
2023-08-08 16:25:04,729 - epoch:29, training loss:0.9392 validation loss:0.1650
Updating learning rate to 5.114376623277983e-10
Updating learning rate to 5.114376623277983e-10
check exp/ECL-PatchTST2023-08-08-15:36:03.288583/0/0.1627_epoch_19.pkl  &  0.15951644722372293
2023-08-08 16:25:09,580 - [*] loss:0.3660
2023-08-08 16:25:09,589 - [*] phase 0, testing
2023-08-08 16:25:09,719 - T:336 MAE     0.396363        RMSE    0.361973        MAPE    158.242869
2023-08-08 16:25:09,720 - 336   mae     0.3964
2023-08-08 16:25:09,720 - 336   rmse    0.3620
2023-08-08 16:25:09,720 - 336   mape    158.2429
2023-08-08 16:25:13,238 - [*] loss:0.3564
2023-08-08 16:25:13,247 - [*] phase 0, testing
2023-08-08 16:25:13,395 - T:336 MAE     0.389810        RMSE    0.352091        MAPE    157.484508
2023-08-08 16:25:19,276 - [*] loss:0.3562
2023-08-08 16:25:19,284 - [*] phase 0, testing
2023-08-08 16:25:19,423 - T:336 MAE     0.390832        RMSE    0.352002        MAPE    155.482769
2023-08-08 16:25:24,711 - [*] loss:0.3655
2023-08-08 16:25:24,721 - [*] phase 0, testing
2023-08-08 16:25:24,875 - T:336 MAE     0.390948        RMSE    0.361249        MAPE    153.552401
2023-08-08 16:25:24,876 - 336   mae     0.3909
2023-08-08 16:25:24,876 - 336   rmse    0.3612
2023-08-08 16:25:24,877 - 336   mape    153.5524
