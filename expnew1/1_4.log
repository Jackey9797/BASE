2023-07-25 01:38:51,000 - logger name:exp/ECL-PatchTST2023-07-25-01:38:51.000683/ECL-PatchTST.log
2023-07-25 01:38:51,001 - params : {'conf': 'ECL-PatchTST', 'data_name': 'electricity', 'iteration': 1, 'load': True, 'build_graph': False, 'grad_norm': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.1, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': 89, 'aligner': 1, 'refiner': 0, 'enhance': 0, 'seed': 2021, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'batch_size': 128, 'lr': 0.0001, 'loss': 'mse', '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-07-25-01:38:51.000683', 'path': 'exp/ECL-PatchTST2023-07-25-01:38:51.000683', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-07-25 01:38:51,001 - [*] phase 0 start training
0 26304
18412 2632 5260 0.7 0.2 26304
train 17981
18412 2632 5260 0.7 0.2 26304
val 2537
18412 2632 5260 0.7 0.2 26304
test 5165
2023-07-25 01:39:04,067 - [*] phase 0 Dataset load!
2023-07-25 01:39:05,042 - [*] phase 0 Training start
18412 2632 5260 0.7 0.2 26304
train 17981
2023-07-25 01:39:30,752 - epoch:0, training loss:0.4970 validation loss:0.1071
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.10705327782779932 0.11094655059278011
Updating learning rate to 1.0445766508069865e-05
Updating learning rate to 1.0445766508069865e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.07395038660615683 0.07364116460084916
2023-07-25 01:40:09,008 - epoch:1, training loss:0.7399 validation loss:0.0740
Updating learning rate to 2.805190328742299e-05
Updating learning rate to 2.805190328742299e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.06235323064029217 0.061446767393499614
2023-07-25 01:40:33,147 - epoch:2, training loss:0.3748 validation loss:0.0624
Updating learning rate to 5.208986672185724e-05
Updating learning rate to 5.208986672185724e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.05424599535763264 0.05492168152704835
2023-07-25 01:41:00,451 - epoch:3, training loss:0.2530 validation loss:0.0542
Updating learning rate to 7.610369432687743e-05
Updating learning rate to 7.610369432687743e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.05039315083995462 0.051341804955154655
2023-07-25 01:41:23,794 - epoch:4, training loss:0.2103 validation loss:0.0504
Updating learning rate to 9.364390586469177e-05
Updating learning rate to 9.364390586469177e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04815642973408103 0.04924201443791389
2023-07-25 01:41:50,267 - epoch:5, training loss:0.1884 validation loss:0.0482
Updating learning rate to 9.999997814456623e-05
Updating learning rate to 9.999997814456623e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04737634165212512 0.047781676054000854
2023-07-25 01:42:14,947 - epoch:6, training loss:0.1751 validation loss:0.0474
Updating learning rate to 9.956612105134122e-05
Updating learning rate to 9.956612105134122e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04621692812070251 0.04649131167680025
2023-07-25 01:42:41,863 - epoch:7, training loss:0.1655 validation loss:0.0462
Updating learning rate to 9.828417730665822e-05
Updating learning rate to 9.828417730665822e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04644560776650906 0.045513245509937406
2023-07-25 01:43:06,726 - epoch:8, training loss:0.1585 validation loss:0.0464
Updating learning rate to 9.617608132341072e-05
Updating learning rate to 9.617608132341072e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04547203667461872 0.04538690447807312
2023-07-25 01:43:33,917 - epoch:9, training loss:0.1521 validation loss:0.0455
Updating learning rate to 9.327790320834669e-05
Updating learning rate to 9.327790320834669e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04529083240777254 0.044516307301819324
2023-07-25 01:44:01,524 - epoch:10, training loss:0.1474 validation loss:0.0453
Updating learning rate to 8.963923159254168e-05
Updating learning rate to 8.963923159254168e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04523510788567364 0.04392931843176484
2023-07-25 01:44:29,976 - epoch:11, training loss:0.1427 validation loss:0.0452
Updating learning rate to 8.532232515617246e-05
Updating learning rate to 8.532232515617246e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.044261632673442367 0.04345861366018653
2023-07-25 01:44:53,010 - epoch:12, training loss:0.1387 validation loss:0.0443
Updating learning rate to 8.04010473652379e-05
Updating learning rate to 8.04010473652379e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04457381288520992 0.04304348635487258
2023-07-25 01:45:22,005 - epoch:13, training loss:0.1356 validation loss:0.0446
Updating learning rate to 7.495960264717686e-05
Updating learning rate to 7.495960264717686e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04440687075257301 0.0432638272177428
2023-07-25 01:45:42,303 - epoch:14, training loss:0.1328 validation loss:0.0444
Updating learning rate to 6.909109562976884e-05
Updating learning rate to 6.909109562976884e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.044238801160827276 0.04247706732712686
2023-07-25 01:46:10,858 - epoch:15, training loss:0.1302 validation loss:0.0442
Updating learning rate to 6.289593809513925e-05
Updating learning rate to 6.289593809513925e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04296516864560544 0.04207446505315602
2023-07-25 01:46:37,059 - epoch:16, training loss:0.1277 validation loss:0.0430
Updating learning rate to 5.6480130906327764e-05
Updating learning rate to 5.6480130906327764e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04299417142756283 0.04197646733373404
2023-07-25 01:47:01,064 - epoch:17, training loss:0.1253 validation loss:0.0430
Updating learning rate to 4.995345030313274e-05
Updating learning rate to 4.995345030313274e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.042757821502164006 0.042004255158826706
2023-07-25 01:47:30,490 - epoch:18, training loss:0.1232 validation loss:0.0428
Updating learning rate to 4.3427569600212594e-05
Updating learning rate to 4.3427569600212594e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04338472727686167 0.04138144021853805
2023-07-25 01:47:52,610 - epoch:19, training loss:0.1218 validation loss:0.0434
Updating learning rate to 3.70141484257102e-05
Updating learning rate to 3.70141484257102e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04215590418316424 0.04129859902895987
2023-07-25 01:48:19,002 - epoch:20, training loss:0.1205 validation loss:0.0422
Updating learning rate to 3.0822922194057626e-05
Updating learning rate to 3.0822922194057626e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.042033830238506195 0.040858917078003286
2023-07-25 01:48:46,635 - epoch:21, training loss:0.1188 validation loss:0.0420
Updating learning rate to 2.4959824502610635e-05
Updating learning rate to 2.4959824502610635e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04176832642406225 0.04078452922403812
2023-07-25 01:49:09,943 - epoch:22, training loss:0.1179 validation loss:0.0418
Updating learning rate to 1.9525174578427625e-05
Updating learning rate to 1.9525174578427625e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04176421258598566 0.04054768993519246
2023-07-25 01:49:39,129 - epoch:23, training loss:0.1170 validation loss:0.0418
Updating learning rate to 1.4611960788481038e-05
Updating learning rate to 1.4611960788481038e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04185633989982307 0.04049969301559031
2023-07-25 01:50:02,073 - epoch:24, training loss:0.1159 validation loss:0.0419
Updating learning rate to 1.0304249582917059e-05
Updating learning rate to 1.0304249582917059e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04198348373174667 0.04056566366925836
2023-07-25 01:50:28,832 - epoch:25, training loss:0.1156 validation loss:0.0420
Updating learning rate to 6.675747094786052e-06
Updating learning rate to 6.675747094786052e-06
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04163025976158678 0.040331332478672266
2023-07-25 01:50:55,773 - epoch:26, training loss:0.1152 validation loss:0.0416
Updating learning rate to 3.7885380076709116e-06
Updating learning rate to 3.7885380076709116e-06
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04164962819777429 0.04031051895581186
2023-07-25 01:51:19,256 - epoch:27, training loss:0.1149 validation loss:0.0416
Updating learning rate to 1.6920232695377565e-06
Updating learning rate to 1.6920232695377565e-06
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04150856267660856 0.0402720816899091
2023-07-25 01:51:48,927 - epoch:28, training loss:0.1148 validation loss:0.0415
Updating learning rate to 4.220748288197363e-07
Updating learning rate to 4.220748288197363e-07
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04156156368553639 0.04027781249023974
2023-07-25 01:52:09,196 - epoch:29, training loss:0.1145 validation loss:0.0416
Updating learning rate to 4.218554337707414e-10
Updating learning rate to 4.218554337707414e-10
2023-07-25 01:52:12,724 - [*] loss:0.0820
2023-07-25 01:52:12,726 - [*] phase 0, testing
2023-07-25 01:52:12,735 - T:96	MAE	0.182734	RMSE	0.082515	MAPE	45.977595
2023-07-25 01:52:12,735 - 96	mae	0.1827	
2023-07-25 01:52:12,735 - 96	rmse	0.0825	
2023-07-25 01:52:12,736 - 96	mape	45.9776	
2023-07-25 01:52:16,040 - [*] loss:0.0813
2023-07-25 01:52:16,041 - [*] phase 0, testing
2023-07-25 01:52:16,049 - T:96	MAE	0.178680	RMSE	0.081825	MAPE	44.933340
2023-07-25 01:52:16,050 - 96	mae	0.1787	
2023-07-25 01:52:16,050 - 96	rmse	0.0818	
2023-07-25 01:52:16,050 - 96	mape	44.9333	
