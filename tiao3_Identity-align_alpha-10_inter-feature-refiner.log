2023-08-08 00:02:47,674 - logger name:exp/ECL-PatchTST2023-08-08-00:02:47.674695/ECL-PatchTST.log
2023-08-08 00:02:47,675 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 64, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-00:02:47.674695', 'path': 'exp/ECL-PatchTST2023-08-08-00:02:47.674695', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 00:02:47,675 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 00:02:47,866 - [*] phase 0 Dataset load!
2023-08-08 00:02:48,818 - [*] phase 0 Training start
train 8209
2023-08-08 00:03:40,287 - epoch:0, training loss:0.2157 validation loss:0.1663
train 8209
vs, vt 0.1663396390662952 0.1671547761017626
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.14052498730068858 0.1379951284351674
need align? ->  True 0.1379951284351674
2023-08-08 00:05:30,014 - epoch:1, training loss:1.8718 validation loss:0.1405
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.13117890965870835 0.12756296670572323
need align? ->  True 0.12756296670572323
2023-08-08 00:07:05,610 - epoch:2, training loss:1.5411 validation loss:0.1312
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12537784391844814 0.1252376733517105
need align? ->  True 0.1252376733517105
2023-08-08 00:08:42,370 - epoch:3, training loss:1.2487 validation loss:0.1254
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12560311120680787 0.12414998772808096
need align? ->  True 0.12414998772808096
2023-08-08 00:10:19,098 - epoch:4, training loss:1.0203 validation loss:0.1256
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12487685223194686 0.12481206045909361
need align? ->  True 0.12414998772808096
2023-08-08 00:11:54,970 - epoch:5, training loss:0.8865 validation loss:0.1249
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12717225622724404 0.12337761787189679
need align? ->  True 0.12337761787189679
2023-08-08 00:13:30,319 - epoch:6, training loss:0.8641 validation loss:0.1272
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12487106262282892 0.1219506562602791
need align? ->  True 0.1219506562602791
2023-08-08 00:15:06,959 - epoch:7, training loss:0.7906 validation loss:0.1249
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12418928056616675 0.12303207442164421
need align? ->  True 0.1219506562602791
2023-08-08 00:16:43,876 - epoch:8, training loss:0.7700 validation loss:0.1242
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12504277429120106 0.12265000949529084
need align? ->  True 0.1219506562602791
2023-08-08 00:18:19,286 - epoch:9, training loss:0.7605 validation loss:0.1250
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12531536250290545 0.12257050197910178
need align? ->  True 0.1219506562602791
2023-08-08 00:19:56,093 - epoch:10, training loss:0.7577 validation loss:0.1253
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.1233779527246952 0.1221190205859867
need align? ->  True 0.1219506562602791
2023-08-08 00:21:32,827 - epoch:11, training loss:0.7578 validation loss:0.1234
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.1239981001073664 0.12154482296583327
need align? ->  True 0.12154482296583327
2023-08-08 00:23:08,096 - epoch:12, training loss:0.7585 validation loss:0.1240
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12383938021957874 0.12220336361364885
need align? ->  True 0.12154482296583327
2023-08-08 00:24:49,873 - epoch:13, training loss:0.7517 validation loss:0.1238
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12461571615527976 0.12251142082227902
need align? ->  True 0.12154482296583327
2023-08-08 00:26:30,566 - epoch:14, training loss:0.7123 validation loss:0.1246
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12366010417992418 0.12330793157558549
need align? ->  True 0.12154482296583327
2023-08-08 00:28:13,811 - epoch:15, training loss:0.7040 validation loss:0.1237
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12385499324988235 0.12273437927731058
need align? ->  True 0.12154482296583327
2023-08-08 00:29:55,743 - epoch:16, training loss:0.7021 validation loss:0.1239
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12356669540432366 0.12294954408637503
need align? ->  True 0.12154482296583327
2023-08-08 00:31:34,100 - epoch:17, training loss:0.7017 validation loss:0.1236
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12422435341233556 0.12239806599576365
need align? ->  True 0.12154482296583327
2023-08-08 00:33:09,790 - epoch:18, training loss:0.6970 validation loss:0.1242
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12360400342467157 0.12263111414557154
need align? ->  True 0.12154482296583327
2023-08-08 00:34:45,082 - epoch:19, training loss:0.6984 validation loss:0.1236
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12385091194036332 0.12250482638112524
need align? ->  True 0.12154482296583327
2023-08-08 00:36:20,855 - epoch:20, training loss:0.6947 validation loss:0.1239
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12383335049856793 0.12260859188708392
need align? ->  True 0.12154482296583327
2023-08-08 00:37:56,135 - epoch:21, training loss:0.6960 validation loss:0.1238
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12326800467615778 0.12235637915066698
need align? ->  True 0.12154482296583327
2023-08-08 00:39:32,213 - epoch:22, training loss:0.6935 validation loss:0.1233
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12382043559442867 0.12241165864874017
need align? ->  True 0.12154482296583327
2023-08-08 00:41:09,465 - epoch:23, training loss:0.6938 validation loss:0.1238
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12388229268518361 0.12228645291179419
need align? ->  True 0.12154482296583327
2023-08-08 00:42:45,048 - epoch:24, training loss:0.6921 validation loss:0.1239
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12401768683709881 0.12237688594243744
need align? ->  True 0.12154482296583327
2023-08-08 00:44:20,365 - epoch:25, training loss:0.6945 validation loss:0.1240
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12370841120454398 0.1224280186844143
need align? ->  True 0.12154482296583327
2023-08-08 00:45:55,706 - epoch:26, training loss:0.6942 validation loss:0.1237
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.1234792120416056 0.12226135190576315
need align? ->  True 0.12154482296583327
2023-08-08 00:47:33,019 - epoch:27, training loss:0.6946 validation loss:0.1235
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12358220319517634 0.12224947051568465
need align? ->  True 0.12154482296583327
2023-08-08 00:49:12,519 - epoch:28, training loss:0.6942 validation loss:0.1236
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12357254351743242 0.12228346522897482
need align? ->  True 0.12154482296583327
2023-08-08 00:50:53,795 - epoch:29, training loss:0.6922 validation loss:0.1236
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-00:02:47.674695/0/0.1233_epoch_22.pkl  &  0.12154482296583327
2023-08-08 00:51:00,432 - [*] loss:0.2745
2023-08-08 00:51:00,437 - [*] phase 0, testing
2023-08-08 00:51:00,492 - T:96	MAE	0.332919	RMSE	0.274754	MAPE	129.841518
2023-08-08 00:51:00,494 - 96	mae	0.3329	
2023-08-08 00:51:00,494 - 96	rmse	0.2748	
2023-08-08 00:51:00,494 - 96	mape	129.8415	
2023-08-08 00:51:04,996 - [*] loss:0.2724
2023-08-08 00:51:04,999 - [*] phase 0, testing
2023-08-08 00:51:05,105 - T:96	MAE	0.329282	RMSE	0.272735	MAPE	131.421041
2023-08-08 00:51:05,107 - 96	mae	0.3293	
2023-08-08 00:51:05,107 - 96	rmse	0.2727	
2023-08-08 00:51:05,107 - 96	mape	131.4210	
2023-08-08 00:51:07,506 - logger name:exp/ECL-PatchTST2023-08-08-00:51:07.506537/ECL-PatchTST.log
2023-08-08 00:51:07,507 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 64, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-00:51:07.506537', 'path': 'exp/ECL-PatchTST2023-08-08-00:51:07.506537', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 00:51:07,507 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 00:51:07,732 - [*] phase 0 Dataset load!
2023-08-08 00:51:08,881 - [*] phase 0 Training start
train 8209
2023-08-08 00:51:59,912 - epoch:0, training loss:0.2181 validation loss:0.1683
train 8209
vs, vt 0.16827843710780144 0.1700424634936181
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.140079256485809 0.13881780605085872
need align? ->  True 0.13881780605085872
2023-08-08 00:53:51,434 - epoch:1, training loss:1.8934 validation loss:0.1401
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.13042508937757125 0.12777911854738538
need align? ->  True 0.12777911854738538
2023-08-08 00:55:33,197 - epoch:2, training loss:1.5685 validation loss:0.1304
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12666368205100298 0.1259527372365648
need align? ->  True 0.1259527372365648
2023-08-08 00:57:18,869 - epoch:3, training loss:1.2861 validation loss:0.1267
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12477207141505046 0.12263894953172315
need align? ->  True 0.12263894953172315
2023-08-08 00:59:00,342 - epoch:4, training loss:1.0658 validation loss:0.1248
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12498768588358705 0.12394695813682946
need align? ->  True 0.12263894953172315
2023-08-08 01:00:40,053 - epoch:5, training loss:0.9349 validation loss:0.1250
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.1243607693944465 0.12290228810161352
need align? ->  True 0.12263894953172315
2023-08-08 01:02:21,686 - epoch:6, training loss:0.9067 validation loss:0.1244
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12951657963408666 0.12320635569366542
need align? ->  True 0.12263894953172315
2023-08-08 01:04:03,595 - epoch:7, training loss:0.8931 validation loss:0.1295
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12863059214908967 0.12394006321714683
need align? ->  True 0.12263894953172315
2023-08-08 01:05:39,364 - epoch:8, training loss:0.8798 validation loss:0.1286
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.1266823405061256 0.12374068542637608
need align? ->  True 0.12263894953172315
2023-08-08 01:07:15,649 - epoch:9, training loss:0.8694 validation loss:0.1267
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12548580215397206 0.12310567117211493
need align? ->  True 0.12263894953172315
2023-08-08 01:08:51,955 - epoch:10, training loss:0.8629 validation loss:0.1255
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12703834016892043 0.12319286760281432
need align? ->  True 0.12263894953172315
2023-08-08 01:10:34,696 - epoch:11, training loss:0.8580 validation loss:0.1270
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.1273791790008545 0.12341223767196591
need align? ->  True 0.12263894953172315
2023-08-08 01:12:16,764 - epoch:12, training loss:0.8521 validation loss:0.1274
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12606097546152092 0.1228551281277429
need align? ->  True 0.12263894953172315
2023-08-08 01:14:00,114 - epoch:13, training loss:0.8479 validation loss:0.1261
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.1276947862722657 0.12302604080601172
need align? ->  True 0.12263894953172315
2023-08-08 01:15:41,953 - epoch:14, training loss:0.8447 validation loss:0.1277
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12645168128338727 0.12365676877512173
need align? ->  True 0.12263894953172315
2023-08-08 01:17:20,414 - epoch:15, training loss:0.8422 validation loss:0.1265
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12630985474044626 0.12301672537895766
need align? ->  True 0.12263894953172315
2023-08-08 01:18:55,989 - epoch:16, training loss:0.8387 validation loss:0.1263
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12541400810534303 0.12310349052263932
need align? ->  True 0.12263894953172315
2023-08-08 01:20:31,496 - epoch:17, training loss:0.8373 validation loss:0.1254
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12766149343753402 0.12229731611230156
need align? ->  True 0.12229731611230156
2023-08-08 01:22:07,736 - epoch:18, training loss:0.8348 validation loss:0.1277
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12734413155439225 0.12316894692114809
need align? ->  True 0.12229731611230156
2023-08-08 01:23:43,338 - epoch:19, training loss:0.7889 validation loss:0.1273
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12631722895259206 0.12260156412693587
need align? ->  True 0.12229731611230156
2023-08-08 01:25:18,494 - epoch:20, training loss:0.7249 validation loss:0.1263
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12583692837506533 0.12235514853488315
need align? ->  True 0.12229731611230156
2023-08-08 01:26:53,815 - epoch:21, training loss:0.7144 validation loss:0.1258
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12642574445767837 0.12227171134542335
need align? ->  True 0.12227171134542335
2023-08-08 01:28:29,291 - epoch:22, training loss:0.7137 validation loss:0.1264
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12685446364974434 0.1226893700659275
need align? ->  True 0.12227171134542335
2023-08-08 01:30:05,727 - epoch:23, training loss:0.7270 validation loss:0.1269
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12621500021354717 0.12248892286284403
need align? ->  True 0.12227171134542335
2023-08-08 01:31:42,127 - epoch:24, training loss:0.7168 validation loss:0.1262
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12649177700619807 0.12281914859671485
need align? ->  True 0.12227171134542335
2023-08-08 01:33:19,797 - epoch:25, training loss:0.7156 validation loss:0.1265
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12647757594558326 0.12272964587265794
need align? ->  True 0.12227171134542335
2023-08-08 01:35:00,717 - epoch:26, training loss:0.7157 validation loss:0.1265
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12651049997657537 0.12277258314530957
need align? ->  True 0.12227171134542335
2023-08-08 01:36:44,310 - epoch:27, training loss:0.7125 validation loss:0.1265
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.1265655676410957 0.12275606436146931
need align? ->  True 0.12227171134542335
2023-08-08 01:38:26,044 - epoch:28, training loss:0.7132 validation loss:0.1266
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12637347931211645 0.1226725396276875
need align? ->  True 0.12227171134542335
2023-08-08 01:40:04,668 - epoch:29, training loss:0.7140 validation loss:0.1264
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-00:51:07.506537/0/0.1244_epoch_6.pkl  &  0.12227171134542335
2023-08-08 01:40:10,029 - [*] loss:0.2762
2023-08-08 01:40:10,033 - [*] phase 0, testing
2023-08-08 01:40:10,072 - T:96	MAE	0.334903	RMSE	0.276106	MAPE	136.217403
2023-08-08 01:40:10,074 - 96	mae	0.3349	
2023-08-08 01:40:10,075 - 96	rmse	0.2761	
2023-08-08 01:40:10,075 - 96	mape	136.2174	
2023-08-08 01:40:14,270 - [*] loss:0.2731
2023-08-08 01:40:14,273 - [*] phase 0, testing
2023-08-08 01:40:14,312 - T:96	MAE	0.329896	RMSE	0.273514	MAPE	132.303560
2023-08-08 01:40:14,314 - 96	mae	0.3299	
2023-08-08 01:40:14,314 - 96	rmse	0.2735	
2023-08-08 01:40:14,314 - 96	mape	132.3036	
2023-08-08 01:40:16,380 - logger name:exp/ECL-PatchTST2023-08-08-01:40:16.379837/ECL-PatchTST.log
2023-08-08 01:40:16,380 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'rec_intra_feature': 1, 'rec_ori': 0, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-01:40:16.379837', 'path': 'exp/ECL-PatchTST2023-08-08-01:40:16.379837', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 01:40:16,380 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 01:40:16,580 - [*] phase 0 Dataset load!
2023-08-08 01:40:17,613 - [*] phase 0 Training start
train 8209
2023-08-08 01:41:15,158 - epoch:0, training loss:0.2127 validation loss:0.1650
train 8209
vs, vt 0.16504068012264642 0.16559637168591673
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13883746347644113 0.13648678243837573
need align? ->  True 0.13648678243837573
2023-08-08 01:43:15,995 - epoch:1, training loss:1.6424 validation loss:0.1388
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12893810444934803 0.1288174150342291
need align? ->  True 0.1288174150342291
2023-08-08 01:44:57,822 - epoch:2, training loss:1.4033 validation loss:0.1289
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12753855826502497 0.12413531491024928
need align? ->  True 0.12413531491024928
2023-08-08 01:46:39,516 - epoch:3, training loss:1.1760 validation loss:0.1275
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12764855160970578 0.12281375463035973
need align? ->  True 0.12281375463035973
2023-08-08 01:48:21,684 - epoch:4, training loss:0.9810 validation loss:0.1276
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12580666767263954 0.12243922545828602
need align? ->  True 0.12243922545828602
2023-08-08 01:50:04,129 - epoch:5, training loss:0.8537 validation loss:0.1258
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12583293206989765 0.12367897912521254
need align? ->  True 0.12243922545828602
2023-08-08 01:51:47,698 - epoch:6, training loss:0.7827 validation loss:0.1258
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12597802162847735 0.1233217137103731
need align? ->  True 0.12243922545828602
2023-08-08 01:53:36,367 - epoch:7, training loss:0.7683 validation loss:0.1260
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12621580665423113 0.12160644350065426
need align? ->  True 0.12160644350065426
2023-08-08 01:55:25,321 - epoch:8, training loss:0.7620 validation loss:0.1262
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12828077375888824 0.1222798755731095
need align? ->  True 0.12160644350065426
2023-08-08 01:57:09,027 - epoch:9, training loss:0.6972 validation loss:0.1283
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.1256891597401012 0.12179448184641925
need align? ->  True 0.12160644350065426
2023-08-08 01:58:51,553 - epoch:10, training loss:0.6590 validation loss:0.1257
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12539928872138262 0.12266159023750912
need align? ->  True 0.12160644350065426
2023-08-08 02:00:34,108 - epoch:11, training loss:0.6498 validation loss:0.1254
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.1276834254915064 0.1224394604055719
need align? ->  True 0.12160644350065426
2023-08-08 02:02:17,076 - epoch:12, training loss:0.6536 validation loss:0.1277
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12699930429119954 0.12263858513059941
need align? ->  True 0.12160644350065426
2023-08-08 02:03:59,698 - epoch:13, training loss:0.6539 validation loss:0.1270
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.1283062748949636 0.12220312409441579
need align? ->  True 0.12160644350065426
2023-08-08 02:05:41,991 - epoch:14, training loss:0.6461 validation loss:0.1283
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12580186331813986 0.12261056586761367
need align? ->  True 0.12160644350065426
2023-08-08 02:07:28,706 - epoch:15, training loss:0.6451 validation loss:0.1258
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12584398907016625 0.12325603785839948
need align? ->  True 0.12160644350065426
2023-08-08 02:09:19,528 - epoch:16, training loss:0.6449 validation loss:0.1258
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12687614661726085 0.12283395137637854
need align? ->  True 0.12160644350065426
2023-08-08 02:11:05,509 - epoch:17, training loss:0.6444 validation loss:0.1269
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12550092928788878 0.12279075511138547
need align? ->  True 0.12160644350065426
2023-08-08 02:12:47,539 - epoch:18, training loss:0.6379 validation loss:0.1255
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12761543420228091 0.1230365519665859
need align? ->  True 0.12160644350065426
2023-08-08 02:14:30,447 - epoch:19, training loss:0.6401 validation loss:0.1276
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12723016942089255 0.12284141668880527
need align? ->  True 0.12160644350065426
2023-08-08 02:16:12,708 - epoch:20, training loss:0.6437 validation loss:0.1272
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12558121860704638 0.12288585703142664
need align? ->  True 0.12160644350065426
2023-08-08 02:17:53,572 - epoch:21, training loss:0.6458 validation loss:0.1256
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12672231896695765 0.12253662528978153
need align? ->  True 0.12160644350065426
2023-08-08 02:19:34,419 - epoch:22, training loss:0.6441 validation loss:0.1267
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.1266024402258071 0.12299595671621236
need align? ->  True 0.12160644350065426
2023-08-08 02:21:16,814 - epoch:23, training loss:0.6415 validation loss:0.1266
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12612382898276503 0.12301906011998653
need align? ->  True 0.12160644350065426
2023-08-08 02:23:04,341 - epoch:24, training loss:0.6405 validation loss:0.1261
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12629876539788462 0.1230233610523018
need align? ->  True 0.12160644350065426
2023-08-08 02:24:54,591 - epoch:25, training loss:0.6382 validation loss:0.1263
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12621882684867491 0.12309555768628012
need align? ->  True 0.12160644350065426
2023-08-08 02:26:39,502 - epoch:26, training loss:0.6415 validation loss:0.1262
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.1260883588851853 0.1230177025903355
need align? ->  True 0.12160644350065426
2023-08-08 02:28:21,433 - epoch:27, training loss:0.6410 validation loss:0.1261
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12626515845344824 0.1231146917085756
need align? ->  True 0.12160644350065426
2023-08-08 02:30:03,406 - epoch:28, training loss:0.6396 validation loss:0.1263
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12625043449754064 0.12311840218237856
need align? ->  True 0.12160644350065426
2023-08-08 02:31:45,768 - epoch:29, training loss:0.6407 validation loss:0.1263
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-01:40:16.379837/0/0.1254_epoch_11.pkl  &  0.12160644350065426
2023-08-08 02:31:51,524 - [*] loss:0.2788
2023-08-08 02:31:51,528 - [*] phase 0, testing
2023-08-08 02:31:51,567 - T:96	MAE	0.337259	RMSE	0.278685	MAPE	133.206081
2023-08-08 02:31:51,569 - 96	mae	0.3373	
2023-08-08 02:31:51,569 - 96	rmse	0.2787	
2023-08-08 02:31:51,569 - 96	mape	133.2061	
2023-08-08 02:31:55,853 - [*] loss:0.2716
2023-08-08 02:31:55,857 - [*] phase 0, testing
2023-08-08 02:31:55,898 - T:96	MAE	0.330165	RMSE	0.271805	MAPE	132.592690
2023-08-08 02:31:55,900 - 96	mae	0.3302	
2023-08-08 02:31:55,900 - 96	rmse	0.2718	
2023-08-08 02:31:55,900 - 96	mape	132.5927	
2023-08-08 02:31:58,340 - logger name:exp/ECL-PatchTST2023-08-08-02:31:58.340368/ECL-PatchTST.log
2023-08-08 02:31:58,341 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'rec_intra_feature': 1, 'rec_ori': 0, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-02:31:58.340368', 'path': 'exp/ECL-PatchTST2023-08-08-02:31:58.340368', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 02:31:58,341 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 02:31:58,556 - [*] phase 0 Dataset load!
2023-08-08 02:31:59,627 - [*] phase 0 Training start
train 8209
2023-08-08 02:32:56,015 - epoch:0, training loss:0.2133 validation loss:0.1653
train 8209
vs, vt 0.16531540216370064 0.16537542929026214
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13797265816141258 0.13578109832649882
need align? ->  True 0.13578109832649882
2023-08-08 02:34:51,022 - epoch:1, training loss:1.6370 validation loss:0.1380
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12937722270461646 0.1283614748919552
need align? ->  True 0.1283614748919552
2023-08-08 02:36:32,531 - epoch:2, training loss:1.3948 validation loss:0.1294
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12710732196203686 0.1260731416669759
need align? ->  True 0.1260731416669759
2023-08-08 02:38:14,407 - epoch:3, training loss:1.1639 validation loss:0.1271
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12582965999503026 0.12259293974123218
need align? ->  True 0.12259293974123218
2023-08-08 02:39:57,701 - epoch:4, training loss:0.9474 validation loss:0.1258
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12460364587605 0.12228086556900632
need align? ->  True 0.12228086556900632
2023-08-08 02:41:45,343 - epoch:5, training loss:0.8207 validation loss:0.1246
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.1253938339650631 0.12317657682367346
need align? ->  True 0.12228086556900632
2023-08-08 02:43:33,976 - epoch:6, training loss:0.7590 validation loss:0.1254
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.1278226494111798 0.12377084690061482
need align? ->  True 0.12228086556900632
2023-08-08 02:45:21,577 - epoch:7, training loss:0.7431 validation loss:0.1278
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.13038176823068748 0.12472232473506169
need align? ->  True 0.12228086556900632
2023-08-08 02:47:03,159 - epoch:8, training loss:0.7424 validation loss:0.1304
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12808744380758566 0.12364212779158895
need align? ->  True 0.12228086556900632
2023-08-08 02:48:45,479 - epoch:9, training loss:0.7357 validation loss:0.1281
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12585434690117836 0.12221046918156472
need align? ->  True 0.12221046918156472
2023-08-08 02:50:27,345 - epoch:10, training loss:0.7343 validation loss:0.1259
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12420089593665166 0.12194224345413121
need align? ->  True 0.12194224345413121
2023-08-08 02:52:08,418 - epoch:11, training loss:0.6810 validation loss:0.1242
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12404117834838954 0.12245543986897577
need align? ->  True 0.12194224345413121
2023-08-08 02:53:53,001 - epoch:12, training loss:0.6009 validation loss:0.1240
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.125350534661927 0.12224054666744037
need align? ->  True 0.12194224345413121
2023-08-08 02:55:38,510 - epoch:13, training loss:0.5905 validation loss:0.1254
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12419045564125884 0.1225157906724648
need align? ->  True 0.12194224345413121
2023-08-08 02:57:27,398 - epoch:14, training loss:0.5919 validation loss:0.1242
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12560323448005048 0.12265158690173518
need align? ->  True 0.12194224345413121
2023-08-08 02:59:11,904 - epoch:15, training loss:0.5936 validation loss:0.1256
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12451472692191601 0.12249982119961218
need align? ->  True 0.12194224345413121
2023-08-08 03:00:52,649 - epoch:16, training loss:0.5931 validation loss:0.1245
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.1240359228104353 0.12310646956955845
need align? ->  True 0.12194224345413121
2023-08-08 03:02:34,945 - epoch:17, training loss:0.5862 validation loss:0.1240
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.1263191555203362 0.12269059653309258
need align? ->  True 0.12194224345413121
2023-08-08 03:04:16,603 - epoch:18, training loss:0.5839 validation loss:0.1263
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12444361468607729 0.12266029705378143
need align? ->  True 0.12194224345413121
2023-08-08 03:06:01,219 - epoch:19, training loss:0.5904 validation loss:0.1244
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12562638961456038 0.12250941314480522
need align? ->  True 0.12194224345413121
2023-08-08 03:07:43,339 - epoch:20, training loss:0.5830 validation loss:0.1256
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.125013391317969 0.12279452815313231
need align? ->  True 0.12194224345413121
2023-08-08 03:09:25,460 - epoch:21, training loss:0.5807 validation loss:0.1250
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12521534878760576 0.12287129639563235
need align? ->  True 0.12194224345413121
2023-08-08 03:11:11,796 - epoch:22, training loss:0.5801 validation loss:0.1252
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.1252841791138053 0.12262270895933564
need align? ->  True 0.12194224345413121
2023-08-08 03:13:05,769 - epoch:23, training loss:0.5786 validation loss:0.1253
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12566462336954745 0.12303811591118574
need align? ->  True 0.12194224345413121
2023-08-08 03:14:54,412 - epoch:24, training loss:0.5793 validation loss:0.1257
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12525262709029697 0.12277461783113805
need align? ->  True 0.12194224345413121
2023-08-08 03:16:37,726 - epoch:25, training loss:0.5775 validation loss:0.1253
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12558135102418336 0.12281336017291654
need align? ->  True 0.12194224345413121
2023-08-08 03:18:20,869 - epoch:26, training loss:0.5770 validation loss:0.1256
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12559095287526195 0.12279038071971048
need align? ->  True 0.12194224345413121
2023-08-08 03:20:02,804 - epoch:27, training loss:0.5798 validation loss:0.1256
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.1253160322085023 0.1226977121762254
need align? ->  True 0.12194224345413121
2023-08-08 03:21:45,085 - epoch:28, training loss:0.5781 validation loss:0.1253
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12587109665301713 0.12288289567963644
need align? ->  True 0.12194224345413121
2023-08-08 03:23:26,887 - epoch:29, training loss:0.5790 validation loss:0.1259
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-02:31:58.340368/0/0.124_epoch_17.pkl  &  0.12194224345413121
2023-08-08 03:23:32,276 - [*] loss:0.2763
2023-08-08 03:23:32,280 - [*] phase 0, testing
2023-08-08 03:23:32,320 - T:96	MAE	0.334294	RMSE	0.276779	MAPE	130.420446
2023-08-08 03:23:32,322 - 96	mae	0.3343	
2023-08-08 03:23:32,322 - 96	rmse	0.2768	
2023-08-08 03:23:32,322 - 96	mape	130.4204	
2023-08-08 03:23:36,255 - [*] loss:0.2728
2023-08-08 03:23:36,259 - [*] phase 0, testing
2023-08-08 03:23:36,296 - T:96	MAE	0.330714	RMSE	0.272865	MAPE	133.952928
2023-08-08 03:23:36,297 - 96	mae	0.3307	
2023-08-08 03:23:36,297 - 96	rmse	0.2729	
2023-08-08 03:23:36,297 - 96	mape	133.9529	
2023-08-08 03:23:38,383 - logger name:exp/ECL-PatchTST2023-08-08-03:23:38.383159/ECL-PatchTST.log
2023-08-08 03:23:38,383 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 1, 'rec_ori': 0, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-03:23:38.383159', 'path': 'exp/ECL-PatchTST2023-08-08-03:23:38.383159', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 03:23:38,383 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 03:23:38,586 - [*] phase 0 Dataset load!
2023-08-08 03:23:39,891 - [*] phase 0 Training start
train 8209
2023-08-08 03:24:35,663 - epoch:0, training loss:0.2127 validation loss:0.1650
train 8209
vs, vt 0.16504068012264642 0.16559637168591673
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13877112638543954 0.13639380291781641
need align? ->  True 0.13639380291781641
2023-08-08 03:26:30,175 - epoch:1, training loss:1.7069 validation loss:0.1388
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.1303275114941326 0.12693382799625397
need align? ->  True 0.12693382799625397
2023-08-08 03:28:13,187 - epoch:2, training loss:1.4765 validation loss:0.1303
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.1271399844607169 0.12337055954743516
need align? ->  True 0.12337055954743516
2023-08-08 03:29:58,267 - epoch:3, training loss:1.2256 validation loss:0.1271
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.1275056443092498 0.12187155767936599
need align? ->  True 0.12187155767936599
2023-08-08 03:31:48,356 - epoch:4, training loss:1.0204 validation loss:0.1275
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12943221086805518 0.1215967924588106
need align? ->  True 0.1215967924588106
2023-08-08 03:33:12,728 - epoch:5, training loss:0.9124 validation loss:0.1294
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.1284157425503839 0.12247982350262729
need align? ->  True 0.1215967924588106
2023-08-08 03:34:36,923 - epoch:6, training loss:0.8586 validation loss:0.1284
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12613519179550084 0.12187255766581405
need align? ->  True 0.1215967924588106
2023-08-08 03:36:00,666 - epoch:7, training loss:0.8420 validation loss:0.1261
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12772074705836448 0.12272746801715005
need align? ->  True 0.1215967924588106
2023-08-08 03:37:24,571 - epoch:8, training loss:0.8301 validation loss:0.1277
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.1274310557849028 0.12277295999228954
need align? ->  True 0.1215967924588106
2023-08-08 03:38:48,248 - epoch:9, training loss:0.8232 validation loss:0.1274
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12636459627273408 0.12241107352416623
need align? ->  True 0.1215967924588106
2023-08-08 03:40:12,071 - epoch:10, training loss:0.8141 validation loss:0.1264
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12664160547270017 0.12216081402518532
need align? ->  True 0.1215967924588106
2023-08-08 03:41:35,824 - epoch:11, training loss:0.8073 validation loss:0.1266
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.1285869845781814 0.12221371018412439
need align? ->  True 0.1215967924588106
2023-08-08 03:42:59,886 - epoch:12, training loss:0.8012 validation loss:0.1286
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.13201830807057294 0.12284610251134093
need align? ->  True 0.1215967924588106
2023-08-08 03:44:23,636 - epoch:13, training loss:0.7960 validation loss:0.1320
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12984420071271333 0.12189197379418394
need align? ->  True 0.1215967924588106
2023-08-08 03:45:47,648 - epoch:14, training loss:0.7928 validation loss:0.1298
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.1277094449509274 0.12262403016740625
need align? ->  True 0.1215967924588106
2023-08-08 03:47:11,303 - epoch:15, training loss:0.7882 validation loss:0.1277
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12772490084171295 0.12247987574135716
need align? ->  True 0.1215967924588106
2023-08-08 03:48:34,965 - epoch:16, training loss:0.7858 validation loss:0.1277
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.13009806790135123 0.1224058096889745
need align? ->  True 0.1215967924588106
2023-08-08 03:49:58,445 - epoch:17, training loss:0.7819 validation loss:0.1301
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12893864436244423 0.12211668262766166
need align? ->  True 0.1215967924588106
2023-08-08 03:51:22,037 - epoch:18, training loss:0.7802 validation loss:0.1289
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.13029246447099882 0.12237878939644857
need align? ->  True 0.1215967924588106
2023-08-08 03:52:45,786 - epoch:19, training loss:0.7762 validation loss:0.1303
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12959298889406703 0.1220506624403325
need align? ->  True 0.1215967924588106
2023-08-08 03:54:09,544 - epoch:20, training loss:0.7744 validation loss:0.1296
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12971499748528004 0.12256318500096147
need align? ->  True 0.1215967924588106
2023-08-08 03:55:32,899 - epoch:21, training loss:0.7732 validation loss:0.1297
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12965682868591763 0.12257945385168899
need align? ->  True 0.1215967924588106
2023-08-08 03:56:56,738 - epoch:22, training loss:0.7713 validation loss:0.1297
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.1301831341433254 0.12232131739570336
need align? ->  True 0.1215967924588106
2023-08-08 03:58:20,561 - epoch:23, training loss:0.7702 validation loss:0.1302
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12996717254546555 0.12237856977365234
need align? ->  True 0.1215967924588106
2023-08-08 03:59:44,154 - epoch:24, training loss:0.7694 validation loss:0.1300
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12942660532214426 0.1225228262218562
need align? ->  True 0.1215967924588106
2023-08-08 04:01:07,847 - epoch:25, training loss:0.7676 validation loss:0.1294
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12949556366286494 0.12234135835685513
need align? ->  True 0.1215967924588106
2023-08-08 04:02:31,607 - epoch:26, training loss:0.7682 validation loss:0.1295
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12960792358287357 0.12227723654359579
need align? ->  True 0.1215967924588106
2023-08-08 04:03:56,486 - epoch:27, training loss:0.7682 validation loss:0.1296
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12957430994984778 0.12233049862764099
need align? ->  True 0.1215967924588106
2023-08-08 04:05:19,979 - epoch:28, training loss:0.7680 validation loss:0.1296
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12949116316369988 0.12225858291441744
need align? ->  True 0.1215967924588106
2023-08-08 04:06:43,701 - epoch:29, training loss:0.7668 validation loss:0.1295
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-03:23:38.383159/0/0.1261_epoch_7.pkl  &  0.1215967924588106
2023-08-08 04:06:45,844 - [*] loss:0.2811
2023-08-08 04:06:45,848 - [*] phase 0, testing
2023-08-08 04:06:45,885 - T:96	MAE	0.337059	RMSE	0.280763	MAPE	130.181134
2023-08-08 04:06:45,886 - 96	mae	0.3371	
2023-08-08 04:06:45,886 - 96	rmse	0.2808	
2023-08-08 04:06:45,886 - 96	mape	130.1811	
2023-08-08 04:06:46,939 - [*] loss:0.2709
2023-08-08 04:06:46,943 - [*] phase 0, testing
2023-08-08 04:06:46,980 - T:96	MAE	0.331063	RMSE	0.271065	MAPE	132.245064
2023-08-08 04:06:46,981 - 96	mae	0.3311	
2023-08-08 04:06:46,981 - 96	rmse	0.2711	
2023-08-08 04:06:46,981 - 96	mape	132.2451	
2023-08-08 04:06:48,962 - logger name:exp/ECL-PatchTST2023-08-08-04:06:48.961746/ECL-PatchTST.log
2023-08-08 04:06:48,962 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 1, 'rec_ori': 0, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-04:06:48.961746', 'path': 'exp/ECL-PatchTST2023-08-08-04:06:48.961746', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 04:06:48,962 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 04:06:49,154 - [*] phase 0 Dataset load!
2023-08-08 04:06:50,104 - [*] phase 0 Training start
train 8209
2023-08-08 04:07:30,860 - epoch:0, training loss:0.2133 validation loss:0.1653
train 8209
vs, vt 0.16531540216370064 0.16537542929026214
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13798030059445987 0.13525610806589777
need align? ->  True 0.13525610806589777
2023-08-08 04:09:01,679 - epoch:1, training loss:1.7008 validation loss:0.1380
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.1296385945752263 0.1273768160661513
need align? ->  True 0.1273768160661513
2023-08-08 04:10:25,361 - epoch:2, training loss:1.4701 validation loss:0.1296
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.1280184979127212 0.12432769533585418
need align? ->  True 0.12432769533585418
2023-08-08 04:11:48,818 - epoch:3, training loss:1.2193 validation loss:0.1280
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12625758510760285 0.1228882587430152
need align? ->  True 0.1228882587430152
2023-08-08 04:13:12,741 - epoch:4, training loss:1.0045 validation loss:0.1263
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12555676978081465 0.12220004057003693
need align? ->  True 0.12220004057003693
2023-08-08 04:14:36,614 - epoch:5, training loss:0.8980 validation loss:0.1256
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.13126576344736599 0.12254722755063664
need align? ->  True 0.12220004057003693
2023-08-08 04:15:59,992 - epoch:6, training loss:0.8541 validation loss:0.1313
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12854325771331787 0.12211764197457921
need align? ->  True 0.12211764197457921
2023-08-08 04:17:23,591 - epoch:7, training loss:0.8371 validation loss:0.1285
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.1265209809961644 0.12222429271787405
need align? ->  True 0.12211764197457921
2023-08-08 04:18:46,932 - epoch:8, training loss:0.8071 validation loss:0.1265
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12762042414397 0.12258153764361684
need align? ->  True 0.12211764197457921
2023-08-08 04:20:10,009 - epoch:9, training loss:0.7926 validation loss:0.1276
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12592404488135467 0.12263537477701902
need align? ->  True 0.12211764197457921
2023-08-08 04:21:33,494 - epoch:10, training loss:0.7853 validation loss:0.1259
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12844372786242853 0.12255349814553153
need align? ->  True 0.12211764197457921
2023-08-08 04:22:57,063 - epoch:11, training loss:0.7766 validation loss:0.1284
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.13059410969303412 0.12237677503038537
need align? ->  True 0.12211764197457921
2023-08-08 04:24:20,694 - epoch:12, training loss:0.7721 validation loss:0.1306
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.1263403773985126 0.12252972867678512
need align? ->  True 0.12211764197457921
2023-08-08 04:25:44,333 - epoch:13, training loss:0.7675 validation loss:0.1263
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.13030546717345715 0.1227734883908521
need align? ->  True 0.12211764197457921
2023-08-08 04:27:07,577 - epoch:14, training loss:0.7642 validation loss:0.1303
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12981317420913416 0.12296080800958654
need align? ->  True 0.12211764197457921
2023-08-08 04:28:31,397 - epoch:15, training loss:0.7599 validation loss:0.1298
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.13045068237591873 0.1229137700389732
need align? ->  True 0.12211764197457921
2023-08-08 04:29:54,791 - epoch:16, training loss:0.7573 validation loss:0.1305
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12961198635060678 0.12296362898566505
need align? ->  True 0.12211764197457921
2023-08-08 04:31:18,210 - epoch:17, training loss:0.7543 validation loss:0.1296
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.1304307227784937 0.12312044728208672
need align? ->  True 0.12211764197457921
2023-08-08 04:32:42,024 - epoch:18, training loss:0.7494 validation loss:0.1304
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12961320917714725 0.12294514087790792
need align? ->  True 0.12211764197457921
2023-08-08 04:34:05,582 - epoch:19, training loss:0.7485 validation loss:0.1296
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12834581309421497 0.12273098587651145
need align? ->  True 0.12211764197457921
2023-08-08 04:35:29,523 - epoch:20, training loss:0.7460 validation loss:0.1283
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12910795702852987 0.12288562911139293
need align? ->  True 0.12211764197457921
2023-08-08 04:36:52,904 - epoch:21, training loss:0.7451 validation loss:0.1291
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.1292964882979339 0.1227278710258278
need align? ->  True 0.12211764197457921
2023-08-08 04:38:16,393 - epoch:22, training loss:0.7445 validation loss:0.1293
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.1300208761610768 0.12327686646445231
need align? ->  True 0.12211764197457921
2023-08-08 04:39:40,004 - epoch:23, training loss:0.7426 validation loss:0.1300
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12905341107398272 0.12260688688944686
need align? ->  True 0.12211764197457921
2023-08-08 04:41:03,695 - epoch:24, training loss:0.7423 validation loss:0.1291
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.1294286451725797 0.12286036216061223
need align? ->  True 0.12211764197457921
2023-08-08 04:42:27,020 - epoch:25, training loss:0.7403 validation loss:0.1294
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.13026668605479327 0.12312121113592928
need align? ->  True 0.12211764197457921
2023-08-08 04:43:50,401 - epoch:26, training loss:0.7409 validation loss:0.1303
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.13024526411159473 0.12298803234642203
need align? ->  True 0.12211764197457921
2023-08-08 04:45:13,582 - epoch:27, training loss:0.7401 validation loss:0.1302
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.13009984325617552 0.1229656266725876
need align? ->  True 0.12211764197457921
2023-08-08 04:46:36,854 - epoch:28, training loss:0.7401 validation loss:0.1301
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.13015266465531153 0.12307123932987452
need align? ->  True 0.12211764197457921
2023-08-08 04:48:00,602 - epoch:29, training loss:0.7382 validation loss:0.1302
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-04:06:48.961746/0/0.1256_epoch_5.pkl  &  0.12211764197457921
2023-08-08 04:48:02,685 - [*] loss:0.2796
2023-08-08 04:48:02,689 - [*] phase 0, testing
2023-08-08 04:48:02,726 - T:96	MAE	0.336888	RMSE	0.279570	MAPE	130.269217
2023-08-08 04:48:02,728 - 96	mae	0.3369	
2023-08-08 04:48:02,728 - 96	rmse	0.2796	
2023-08-08 04:48:02,728 - 96	mape	130.2692	
2023-08-08 04:48:03,834 - [*] loss:0.2728
2023-08-08 04:48:03,837 - [*] phase 0, testing
2023-08-08 04:48:03,878 - T:96	MAE	0.330050	RMSE	0.273015	MAPE	132.951701
2023-08-08 04:48:03,879 - 96	mae	0.3301	
2023-08-08 04:48:03,879 - 96	rmse	0.2730	
2023-08-08 04:48:03,879 - 96	mape	132.9517	
2023-08-08 04:48:05,965 - logger name:exp/ECL-PatchTST2023-08-08-04:48:05.964808/ECL-PatchTST.log
2023-08-08 04:48:05,965 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'rec_intra_feature': 1, 'rec_ori': 1, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-04:48:05.964808', 'path': 'exp/ECL-PatchTST2023-08-08-04:48:05.964808', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 04:48:05,965 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 04:48:06,159 - [*] phase 0 Dataset load!
2023-08-08 04:48:07,077 - [*] phase 0 Training start
train 8209
2023-08-08 04:48:47,557 - epoch:0, training loss:0.2127 validation loss:0.1650
train 8209
vs, vt 0.16504068012264642 0.16559637168591673
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.1399047024209391 0.13626709885217927
need align? ->  True 0.13626709885217927
2023-08-08 04:50:18,854 - epoch:1, training loss:1.8379 validation loss:0.1399
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.1291468030011112 0.12844331791116434
need align? ->  True 0.12844331791116434
2023-08-08 04:51:42,837 - epoch:2, training loss:1.5205 validation loss:0.1291
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.1270905054089698 0.12394525559449737
need align? ->  True 0.12394525559449737
2023-08-08 04:53:06,455 - epoch:3, training loss:1.2527 validation loss:0.1271
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12569199248471044 0.12285815611143004
need align? ->  True 0.12285815611143004
2023-08-08 04:54:29,958 - epoch:4, training loss:1.0371 validation loss:0.1257
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12473497387360442 0.12330499672415582
need align? ->  True 0.12285815611143004
2023-08-08 04:55:53,861 - epoch:5, training loss:0.8966 validation loss:0.1247
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.1236162084070119 0.12378565763885324
need align? ->  True 0.12285815611143004
2023-08-08 04:57:17,451 - epoch:6, training loss:0.8678 validation loss:0.1236
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.1233331412923607 0.12353399616073478
need align? ->  True 0.12285815611143004
2023-08-08 04:58:40,983 - epoch:7, training loss:0.8540 validation loss:0.1233
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12360765954310243 0.12175968958234246
need align? ->  True 0.12175968958234246
2023-08-08 05:00:06,147 - epoch:8, training loss:0.8470 validation loss:0.1236
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.1239206584339792 0.1215972937643528
need align? ->  True 0.1215972937643528
2023-08-08 05:01:30,025 - epoch:9, training loss:0.7619 validation loss:0.1239
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12270413068207828 0.12196850319477645
need align? ->  True 0.1215972937643528
2023-08-08 05:02:53,730 - epoch:10, training loss:0.7079 validation loss:0.1227
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12249116506427526 0.12221077914265069
need align? ->  True 0.1215972937643528
2023-08-08 05:04:17,412 - epoch:11, training loss:0.6914 validation loss:0.1225
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12268713048913261 0.12206240455535325
need align? ->  True 0.1215972937643528
2023-08-08 05:05:40,979 - epoch:12, training loss:0.6884 validation loss:0.1227
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12483912985771894 0.12213821489025246
need align? ->  True 0.1215972937643528
2023-08-08 05:07:05,134 - epoch:13, training loss:0.6877 validation loss:0.1248
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.1245988297530196 0.12204375482079657
need align? ->  True 0.1215972937643528
2023-08-08 05:08:29,270 - epoch:14, training loss:0.6922 validation loss:0.1246
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12258577465333721 0.12190002998845144
need align? ->  True 0.1215972937643528
2023-08-08 05:09:52,763 - epoch:15, training loss:0.6905 validation loss:0.1226
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12352968261323193 0.12186138924549926
need align? ->  True 0.1215972937643528
2023-08-08 05:11:16,415 - epoch:16, training loss:0.6873 validation loss:0.1235
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12414170462976802 0.121927208351818
need align? ->  True 0.1215972937643528
2023-08-08 05:12:40,275 - epoch:17, training loss:0.6926 validation loss:0.1241
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12413769173012539 0.12168866776945916
need align? ->  True 0.1215972937643528
2023-08-08 05:14:04,091 - epoch:18, training loss:0.6892 validation loss:0.1241
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12359561043029482 0.1215168966657736
need align? ->  True 0.1215168966657736
2023-08-08 05:15:28,211 - epoch:19, training loss:0.6901 validation loss:0.1236
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12241758948022669 0.121340461329303
need align? ->  True 0.121340461329303
2023-08-08 05:16:51,875 - epoch:20, training loss:0.6997 validation loss:0.1224
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12250283411280675 0.12124780328436331
need align? ->  True 0.12124780328436331
2023-08-08 05:18:16,048 - epoch:21, training loss:0.6444 validation loss:0.1225
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.1222472247582945 0.12144241007891568
need align? ->  True 0.12124780328436331
2023-08-08 05:19:42,185 - epoch:22, training loss:0.6414 validation loss:0.1222
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12286408287896351 0.12172751272605224
need align? ->  True 0.12124780328436331
2023-08-08 05:21:06,758 - epoch:23, training loss:0.6291 validation loss:0.1229
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12240452598780394 0.12148505229164254
need align? ->  True 0.12124780328436331
2023-08-08 05:22:31,378 - epoch:24, training loss:0.6298 validation loss:0.1224
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.1227265741168098 0.1214216211810708
need align? ->  True 0.12124780328436331
2023-08-08 05:23:55,512 - epoch:25, training loss:0.6219 validation loss:0.1227
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.1227195853875442 0.12152924320914528
need align? ->  True 0.12124780328436331
2023-08-08 05:25:19,358 - epoch:26, training loss:0.6287 validation loss:0.1227
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.1227878826585683 0.1215977060862563
need align? ->  True 0.12124780328436331
2023-08-08 05:26:57,678 - epoch:27, training loss:0.6286 validation loss:0.1228
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12267345731908624 0.12149690345607021
need align? ->  True 0.12124780328436331
2023-08-08 05:28:40,728 - epoch:28, training loss:0.6240 validation loss:0.1227
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.1227617095309225 0.12151031543246725
need align? ->  True 0.12124780328436331
2023-08-08 05:30:22,817 - epoch:29, training loss:0.6276 validation loss:0.1228
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-04:48:05.964808/0/0.1222_epoch_22.pkl  &  0.12124780328436331
2023-08-08 05:30:28,540 - [*] loss:0.2717
2023-08-08 05:30:28,552 - [*] phase 0, testing
2023-08-08 05:30:28,632 - T:96	MAE	0.331855	RMSE	0.272118	MAPE	129.013872
2023-08-08 05:30:28,633 - 96	mae	0.3319	
2023-08-08 05:30:28,633 - 96	rmse	0.2721	
2023-08-08 05:30:28,633 - 96	mape	129.0139	
2023-08-08 05:30:33,407 - [*] loss:0.2709
2023-08-08 05:30:33,524 - [*] phase 0, testing
2023-08-08 05:30:33,620 - T:96	MAE	0.328148	RMSE	0.271170	MAPE	131.899428
2023-08-08 05:30:33,623 - 96	mae	0.3281	
2023-08-08 05:30:33,623 - 96	rmse	0.2712	
2023-08-08 05:30:33,623 - 96	mape	131.8994	
2023-08-08 05:30:36,205 - logger name:exp/ECL-PatchTST2023-08-08-05:30:36.204425/ECL-PatchTST.log
2023-08-08 05:30:36,205 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'rec_intra_feature': 1, 'rec_ori': 1, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-05:30:36.204425', 'path': 'exp/ECL-PatchTST2023-08-08-05:30:36.204425', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 05:30:36,205 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 05:30:36,434 - [*] phase 0 Dataset load!
2023-08-08 05:30:37,678 - [*] phase 0 Training start
train 8209
2023-08-08 05:31:33,348 - epoch:0, training loss:0.2133 validation loss:0.1653
train 8209
vs, vt 0.16531540216370064 0.16537542929026214
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13974769904532217 0.13564884069968353
need align? ->  True 0.13564884069968353
2023-08-08 05:33:28,493 - epoch:1, training loss:1.8372 validation loss:0.1397
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.1299756866964427 0.127952754243531
need align? ->  True 0.127952754243531
2023-08-08 05:35:08,877 - epoch:2, training loss:1.5117 validation loss:0.1300
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12660174766047436 0.12623458160934123
need align? ->  True 0.12623458160934123
2023-08-08 05:36:40,268 - epoch:3, training loss:1.2441 validation loss:0.1266
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12521350172094323 0.1226444887843999
need align? ->  True 0.1226444887843999
2023-08-08 05:38:19,580 - epoch:4, training loss:1.0045 validation loss:0.1252
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12372915091162379 0.12175694209608165
need align? ->  True 0.12175694209608165
2023-08-08 05:39:55,252 - epoch:5, training loss:0.8660 validation loss:0.1237
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12602765663442286 0.12401674476198175
need align? ->  True 0.12175694209608165
2023-08-08 05:41:33,796 - epoch:6, training loss:0.8018 validation loss:0.1260
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12561307559636506 0.12415025535632264
need align? ->  True 0.12175694209608165
2023-08-08 05:43:10,150 - epoch:7, training loss:0.7800 validation loss:0.1256
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12718197576362977 0.1259837348691442
need align? ->  True 0.12175694209608165
2023-08-08 05:44:43,397 - epoch:8, training loss:0.7752 validation loss:0.1272
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.1263535273346034 0.12391382007097657
need align? ->  True 0.12175694209608165
2023-08-08 05:46:20,696 - epoch:9, training loss:0.7698 validation loss:0.1264
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12364813600751487 0.12283286748623307
need align? ->  True 0.12175694209608165
2023-08-08 05:47:57,128 - epoch:10, training loss:0.7723 validation loss:0.1236
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12455689001151106 0.12243856261060997
need align? ->  True 0.12175694209608165
2023-08-08 05:49:38,219 - epoch:11, training loss:0.7709 validation loss:0.1246
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12870760101147674 0.12384066815403375
need align? ->  True 0.12175694209608165
2023-08-08 05:51:15,973 - epoch:12, training loss:0.7729 validation loss:0.1287
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12536364920776 0.12334463355893438
need align? ->  True 0.12175694209608165
2023-08-08 05:52:56,633 - epoch:13, training loss:0.7686 validation loss:0.1254
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.1258832004598596 0.12286624981259758
need align? ->  True 0.12175694209608165
2023-08-08 05:54:37,215 - epoch:14, training loss:0.7679 validation loss:0.1259
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.1265018108216199 0.12298331604423848
need align? ->  True 0.12175694209608165
2023-08-08 05:56:17,879 - epoch:15, training loss:0.7666 validation loss:0.1265
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12380569343539802 0.12255381310189312
need align? ->  True 0.12175694209608165
2023-08-08 05:57:57,169 - epoch:16, training loss:0.7669 validation loss:0.1238
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12668826083906673 0.12281905622644858
need align? ->  True 0.12175694209608165
2023-08-08 05:59:37,466 - epoch:17, training loss:0.7663 validation loss:0.1267
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12781330553645437 0.12290504456243732
need align? ->  True 0.12175694209608165
2023-08-08 06:01:16,005 - epoch:18, training loss:0.7647 validation loss:0.1278
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.1255495665933598 0.12265806763686916
need align? ->  True 0.12175694209608165
2023-08-08 06:02:49,699 - epoch:19, training loss:0.7618 validation loss:0.1255
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.1257935440675779 0.12274057646705346
need align? ->  True 0.12175694209608165
2023-08-08 06:04:16,958 - epoch:20, training loss:0.7611 validation loss:0.1258
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.1256432677195831 0.12281742395663803
need align? ->  True 0.12175694209608165
2023-08-08 06:05:40,559 - epoch:21, training loss:0.7611 validation loss:0.1256
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12751143662766976 0.12286172333088788
need align? ->  True 0.12175694209608165
2023-08-08 06:07:04,998 - epoch:22, training loss:0.7603 validation loss:0.1275
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12685769821771167 0.12291031423956156
need align? ->  True 0.12175694209608165
2023-08-08 06:08:28,737 - epoch:23, training loss:0.7576 validation loss:0.1269
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.1270067836581306 0.12305011845786464
need align? ->  True 0.12175694209608165
2023-08-08 06:09:52,456 - epoch:24, training loss:0.7580 validation loss:0.1270
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12650908673690123 0.12291318372907964
need align? ->  True 0.12175694209608165
2023-08-08 06:11:16,099 - epoch:25, training loss:0.7571 validation loss:0.1265
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12650518500330774 0.12278588124635545
need align? ->  True 0.12175694209608165
2023-08-08 06:12:39,728 - epoch:26, training loss:0.7567 validation loss:0.1265
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12671680392866785 0.12283271593465046
need align? ->  True 0.12175694209608165
2023-08-08 06:14:03,352 - epoch:27, training loss:0.7569 validation loss:0.1267
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12641491055149923 0.12280331162566488
need align? ->  True 0.12175694209608165
2023-08-08 06:15:28,147 - epoch:28, training loss:0.7572 validation loss:0.1264
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12646765194155954 0.12289451697671955
need align? ->  True 0.12175694209608165
2023-08-08 06:16:51,123 - epoch:29, training loss:0.7573 validation loss:0.1265
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-05:30:36.204425/0/0.1236_epoch_10.pkl  &  0.12175694209608165
2023-08-08 06:16:53,346 - [*] loss:0.2737
2023-08-08 06:16:53,350 - [*] phase 0, testing
2023-08-08 06:16:53,402 - T:96	MAE	0.335241	RMSE	0.273588	MAPE	132.502818
2023-08-08 06:16:53,403 - 96	mae	0.3352	
2023-08-08 06:16:53,403 - 96	rmse	0.2736	
2023-08-08 06:16:53,403 - 96	mape	132.5028	
2023-08-08 06:16:54,536 - [*] loss:0.2711
2023-08-08 06:16:54,539 - [*] phase 0, testing
2023-08-08 06:16:54,576 - T:96	MAE	0.330353	RMSE	0.271369	MAPE	134.263694
2023-08-08 06:16:54,577 - 96	mae	0.3304	
2023-08-08 06:16:54,577 - 96	rmse	0.2714	
2023-08-08 06:16:54,577 - 96	mape	134.2637	
2023-08-08 06:16:57,500 - logger name:exp/ECL-PatchTST2023-08-08-06:16:57.500409/ECL-PatchTST.log
2023-08-08 06:16:57,500 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 1, 'rec_ori': 1, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-06:16:57.500409', 'path': 'exp/ECL-PatchTST2023-08-08-06:16:57.500409', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 06:16:57,501 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 06:16:57,700 - [*] phase 0 Dataset load!
2023-08-08 06:16:59,420 - [*] phase 0 Training start
train 8209
2023-08-08 06:17:40,098 - epoch:0, training loss:0.2127 validation loss:0.1650
train 8209
vs, vt 0.16504068012264642 0.16559637168591673
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.14047758958556436 0.1360680346631191
need align? ->  True 0.1360680346631191
2023-08-08 06:19:10,809 - epoch:1, training loss:1.9675 validation loss:0.1405
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12987968901341612 0.12632461154664104
need align? ->  True 0.12632461154664104
2023-08-08 06:20:34,335 - epoch:2, training loss:1.6610 validation loss:0.1299
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12582805583422835 0.12314669897949154
need align? ->  True 0.12314669897949154
2023-08-08 06:21:58,252 - epoch:3, training loss:1.3534 validation loss:0.1258
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12419940810650587 0.12221794058992104
need align? ->  True 0.12221794058992104
2023-08-08 06:23:21,498 - epoch:4, training loss:1.1168 validation loss:0.1242
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12529233868487857 0.12175020567056807
need align? ->  True 0.12175020567056807
2023-08-08 06:24:45,094 - epoch:5, training loss:0.9958 validation loss:0.1253
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12476992928846316 0.12258723496713421
need align? ->  True 0.12175020567056807
2023-08-08 06:26:08,644 - epoch:6, training loss:0.9407 validation loss:0.1248
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12302749510854483 0.1215014314617623
need align? ->  True 0.1215014314617623
2023-08-08 06:27:32,375 - epoch:7, training loss:0.9179 validation loss:0.1230
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12541518326510082 0.12222704833204096
need align? ->  True 0.1215014314617623
2023-08-08 06:28:56,349 - epoch:8, training loss:0.8929 validation loss:0.1254
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.1238622237843546 0.12306749016385186
need align? ->  True 0.1215014314617623
2023-08-08 06:30:19,835 - epoch:9, training loss:0.8752 validation loss:0.1239
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12333123445172202 0.12178439257497137
need align? ->  True 0.1215014314617623
2023-08-08 06:31:43,391 - epoch:10, training loss:0.8638 validation loss:0.1233
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.1229822129349817 0.12155193547633561
need align? ->  True 0.1215014314617623
2023-08-08 06:33:06,798 - epoch:11, training loss:0.8527 validation loss:0.1230
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12552727479487658 0.12114956890317527
need align? ->  True 0.12114956890317527
2023-08-08 06:34:30,821 - epoch:12, training loss:0.8437 validation loss:0.1255
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12509017962623725 0.1229645570909435
need align? ->  True 0.12114956890317527
2023-08-08 06:35:54,200 - epoch:13, training loss:0.8510 validation loss:0.1251
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12548234601589767 0.12033281653103503
need align? ->  True 0.12033281653103503
2023-08-08 06:37:18,043 - epoch:14, training loss:0.8331 validation loss:0.1255
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12432093046266925 0.12218882135030898
need align? ->  True 0.12033281653103503
2023-08-08 06:38:41,618 - epoch:15, training loss:0.8520 validation loss:0.1243
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12382431302896955 0.12154315936971795
need align? ->  True 0.12033281653103503
2023-08-08 06:40:05,144 - epoch:16, training loss:0.8393 validation loss:0.1238
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12438157759606838 0.12138544861227274
need align? ->  True 0.12033281653103503
2023-08-08 06:41:28,721 - epoch:17, training loss:0.8322 validation loss:0.1244
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12486979475414212 0.12122054457325827
need align? ->  True 0.12033281653103503
2023-08-08 06:42:52,886 - epoch:18, training loss:0.8284 validation loss:0.1249
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12418953329324722 0.12160524040121924
need align? ->  True 0.12033281653103503
2023-08-08 06:44:16,547 - epoch:19, training loss:0.8237 validation loss:0.1242
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12428349248048934 0.12125276613303206
need align? ->  True 0.12033281653103503
2023-08-08 06:45:40,198 - epoch:20, training loss:0.8202 validation loss:0.1243
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12325799913907592 0.12175965715538371
need align? ->  True 0.12033281653103503
2023-08-08 06:47:03,589 - epoch:21, training loss:0.8176 validation loss:0.1233
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12322446720843966 0.12168048559264703
need align? ->  True 0.12033281653103503
2023-08-08 06:48:27,041 - epoch:22, training loss:0.8163 validation loss:0.1232
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12386953898451546 0.12147418079389767
need align? ->  True 0.12033281653103503
2023-08-08 06:49:50,391 - epoch:23, training loss:0.8141 validation loss:0.1239
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12445183851840821 0.12152880548753521
need align? ->  True 0.12033281653103503
2023-08-08 06:51:14,299 - epoch:24, training loss:0.8130 validation loss:0.1245
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12353987145152959 0.12166813583197919
need align? ->  True 0.12033281653103503
2023-08-08 06:52:38,139 - epoch:25, training loss:0.8114 validation loss:0.1235
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.123536323197186 0.12150776369327848
need align? ->  True 0.12033281653103503
2023-08-08 06:54:01,634 - epoch:26, training loss:0.8114 validation loss:0.1235
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.1236810487779704 0.12151529851623556
need align? ->  True 0.12033281653103503
2023-08-08 06:55:25,266 - epoch:27, training loss:0.8107 validation loss:0.1237
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12373566754500974 0.12150049438192086
need align? ->  True 0.12033281653103503
2023-08-08 06:56:48,843 - epoch:28, training loss:0.8118 validation loss:0.1237
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12360203537074002 0.12150366655127569
need align? ->  True 0.12033281653103503
2023-08-08 06:58:13,739 - epoch:29, training loss:0.8095 validation loss:0.1236
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-06:16:57.500409/0/0.123_epoch_11.pkl  &  0.12033281653103503
2023-08-08 06:58:16,033 - [*] loss:0.2743
2023-08-08 06:58:16,037 - [*] phase 0, testing
2023-08-08 06:58:16,074 - T:96	MAE	0.332322	RMSE	0.274480	MAPE	129.982746
2023-08-08 06:58:16,076 - 96	mae	0.3323	
2023-08-08 06:58:16,076 - 96	rmse	0.2745	
2023-08-08 06:58:16,076 - 96	mape	129.9827	
2023-08-08 06:58:17,114 - [*] loss:0.2689
2023-08-08 06:58:17,117 - [*] phase 0, testing
2023-08-08 06:58:17,154 - T:96	MAE	0.328369	RMSE	0.269066	MAPE	129.051745
2023-08-08 06:58:17,156 - 96	mae	0.3284	
2023-08-08 06:58:17,156 - 96	rmse	0.2691	
2023-08-08 06:58:17,156 - 96	mape	129.0517	
2023-08-08 06:58:19,153 - logger name:exp/ECL-PatchTST2023-08-08-06:58:19.153234/ECL-PatchTST.log
2023-08-08 06:58:19,153 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 1, 'rec_ori': 1, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-06:58:19.153234', 'path': 'exp/ECL-PatchTST2023-08-08-06:58:19.153234', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 06:58:19,154 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 06:58:19,345 - [*] phase 0 Dataset load!
2023-08-08 06:58:20,298 - [*] phase 0 Training start
train 8209
2023-08-08 06:59:00,557 - epoch:0, training loss:0.2133 validation loss:0.1653
train 8209
vs, vt 0.16531540216370064 0.16537542929026214
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.14046313224191015 0.13501011571762236
need align? ->  True 0.13501011571762236
2023-08-08 07:00:31,569 - epoch:1, training loss:1.9670 validation loss:0.1405
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.13028268643062224 0.12707252106205982
need align? ->  True 0.12707252106205982
2023-08-08 07:01:55,287 - epoch:2, training loss:1.6553 validation loss:0.1303
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12702209138396112 0.12372381206263196
need align? ->  True 0.12372381206263196
2023-08-08 07:03:18,922 - epoch:3, training loss:1.3478 validation loss:0.1270
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.1249740630896254 0.12322421194138852
need align? ->  True 0.12322421194138852
2023-08-08 07:04:42,844 - epoch:4, training loss:1.1005 validation loss:0.1250
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12444723549891602 0.12297446212985298
need align? ->  True 0.12297446212985298
2023-08-08 07:06:06,790 - epoch:5, training loss:0.9817 validation loss:0.1244
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.1255802812732079 0.12251868056641384
need align? ->  True 0.12251868056641384
2023-08-08 07:07:30,722 - epoch:6, training loss:0.9402 validation loss:0.1256
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12492691175165502 0.12117903865873814
need align? ->  True 0.12117903865873814
2023-08-08 07:08:55,127 - epoch:7, training loss:0.9054 validation loss:0.1249
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12413031379268928 0.12198947480117733
need align? ->  True 0.12117903865873814
2023-08-08 07:10:19,124 - epoch:8, training loss:0.8927 validation loss:0.1241
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12532614264637232 0.12216848829253153
need align? ->  True 0.12117903865873814
2023-08-08 07:11:45,466 - epoch:9, training loss:0.8748 validation loss:0.1253
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12434724735265429 0.12211609267714349
need align? ->  True 0.12117903865873814
2023-08-08 07:13:09,308 - epoch:10, training loss:0.8633 validation loss:0.1243
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12711417217823592 0.12151203787123616
need align? ->  True 0.12117903865873814
2023-08-08 07:14:32,772 - epoch:11, training loss:0.8520 validation loss:0.1271
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12471735308116133 0.12105536579408428
need align? ->  True 0.12105536579408428
2023-08-08 07:15:56,600 - epoch:12, training loss:0.8452 validation loss:0.1247
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12391163611953909 0.12105438968336041
need align? ->  True 0.12105438968336041
2023-08-08 07:17:20,616 - epoch:13, training loss:0.8487 validation loss:0.1239
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12604438327252865 0.12163975758647377
need align? ->  True 0.12105438968336041
2023-08-08 07:18:44,376 - epoch:14, training loss:0.8525 validation loss:0.1260
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12662248584357175 0.12135383622212843
need align? ->  True 0.12105438968336041
2023-08-08 07:20:08,358 - epoch:15, training loss:0.8374 validation loss:0.1266
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12492768220942128 0.12116979426619681
need align? ->  True 0.12105438968336041
2023-08-08 07:21:32,050 - epoch:16, training loss:0.8325 validation loss:0.1249
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12554662530733782 0.12126981509341435
need align? ->  True 0.12105438968336041
2023-08-08 07:22:55,714 - epoch:17, training loss:0.8275 validation loss:0.1255
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.1254531443965706 0.1215624554421414
need align? ->  True 0.12105438968336041
2023-08-08 07:24:19,354 - epoch:18, training loss:0.8205 validation loss:0.1255
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12509418092668056 0.12104292899708856
need align? ->  True 0.12104292899708856
2023-08-08 07:25:44,735 - epoch:19, training loss:0.8181 validation loss:0.1251
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12451765974136916 0.12113801559264009
need align? ->  True 0.12104292899708856
2023-08-08 07:27:08,769 - epoch:20, training loss:0.8539 validation loss:0.1245
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12558031497015196 0.12081077415496111
need align? ->  True 0.12081077415496111
2023-08-08 07:28:44,187 - epoch:21, training loss:0.8385 validation loss:0.1256
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.1246093763885173 0.1208611282265999
need align? ->  True 0.12081077415496111
2023-08-08 07:30:08,463 - epoch:22, training loss:0.8612 validation loss:0.1246
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.1264626785096797 0.12114622884175995
need align? ->  True 0.12081077415496111
2023-08-08 07:31:33,281 - epoch:23, training loss:0.8503 validation loss:0.1265
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12486510198902 0.12059733635661277
need align? ->  True 0.12059733635661277
2023-08-08 07:32:57,520 - epoch:24, training loss:0.8474 validation loss:0.1249
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.1246555180881511 0.12070542133667252
need align? ->  True 0.12059733635661277
2023-08-08 07:34:21,095 - epoch:25, training loss:0.8777 validation loss:0.1247
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12516186547211625 0.12072548515756022
need align? ->  True 0.12059733635661277
2023-08-08 07:35:44,999 - epoch:26, training loss:0.8676 validation loss:0.1252
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.1251345593482256 0.12078429343686863
need align? ->  True 0.12059733635661277
2023-08-08 07:37:08,893 - epoch:27, training loss:0.8649 validation loss:0.1251
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12503868189047684 0.12070446419106289
need align? ->  True 0.12059733635661277
2023-08-08 07:38:32,905 - epoch:28, training loss:0.8642 validation loss:0.1250
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.1250682007521391 0.12070429951629856
need align? ->  True 0.12059733635661277
2023-08-08 07:39:56,705 - epoch:29, training loss:0.8627 validation loss:0.1251
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-06:58:19.153234/0/0.1239_epoch_13.pkl  &  0.12059733635661277
2023-08-08 07:39:58,973 - [*] loss:0.2757
2023-08-08 07:39:58,977 - [*] phase 0, testing
2023-08-08 07:39:59,013 - T:96	MAE	0.334411	RMSE	0.275444	MAPE	132.520330
2023-08-08 07:39:59,014 - 96	mae	0.3344	
2023-08-08 07:39:59,014 - 96	rmse	0.2754	
2023-08-08 07:39:59,014 - 96	mape	132.5203	
2023-08-08 07:39:59,982 - [*] loss:0.2680
2023-08-08 07:39:59,985 - [*] phase 0, testing
2023-08-08 07:40:00,022 - T:96	MAE	0.328490	RMSE	0.268247	MAPE	130.307472
2023-08-08 07:40:00,023 - 96	mae	0.3285	
2023-08-08 07:40:00,023 - 96	rmse	0.2682	
2023-08-08 07:40:00,023 - 96	mape	130.3075	
