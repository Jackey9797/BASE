2023-08-02 19:06:15,862 - logger name:exp/ECL-PatchTST2023-08-02-19:06:15.861519/ECL-PatchTST.log
2023-08-02 19:06:15,862 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 3, 'seed': 42033, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-02-19:06:15.861519', 'path': 'exp/ECL-PatchTST2023-08-02-19:06:15.861519', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-02 19:06:15,862 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-02 19:06:16,080 - [*] phase 0 Dataset load!
2023-08-02 19:06:17,047 - [*] phase 0 Training start
train 8209
2023-08-02 19:06:29,359 - epoch:0, training loss:0.2361 validation loss:0.1664
train 8209
vs, vt 0.16641259074888445 0.1659279042346911
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.14063907211477106 0.13619721621613612
2023-08-02 19:07:02,068 - epoch:1, training loss:2.7031 validation loss:0.1406
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.13087131193077023 0.12786784073845905
2023-08-02 19:07:26,398 - epoch:2, training loss:2.0741 validation loss:0.1309
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.1283375101841309 0.12538351711224427
2023-08-02 19:07:51,089 - epoch:3, training loss:1.3522 validation loss:0.1283
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12529801949858665 0.1261532443829558
2023-08-02 19:08:16,192 - epoch:4, training loss:0.9383 validation loss:0.1253
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12435697696425697 0.12345653205094012
2023-08-02 19:08:41,314 - epoch:5, training loss:0.8010 validation loss:0.1244
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12467266331342133 0.1250790274617347
2023-08-02 19:09:06,423 - epoch:6, training loss:0.7401 validation loss:0.1247
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12452749687839638 0.12283780002458529
2023-08-02 19:09:31,431 - epoch:7, training loss:0.7029 validation loss:0.1245
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12519651693715292 0.12378650632771579
2023-08-02 19:09:55,652 - epoch:8, training loss:0.6741 validation loss:0.1252
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.1252874710851095 0.12309062201529741
2023-08-02 19:10:20,713 - epoch:9, training loss:0.6503 validation loss:0.1253
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.1259769688953053 0.12515893807126718
2023-08-02 19:10:45,606 - epoch:10, training loss:0.6245 validation loss:0.1260
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12353008401326158 0.12431316221640869
2023-08-02 19:11:10,925 - epoch:11, training loss:0.6075 validation loss:0.1235
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.1241268850016323 0.12337685867466709
2023-08-02 19:11:35,661 - epoch:12, training loss:0.5866 validation loss:0.1241
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12366366674276916 0.1233528180217201
2023-08-02 19:12:00,831 - epoch:13, training loss:0.5705 validation loss:0.1237
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12312035431916063 0.12271868750791658
2023-08-02 19:12:25,695 - epoch:14, training loss:0.5548 validation loss:0.1231
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12332540259442547 0.12296950241381471
2023-08-02 19:12:50,917 - epoch:15, training loss:0.5424 validation loss:0.1233
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12272311958738348 0.12311630442061207
2023-08-02 19:13:15,691 - epoch:16, training loss:0.5332 validation loss:0.1227
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.1229093327948993 0.1228282085873864
2023-08-02 19:13:41,000 - epoch:17, training loss:0.5208 validation loss:0.1229
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.1227970684624531 0.12287426181137562
2023-08-02 19:14:06,945 - epoch:18, training loss:0.5201 validation loss:0.1228
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12345697840845044 0.12322243379259651
2023-08-02 19:14:32,436 - epoch:19, training loss:0.5150 validation loss:0.1235
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12338289423761042 0.1224630144001408
2023-08-02 19:15:00,155 - epoch:20, training loss:0.5142 validation loss:0.1234
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12298176962543618 0.12217365924946287
2023-08-02 19:15:24,716 - epoch:21, training loss:0.5096 validation loss:0.1230
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12341397399590774 0.12225237632678314
2023-08-02 19:15:49,271 - epoch:22, training loss:0.5066 validation loss:0.1234
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12287661831148645 0.12249701224606145
2023-08-02 19:16:14,866 - epoch:23, training loss:0.5065 validation loss:0.1229
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.1233366724442352 0.12257323515686122
2023-08-02 19:16:39,647 - epoch:24, training loss:0.5046 validation loss:0.1233
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12327133110639724 0.12252442962066694
2023-08-02 19:17:05,117 - epoch:25, training loss:0.5050 validation loss:0.1233
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.1229519380933859 0.12240807932208884
2023-08-02 19:17:30,723 - epoch:26, training loss:0.5018 validation loss:0.1230
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12294664183123545 0.12238099527629939
2023-08-02 19:18:01,106 - epoch:27, training loss:0.5027 validation loss:0.1229
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12292594979093834 0.12232936461540786
2023-08-02 19:18:26,175 - epoch:28, training loss:0.4997 validation loss:0.1229
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12303985782306302 0.12243923536417159
2023-08-02 19:18:51,413 - epoch:29, training loss:0.4984 validation loss:0.1230
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-02-19:06:15.861519/0/0.1227_epoch_16.pkl  &  0.12217365924946287
2023-08-02 19:18:52,704 - [*] loss:0.2724
2023-08-02 19:18:52,722 - [*] phase 0, testing
2023-08-02 19:18:52,792 - T:96	MAE	0.332047	RMSE	0.272465	MAPE	132.455802
2023-08-02 19:18:52,794 - 96	mae	0.3320	
2023-08-02 19:18:52,794 - 96	rmse	0.2725	
2023-08-02 19:18:52,794 - 96	mape	132.4558	
2023-08-02 19:18:54,902 - [*] loss:0.2723
2023-08-02 19:18:54,918 - [*] phase 0, testing
2023-08-02 19:18:54,997 - T:96	MAE	0.330064	RMSE	0.272628	MAPE	129.905283
2023-08-02 19:18:55,000 - 96	mae	0.3301	
2023-08-02 19:18:55,000 - 96	rmse	0.2726	
2023-08-02 19:18:55,000 - 96	mape	129.9053	
