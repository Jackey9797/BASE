2023-05-11 19:12:20,342 - logger name:exp/district3F11T17/incremental-linear2023-05-11-19:12:20.342025/incremental-linear.log
2023-05-11 19:12:20,343 - params : {'conf': 'incremental-linear', 'load_config': 'configs/', 'data_process': False, 'auto_test': 1, 'load': True, 'device': device(type='cuda', index=1), 'build_graph': False, 'dynamic_graph': False, 'graph_input': False, 'model_name': 'Linear', 'data_name': 'PEMS3-Stream', 'raw_data_path': 'data/district3F11T17/finaldata/', 'graph_path': 'data/district3F11T17/graph/', 'save_data_path': 'data/district3F11T17/FastData/', 'model_path': 'exp/district3F11T17/', 'year': 2012, 'days': 31, 'logname': 'incremental-linear', '/* model related args*/': '//', 'x_len': 12, 'y_len': 12, 'dropout': 0.0, 'individual': False, '/*train related args*/': '//', 'train': True, 'begin_year': 2011, 'end_year': 2017, 'epoch': 100, 'batch_size': 128, 'lr': 0.01, 'loss': 'mse', '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'time': '2023-05-11-19:12:20.342025', 'path': 'exp/district3F11T17/incremental-linear2023-05-11-19:12:20.342025', 'logger': <Logger __main__ (INFO)>}
2023-05-11 19:12:20,343 - [*] Year 2011 load from data/district3F11T17/FastData/2011_30day.npz
2023-05-11 19:12:21,270 - [*] Year 2011 Dataset load!
2023-05-11 19:12:22,312 - [*] Year 2011 Training start
2023-05-11 19:12:22,447 - node number torch.Size([83840, 12])
2023-05-11 19:12:23,585 - epoch:0, training loss:138.5095 validation loss:139.9771
2023-05-11 19:12:24,544 - epoch:1, training loss:138.0724 validation loss:139.5064
2023-05-11 19:12:25,491 - epoch:2, training loss:137.5616 validation loss:139.0390
2023-05-11 19:12:26,438 - epoch:3, training loss:137.2082 validation loss:138.5757
2023-05-11 19:12:27,414 - epoch:4, training loss:136.7007 validation loss:138.1171
2023-05-11 19:12:28,375 - epoch:5, training loss:136.2895 validation loss:137.6604
2023-05-11 19:12:29,329 - epoch:6, training loss:135.8399 validation loss:137.2084
2023-05-11 19:12:30,303 - epoch:7, training loss:135.3617 validation loss:136.7591
2023-05-11 19:12:31,257 - epoch:8, training loss:134.9233 validation loss:136.3145
2023-05-11 19:12:32,235 - epoch:9, training loss:134.5751 validation loss:135.8731
2023-05-11 19:12:33,207 - epoch:10, training loss:134.1489 validation loss:135.4359
2023-05-11 19:12:34,189 - epoch:11, training loss:133.6344 validation loss:134.9999
2023-05-11 19:12:35,171 - epoch:12, training loss:133.2538 validation loss:134.5688
2023-05-11 19:12:36,119 - epoch:13, training loss:132.8293 validation loss:134.1413
2023-05-11 19:12:37,078 - epoch:14, training loss:132.4210 validation loss:133.7180
2023-05-11 19:12:38,054 - epoch:15, training loss:131.9849 validation loss:133.2982
2023-05-11 19:12:39,030 - epoch:16, training loss:131.5879 validation loss:132.8813
2023-05-11 19:12:39,995 - epoch:17, training loss:131.1529 validation loss:132.4694
2023-05-11 19:12:40,959 - epoch:18, training loss:130.7913 validation loss:132.0586
2023-05-11 19:12:41,911 - epoch:19, training loss:130.3826 validation loss:131.6521
2023-05-11 19:12:42,877 - epoch:20, training loss:129.9759 validation loss:131.2479
2023-05-11 19:12:43,839 - epoch:21, training loss:129.5549 validation loss:130.8487
2023-05-11 19:12:44,810 - epoch:22, training loss:129.1528 validation loss:130.4519
2023-05-11 19:12:45,792 - epoch:23, training loss:128.8086 validation loss:130.0586
2023-05-11 19:12:46,754 - epoch:24, training loss:128.3921 validation loss:129.6680
2023-05-11 19:12:47,722 - epoch:25, training loss:128.0056 validation loss:129.2818
2023-05-11 19:12:48,686 - epoch:26, training loss:127.6460 validation loss:128.8995
2023-05-11 19:12:49,683 - epoch:27, training loss:127.2638 validation loss:128.5201
2023-05-11 19:12:50,661 - epoch:28, training loss:126.8274 validation loss:128.1436
2023-05-11 19:12:51,626 - epoch:29, training loss:126.5082 validation loss:127.7715
2023-05-11 19:12:52,605 - epoch:30, training loss:126.1280 validation loss:127.4020
2023-05-11 19:12:53,577 - epoch:31, training loss:125.7841 validation loss:127.0355
2023-05-11 19:12:54,568 - epoch:32, training loss:125.3903 validation loss:126.6733
2023-05-11 19:12:55,548 - epoch:33, training loss:125.0507 validation loss:126.3137
2023-05-11 19:12:56,508 - epoch:34, training loss:124.7123 validation loss:125.9571
2023-05-11 19:12:57,515 - epoch:35, training loss:124.3661 validation loss:125.6032
2023-05-11 19:12:58,475 - epoch:36, training loss:123.9753 validation loss:125.2520
2023-05-11 19:12:59,427 - epoch:37, training loss:123.6377 validation loss:124.9040
2023-05-11 19:13:00,384 - epoch:38, training loss:123.2976 validation loss:124.5589
2023-05-11 19:13:01,334 - epoch:39, training loss:122.9726 validation loss:124.2159
2023-05-11 19:13:02,286 - epoch:40, training loss:122.6358 validation loss:123.8752
2023-05-11 19:13:03,247 - epoch:41, training loss:122.3601 validation loss:123.5367
2023-05-11 19:13:04,209 - epoch:42, training loss:121.9676 validation loss:123.2008
2023-05-11 19:13:05,160 - epoch:43, training loss:121.5877 validation loss:122.8656
2023-05-11 19:13:06,108 - epoch:44, training loss:121.2950 validation loss:122.5350
2023-05-11 19:13:07,102 - epoch:45, training loss:120.9943 validation loss:122.2053
2023-05-11 19:13:08,089 - epoch:46, training loss:120.6255 validation loss:121.8772
2023-05-11 19:13:09,062 - epoch:47, training loss:120.3156 validation loss:121.5516
2023-05-11 19:13:10,017 - epoch:48, training loss:119.9811 validation loss:121.2272
2023-05-11 19:13:10,995 - epoch:49, training loss:119.7159 validation loss:120.9047
2023-05-11 19:13:11,970 - epoch:50, training loss:119.3479 validation loss:120.5836
2023-05-11 19:13:12,941 - epoch:51, training loss:119.0404 validation loss:120.2646
2023-05-11 19:13:13,901 - epoch:52, training loss:118.7084 validation loss:119.9472
2023-05-11 19:13:14,868 - epoch:53, training loss:118.3954 validation loss:119.6314
2023-05-11 19:13:15,823 - epoch:54, training loss:118.0922 validation loss:119.3176
2023-05-11 19:13:16,784 - epoch:55, training loss:117.7548 validation loss:119.0044
2023-05-11 19:13:17,740 - epoch:56, training loss:117.4937 validation loss:118.6935
2023-05-11 19:13:18,697 - epoch:57, training loss:117.1253 validation loss:118.3836
2023-05-11 19:13:19,667 - epoch:58, training loss:116.8551 validation loss:118.0760
2023-05-11 19:13:20,628 - epoch:59, training loss:116.5654 validation loss:117.7694
2023-05-11 19:13:21,584 - epoch:60, training loss:116.2236 validation loss:117.4635
2023-05-11 19:13:22,562 - epoch:61, training loss:115.9262 validation loss:117.1602
2023-05-11 19:13:23,527 - epoch:62, training loss:115.6150 validation loss:116.8572
2023-05-11 19:13:24,491 - epoch:63, training loss:115.3843 validation loss:116.5568
2023-05-11 19:13:25,443 - epoch:64, training loss:114.9990 validation loss:116.2567
2023-05-11 19:13:26,402 - epoch:65, training loss:114.7115 validation loss:115.9583
2023-05-11 19:13:27,358 - epoch:66, training loss:114.4310 validation loss:115.6623
2023-05-11 19:13:28,313 - epoch:67, training loss:114.1600 validation loss:115.3669
2023-05-11 19:13:29,265 - epoch:68, training loss:113.8621 validation loss:115.0723
2023-05-11 19:13:30,237 - epoch:69, training loss:113.5592 validation loss:114.7790
2023-05-11 19:13:31,193 - epoch:70, training loss:113.2393 validation loss:114.4883
2023-05-11 19:13:32,142 - epoch:71, training loss:112.9744 validation loss:114.1971
2023-05-11 19:13:33,096 - epoch:72, training loss:112.6642 validation loss:113.9088
2023-05-11 19:13:34,060 - epoch:73, training loss:112.3557 validation loss:113.6211
2023-05-11 19:13:35,017 - epoch:74, training loss:112.1248 validation loss:113.3348
2023-05-11 19:13:35,959 - epoch:75, training loss:111.7819 validation loss:113.0502
2023-05-11 19:13:36,916 - epoch:76, training loss:111.5721 validation loss:112.7670
2023-05-11 19:13:37,885 - epoch:77, training loss:111.2460 validation loss:112.4840
2023-05-11 19:13:38,870 - epoch:78, training loss:110.9834 validation loss:112.2028
2023-05-11 19:13:39,837 - epoch:79, training loss:110.6887 validation loss:111.9234
2023-05-11 19:13:40,818 - epoch:80, training loss:110.3735 validation loss:111.6432
2023-05-11 19:13:41,764 - epoch:81, training loss:110.0947 validation loss:111.3657
2023-05-11 19:13:42,715 - epoch:82, training loss:109.8790 validation loss:111.0908
2023-05-11 19:13:43,656 - epoch:83, training loss:109.5747 validation loss:110.8152
2023-05-11 19:13:44,611 - epoch:84, training loss:109.2747 validation loss:110.5420
2023-05-11 19:13:45,576 - epoch:85, training loss:108.9910 validation loss:110.2688
2023-05-11 19:13:46,548 - epoch:86, training loss:108.7524 validation loss:109.9966
2023-05-11 19:13:47,512 - epoch:87, training loss:108.4933 validation loss:109.7271
2023-05-11 19:13:48,468 - epoch:88, training loss:108.2109 validation loss:109.4586
2023-05-11 19:13:49,436 - epoch:89, training loss:107.9433 validation loss:109.1907
2023-05-11 19:13:50,401 - epoch:90, training loss:107.6814 validation loss:108.9246
2023-05-11 19:13:51,354 - epoch:91, training loss:107.4238 validation loss:108.6582
2023-05-11 19:13:52,317 - epoch:92, training loss:107.1575 validation loss:108.3933
2023-05-11 19:13:53,315 - epoch:93, training loss:106.9016 validation loss:108.1298
2023-05-11 19:13:54,269 - epoch:94, training loss:106.6207 validation loss:107.8666
2023-05-11 19:13:55,247 - epoch:95, training loss:106.3596 validation loss:107.6077
2023-05-11 19:13:56,206 - epoch:96, training loss:106.1041 validation loss:107.3461
2023-05-11 19:13:57,183 - epoch:97, training loss:105.8121 validation loss:107.0876
2023-05-11 19:13:58,152 - epoch:98, training loss:105.5450 validation loss:106.8304
2023-05-11 19:13:59,139 - epoch:99, training loss:105.3100 validation loss:106.5745
2023-05-11 19:13:59,141 - Finished optimization, total time:65.62 s, best model:exp/district3F11T17/incremental-linear2023-05-11-19:12:20.342025/2011/106.5745_epoch_99.pkl
2023-05-11 19:13:59,436 - [*] loss:11972.8369
2023-05-11 19:13:59,452 - [*] year 2011, testing
2023-05-11 19:13:59,563 - T:3	MAE	104.0702	RMSE	107.1206	MAPE	366.4801
2023-05-11 19:13:59,740 - T:6	MAE	104.0785	RMSE	107.9003	MAPE	363.0353
2023-05-11 19:13:59,970 - T:12	MAE	104.1223	RMSE	109.7458	MAPE	354.6253
2023-05-11 19:13:59,971 - [*] Year 2012 load from data/district3F11T17/FastData/2012_30day.npz
2023-05-11 19:13:59,981 - [*] Year 2012 Dataset load!
2023-05-11 19:13:59,981 - [*] load from exp/district3F11T17/incremental-linear2023-05-11-19:12:20.342025/2011/best_model.pkl
2023-05-11 19:14:01,899 - [*] Year 2012 Training start
2023-05-11 19:14:02,026 - node number torch.Size([91520, 12])
2023-05-11 19:14:03,065 - epoch:0, training loss:103.9680 validation loss:104.4832
2023-05-11 19:14:04,130 - epoch:1, training loss:103.7174 validation loss:104.2061
2023-05-11 19:14:05,162 - epoch:2, training loss:103.4869 validation loss:103.9373
2023-05-11 19:14:06,191 - epoch:3, training loss:103.1754 validation loss:103.6524
2023-05-11 19:14:07,206 - epoch:4, training loss:102.8757 validation loss:103.3828
2023-05-11 19:14:08,225 - epoch:5, training loss:102.6459 validation loss:103.1101
2023-05-11 19:14:09,250 - epoch:6, training loss:102.3765 validation loss:102.8373
2023-05-11 19:14:10,266 - epoch:7, training loss:102.0753 validation loss:102.5659
2023-05-11 19:14:11,285 - epoch:8, training loss:101.8434 validation loss:102.3045
2023-05-11 19:14:12,302 - epoch:9, training loss:101.5303 validation loss:102.0253
2023-05-11 19:14:13,314 - epoch:10, training loss:101.2839 validation loss:101.7689
2023-05-11 19:14:14,323 - epoch:11, training loss:100.9892 validation loss:101.4996
2023-05-11 19:14:15,358 - epoch:12, training loss:100.7459 validation loss:101.2359
2023-05-11 19:14:16,363 - epoch:13, training loss:100.4869 validation loss:100.9818
2023-05-11 19:14:17,374 - epoch:14, training loss:100.2310 validation loss:100.7206
2023-05-11 19:14:18,400 - epoch:15, training loss:99.9583 validation loss:100.4621
2023-05-11 19:14:19,426 - epoch:16, training loss:99.7228 validation loss:100.2078
2023-05-11 19:14:20,445 - epoch:17, training loss:99.4611 validation loss:99.9543
2023-05-11 19:14:21,450 - epoch:18, training loss:99.2321 validation loss:99.7007
2023-05-11 19:14:22,471 - epoch:19, training loss:98.9247 validation loss:99.4492
2023-05-11 19:14:23,537 - epoch:20, training loss:98.6956 validation loss:99.1997
2023-05-11 19:14:24,552 - epoch:21, training loss:98.4565 validation loss:98.9525
2023-05-11 19:14:25,563 - epoch:22, training loss:98.1900 validation loss:98.7021
2023-05-11 19:14:26,565 - epoch:23, training loss:97.9482 validation loss:98.4553
2023-05-11 19:14:27,576 - epoch:24, training loss:97.7484 validation loss:98.2128
2023-05-11 19:14:28,591 - epoch:25, training loss:97.4466 validation loss:97.9700
2023-05-11 19:14:29,607 - epoch:26, training loss:97.2027 validation loss:97.7287
2023-05-11 19:14:30,631 - epoch:27, training loss:96.9635 validation loss:97.4895
2023-05-11 19:14:31,633 - epoch:28, training loss:96.7368 validation loss:97.2508
2023-05-11 19:14:32,692 - epoch:29, training loss:96.5102 validation loss:97.0118
2023-05-11 19:14:33,704 - epoch:30, training loss:96.2556 validation loss:96.7784
2023-05-11 19:14:34,733 - epoch:31, training loss:95.9920 validation loss:96.5441
2023-05-11 19:14:35,875 - epoch:32, training loss:95.7931 validation loss:96.3092
2023-05-11 19:14:36,888 - epoch:33, training loss:95.5103 validation loss:96.0755
2023-05-11 19:14:37,906 - epoch:34, training loss:95.3337 validation loss:95.8456
2023-05-11 19:14:38,924 - epoch:35, training loss:95.1133 validation loss:95.6174
2023-05-11 19:14:39,960 - epoch:36, training loss:94.8541 validation loss:95.3881
2023-05-11 19:14:40,970 - epoch:37, training loss:94.6395 validation loss:95.1563
2023-05-11 19:14:41,975 - epoch:38, training loss:94.4168 validation loss:94.9383
2023-05-11 19:14:42,982 - epoch:39, training loss:94.1743 validation loss:94.7098
2023-05-11 19:14:43,992 - epoch:40, training loss:93.9364 validation loss:94.4869
2023-05-11 19:14:45,024 - epoch:41, training loss:93.7503 validation loss:94.2623
2023-05-11 19:14:46,040 - epoch:42, training loss:93.5396 validation loss:94.0448
2023-05-11 19:14:47,066 - epoch:43, training loss:93.2818 validation loss:93.8195
2023-05-11 19:14:48,080 - epoch:44, training loss:93.0882 validation loss:93.6047
2023-05-11 19:14:49,091 - epoch:45, training loss:92.8519 validation loss:93.3851
2023-05-11 19:14:50,131 - epoch:46, training loss:92.6310 validation loss:93.1689
2023-05-11 19:14:51,151 - epoch:47, training loss:92.4459 validation loss:92.9578
2023-05-11 19:14:52,175 - epoch:48, training loss:92.2167 validation loss:92.7371
2023-05-11 19:14:53,223 - epoch:49, training loss:92.0087 validation loss:92.5299
2023-05-11 19:14:54,270 - epoch:50, training loss:91.7717 validation loss:92.3108
2023-05-11 19:14:55,302 - epoch:51, training loss:91.5593 validation loss:92.1053
2023-05-11 19:14:56,311 - epoch:52, training loss:91.3467 validation loss:91.8928
2023-05-11 19:14:57,327 - epoch:53, training loss:91.1552 validation loss:91.6824
2023-05-11 19:14:58,343 - epoch:54, training loss:90.9406 validation loss:91.4769
2023-05-11 19:14:59,355 - epoch:55, training loss:90.7310 validation loss:91.2701
2023-05-11 19:15:00,373 - epoch:56, training loss:90.5071 validation loss:91.0654
2023-05-11 19:15:01,385 - epoch:57, training loss:90.2880 validation loss:90.8565
2023-05-11 19:15:02,392 - epoch:58, training loss:90.1149 validation loss:90.6567
2023-05-11 19:15:03,425 - epoch:59, training loss:89.9177 validation loss:90.4518
2023-05-11 19:15:04,463 - epoch:60, training loss:89.6936 validation loss:90.2516
2023-05-11 19:15:05,479 - epoch:61, training loss:89.5025 validation loss:90.0484
2023-05-11 19:15:06,513 - epoch:62, training loss:89.2966 validation loss:89.8528
2023-05-11 19:15:07,530 - epoch:63, training loss:89.0710 validation loss:89.6566
2023-05-11 19:15:08,671 - epoch:64, training loss:88.9084 validation loss:89.4538
2023-05-11 19:15:09,737 - epoch:65, training loss:88.6892 validation loss:89.2634
2023-05-11 19:15:10,763 - epoch:66, training loss:88.5004 validation loss:89.0600
2023-05-11 19:15:11,776 - epoch:67, training loss:88.2945 validation loss:88.8642
2023-05-11 19:15:12,805 - epoch:68, training loss:88.1384 validation loss:88.6754
2023-05-11 19:15:13,821 - epoch:69, training loss:87.9212 validation loss:88.4798
2023-05-11 19:15:14,859 - epoch:70, training loss:87.7166 validation loss:88.2870
2023-05-11 19:15:15,879 - epoch:71, training loss:87.5380 validation loss:88.0964
2023-05-11 19:15:16,915 - epoch:72, training loss:87.3713 validation loss:87.9089
2023-05-11 19:15:17,965 - epoch:73, training loss:87.1312 validation loss:87.7122
2023-05-11 19:15:18,985 - epoch:74, training loss:86.9613 validation loss:87.5262
2023-05-11 19:15:20,005 - epoch:75, training loss:86.7816 validation loss:87.3396
2023-05-11 19:15:21,021 - epoch:76, training loss:86.6222 validation loss:87.1619
2023-05-11 19:15:22,023 - epoch:77, training loss:86.4010 validation loss:86.9645
2023-05-11 19:15:23,036 - epoch:78, training loss:86.2208 validation loss:86.7832
2023-05-11 19:15:24,057 - epoch:79, training loss:86.0632 validation loss:86.6060
2023-05-11 19:15:25,064 - epoch:80, training loss:85.8600 validation loss:86.4108
2023-05-11 19:15:26,085 - epoch:81, training loss:85.6860 validation loss:86.2324
2023-05-11 19:15:27,117 - epoch:82, training loss:85.4857 validation loss:86.0534
2023-05-11 19:15:28,136 - epoch:83, training loss:85.3271 validation loss:85.8724
2023-05-11 19:15:29,159 - epoch:84, training loss:85.1267 validation loss:85.6931
2023-05-11 19:15:30,179 - epoch:85, training loss:84.9488 validation loss:85.5124
2023-05-11 19:15:31,199 - epoch:86, training loss:84.7705 validation loss:85.3320
2023-05-11 19:15:32,229 - epoch:87, training loss:84.6012 validation loss:85.1480
2023-05-11 19:15:33,252 - epoch:88, training loss:84.4041 validation loss:84.9771
2023-05-11 19:15:34,288 - epoch:89, training loss:84.2194 validation loss:84.8018
2023-05-11 19:15:35,327 - epoch:90, training loss:84.0474 validation loss:84.6297
2023-05-11 19:15:36,358 - epoch:91, training loss:83.8805 validation loss:84.4541
2023-05-11 19:15:37,397 - epoch:92, training loss:83.7458 validation loss:84.2805
2023-05-11 19:15:38,437 - epoch:93, training loss:83.5518 validation loss:84.1037
2023-05-11 19:15:39,463 - epoch:94, training loss:83.3518 validation loss:83.9335
2023-05-11 19:15:40,478 - epoch:95, training loss:83.1840 validation loss:83.7628
2023-05-11 19:15:41,621 - epoch:96, training loss:83.0175 validation loss:83.5932
2023-05-11 19:15:42,643 - epoch:97, training loss:82.8360 validation loss:83.4185
2023-05-11 19:15:43,663 - epoch:98, training loss:82.6760 validation loss:83.2510
2023-05-11 19:15:44,680 - epoch:99, training loss:82.5217 validation loss:83.0841
2023-05-11 19:15:44,681 - Finished optimization, total time:69.78 s, best model:exp/district3F11T17/incremental-linear2023-05-11-19:12:20.342025/2012/83.0841_epoch_99.pkl
2023-05-11 19:15:45,026 - [*] loss:7017.5903
2023-05-11 19:15:45,047 - [*] year 2012, testing
2023-05-11 19:15:45,167 - T:3	MAE	78.7896	RMSE	81.5338	MAPE	286.9954
2023-05-11 19:15:45,366 - T:6	MAE	78.8619	RMSE	82.2576	MAPE	283.9083
2023-05-11 19:15:45,635 - T:12	MAE	79.0657	RMSE	84.0556	MAPE	276.5296
2023-05-11 19:15:45,636 - [*] Year 2013 load from data/district3F11T17/FastData/2013_30day.npz
2023-05-11 19:15:45,647 - [*] Year 2013 Dataset load!
2023-05-11 19:15:45,647 - [*] load from exp/district3F11T17/incremental-linear2023-05-11-19:12:20.342025/2012/best_model.pkl
2023-05-11 19:15:47,608 - [*] Year 2013 Training start
2023-05-11 19:15:47,732 - node number torch.Size([100608, 12])
2023-05-11 19:15:48,676 - epoch:0, training loss:82.4890 validation loss:86.1403
2023-05-11 19:15:49,736 - epoch:1, training loss:82.2975 validation loss:85.9571
2023-05-11 19:15:50,816 - epoch:2, training loss:82.1753 validation loss:85.7737
2023-05-11 19:15:51,874 - epoch:3, training loss:81.9529 validation loss:85.5862
2023-05-11 19:15:52,951 - epoch:4, training loss:81.7503 validation loss:85.4094
2023-05-11 19:15:54,023 - epoch:5, training loss:81.5738 validation loss:85.2261
2023-05-11 19:15:55,084 - epoch:6, training loss:81.3864 validation loss:85.0422
2023-05-11 19:15:56,162 - epoch:7, training loss:81.2006 validation loss:84.8643
2023-05-11 19:15:57,240 - epoch:8, training loss:81.0531 validation loss:84.6924
2023-05-11 19:15:58,331 - epoch:9, training loss:80.8668 validation loss:84.5073
2023-05-11 19:15:59,410 - epoch:10, training loss:80.6911 validation loss:84.3364
2023-05-11 19:16:00,505 - epoch:11, training loss:80.5131 validation loss:84.1615
2023-05-11 19:16:01,567 - epoch:12, training loss:80.3233 validation loss:83.9890
2023-05-11 19:16:02,665 - epoch:13, training loss:80.1433 validation loss:83.8113
2023-05-11 19:16:03,724 - epoch:14, training loss:80.0027 validation loss:83.6424
2023-05-11 19:16:04,798 - epoch:15, training loss:79.8375 validation loss:83.4737
2023-05-11 19:16:05,879 - epoch:16, training loss:79.6531 validation loss:83.3039
2023-05-11 19:16:06,948 - epoch:17, training loss:79.4964 validation loss:83.1376
2023-05-11 19:16:08,026 - epoch:18, training loss:79.3162 validation loss:82.9663
2023-05-11 19:16:09,125 - epoch:19, training loss:79.1189 validation loss:82.8035
2023-05-11 19:16:10,221 - epoch:20, training loss:78.9777 validation loss:82.6368
2023-05-11 19:16:11,287 - epoch:21, training loss:78.8246 validation loss:82.4720
2023-05-11 19:16:12,365 - epoch:22, training loss:78.6463 validation loss:82.3087
2023-05-11 19:16:13,455 - epoch:23, training loss:78.5139 validation loss:82.1541
2023-05-11 19:16:14,691 - epoch:24, training loss:78.3199 validation loss:81.9883
2023-05-11 19:16:15,779 - epoch:25, training loss:78.1787 validation loss:81.8307
2023-05-11 19:16:16,856 - epoch:26, training loss:78.0264 validation loss:81.6694
2023-05-11 19:16:17,949 - epoch:27, training loss:77.8360 validation loss:81.5106
2023-05-11 19:16:19,051 - epoch:28, training loss:77.6851 validation loss:81.3527
2023-05-11 19:16:20,146 - epoch:29, training loss:77.5253 validation loss:81.1991
2023-05-11 19:16:21,224 - epoch:30, training loss:77.3861 validation loss:81.0419
2023-05-11 19:16:22,297 - epoch:31, training loss:77.2210 validation loss:80.8843
2023-05-11 19:16:23,434 - epoch:32, training loss:77.0530 validation loss:80.7346
2023-05-11 19:16:24,512 - epoch:33, training loss:76.9079 validation loss:80.5840
2023-05-11 19:16:25,590 - epoch:34, training loss:76.7563 validation loss:80.4331
2023-05-11 19:16:26,662 - epoch:35, training loss:76.6137 validation loss:80.2794
2023-05-11 19:16:27,747 - epoch:36, training loss:76.4501 validation loss:80.1256
2023-05-11 19:16:28,829 - epoch:37, training loss:76.2982 validation loss:79.9812
2023-05-11 19:16:29,901 - epoch:38, training loss:76.1742 validation loss:79.8324
2023-05-11 19:16:30,988 - epoch:39, training loss:76.0097 validation loss:79.6844
2023-05-11 19:16:32,092 - epoch:40, training loss:75.8873 validation loss:79.5352
2023-05-11 19:16:33,175 - epoch:41, training loss:75.7089 validation loss:79.3929
2023-05-11 19:16:34,272 - epoch:42, training loss:75.5594 validation loss:79.2478
2023-05-11 19:16:35,366 - epoch:43, training loss:75.4297 validation loss:79.1040
