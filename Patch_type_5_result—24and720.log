2023-08-04 16:02:50,481 - logger name:exp/ECL-PatchTST2023-08-04-16:02:50.481098/ECL-PatchTST.log
2023-08-04 16:02:50,481 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-16:02:50.481098', 'path': 'exp/ECL-PatchTST2023-08-04-16:02:50.481098', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 16:02:50,481 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 16:02:50,678 - [*] phase 0 Dataset load!
2023-08-04 16:02:51,647 - [*] phase 0 Training start
train 8281
2023-08-04 16:03:03,916 - epoch:0, training loss:0.5472 validation loss:0.3054
train 8281
vs, vt 0.3054146449203077 0.318247155326864
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.21313134192124658 0.24177021743810695
2023-08-04 16:03:30,485 - epoch:1, training loss:0.4381 validation loss:0.2131
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.18069265701848528 0.19171883457380792
2023-08-04 16:03:49,801 - epoch:2, training loss:0.3233 validation loss:0.1807
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.17325786323003148 0.17075247398537138
2023-08-04 16:04:08,368 - epoch:3, training loss:0.2793 validation loss:0.1733
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.16867771275017573 0.16364618309813997
2023-08-04 16:04:27,300 - epoch:4, training loss:0.2607 validation loss:0.1687
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.16660775438598965 0.1612817839435909
2023-08-04 16:04:46,017 - epoch:5, training loss:0.2516 validation loss:0.1666
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.17031088260852772 0.1626253072982249
2023-08-04 16:05:04,999 - epoch:6, training loss:0.2449 validation loss:0.1703
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.16631172046713208 0.16323853948194048
2023-08-04 16:05:23,389 - epoch:7, training loss:0.2400 validation loss:0.1663
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.16685887102199637 0.1610547794919947
2023-08-04 16:05:43,032 - epoch:8, training loss:0.2357 validation loss:0.1669
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.17008605988129324 0.15933782412953998
2023-08-04 16:06:03,042 - epoch:9, training loss:0.2322 validation loss:0.1701
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.16917444490220235 0.16107898335094037
2023-08-04 16:06:21,711 - epoch:10, training loss:0.2287 validation loss:0.1692
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.17027778437604074 0.16028652858474982
2023-08-04 16:06:40,813 - epoch:11, training loss:0.2265 validation loss:0.1703
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.1707391897621362 0.1608753047235634
2023-08-04 16:07:00,274 - epoch:12, training loss:0.2221 validation loss:0.1707
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.1707020831172881 0.16046110265280888
2023-08-04 16:07:18,709 - epoch:13, training loss:0.2212 validation loss:0.1707
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.17254828969421593 0.16088737676972928
2023-08-04 16:07:37,994 - epoch:14, training loss:0.2186 validation loss:0.1725
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.17326275291650192 0.16100777458885443
2023-08-04 16:07:56,875 - epoch:15, training loss:0.2165 validation loss:0.1733
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.17206769744339195 0.16143628258420073
2023-08-04 16:08:16,480 - epoch:16, training loss:0.2151 validation loss:0.1721
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.17255585219549097 0.16060966820172642
2023-08-04 16:08:35,043 - epoch:17, training loss:0.2136 validation loss:0.1726
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.17367158230880034 0.1613834417708542
2023-08-04 16:08:54,311 - epoch:18, training loss:0.2113 validation loss:0.1737
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.17279809486606848 0.1614517610034217
2023-08-04 16:09:13,165 - epoch:19, training loss:0.2116 validation loss:0.1728
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.17336280320001685 0.1607551783647226
2023-08-04 16:09:31,702 - epoch:20, training loss:0.2091 validation loss:0.1734
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.17254191199722496 0.16106865571244902
2023-08-04 16:09:49,919 - epoch:21, training loss:0.2083 validation loss:0.1725
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.1739433592428332 0.16015953878345696
2023-08-04 16:10:09,357 - epoch:22, training loss:0.2072 validation loss:0.1739
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.1730768431140029 0.16094791176526443
2023-08-04 16:10:28,017 - epoch:23, training loss:0.2076 validation loss:0.1731
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.1731203559624112 0.160793406807858
2023-08-04 16:10:47,275 - epoch:24, training loss:0.2068 validation loss:0.1731
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.17348244543308797 0.16060083278495332
2023-08-04 16:11:05,733 - epoch:25, training loss:0.2069 validation loss:0.1735
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.17322846421081087 0.1606979574198308
2023-08-04 16:11:25,224 - epoch:26, training loss:0.2056 validation loss:0.1732
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.1734564927933009 0.16067519087506377
2023-08-04 16:11:43,956 - epoch:27, training loss:0.2052 validation loss:0.1735
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.1729488065061362 0.16060584718766419
2023-08-04 16:12:02,909 - epoch:28, training loss:0.2051 validation loss:0.1729
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.17311243405160698 0.16063962520464606
2023-08-04 16:12:21,276 - epoch:29, training loss:0.2056 validation loss:0.1731
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-16:02:50.481098/0/0.1663_epoch_7.pkl  &  0.15933782412953998
2023-08-04 16:12:23,050 - [*] loss:0.1663
2023-08-04 16:12:23,051 - [*] phase 0, testing
2023-08-04 16:12:23,059 - T:24	MAE	0.263155	RMSE	0.168498	MAPE	116.675234
2023-08-04 16:12:23,060 - 24	mae	0.2632	
2023-08-04 16:12:23,060 - 24	rmse	0.1685	
2023-08-04 16:12:23,060 - 24	mape	116.6752	
2023-08-04 16:12:24,278 - [*] loss:0.1593
2023-08-04 16:12:24,279 - [*] phase 0, testing
2023-08-04 16:12:24,289 - T:24	MAE	0.255466	RMSE	0.161488	MAPE	114.994669
2023-08-04 16:12:24,289 - 24	mae	0.2555	
2023-08-04 16:12:24,289 - 24	rmse	0.1615	
2023-08-04 16:12:24,289 - 24	mape	114.9947	
2023-08-04 16:12:26,423 - logger name:exp/ECL-PatchTST2023-08-04-16:12:26.423274/ECL-PatchTST.log
2023-08-04 16:12:26,423 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-16:12:26.423274', 'path': 'exp/ECL-PatchTST2023-08-04-16:12:26.423274', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 16:12:26,423 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 16:12:26,618 - [*] phase 0 Dataset load!
2023-08-04 16:12:27,595 - [*] phase 0 Training start
train 8281
2023-08-04 16:12:40,407 - epoch:0, training loss:0.5004 validation loss:0.3072
train 8281
vs, vt 0.3072404446809188 0.3342122979786085
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.22581226232907045 0.2712437090990336
2023-08-04 16:13:12,772 - epoch:1, training loss:7.8261 validation loss:0.2258
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.1849356343888718 0.21233370268474455
2023-08-04 16:13:36,905 - epoch:2, training loss:5.8824 validation loss:0.1849
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.171759817911231 0.20489638185371523
2023-08-04 16:14:01,348 - epoch:3, training loss:3.9592 validation loss:0.1718
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.16780171527162843 0.19369693634950597
2023-08-04 16:14:25,456 - epoch:4, training loss:2.6566 validation loss:0.1678
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.16654287796953451 0.18125242610340533
2023-08-04 16:14:49,885 - epoch:5, training loss:2.1091 validation loss:0.1665
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.169755634892246 0.18064192439550938
2023-08-04 16:15:14,726 - epoch:6, training loss:1.8274 validation loss:0.1698
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.16858991154510042 0.17364642590932225
2023-08-04 16:15:39,391 - epoch:7, training loss:1.6828 validation loss:0.1686
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.17083870830095332 0.18025693394567655
2023-08-04 16:16:03,309 - epoch:8, training loss:1.5999 validation loss:0.1708
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.1777783996709015 0.17916189522846884
2023-08-04 16:16:27,698 - epoch:9, training loss:1.5278 validation loss:0.1778
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.1720746966159862 0.17869188733722852
2023-08-04 16:16:52,564 - epoch:10, training loss:1.4885 validation loss:0.1721
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.1759210429761721 0.1836693902378497
2023-08-04 16:17:17,048 - epoch:11, training loss:1.3865 validation loss:0.1759
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.17399623487954555 0.18573115231550258
2023-08-04 16:17:40,927 - epoch:12, training loss:1.3510 validation loss:0.1740
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.1715374822201936 0.18202401908195537
2023-08-04 16:18:05,049 - epoch:13, training loss:1.2809 validation loss:0.1715
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.16966026972817339 0.17927963497198146
2023-08-04 16:18:28,940 - epoch:14, training loss:1.2220 validation loss:0.1697
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.16954318612166072 0.18512951307322667
2023-08-04 16:18:53,138 - epoch:15, training loss:1.2549 validation loss:0.1695
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.16878359278906946 0.18514598501117333
2023-08-04 16:19:18,095 - epoch:16, training loss:1.2063 validation loss:0.1688
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.16781188110294548 0.18198078708804172
2023-08-04 16:19:42,897 - epoch:17, training loss:1.1925 validation loss:0.1678
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.1672839091523834 0.18141203972956407
2023-08-04 16:20:06,338 - epoch:18, training loss:1.2059 validation loss:0.1673
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.16698923836583676 0.18270823829199956
2023-08-04 16:20:30,810 - epoch:19, training loss:1.2067 validation loss:0.1670
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.16701598060519798 0.18370126495542732
2023-08-04 16:20:55,587 - epoch:20, training loss:1.1817 validation loss:0.1670
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.16653876946024274 0.18378167955771738
2023-08-04 16:21:19,480 - epoch:21, training loss:1.1996 validation loss:0.1665
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.16648486985460573 0.18188443067281143
2023-08-04 16:21:43,877 - epoch:22, training loss:1.1494 validation loss:0.1665
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.1662535986498646 0.18481780637217604
2023-08-04 16:22:08,506 - epoch:23, training loss:1.1657 validation loss:0.1663
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.16610120933340944 0.18347774365026018
2023-08-04 16:22:32,224 - epoch:24, training loss:1.1603 validation loss:0.1661
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.16594487218105275 0.18392016819637755
2023-08-04 16:22:56,780 - epoch:25, training loss:1.1712 validation loss:0.1659
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.16614199705097987 0.18330398461093073
2023-08-04 16:23:21,222 - epoch:26, training loss:1.1524 validation loss:0.1661
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.16624271885856337 0.18518885710965033
2023-08-04 16:23:45,953 - epoch:27, training loss:1.1326 validation loss:0.1662
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.16606830433011055 0.1841255865343239
2023-08-04 16:24:09,770 - epoch:28, training loss:1.1715 validation loss:0.1661
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.16606664770971175 0.1834015881885653
2023-08-04 16:24:34,355 - epoch:29, training loss:1.1395 validation loss:0.1661
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-16:12:26.423274/0/0.1659_epoch_25.pkl  &  0.17364642590932225
2023-08-04 16:24:35,573 - [*] loss:0.1659
2023-08-04 16:24:35,574 - [*] phase 0, testing
2023-08-04 16:24:35,583 - T:24	MAE	0.261467	RMSE	0.168156	MAPE	113.411772
2023-08-04 16:24:35,583 - 24	mae	0.2615	
2023-08-04 16:24:35,583 - 24	rmse	0.1682	
2023-08-04 16:24:35,584 - 24	mape	113.4118	
2023-08-04 16:24:37,153 - [*] loss:0.1736
2023-08-04 16:24:37,154 - [*] phase 0, testing
2023-08-04 16:24:37,162 - T:24	MAE	0.271851	RMSE	0.175116	MAPE	117.187726
2023-08-04 16:24:37,163 - 24	mae	0.2719	
2023-08-04 16:24:37,163 - 24	rmse	0.1751	
2023-08-04 16:24:37,163 - 24	mape	117.1877	
2023-08-04 16:24:39,485 - logger name:exp/ECL-PatchTST2023-08-04-16:24:39.485149/ECL-PatchTST.log
2023-08-04 16:24:39,485 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-16:24:39.485149', 'path': 'exp/ECL-PatchTST2023-08-04-16:24:39.485149', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 16:24:39,485 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 16:24:39,676 - [*] phase 0 Dataset load!
2023-08-04 16:24:40,644 - [*] phase 0 Training start
train 8281
2023-08-04 16:24:53,138 - epoch:0, training loss:0.1831 validation loss:0.1379
train 8281
vs, vt 0.1379224541394607 0.1508067768553029
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10035326963533527 0.12298728428457094
2023-08-04 16:25:25,232 - epoch:1, training loss:0.5160 validation loss:0.1004
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08148129422055639 0.09680971900082153
2023-08-04 16:25:49,010 - epoch:2, training loss:0.4096 validation loss:0.0815
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07805090932094533 0.09284160780194013
2023-08-04 16:26:13,585 - epoch:3, training loss:0.3439 validation loss:0.0781
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.0764593196141979 0.08612049962191479
2023-08-04 16:26:37,785 - epoch:4, training loss:0.3021 validation loss:0.0765
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07625924042709496 0.08397665830410045
2023-08-04 16:27:02,617 - epoch:5, training loss:0.2782 validation loss:0.0763
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.0767479419870221 0.0834301476083372
2023-08-04 16:27:26,897 - epoch:6, training loss:0.2658 validation loss:0.0767
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07569108765734278 0.07886982935926189
2023-08-04 16:27:51,003 - epoch:7, training loss:0.2600 validation loss:0.0757
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07588751832752125 0.08328920084497203
2023-08-04 16:28:16,086 - epoch:8, training loss:0.2571 validation loss:0.0759
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07740185531261175 0.08145373695246551
2023-08-04 16:28:40,055 - epoch:9, training loss:0.2597 validation loss:0.0774
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07607578308038089 0.08196154738897862
2023-08-04 16:29:04,473 - epoch:10, training loss:0.2531 validation loss:0.0761
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07672545022290686 0.08339997672516367
2023-08-04 16:29:29,082 - epoch:11, training loss:0.2523 validation loss:0.0767
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07600251852494219 0.08544621819063374
2023-08-04 16:29:53,647 - epoch:12, training loss:0.2476 validation loss:0.0760
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07660160942570023 0.08216053117876468
2023-08-04 16:30:17,714 - epoch:13, training loss:0.2385 validation loss:0.0766
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07689520894833234 0.0818138470792252
2023-08-04 16:30:41,052 - epoch:14, training loss:0.2386 validation loss:0.0769
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07719129777472952 0.08125793925769952
2023-08-04 16:31:05,489 - epoch:15, training loss:0.2338 validation loss:0.0772
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07695451233050098 0.0816625844201316
2023-08-04 16:31:29,221 - epoch:16, training loss:0.2287 validation loss:0.0770
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07663973607122898 0.08119837299961111
2023-08-04 16:31:53,359 - epoch:17, training loss:0.2277 validation loss:0.0766
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.076972469849431 0.08098854942490226
2023-08-04 16:32:18,299 - epoch:18, training loss:0.2289 validation loss:0.0770
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07669937610626221 0.08128295271940854
2023-08-04 16:32:42,181 - epoch:19, training loss:0.2270 validation loss:0.0767
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07702273924065672 0.08133840075005656
2023-08-04 16:33:06,619 - epoch:20, training loss:0.2244 validation loss:0.0770
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07669443442769673 0.08076237530811974
2023-08-04 16:33:30,646 - epoch:21, training loss:0.2234 validation loss:0.0767
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07699675258734952 0.08057873777073363
2023-08-04 16:33:54,726 - epoch:22, training loss:0.2216 validation loss:0.0770
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07681584301526131 0.08100152436805808
2023-08-04 16:34:18,445 - epoch:23, training loss:0.2238 validation loss:0.0768
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07678284212622953 0.08089527317687221
2023-08-04 16:34:42,750 - epoch:24, training loss:0.2228 validation loss:0.0768
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07682453962447851 0.08082015545147916
2023-08-04 16:35:07,260 - epoch:25, training loss:0.2218 validation loss:0.0768
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07677225177378758 0.080485080814232
2023-08-04 16:35:31,233 - epoch:26, training loss:0.2204 validation loss:0.0768
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07683082127376743 0.0810287743806839
2023-08-04 16:35:55,706 - epoch:27, training loss:0.2212 validation loss:0.0768
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07676141887255337 0.08058996814424577
2023-08-04 16:36:19,346 - epoch:28, training loss:0.2218 validation loss:0.0768
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07678469027514043 0.08033096636443035
2023-08-04 16:36:44,092 - epoch:29, training loss:0.2215 validation loss:0.0768
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-16:24:39.485149/0/0.0757_epoch_7.pkl  &  0.07886982935926189
2023-08-04 16:36:45,891 - [*] loss:0.1611
2023-08-04 16:36:45,892 - [*] phase 0, testing
2023-08-04 16:36:45,901 - T:24	MAE	0.256901	RMSE	0.163377	MAPE	112.356055
2023-08-04 16:36:45,901 - 24	mae	0.2569	
2023-08-04 16:36:45,901 - 24	rmse	0.1634	
2023-08-04 16:36:45,901 - 24	mape	112.3561	
2023-08-04 16:36:46,912 - [*] loss:0.1674
2023-08-04 16:36:46,913 - [*] phase 0, testing
2023-08-04 16:36:46,922 - T:24	MAE	0.264572	RMSE	0.169669	MAPE	115.245855
2023-08-04 16:36:46,922 - 24	mae	0.2646	
2023-08-04 16:36:46,922 - 24	rmse	0.1697	
2023-08-04 16:36:46,922 - 24	mape	115.2459	
2023-08-04 16:36:49,076 - logger name:exp/ECL-PatchTST2023-08-04-16:36:49.075985/ECL-PatchTST.log
2023-08-04 16:36:49,076 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-16:36:49.075985', 'path': 'exp/ECL-PatchTST2023-08-04-16:36:49.075985', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 16:36:49,076 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 16:36:49,273 - [*] phase 0 Dataset load!
2023-08-04 16:36:50,249 - [*] phase 0 Training start
train 8281
2023-08-04 16:37:02,351 - epoch:0, training loss:0.2000 validation loss:0.1367
train 8281
vs, vt 0.13673778509964113 0.1439287234907565
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.09875239277987377 0.11190648312154024
2023-08-04 16:37:30,129 - epoch:1, training loss:1.3122 validation loss:0.0988
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08259314695454162 0.09040799664090508
2023-08-04 16:37:48,626 - epoch:2, training loss:1.0693 validation loss:0.0826
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07864360624681348 0.08059934048872927
2023-08-04 16:38:07,588 - epoch:3, training loss:0.8400 validation loss:0.0786
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07730839409581992 0.07711455647064291
2023-08-04 16:38:26,518 - epoch:4, training loss:0.6741 validation loss:0.0773
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.0766924781643826 0.0756454909949199
2023-08-04 16:38:45,871 - epoch:5, training loss:0.5820 validation loss:0.0767
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07749283621492593 0.07625020416858404
2023-08-04 16:39:04,732 - epoch:6, training loss:0.5322 validation loss:0.0775
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07652100386179012 0.07619548548498879
2023-08-04 16:39:23,396 - epoch:7, training loss:0.5051 validation loss:0.0765
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07642082351705302 0.07540522831613602
2023-08-04 16:39:43,392 - epoch:8, training loss:0.4832 validation loss:0.0764
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.0773338894157306 0.07540794194716474
2023-08-04 16:40:02,413 - epoch:9, training loss:0.4682 validation loss:0.0773
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07694005480279094 0.07547593991393628
2023-08-04 16:40:21,844 - epoch:10, training loss:0.4583 validation loss:0.0769
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.0761026716588632 0.07534047702084416
2023-08-04 16:40:40,591 - epoch:11, training loss:0.4458 validation loss:0.0761
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07576727575582007 0.07507741054439027
2023-08-04 16:41:00,106 - epoch:12, training loss:0.4386 validation loss:0.0758
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07552903898708198 0.0747311646523683
2023-08-04 16:41:19,044 - epoch:13, training loss:0.4341 validation loss:0.0755
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07593809432633546 0.07488824629589268
2023-08-04 16:41:38,165 - epoch:14, training loss:0.4284 validation loss:0.0759
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07569314053525096 0.07479855385811432
2023-08-04 16:41:57,060 - epoch:15, training loss:0.4250 validation loss:0.0757
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07580444297712782 0.07475608975990959
2023-08-04 16:42:16,441 - epoch:16, training loss:0.4180 validation loss:0.0758
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07601148270718429 0.07455995816575445
2023-08-04 16:42:36,056 - epoch:17, training loss:0.4180 validation loss:0.0760
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07575353307892448 0.0746744483223428
2023-08-04 16:42:54,959 - epoch:18, training loss:0.4164 validation loss:0.0758
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.0753528832093529 0.07472987221958845
2023-08-04 16:43:14,411 - epoch:19, training loss:0.4122 validation loss:0.0754
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07533568977985693 0.07438573808125827
2023-08-04 16:43:33,411 - epoch:20, training loss:0.4127 validation loss:0.0753
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07512832918892735 0.07444800979093365
2023-08-04 16:43:52,761 - epoch:21, training loss:0.4069 validation loss:0.0751
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07533570354723412 0.07416871481615564
2023-08-04 16:44:12,271 - epoch:22, training loss:0.4066 validation loss:0.0753
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07518506325457407 0.07420807728624862
2023-08-04 16:44:31,138 - epoch:23, training loss:0.4052 validation loss:0.0752
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07507672630574393 0.07426940649747849
2023-08-04 16:44:50,321 - epoch:24, training loss:0.4055 validation loss:0.0751
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07507515075090139 0.0741367067169884
2023-08-04 16:45:09,295 - epoch:25, training loss:0.4040 validation loss:0.0751
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07518141198417415 0.07407794269206731
2023-08-04 16:45:28,923 - epoch:26, training loss:0.4032 validation loss:0.0752
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07510214007419089 0.07410625497932019
2023-08-04 16:45:47,807 - epoch:27, training loss:0.4025 validation loss:0.0751
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07512740920419278 0.07408695020105528
2023-08-04 16:46:06,653 - epoch:28, training loss:0.4032 validation loss:0.0751
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07509596030349316 0.07407866361672463
2023-08-04 16:46:25,277 - epoch:29, training loss:0.4033 validation loss:0.0751
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-16:36:49.075985/0/0.0751_epoch_25.pkl  &  0.07407794269206731
2023-08-04 16:46:26,937 - [*] loss:0.1597
2023-08-04 16:46:26,939 - [*] phase 0, testing
2023-08-04 16:46:26,948 - T:24	MAE	0.254892	RMSE	0.161973	MAPE	114.432406
2023-08-04 16:46:26,948 - 24	mae	0.2549	
2023-08-04 16:46:26,948 - 24	rmse	0.1620	
2023-08-04 16:46:26,948 - 24	mape	114.4324	
2023-08-04 16:46:27,949 - [*] loss:0.1578
2023-08-04 16:46:27,951 - [*] phase 0, testing
2023-08-04 16:46:27,960 - T:24	MAE	0.251819	RMSE	0.160093	MAPE	113.275468
2023-08-04 16:46:27,961 - 24	mae	0.2518	
2023-08-04 16:46:27,961 - 24	rmse	0.1601	
2023-08-04 16:46:27,961 - 24	mape	113.2755	
2023-08-04 16:46:30,251 - logger name:exp/ECL-PatchTST2023-08-04-16:46:30.251053/ECL-PatchTST.log
2023-08-04 16:46:30,251 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-16:46:30.251053', 'path': 'exp/ECL-PatchTST2023-08-04-16:46:30.251053', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 16:46:30,251 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 16:46:30,453 - [*] phase 0 Dataset load!
2023-08-04 16:46:31,552 - [*] phase 0 Training start
train 8281
2023-08-04 16:46:46,871 - epoch:0, training loss:0.1831 validation loss:0.1379
train 8281
vs, vt 0.1379224541394607 0.1508067768553029
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10306255924312965 0.12300778828237367
2023-08-04 16:47:19,712 - epoch:1, training loss:2.8623 validation loss:0.1031
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08479165957997674 0.09772249084451924
2023-08-04 16:47:44,671 - epoch:2, training loss:2.2503 validation loss:0.0848
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07875752651497074 0.0941489121026319
2023-08-04 16:48:08,970 - epoch:3, training loss:1.5955 validation loss:0.0788
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07700427597307641 0.08688999825845593
2023-08-04 16:48:33,716 - epoch:4, training loss:1.1159 validation loss:0.0770
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07716159757388674 0.08419448485516984
2023-08-04 16:48:58,969 - epoch:5, training loss:0.8856 validation loss:0.0772
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07770582482866618 0.08533821514119273
2023-08-04 16:49:23,424 - epoch:6, training loss:0.7596 validation loss:0.0777
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07751739924044712 0.08015822092800037
2023-08-04 16:49:47,590 - epoch:7, training loss:0.6869 validation loss:0.0775
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07775620654549288 0.08223173605359119
2023-08-04 16:50:12,400 - epoch:8, training loss:0.6313 validation loss:0.0778
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.0804414903016194 0.08248722488465517
2023-08-04 16:50:37,108 - epoch:9, training loss:0.5915 validation loss:0.0804
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07866243839911792 0.08315809937598913
2023-08-04 16:51:01,149 - epoch:10, training loss:0.5603 validation loss:0.0787
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07871169613107391 0.08463138439085173
2023-08-04 16:51:25,943 - epoch:11, training loss:0.5113 validation loss:0.0787
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07904846043042514 0.08501974201720694
2023-08-04 16:51:50,966 - epoch:12, training loss:0.4929 validation loss:0.0790
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07808045751374701 0.08408095282704933
2023-08-04 16:52:15,601 - epoch:13, training loss:0.4733 validation loss:0.0781
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07695965752329516 0.0830783101365618
2023-08-04 16:52:39,795 - epoch:14, training loss:0.4425 validation loss:0.0770
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07728086225688457 0.08463420127720936
2023-08-04 16:53:03,886 - epoch:15, training loss:0.4388 validation loss:0.0773
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.0768499469951443 0.08439675046373969
2023-08-04 16:53:29,239 - epoch:16, training loss:0.4310 validation loss:0.0768
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07654609992776228 0.08376903702383456
2023-08-04 16:53:53,275 - epoch:17, training loss:0.4192 validation loss:0.0765
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07644507418508115 0.08267385229144407
2023-08-04 16:54:17,881 - epoch:18, training loss:0.4203 validation loss:0.0764
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07637480694962584 0.08411450862236645
2023-08-04 16:54:42,841 - epoch:19, training loss:0.4190 validation loss:0.0764
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07649194084755752 0.08386178760100967
2023-08-04 16:55:06,655 - epoch:20, training loss:0.4078 validation loss:0.0765
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07625848362627237 0.08433808523999609
2023-08-04 16:55:31,020 - epoch:21, training loss:0.4096 validation loss:0.0763
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07614495977759361 0.08310429290260958
2023-08-04 16:55:56,145 - epoch:22, training loss:0.4072 validation loss:0.0761
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07605899190125258 0.08433937710588393
2023-08-04 16:56:20,275 - epoch:23, training loss:0.4052 validation loss:0.0761
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07601340082676514 0.08393799983288931
2023-08-04 16:56:44,588 - epoch:24, training loss:0.4038 validation loss:0.0760
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07600165741599124 0.08400640620485596
2023-08-04 16:57:09,538 - epoch:25, training loss:0.4030 validation loss:0.0760
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07607819220942 0.08391489955070226
2023-08-04 16:57:33,711 - epoch:26, training loss:0.4054 validation loss:0.0761
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07609435187085815 0.08462201374704423
2023-08-04 16:57:58,033 - epoch:27, training loss:0.3987 validation loss:0.0761
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.0760390968264445 0.08417428268686585
2023-08-04 16:58:22,852 - epoch:28, training loss:0.4055 validation loss:0.0760
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07604436705941739 0.08390157353942809
2023-08-04 16:58:47,059 - epoch:29, training loss:0.4014 validation loss:0.0760
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-16:46:30.251053/0/0.076_epoch_25.pkl  &  0.08015822092800037
2023-08-04 16:58:48,355 - [*] loss:0.1618
2023-08-04 16:58:48,356 - [*] phase 0, testing
2023-08-04 16:58:48,365 - T:24	MAE	0.256113	RMSE	0.163987	MAPE	112.591517
2023-08-04 16:58:48,365 - 24	mae	0.2561	
2023-08-04 16:58:48,365 - 24	rmse	0.1640	
2023-08-04 16:58:48,365 - 24	mape	112.5915	
2023-08-04 16:58:50,398 - [*] loss:0.1699
2023-08-04 16:58:50,400 - [*] phase 0, testing
2023-08-04 16:58:50,408 - T:24	MAE	0.267209	RMSE	0.171993	MAPE	115.793586
2023-08-04 16:58:50,408 - 24	mae	0.2672	
2023-08-04 16:58:50,408 - 24	rmse	0.1720	
2023-08-04 16:58:50,408 - 24	mape	115.7936	
2023-08-04 16:58:52,426 - logger name:exp/ECL-PatchTST2023-08-04-16:58:52.426272/ECL-PatchTST.log
2023-08-04 16:58:52,426 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-16:58:52.426272', 'path': 'exp/ECL-PatchTST2023-08-04-16:58:52.426272', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 16:58:52,427 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 16:58:52,634 - [*] phase 0 Dataset load!
2023-08-04 16:58:53,597 - [*] phase 0 Training start
train 8281
2023-08-04 16:59:05,798 - epoch:0, training loss:0.5504 validation loss:0.3015
train 8281
vs, vt 0.3015339902561644 0.31067220704711
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.21800675972000413 0.23808618116637933
2023-08-04 16:59:33,054 - epoch:1, training loss:0.4378 validation loss:0.2180
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.18271404747729716 0.19301421243859374
2023-08-04 16:59:52,163 - epoch:2, training loss:0.3311 validation loss:0.1827
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.17265538919879042 0.17245714674177376
2023-08-04 17:00:12,187 - epoch:3, training loss:0.2829 validation loss:0.1727
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.1693479277841423 0.16441149718087653
2023-08-04 17:00:31,533 - epoch:4, training loss:0.2643 validation loss:0.1693
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.16761297603016315 0.16439700337207835
2023-08-04 17:00:50,319 - epoch:5, training loss:0.2544 validation loss:0.1676
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.16706161881270615 0.160809474794761
2023-08-04 17:01:09,630 - epoch:6, training loss:0.2465 validation loss:0.1671
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.1681578492988711 0.16004909947514534
2023-08-04 17:01:27,901 - epoch:7, training loss:0.2421 validation loss:0.1682
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.16903230450723483 0.16086598696268123
2023-08-04 17:01:47,233 - epoch:8, training loss:0.2374 validation loss:0.1690
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.16764498129487038 0.16133730955745862
2023-08-04 17:02:05,864 - epoch:9, training loss:0.2340 validation loss:0.1676
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.16827940276783446 0.15945154687632684
2023-08-04 17:02:25,897 - epoch:10, training loss:0.2305 validation loss:0.1683
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.17157244099223096 0.1600074972147527
2023-08-04 17:02:44,759 - epoch:11, training loss:0.2269 validation loss:0.1716
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.17318687701354857 0.16003848709489987
2023-08-04 17:03:04,142 - epoch:12, training loss:0.2236 validation loss:0.1732
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.17192906125084215 0.16045407238213913
2023-08-04 17:03:22,882 - epoch:13, training loss:0.2207 validation loss:0.1719
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.17460998390679774 0.15895586509419524
2023-08-04 17:03:42,306 - epoch:14, training loss:0.2197 validation loss:0.1746
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.17319361752142076 0.15984393024574156
2023-08-04 17:04:00,719 - epoch:15, training loss:0.2157 validation loss:0.1732
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.1725588113717411 0.1588121762742167
2023-08-04 17:04:20,220 - epoch:16, training loss:0.2148 validation loss:0.1726
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.17413204584432684 0.15864157919650493
2023-08-04 17:04:39,197 - epoch:17, training loss:0.2127 validation loss:0.1741
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.17444471124073732 0.15943282826439195
2023-08-04 17:04:58,680 - epoch:18, training loss:0.2117 validation loss:0.1744
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.1737711074883523 0.15945494142563446
2023-08-04 17:05:17,491 - epoch:19, training loss:0.2098 validation loss:0.1738
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.17334771123917206 0.15854452372245167
2023-08-04 17:05:36,564 - epoch:20, training loss:0.2082 validation loss:0.1733
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.17420914995929468 0.15869535408590152
2023-08-04 17:05:55,785 - epoch:21, training loss:0.2075 validation loss:0.1742
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.17293084007890328 0.15884003464294516
2023-08-04 17:06:15,363 - epoch:22, training loss:0.2068 validation loss:0.1729
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.17381384887773058 0.15832961269694826
2023-08-04 17:06:34,086 - epoch:23, training loss:0.2055 validation loss:0.1738
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.17362714251098427 0.1587067191367564
2023-08-04 17:06:53,296 - epoch:24, training loss:0.2057 validation loss:0.1736
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.17350227499137755 0.1585274902374848
2023-08-04 17:07:12,038 - epoch:25, training loss:0.2050 validation loss:0.1735
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.17321235816115918 0.15848414437926334
2023-08-04 17:07:31,361 - epoch:26, training loss:0.2052 validation loss:0.1732
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.17344962125239166 0.1585131339404894
2023-08-04 17:07:50,025 - epoch:27, training loss:0.2050 validation loss:0.1734
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.1732862837936567 0.15848651576949202
2023-08-04 17:08:09,062 - epoch:28, training loss:0.2041 validation loss:0.1733
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.1734368299012599 0.1585058584485365
2023-08-04 17:08:27,912 - epoch:29, training loss:0.2038 validation loss:0.1734
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-16:58:52.426272/0/0.1671_epoch_6.pkl  &  0.15832961269694826
2023-08-04 17:08:29,214 - [*] loss:0.1671
2023-08-04 17:08:29,215 - [*] phase 0, testing
2023-08-04 17:08:29,225 - T:24	MAE	0.263950	RMSE	0.169279	MAPE	120.667815
2023-08-04 17:08:29,226 - 24	mae	0.2640	
2023-08-04 17:08:29,226 - 24	rmse	0.1693	
2023-08-04 17:08:29,226 - 24	mape	120.6678	
2023-08-04 17:08:30,485 - [*] loss:0.1583
2023-08-04 17:08:30,486 - [*] phase 0, testing
2023-08-04 17:08:30,494 - T:24	MAE	0.253904	RMSE	0.160600	MAPE	113.882422
2023-08-04 17:08:30,495 - 24	mae	0.2539	
2023-08-04 17:08:30,495 - 24	rmse	0.1606	
2023-08-04 17:08:30,495 - 24	mape	113.8824	
2023-08-04 17:08:32,607 - logger name:exp/ECL-PatchTST2023-08-04-17:08:32.606867/ECL-PatchTST.log
2023-08-04 17:08:32,607 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-17:08:32.606867', 'path': 'exp/ECL-PatchTST2023-08-04-17:08:32.606867', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 17:08:32,607 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 17:08:32,803 - [*] phase 0 Dataset load!
2023-08-04 17:08:33,765 - [*] phase 0 Training start
train 8281
2023-08-04 17:08:46,180 - epoch:0, training loss:0.4958 validation loss:0.2965
train 8281
vs, vt 0.29652937534062757 0.33546551381764206
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.22483758029082548 0.2984701229826264
2023-08-04 17:09:18,839 - epoch:1, training loss:7.4612 validation loss:0.2248
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.18623299232643584 0.22906066056178964
2023-08-04 17:09:46,187 - epoch:2, training loss:5.5794 validation loss:0.1862
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.17229684004965035 0.1956814234347447
2023-08-04 17:10:12,251 - epoch:3, training loss:3.7405 validation loss:0.1723
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.16775292271505232 0.1857950940080311
2023-08-04 17:10:37,372 - epoch:4, training loss:2.4748 validation loss:0.1678
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.1669305247133193 0.17305383685490358
2023-08-04 17:11:03,141 - epoch:5, training loss:1.9203 validation loss:0.1669
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.16783993318676949 0.17291549010121304
2023-08-04 17:11:28,428 - epoch:6, training loss:1.6705 validation loss:0.1678
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.17132703214883804 0.18624570625631706
2023-08-04 17:11:53,686 - epoch:7, training loss:1.5529 validation loss:0.1713
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.17507714373262032 0.20013280893149582
2023-08-04 17:12:17,818 - epoch:8, training loss:1.4928 validation loss:0.1751
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.17376679603172385 0.19203190408323123
2023-08-04 17:12:45,041 - epoch:9, training loss:1.4899 validation loss:0.1738
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.17248823477522188 0.18929903257800185
2023-08-04 17:13:14,198 - epoch:10, training loss:1.3721 validation loss:0.1725
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.17155542273236357 0.1961595058117224
2023-08-04 17:13:39,394 - epoch:11, training loss:1.3002 validation loss:0.1716
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.1709197885964228 0.19996463021506433
2023-08-04 17:14:04,093 - epoch:12, training loss:1.2621 validation loss:0.1709
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.16934869438409805 0.2066418470247932
2023-08-04 17:14:36,273 - epoch:13, training loss:1.2195 validation loss:0.1693
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.16896347750140273 0.20457896395869876
2023-08-04 17:15:02,182 - epoch:14, training loss:1.2155 validation loss:0.1690
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.1684220259928185 0.2030293394042098
2023-08-04 17:15:31,626 - epoch:15, training loss:1.1938 validation loss:0.1684
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.16821349849519524 0.21193909612686737
2023-08-04 17:15:57,739 - epoch:16, training loss:1.1928 validation loss:0.1682
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.16721250926670822 0.20358829993916594
2023-08-04 17:16:22,725 - epoch:17, training loss:1.1528 validation loss:0.1672
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.16731925292507463 0.2061643961655057
2023-08-04 17:16:47,135 - epoch:18, training loss:1.1674 validation loss:0.1673
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.16745052810596384 0.2034374736249447
2023-08-04 17:17:11,488 - epoch:19, training loss:1.1518 validation loss:0.1675
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.16717435516740964 0.20401412831700366
2023-08-04 17:17:36,283 - epoch:20, training loss:1.1301 validation loss:0.1672
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.1670744137271591 0.20459198044693988
2023-08-04 17:18:00,252 - epoch:21, training loss:1.1283 validation loss:0.1671
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.16640795443368994 0.20127696932657904
2023-08-04 17:18:25,042 - epoch:22, training loss:1.1323 validation loss:0.1664
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.16664363864971243 0.20452979644355568
2023-08-04 17:18:50,425 - epoch:23, training loss:1.1519 validation loss:0.1666
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.16634684023649796 0.20518314482077307
2023-08-04 17:19:14,611 - epoch:24, training loss:1.1174 validation loss:0.1663
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.16655854459690012 0.20584828924873602
2023-08-04 17:19:39,112 - epoch:25, training loss:1.1374 validation loss:0.1666
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.1663103587925434 0.20375423635477605
2023-08-04 17:20:04,134 - epoch:26, training loss:1.1145 validation loss:0.1663
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.16631133465663248 0.2043508534198222
2023-08-04 17:20:28,358 - epoch:27, training loss:1.1145 validation loss:0.1663
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.16638102916919667 0.20565417992032092
2023-08-04 17:20:53,007 - epoch:28, training loss:1.0969 validation loss:0.1664
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.16633197859577512 0.20434231877974843
2023-08-04 17:21:17,331 - epoch:29, training loss:1.1240 validation loss:0.1663
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-17:08:32.606867/0/0.1663_epoch_26.pkl  &  0.17291549010121304
2023-08-04 17:21:18,641 - [*] loss:0.1663
2023-08-04 17:21:18,642 - [*] phase 0, testing
2023-08-04 17:21:18,651 - T:24	MAE	0.262161	RMSE	0.168670	MAPE	113.522422
2023-08-04 17:21:18,651 - 24	mae	0.2622	
2023-08-04 17:21:18,651 - 24	rmse	0.1687	
2023-08-04 17:21:18,651 - 24	mape	113.5224	
2023-08-04 17:21:19,776 - [*] loss:0.1729
2023-08-04 17:21:19,778 - [*] phase 0, testing
2023-08-04 17:21:19,787 - T:24	MAE	0.271848	RMSE	0.174916	MAPE	116.334224
2023-08-04 17:21:19,787 - 24	mae	0.2718	
2023-08-04 17:21:19,787 - 24	rmse	0.1749	
2023-08-04 17:21:19,787 - 24	mape	116.3342	
2023-08-04 17:21:21,967 - logger name:exp/ECL-PatchTST2023-08-04-17:21:21.966683/ECL-PatchTST.log
2023-08-04 17:21:21,967 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-17:21:21.966683', 'path': 'exp/ECL-PatchTST2023-08-04-17:21:21.966683', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 17:21:21,967 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 17:21:22,162 - [*] phase 0 Dataset load!
2023-08-04 17:21:23,156 - [*] phase 0 Training start
train 8281
2023-08-04 17:21:35,734 - epoch:0, training loss:0.1810 validation loss:0.1336
train 8281
vs, vt 0.13355287617963293 0.1517282784309076
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.09933155167685903 0.1335544290587954
2023-08-04 17:22:09,458 - epoch:1, training loss:0.5241 validation loss:0.0993
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08109408552231996 0.10530209500828515
2023-08-04 17:22:33,239 - epoch:2, training loss:0.4210 validation loss:0.0811
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07783296177892582 0.09242715087273846
2023-08-04 17:22:57,807 - epoch:3, training loss:0.3527 validation loss:0.0778
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07688207576132339 0.08926458794461645
2023-08-04 17:23:21,967 - epoch:4, training loss:0.3059 validation loss:0.0769
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07601215641783632 0.08422808503003223
2023-08-04 17:23:46,600 - epoch:5, training loss:0.2801 validation loss:0.0760
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07595832370545554 0.07929832318230816
2023-08-04 17:24:10,636 - epoch:6, training loss:0.2679 validation loss:0.0760
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07603678172049315 0.0811506134012471
2023-08-04 17:24:35,232 - epoch:7, training loss:0.2618 validation loss:0.0760
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07631481365989083 0.08674836758038272
2023-08-04 17:24:59,300 - epoch:8, training loss:0.2586 validation loss:0.0763
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07591615714456725 0.08351598922973094
2023-08-04 17:25:23,830 - epoch:9, training loss:0.2621 validation loss:0.0759
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07568517189634882 0.08314642026696516
2023-08-04 17:25:48,453 - epoch:10, training loss:0.2596 validation loss:0.0757
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07636632162915624 0.08479992567521075
2023-08-04 17:26:13,067 - epoch:11, training loss:0.2614 validation loss:0.0764
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07708677754777929 0.08616079292867494
2023-08-04 17:26:36,914 - epoch:12, training loss:0.2589 validation loss:0.0771
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.076623334346906 0.08548529632389545
2023-08-04 17:27:01,893 - epoch:13, training loss:0.2580 validation loss:0.0766
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.0770247743829437 0.08671865102065646
2023-08-04 17:27:25,848 - epoch:14, training loss:0.2586 validation loss:0.0770
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07666882815892281 0.08579486770474393
2023-08-04 17:27:50,529 - epoch:15, training loss:0.2509 validation loss:0.0767
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07648936221780984 0.08667768586588942
2023-08-04 17:28:15,824 - epoch:16, training loss:0.2507 validation loss:0.0765
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.077117752769719 0.08516484134547088
2023-08-04 17:28:39,989 - epoch:17, training loss:0.2439 validation loss:0.0771
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07677755480551202 0.08482415463937365
2023-08-04 17:29:04,654 - epoch:18, training loss:0.2442 validation loss:0.0768
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07710106071570645 0.08510279169549113
2023-08-04 17:29:28,591 - epoch:19, training loss:0.2460 validation loss:0.0771
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07682110685045304 0.08520607310144798
2023-08-04 17:29:53,259 - epoch:20, training loss:0.2442 validation loss:0.0768
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07694288351289603 0.0845205967678972
2023-08-04 17:30:17,202 - epoch:21, training loss:0.2432 validation loss:0.0769
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07674989752147508 0.08299790654817353
2023-08-04 17:30:41,956 - epoch:22, training loss:0.2404 validation loss:0.0767
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.0767207403221856 0.08386745190490848
2023-08-04 17:31:05,486 - epoch:23, training loss:0.2436 validation loss:0.0767
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07680394856826118 0.08409891860640567
2023-08-04 17:31:29,943 - epoch:24, training loss:0.2395 validation loss:0.0768
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07684274633293567 0.08426033284353174
2023-08-04 17:31:53,588 - epoch:25, training loss:0.2405 validation loss:0.0768
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.0768287368118763 0.08385698116667893
2023-08-04 17:32:18,478 - epoch:26, training loss:0.2377 validation loss:0.0768
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07685431287340495 0.08379068549560464
2023-08-04 17:32:42,723 - epoch:27, training loss:0.2376 validation loss:0.0769
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07683610033405863 0.08425425365567207
2023-08-04 17:33:07,006 - epoch:28, training loss:0.2379 validation loss:0.0768
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.076848813859017 0.0838655521688254
2023-08-04 17:33:30,689 - epoch:29, training loss:0.2392 validation loss:0.0768
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-17:21:21.966683/0/0.0757_epoch_10.pkl  &  0.07929832318230816
2023-08-04 17:33:32,452 - [*] loss:0.1617
2023-08-04 17:33:32,453 - [*] phase 0, testing
2023-08-04 17:33:32,461 - T:24	MAE	0.255844	RMSE	0.164029	MAPE	113.584065
2023-08-04 17:33:32,462 - 24	mae	0.2558	
2023-08-04 17:33:32,462 - 24	rmse	0.1640	
2023-08-04 17:33:32,462 - 24	mape	113.5841	
2023-08-04 17:33:33,427 - [*] loss:0.1676
2023-08-04 17:33:33,428 - [*] phase 0, testing
2023-08-04 17:33:33,437 - T:24	MAE	0.266718	RMSE	0.169142	MAPE	115.635693
2023-08-04 17:33:33,437 - 24	mae	0.2667	
2023-08-04 17:33:33,437 - 24	rmse	0.1691	
2023-08-04 17:33:33,437 - 24	mape	115.6357	
2023-08-04 17:33:35,596 - logger name:exp/ECL-PatchTST2023-08-04-17:33:35.596551/ECL-PatchTST.log
2023-08-04 17:33:35,597 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-17:33:35.596551', 'path': 'exp/ECL-PatchTST2023-08-04-17:33:35.596551', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 17:33:35,597 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 17:33:35,789 - [*] phase 0 Dataset load!
2023-08-04 17:33:36,789 - [*] phase 0 Training start
train 8281
2023-08-04 17:33:48,784 - epoch:0, training loss:0.1993 validation loss:0.1351
train 8281
vs, vt 0.13509844648449318 0.140629466785037
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10031487201542957 0.11015384717156058
2023-08-04 17:34:16,489 - epoch:1, training loss:1.2883 validation loss:0.1003
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08324423764386903 0.09052359571923381
2023-08-04 17:34:35,396 - epoch:2, training loss:1.0477 validation loss:0.0832
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07881051747371322 0.08065101585310439
2023-08-04 17:34:55,079 - epoch:3, training loss:0.8237 validation loss:0.0788
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07778314395767191 0.07693301999698514
2023-08-04 17:35:14,536 - epoch:4, training loss:0.6690 validation loss:0.0778
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.0768091377356778 0.07690356113016605
2023-08-04 17:35:34,134 - epoch:5, training loss:0.5844 validation loss:0.0768
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07623968994163949 0.07588828128317128
2023-08-04 17:35:53,097 - epoch:6, training loss:0.5363 validation loss:0.0762
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07664411482603653 0.07512930667270785
2023-08-04 17:36:12,838 - epoch:7, training loss:0.5081 validation loss:0.0766
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07713280367138593 0.07517154522888038
2023-08-04 17:36:31,845 - epoch:8, training loss:0.4875 validation loss:0.0771
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07627779399247273 0.07563049913100574
2023-08-04 17:36:51,177 - epoch:9, training loss:0.4735 validation loss:0.0763
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07579756989751173 0.07519815003742343
2023-08-04 17:37:09,806 - epoch:10, training loss:0.4596 validation loss:0.0758
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07598908367040365 0.07519306534010431
2023-08-04 17:37:29,388 - epoch:11, training loss:0.4521 validation loss:0.0760
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07644361873035846 0.0750216640855955
2023-08-04 17:37:48,598 - epoch:12, training loss:0.4447 validation loss:0.0764
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07621805353657059 0.0753765835872163
2023-08-04 17:38:08,421 - epoch:13, training loss:0.4355 validation loss:0.0762
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07612803270635397 0.075010368924426
2023-08-04 17:38:27,572 - epoch:14, training loss:0.4314 validation loss:0.0761
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07543810967193998 0.07503368988957095
2023-08-04 17:38:46,763 - epoch:15, training loss:0.4251 validation loss:0.0754
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07561602402964364 0.07448558752303538
2023-08-04 17:39:05,809 - epoch:16, training loss:0.4209 validation loss:0.0756
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07544399085252182 0.07453662813033747
2023-08-04 17:39:25,026 - epoch:17, training loss:0.4187 validation loss:0.0754
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07543599492181903 0.0747046587259873
2023-08-04 17:39:44,082 - epoch:18, training loss:0.4154 validation loss:0.0754
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07561120868701002 0.07448859950122626
2023-08-04 17:40:03,882 - epoch:19, training loss:0.4126 validation loss:0.0756
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07526272089908952 0.07443648900674738
2023-08-04 17:40:22,951 - epoch:20, training loss:0.4107 validation loss:0.0753
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07525133268664712 0.07429193672926529
2023-08-04 17:40:42,355 - epoch:21, training loss:0.4080 validation loss:0.0753
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07503009798086208 0.07431484532097112
2023-08-04 17:41:01,549 - epoch:22, training loss:0.4065 validation loss:0.0750
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07501193330339763 0.07399326514290727
2023-08-04 17:41:21,053 - epoch:23, training loss:0.4037 validation loss:0.0750
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07515946014419846 0.07406378699385602
2023-08-04 17:41:40,193 - epoch:24, training loss:0.4046 validation loss:0.0752
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.0751804587147806 0.07411387465570284
2023-08-04 17:41:59,579 - epoch:25, training loss:0.4043 validation loss:0.0752
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07502579211216906 0.07403198305679404
2023-08-04 17:42:18,658 - epoch:26, training loss:0.4032 validation loss:0.0750
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07510231200443662 0.07397127872251946
2023-08-04 17:42:38,454 - epoch:27, training loss:0.4028 validation loss:0.0751
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07509256810273814 0.07399640013666256
2023-08-04 17:42:58,494 - epoch:28, training loss:0.4024 validation loss:0.0751
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07507836373279923 0.07398593077517074
2023-08-04 17:43:17,296 - epoch:29, training loss:0.4021 validation loss:0.0751
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-17:33:35.596551/0/0.075_epoch_23.pkl  &  0.07397127872251946
2023-08-04 17:43:19,092 - [*] loss:0.1594
2023-08-04 17:43:19,093 - [*] phase 0, testing
2023-08-04 17:43:19,103 - T:24	MAE	0.255141	RMSE	0.161648	MAPE	113.802660
2023-08-04 17:43:19,103 - 24	mae	0.2551	
2023-08-04 17:43:19,103 - 24	rmse	0.1616	
2023-08-04 17:43:19,103 - 24	mape	113.8027	
2023-08-04 17:43:20,187 - [*] loss:0.1574
2023-08-04 17:43:20,188 - [*] phase 0, testing
2023-08-04 17:43:20,197 - T:24	MAE	0.251736	RMSE	0.159711	MAPE	113.387275
2023-08-04 17:43:20,197 - 24	mae	0.2517	
2023-08-04 17:43:20,197 - 24	rmse	0.1597	
2023-08-04 17:43:20,197 - 24	mape	113.3873	
2023-08-04 17:43:22,310 - logger name:exp/ECL-PatchTST2023-08-04-17:43:22.310069/ECL-PatchTST.log
2023-08-04 17:43:22,310 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-17:43:22.310069', 'path': 'exp/ECL-PatchTST2023-08-04-17:43:22.310069', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 17:43:22,310 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 17:43:22,504 - [*] phase 0 Dataset load!
2023-08-04 17:43:23,489 - [*] phase 0 Training start
train 8281
2023-08-04 17:43:35,560 - epoch:0, training loss:0.1810 validation loss:0.1336
train 8281
vs, vt 0.13355287617963293 0.1517282784309076
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10223970311167448 0.1335687896479731
2023-08-04 17:44:08,096 - epoch:1, training loss:2.7542 validation loss:0.1022
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08548703149932882 0.10600238204326319
2023-08-04 17:44:33,006 - epoch:2, training loss:2.1589 validation loss:0.0855
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.0790059070872224 0.09174610390935255
2023-08-04 17:44:57,224 - epoch:3, training loss:1.5198 validation loss:0.0790
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07768009975552559 0.08886511486185633
2023-08-04 17:45:21,624 - epoch:4, training loss:1.0327 validation loss:0.0777
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07694233945854333 0.08375876332106798
2023-08-04 17:45:45,983 - epoch:5, training loss:0.8047 validation loss:0.0769
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07728395359995573 0.08224682558489882
2023-08-04 17:46:10,752 - epoch:6, training loss:0.6970 validation loss:0.0773
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07878536042635856 0.08808271489713503
2023-08-04 17:46:35,412 - epoch:7, training loss:0.6375 validation loss:0.0788
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07959793451363625 0.09440951357069223
2023-08-04 17:47:00,074 - epoch:8, training loss:0.5842 validation loss:0.0796
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07842850587938142 0.08968921467337919
2023-08-04 17:47:24,440 - epoch:9, training loss:0.5440 validation loss:0.0784
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07813072277475959 0.09090414457023144
2023-08-04 17:47:49,059 - epoch:10, training loss:0.5014 validation loss:0.0781
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07807776550559895 0.09076204264293546
2023-08-04 17:48:13,439 - epoch:11, training loss:0.4736 validation loss:0.0781
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07769173480894255 0.09340783241002457
2023-08-04 17:48:37,823 - epoch:12, training loss:0.4543 validation loss:0.0777
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07713787694988043 0.09169705681826758
2023-08-04 17:49:02,367 - epoch:13, training loss:0.4380 validation loss:0.0771
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07708054934830769 0.09153951421056104
2023-08-04 17:49:27,280 - epoch:14, training loss:0.4407 validation loss:0.0771
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07675437774995099 0.0934316986442908
2023-08-04 17:49:51,593 - epoch:15, training loss:0.4312 validation loss:0.0768
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07653564538644708 0.09403576499418072
2023-08-04 17:50:16,997 - epoch:16, training loss:0.4121 validation loss:0.0765
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07624151120367258 0.09219695271357246
2023-08-04 17:50:41,920 - epoch:17, training loss:0.4063 validation loss:0.0762
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07639657787006834 0.09176857121612715
2023-08-04 17:51:06,114 - epoch:18, training loss:0.3998 validation loss:0.0764
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07631849643328915 0.09163191703998524
2023-08-04 17:51:31,266 - epoch:19, training loss:0.3991 validation loss:0.0763
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07612441010449243 0.09229572449365388
2023-08-04 17:51:56,494 - epoch:20, training loss:0.3924 validation loss:0.0761
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07612971759036831 0.09252286856265171
2023-08-04 17:52:20,304 - epoch:21, training loss:0.3904 validation loss:0.0761
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07594513042789439 0.09125112369656563
2023-08-04 17:52:44,783 - epoch:22, training loss:0.3868 validation loss:0.0759
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07600477046292761 0.0924914946536655
2023-08-04 17:53:08,783 - epoch:23, training loss:0.3946 validation loss:0.0760
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07590198800291705 0.09235888978709346
2023-08-04 17:53:33,685 - epoch:24, training loss:0.3868 validation loss:0.0759
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07594001374166945 0.09232019665448563
2023-08-04 17:53:58,784 - epoch:25, training loss:0.3879 validation loss:0.0759
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07590637080695319 0.09232041272132294
2023-08-04 17:54:23,735 - epoch:26, training loss:0.3823 validation loss:0.0759
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07587551438938016 0.09215300513998322
2023-08-04 17:54:47,889 - epoch:27, training loss:0.3830 validation loss:0.0759
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07590782925810503 0.09278483154333156
2023-08-04 17:55:12,814 - epoch:28, training loss:0.3818 validation loss:0.0759
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07588166411480178 0.09218714167566402
2023-08-04 17:55:36,885 - epoch:29, training loss:0.3880 validation loss:0.0759
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-17:43:22.310069/0/0.0759_epoch_27.pkl  &  0.08224682558489882
2023-08-04 17:55:38,579 - [*] loss:0.1616
2023-08-04 17:55:38,580 - [*] phase 0, testing
2023-08-04 17:55:38,589 - T:24	MAE	0.255650	RMSE	0.163905	MAPE	112.715983
2023-08-04 17:55:38,589 - 24	mae	0.2556	
2023-08-04 17:55:38,589 - 24	rmse	0.1639	
2023-08-04 17:55:38,589 - 24	mape	112.7160	
2023-08-04 17:55:39,924 - [*] loss:0.1739
2023-08-04 17:55:39,925 - [*] phase 0, testing
2023-08-04 17:55:39,935 - T:24	MAE	0.273058	RMSE	0.175696	MAPE	115.316057
2023-08-04 17:55:39,935 - 24	mae	0.2731	
2023-08-04 17:55:39,935 - 24	rmse	0.1757	
2023-08-04 17:55:39,935 - 24	mape	115.3161	
2023-08-04 17:55:42,027 - logger name:exp/ECL-PatchTST2023-08-04-17:55:42.026970/ECL-PatchTST.log
2023-08-04 17:55:42,027 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-17:55:42.026970', 'path': 'exp/ECL-PatchTST2023-08-04-17:55:42.026970', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 17:55:42,027 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 17:55:42,222 - [*] phase 0 Dataset load!
2023-08-04 17:55:43,202 - [*] phase 0 Training start
train 8281
2023-08-04 17:55:55,863 - epoch:0, training loss:0.5458 validation loss:0.2977
train 8281
vs, vt 0.2976545756277831 0.30725417318551435
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.21659372277233913 0.23896820470690727
2023-08-04 17:56:22,858 - epoch:1, training loss:0.4344 validation loss:0.2166
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.18207376515087875 0.19322627475080284
2023-08-04 17:56:41,529 - epoch:2, training loss:0.3261 validation loss:0.1821
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.1714693737094817 0.1725511374356954
2023-08-04 17:57:01,352 - epoch:3, training loss:0.2794 validation loss:0.1715
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.16751334492279135 0.1653032484261886
2023-08-04 17:57:20,300 - epoch:4, training loss:0.2604 validation loss:0.1675
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.16772457827692447 0.1626207580708939
2023-08-04 17:57:39,871 - epoch:5, training loss:0.2529 validation loss:0.1677
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.16644811273916907 0.16231227196429088
2023-08-04 17:57:58,975 - epoch:6, training loss:0.2457 validation loss:0.1664
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.1687144983721816 0.1604829616844654
2023-08-04 17:58:18,441 - epoch:7, training loss:0.2416 validation loss:0.1687
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.16699912528628888 0.16208694616089697
2023-08-04 17:58:37,472 - epoch:8, training loss:0.2363 validation loss:0.1670
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.16849171224495638 0.1611873373065306
2023-08-04 17:58:57,053 - epoch:9, training loss:0.2323 validation loss:0.1685
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.17009562237755113 0.16231532362492188
2023-08-04 17:59:16,041 - epoch:10, training loss:0.2296 validation loss:0.1701
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.17040457657497862 0.1608849836756354
2023-08-04 17:59:34,781 - epoch:11, training loss:0.2262 validation loss:0.1704
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.17038254634193753 0.16124508756658304
2023-08-04 17:59:53,924 - epoch:12, training loss:0.2228 validation loss:0.1704
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.1720980995375177 0.16143305729264798
2023-08-04 18:00:12,772 - epoch:13, training loss:0.2217 validation loss:0.1721
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.1718579159806604 0.16131091862916946
2023-08-04 18:00:31,693 - epoch:14, training loss:0.2206 validation loss:0.1719
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.17160318613700246 0.1613167760812718
2023-08-04 18:00:50,938 - epoch:15, training loss:0.2174 validation loss:0.1716
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.1717120726471362 0.15941861288055129
2023-08-04 18:01:10,174 - epoch:16, training loss:0.2146 validation loss:0.1717
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.17318395071703455 0.16014275732247726
2023-08-04 18:01:29,630 - epoch:17, training loss:0.2134 validation loss:0.1732
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.17268521889396335 0.15969629780105923
2023-08-04 18:01:48,515 - epoch:18, training loss:0.2119 validation loss:0.1727
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.17320430019627447 0.15985542778735576
2023-08-04 18:02:07,923 - epoch:19, training loss:0.2103 validation loss:0.1732
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.17354030550821967 0.16003428464350494
2023-08-04 18:02:27,158 - epoch:20, training loss:0.2089 validation loss:0.1735
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.17340217708893443 0.15957918827948364
2023-08-04 18:02:46,786 - epoch:21, training loss:0.2081 validation loss:0.1734
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.17272913099631018 0.15962745346452878
2023-08-04 18:03:05,394 - epoch:22, training loss:0.2073 validation loss:0.1727
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.17286255291622618 0.15929062794084134
2023-08-04 18:03:24,132 - epoch:23, training loss:0.2065 validation loss:0.1729
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.1727019828622756 0.15972239540322966
2023-08-04 18:03:43,658 - epoch:24, training loss:0.2058 validation loss:0.1727
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.17246487931064938 0.15964959448446397
2023-08-04 18:04:02,473 - epoch:25, training loss:0.2052 validation loss:0.1725
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.17305840130733408 0.15979484986999762
2023-08-04 18:04:21,845 - epoch:26, training loss:0.2053 validation loss:0.1731
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.17280852324936702 0.1597420521404432
2023-08-04 18:04:40,281 - epoch:27, training loss:0.2055 validation loss:0.1728
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.1729266807113005 0.15972610433464465
2023-08-04 18:04:58,917 - epoch:28, training loss:0.2051 validation loss:0.1729
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.17299577852954034 0.15972683190003686
2023-08-04 18:05:18,294 - epoch:29, training loss:0.2048 validation loss:0.1730
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-17:55:42.026970/0/0.1664_epoch_6.pkl  &  0.15929062794084134
2023-08-04 18:05:19,314 - [*] loss:0.1664
2023-08-04 18:05:19,315 - [*] phase 0, testing
2023-08-04 18:05:19,324 - T:24	MAE	0.263660	RMSE	0.168603	MAPE	119.294512
2023-08-04 18:05:19,324 - 24	mae	0.2637	
2023-08-04 18:05:19,324 - 24	rmse	0.1686	
2023-08-04 18:05:19,324 - 24	mape	119.2945	
2023-08-04 18:05:21,104 - [*] loss:0.1593
2023-08-04 18:05:21,105 - [*] phase 0, testing
2023-08-04 18:05:21,113 - T:24	MAE	0.254806	RMSE	0.161584	MAPE	113.393915
2023-08-04 18:05:21,113 - 24	mae	0.2548	
2023-08-04 18:05:21,113 - 24	rmse	0.1616	
2023-08-04 18:05:21,113 - 24	mape	113.3939	
2023-08-04 18:05:23,165 - logger name:exp/ECL-PatchTST2023-08-04-18:05:23.164812/ECL-PatchTST.log
2023-08-04 18:05:23,165 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-18:05:23.164812', 'path': 'exp/ECL-PatchTST2023-08-04-18:05:23.164812', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 18:05:23,165 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 18:05:23,361 - [*] phase 0 Dataset load!
2023-08-04 18:05:24,357 - [*] phase 0 Training start
train 8281
2023-08-04 18:05:36,885 - epoch:0, training loss:0.5077 validation loss:0.3099
train 8281
vs, vt 0.30990037581195 0.33043100717275037
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.23168337069775746 0.29381926973228867
2023-08-04 18:06:09,755 - epoch:1, training loss:7.6978 validation loss:0.2317
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.18574159462814746 0.2249232911221359
2023-08-04 18:06:33,667 - epoch:2, training loss:5.6483 validation loss:0.1857
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.17186217593110126 0.19568605720996857
2023-08-04 18:06:58,423 - epoch:3, training loss:3.7449 validation loss:0.1719
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.1674495788048143 0.1741018452398155
2023-08-04 18:07:22,414 - epoch:4, training loss:2.4515 validation loss:0.1674
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.16801966046509537 0.17301343818721565
2023-08-04 18:07:47,104 - epoch:5, training loss:1.9210 validation loss:0.1680
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.1680838216258132 0.17081165135554646
2023-08-04 18:08:11,785 - epoch:6, training loss:1.7561 validation loss:0.1681
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.17391350185093674 0.18636613970865373
2023-08-04 18:08:36,440 - epoch:7, training loss:1.6159 validation loss:0.1739
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.1716877314383569 0.1767897863426934
2023-08-04 18:09:00,614 - epoch:8, training loss:1.5318 validation loss:0.1717
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.17563626526490503 0.1726074521632298
2023-08-04 18:09:24,880 - epoch:9, training loss:1.3775 validation loss:0.1756
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.17459437558832375 0.17999435896458832
2023-08-04 18:09:49,111 - epoch:10, training loss:1.4003 validation loss:0.1746
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.17278198671081793 0.18523956981042158
2023-08-04 18:10:13,895 - epoch:11, training loss:1.2961 validation loss:0.1728
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.17043284699320793 0.18280772087366684
2023-08-04 18:10:37,853 - epoch:12, training loss:1.2305 validation loss:0.1704
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.16920744498138843 0.18581973583154057
2023-08-04 18:11:02,320 - epoch:13, training loss:1.2128 validation loss:0.1692
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.16864304455078166 0.19008039456346762
2023-08-04 18:11:27,106 - epoch:14, training loss:1.2416 validation loss:0.1686
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.1673407222589721 0.18692621637297713
2023-08-04 18:11:51,737 - epoch:15, training loss:1.1641 validation loss:0.1673
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.16700585186481476 0.18858160386266914
2023-08-04 18:12:14,678 - epoch:16, training loss:1.1968 validation loss:0.1670
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.16692818523101186 0.1876775931728923
2023-08-04 18:12:38,340 - epoch:17, training loss:1.1562 validation loss:0.1669
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.16598817365972893 0.1799035370349884
2023-08-04 18:13:01,904 - epoch:18, training loss:1.1334 validation loss:0.1660
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.1662188810498818 0.18734221419562463
2023-08-04 18:13:25,247 - epoch:19, training loss:1.1543 validation loss:0.1662
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.1667100231608619 0.18446676384495653
2023-08-04 18:13:48,588 - epoch:20, training loss:1.1562 validation loss:0.1667
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.16582372412085533 0.18633134906058726
2023-08-04 18:14:12,260 - epoch:21, training loss:1.1590 validation loss:0.1658
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.16583754721543062 0.1843822700497897
2023-08-04 18:14:35,843 - epoch:22, training loss:1.1246 validation loss:0.1658
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.16565243608277777 0.1887480325025061
2023-08-04 18:14:59,358 - epoch:23, training loss:1.1834 validation loss:0.1657
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.16530523432985597 0.1837224025765191
2023-08-04 18:15:22,513 - epoch:24, training loss:1.1228 validation loss:0.1653
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.16576514182531316 0.18538116372149924
2023-08-04 18:15:45,760 - epoch:25, training loss:1.1511 validation loss:0.1658
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.16549510184837424 0.1836800031040026
2023-08-04 18:16:09,461 - epoch:26, training loss:1.1708 validation loss:0.1655
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.16548264949865962 0.18545994113968767
2023-08-04 18:16:32,898 - epoch:27, training loss:1.1056 validation loss:0.1655
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.16552980001205983 0.18413125140511472
2023-08-04 18:16:56,024 - epoch:28, training loss:1.1333 validation loss:0.1655
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.165435409254354 0.18355671575535898
2023-08-04 18:17:18,954 - epoch:29, training loss:1.1361 validation loss:0.1654
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-18:05:23.164812/0/0.1653_epoch_24.pkl  &  0.17081165135554646
2023-08-04 18:17:20,680 - [*] loss:0.1653
2023-08-04 18:17:20,682 - [*] phase 0, testing
2023-08-04 18:17:20,690 - T:24	MAE	0.260742	RMSE	0.167579	MAPE	113.752651
2023-08-04 18:17:20,690 - 24	mae	0.2607	
2023-08-04 18:17:20,690 - 24	rmse	0.1676	
2023-08-04 18:17:20,690 - 24	mape	113.7527	
2023-08-04 18:17:22,134 - [*] loss:0.1708
2023-08-04 18:17:22,136 - [*] phase 0, testing
2023-08-04 18:17:22,144 - T:24	MAE	0.267192	RMSE	0.172640	MAPE	113.975155
2023-08-04 18:17:22,144 - 24	mae	0.2672	
2023-08-04 18:17:22,144 - 24	rmse	0.1726	
2023-08-04 18:17:22,144 - 24	mape	113.9752	
2023-08-04 18:17:24,217 - logger name:exp/ECL-PatchTST2023-08-04-18:17:24.217006/ECL-PatchTST.log
2023-08-04 18:17:24,217 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-18:17:24.217006', 'path': 'exp/ECL-PatchTST2023-08-04-18:17:24.217006', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 18:17:24,217 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 18:17:24,409 - [*] phase 0 Dataset load!
2023-08-04 18:17:25,363 - [*] phase 0 Training start
train 8281
2023-08-04 18:17:37,202 - epoch:0, training loss:0.1858 validation loss:0.1396
train 8281
vs, vt 0.1396036904467189 0.14938804523452467
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.1014589820381092 0.13110748087258442
2023-08-04 18:18:07,846 - epoch:1, training loss:0.5213 validation loss:0.1015
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08152995979332406 0.10171225707492103
2023-08-04 18:18:31,058 - epoch:2, training loss:0.4145 validation loss:0.0815
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07823079014601915 0.09839581552407016
2023-08-04 18:18:54,474 - epoch:3, training loss:0.3456 validation loss:0.0782
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.0761565867325534 0.0859970630996901
2023-08-04 18:19:18,138 - epoch:4, training loss:0.3012 validation loss:0.0762
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07670574659562629 0.08223601454949897
2023-08-04 18:19:42,263 - epoch:5, training loss:0.2758 validation loss:0.0767
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07634283092034899 0.08019301287182
2023-08-04 18:20:05,740 - epoch:6, training loss:0.2661 validation loss:0.0763
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07688095177645268 0.08469751007531
2023-08-04 18:20:28,588 - epoch:7, training loss:0.2642 validation loss:0.0769
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07560366408332535 0.08352371637264024
2023-08-04 18:20:52,230 - epoch:8, training loss:0.2645 validation loss:0.0756
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07592245539569337 0.08073348033687343
2023-08-04 18:21:15,875 - epoch:9, training loss:0.2601 validation loss:0.0759
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07638645471762055 0.08173494939894779
2023-08-04 18:21:38,538 - epoch:10, training loss:0.2578 validation loss:0.0764
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07663619332015514 0.0821464289303707
2023-08-04 18:22:02,077 - epoch:11, training loss:0.2561 validation loss:0.0766
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07635170488577822 0.0800568598282078
2023-08-04 18:22:25,410 - epoch:12, training loss:0.2482 validation loss:0.0764
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07640030097378336 0.081053351416536
2023-08-04 18:22:48,366 - epoch:13, training loss:0.2423 validation loss:0.0764
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07725881323542284 0.08087179510165816
2023-08-04 18:23:11,880 - epoch:14, training loss:0.2384 validation loss:0.0773
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.0766683216004268 0.07949596246623475
2023-08-04 18:23:35,211 - epoch:15, training loss:0.2364 validation loss:0.0767
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07684331508758276 0.07982812360253023
2023-08-04 18:23:58,436 - epoch:16, training loss:0.2333 validation loss:0.0768
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07690486077057279 0.0808422980911058
2023-08-04 18:24:21,331 - epoch:17, training loss:0.2297 validation loss:0.0769
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07689496648052464 0.07874066860455534
2023-08-04 18:24:44,350 - epoch:18, training loss:0.2280 validation loss:0.0769
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07694814686218034 0.08022912907535615
2023-08-04 18:25:07,214 - epoch:19, training loss:0.2286 validation loss:0.0769
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07728021737674008 0.07938534750238709
2023-08-04 18:25:30,636 - epoch:20, training loss:0.2300 validation loss:0.0773
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07697480001851269 0.07957569938962875
2023-08-04 18:25:53,587 - epoch:21, training loss:0.2261 validation loss:0.0770
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.076906006783247 0.07923202402889729
2023-08-04 18:26:17,006 - epoch:22, training loss:0.2288 validation loss:0.0769
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07683400372448175 0.08035389657901681
2023-08-04 18:26:40,578 - epoch:23, training loss:0.2259 validation loss:0.0768
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07697464444715044 0.07899550243240336
2023-08-04 18:27:03,744 - epoch:24, training loss:0.2249 validation loss:0.0770
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07689043749933658 0.07940662603663362
2023-08-04 18:27:26,975 - epoch:25, training loss:0.2242 validation loss:0.0769
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07701630687907986 0.07907219028667263
2023-08-04 18:27:49,806 - epoch:26, training loss:0.2243 validation loss:0.0770
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.0769439329109762 0.07963298593202363
2023-08-04 18:28:13,621 - epoch:27, training loss:0.2270 validation loss:0.0769
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07696923726926679 0.07908652941493885
2023-08-04 18:28:36,591 - epoch:28, training loss:0.2239 validation loss:0.0770
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07697490570337875 0.07907043688971063
2023-08-04 18:28:59,828 - epoch:29, training loss:0.2234 validation loss:0.0770
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-18:17:24.217006/0/0.0756_epoch_8.pkl  &  0.07874066860455534
2023-08-04 18:29:01,230 - [*] loss:0.1611
2023-08-04 18:29:01,231 - [*] phase 0, testing
2023-08-04 18:29:01,241 - T:24	MAE	0.256369	RMSE	0.163469	MAPE	113.564289
2023-08-04 18:29:01,241 - 24	mae	0.2564	
2023-08-04 18:29:01,241 - 24	rmse	0.1635	
2023-08-04 18:29:01,241 - 24	mape	113.5643	
2023-08-04 18:29:02,773 - [*] loss:0.1672
2023-08-04 18:29:02,774 - [*] phase 0, testing
2023-08-04 18:29:02,783 - T:24	MAE	0.262455	RMSE	0.169060	MAPE	113.922977
2023-08-04 18:29:02,783 - 24	mae	0.2625	
2023-08-04 18:29:02,783 - 24	rmse	0.1691	
2023-08-04 18:29:02,783 - 24	mape	113.9230	
2023-08-04 18:29:04,923 - logger name:exp/ECL-PatchTST2023-08-04-18:29:04.923155/ECL-PatchTST.log
2023-08-04 18:29:04,923 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-18:29:04.923155', 'path': 'exp/ECL-PatchTST2023-08-04-18:29:04.923155', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 18:29:04,923 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 18:29:05,144 - [*] phase 0 Dataset load!
2023-08-04 18:29:06,142 - [*] phase 0 Training start
train 8281
2023-08-04 18:29:17,708 - epoch:0, training loss:0.1973 validation loss:0.1336
train 8281
vs, vt 0.13359046646434328 0.13925113528966904
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.09995004640001318 0.11066164343577364
2023-08-04 18:29:44,705 - epoch:1, training loss:1.2974 validation loss:0.1000
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08297993339922118 0.09071644654740459
2023-08-04 18:30:02,586 - epoch:2, training loss:1.0554 validation loss:0.0830
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07880234459172124 0.080772600096205
2023-08-04 18:30:20,843 - epoch:3, training loss:0.8241 validation loss:0.0788
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.0769207546406466 0.0774339744416268
2023-08-04 18:30:39,261 - epoch:4, training loss:0.6564 validation loss:0.0769
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07748035819310209 0.07614844486765239
2023-08-04 18:30:57,871 - epoch:5, training loss:0.5738 validation loss:0.0775
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.076428166307185 0.07614495540442674
2023-08-04 18:31:16,707 - epoch:6, training loss:0.5253 validation loss:0.0764
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07839788431706636 0.0752540012092694
2023-08-04 18:31:34,588 - epoch:7, training loss:0.4968 validation loss:0.0784
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.0769209040571814 0.07687407302791657
2023-08-04 18:31:52,888 - epoch:8, training loss:0.4798 validation loss:0.0769
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07684243551414946 0.0761099359749452
2023-08-04 18:32:11,522 - epoch:9, training loss:0.4692 validation loss:0.0768
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07731791601880737 0.07680001749616602
2023-08-04 18:32:29,700 - epoch:10, training loss:0.4558 validation loss:0.0773
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07646082738495391 0.07588693013657695
2023-08-04 18:32:48,018 - epoch:11, training loss:0.4435 validation loss:0.0765
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07642995428455912 0.07504622759702413
2023-08-04 18:33:06,266 - epoch:12, training loss:0.4328 validation loss:0.0764
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07624088648868643 0.07578326858904051
2023-08-04 18:33:24,590 - epoch:13, training loss:0.4358 validation loss:0.0762
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07627685740590096 0.07541704582779304
2023-08-04 18:33:42,785 - epoch:14, training loss:0.4269 validation loss:0.0763
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07538204138045726 0.07465231183754362
2023-08-04 18:34:01,025 - epoch:15, training loss:0.4182 validation loss:0.0754
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07551093634379946 0.07430901167833287
2023-08-04 18:34:19,718 - epoch:16, training loss:0.4169 validation loss:0.0755
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07540017495984616 0.07445558573564758
2023-08-04 18:34:38,049 - epoch:17, training loss:0.4100 validation loss:0.0754
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07530526395725168 0.07430311349099097
2023-08-04 18:34:56,854 - epoch:18, training loss:0.4044 validation loss:0.0753
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07545576592826325 0.07442471028670021
2023-08-04 18:35:15,626 - epoch:19, training loss:0.4029 validation loss:0.0755
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07553111259704051 0.07443952066418917
2023-08-04 18:35:33,596 - epoch:20, training loss:0.4091 validation loss:0.0755
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07519330343474513 0.07431540630109933
2023-08-04 18:35:51,953 - epoch:21, training loss:0.3985 validation loss:0.0752
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07515682207177514 0.07403763626580653
2023-08-04 18:36:10,593 - epoch:22, training loss:0.3979 validation loss:0.0752
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.0750703273098106 0.07403961228935615
2023-08-04 18:36:29,260 - epoch:23, training loss:0.3963 validation loss:0.0751
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07512588515553785 0.07393399553130502
2023-08-04 18:36:47,648 - epoch:24, training loss:0.3959 validation loss:0.0751
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07505102998212627 0.0739947505619215
2023-08-04 18:37:05,818 - epoch:25, training loss:0.3958 validation loss:0.0751
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07515122862937658 0.0739482285697823
2023-08-04 18:37:24,414 - epoch:26, training loss:0.3961 validation loss:0.0752
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07514892623800298 0.07396823451246905
2023-08-04 18:37:43,356 - epoch:27, training loss:0.3941 validation loss:0.0751
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07514631359473518 0.0739822535087233
2023-08-04 18:38:01,817 - epoch:28, training loss:0.3944 validation loss:0.0751
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07510919196774131 0.07396303676068783
2023-08-04 18:38:19,913 - epoch:29, training loss:0.3944 validation loss:0.0751
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-18:29:04.923155/0/0.0751_epoch_25.pkl  &  0.07393399553130502
2023-08-04 18:38:21,112 - [*] loss:0.1595
2023-08-04 18:38:21,113 - [*] phase 0, testing
2023-08-04 18:38:21,122 - T:24	MAE	0.255062	RMSE	0.161742	MAPE	114.790869
2023-08-04 18:38:21,122 - 24	mae	0.2551	
2023-08-04 18:38:21,122 - 24	rmse	0.1617	
2023-08-04 18:38:21,122 - 24	mape	114.7909	
2023-08-04 18:38:22,539 - [*] loss:0.1573
2023-08-04 18:38:22,540 - [*] phase 0, testing
2023-08-04 18:38:22,549 - T:24	MAE	0.251837	RMSE	0.159603	MAPE	113.461030
2023-08-04 18:38:22,549 - 24	mae	0.2518	
2023-08-04 18:38:22,549 - 24	rmse	0.1596	
2023-08-04 18:38:22,549 - 24	mape	113.4610	
2023-08-04 18:38:24,669 - logger name:exp/ECL-PatchTST2023-08-04-18:38:24.669459/ECL-PatchTST.log
2023-08-04 18:38:24,670 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-18:38:24.669459', 'path': 'exp/ECL-PatchTST2023-08-04-18:38:24.669459', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 18:38:24,670 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-04 18:38:24,861 - [*] phase 0 Dataset load!
2023-08-04 18:38:25,837 - [*] phase 0 Training start
train 8281
2023-08-04 18:38:37,664 - epoch:0, training loss:0.1858 validation loss:0.1396
train 8281
vs, vt 0.1396036904467189 0.14938804523452467
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10542470807938473 0.13114703300854433
2023-08-04 18:39:09,475 - epoch:1, training loss:2.8295 validation loss:0.1054
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08545612260375333 0.10192518308758736
2023-08-04 18:39:32,964 - epoch:2, training loss:2.1803 validation loss:0.0855
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07919826099406117 0.0911391886032146
2023-08-04 18:39:57,122 - epoch:3, training loss:1.5177 validation loss:0.0792
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07683462260857872 0.08079882484415303
2023-08-04 18:40:21,412 - epoch:4, training loss:1.0225 validation loss:0.0768
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.0773318323266247 0.07885179223249787
2023-08-04 18:40:45,151 - epoch:5, training loss:0.7942 validation loss:0.0773
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07657432126934113 0.07801871544317059
2023-08-04 18:41:09,352 - epoch:6, training loss:0.6919 validation loss:0.0766
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07823167287785074 0.08241368360493494
2023-08-04 18:41:32,996 - epoch:7, training loss:0.6328 validation loss:0.0782
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07755124066834865 0.08042503375074138
2023-08-04 18:41:56,921 - epoch:8, training loss:0.5863 validation loss:0.0776
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07847639400026073 0.07802687998375167
2023-08-04 18:42:20,439 - epoch:9, training loss:0.5501 validation loss:0.0785
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.079810314933243 0.08195941255468389
2023-08-04 18:42:44,042 - epoch:10, training loss:0.5341 validation loss:0.0798
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07856229441645353 0.0846936137455961
2023-08-04 18:43:07,812 - epoch:11, training loss:0.5034 validation loss:0.0786
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07784423272570838 0.08383214392739793
2023-08-04 18:43:31,975 - epoch:12, training loss:0.4594 validation loss:0.0778
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07778491403745569 0.08523681524979032
2023-08-04 18:43:55,851 - epoch:13, training loss:0.4410 validation loss:0.0778
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07757580790506757 0.08424566499888897
2023-08-04 18:44:19,304 - epoch:14, training loss:0.4192 validation loss:0.0776
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07711177124925282 0.08585683248289254
2023-08-04 18:44:42,800 - epoch:15, training loss:0.4225 validation loss:0.0771
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07663646988246751 0.08602727439416491
2023-08-04 18:45:06,132 - epoch:16, training loss:0.4176 validation loss:0.0766
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07673602154397446 0.08728935229389564
2023-08-04 18:45:29,769 - epoch:17, training loss:0.4087 validation loss:0.0767
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07626277499872705 0.08459267279376155
2023-08-04 18:45:52,916 - epoch:18, training loss:0.3968 validation loss:0.0763
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07637177626399891 0.08682781295931857
2023-08-04 18:46:17,024 - epoch:19, training loss:0.4060 validation loss:0.0764
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07667438896453899 0.08590920431458432
2023-08-04 18:46:40,510 - epoch:20, training loss:0.4060 validation loss:0.0767
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07616325216772764 0.08722203305881956
2023-08-04 18:47:03,811 - epoch:21, training loss:0.4029 validation loss:0.0762
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07614152724652187 0.0858955389779547
2023-08-04 18:47:27,531 - epoch:22, training loss:0.3911 validation loss:0.0761
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07615513808053473 0.08776655444956344
2023-08-04 18:47:50,716 - epoch:23, training loss:0.4000 validation loss:0.0762
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07588415754878003 0.08581067573117174
2023-08-04 18:48:14,889 - epoch:24, training loss:0.3934 validation loss:0.0759
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07611094188431035 0.08700541892777318
2023-08-04 18:48:38,652 - epoch:25, training loss:0.3966 validation loss:0.0761
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07589685641553091 0.08598333913023057
2023-08-04 18:49:02,246 - epoch:26, training loss:0.3995 validation loss:0.0759
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07602313444342303 0.08699471033785654
2023-08-04 18:49:25,654 - epoch:27, training loss:0.3861 validation loss:0.0760
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07598246010425298 0.08615088252269704
2023-08-04 18:49:49,052 - epoch:28, training loss:0.3849 validation loss:0.0760
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07592063413366028 0.08595786203184853
2023-08-04 18:50:12,350 - epoch:29, training loss:0.3894 validation loss:0.0759
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-04-18:38:24.669459/0/0.0759_epoch_24.pkl  &  0.07801871544317059
2023-08-04 18:50:13,749 - [*] loss:0.1615
2023-08-04 18:50:13,750 - [*] phase 0, testing
2023-08-04 18:50:13,759 - T:24	MAE	0.255853	RMSE	0.163767	MAPE	113.055098
2023-08-04 18:50:13,759 - 24	mae	0.2559	
2023-08-04 18:50:13,759 - 24	rmse	0.1638	
2023-08-04 18:50:13,759 - 24	mape	113.0551	
2023-08-04 18:50:15,204 - [*] loss:0.1659
2023-08-04 18:50:15,205 - [*] phase 0, testing
2023-08-04 18:50:15,213 - T:24	MAE	0.261834	RMSE	0.168065	MAPE	114.441192
2023-08-04 18:50:15,214 - 24	mae	0.2618	
2023-08-04 18:50:15,214 - 24	rmse	0.1681	
2023-08-04 18:50:15,214 - 24	mape	114.4412	
2023-08-04 18:50:17,284 - logger name:exp/ECL-PatchTST2023-08-04-18:50:17.283840/ECL-PatchTST.log
2023-08-04 18:50:17,284 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-18:50:17.283840', 'path': 'exp/ECL-PatchTST2023-08-04-18:50:17.283840', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 18:50:17,284 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 18:50:17,476 - [*] phase 0 Dataset load!
2023-08-04 18:50:18,461 - [*] phase 0 Training start
train 7585
2023-08-04 18:50:30,633 - epoch:0, training loss:0.9536 validation loss:0.4443
train 7585
vs, vt 0.444336894680472 0.44589298612931194
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.4128183433238198 0.4208231506978764
2023-08-04 18:50:57,985 - epoch:1, training loss:1.0064 validation loss:0.4128
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.4021657004075892 0.4092982060769025
2023-08-04 18:51:17,035 - epoch:2, training loss:0.9528 validation loss:0.4022
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.39415785451145735 0.40484360693132176
2023-08-04 18:51:35,364 - epoch:3, training loss:0.9235 validation loss:0.3942
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.39031947667107864 0.39555687632630854
2023-08-04 18:51:54,319 - epoch:4, training loss:0.8951 validation loss:0.3903
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.40379613374962525 0.39275239188881483
2023-08-04 18:52:13,028 - epoch:5, training loss:0.8631 validation loss:0.4038
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.41647958755493164 0.3945714178330758
2023-08-04 18:52:31,791 - epoch:6, training loss:0.8314 validation loss:0.4165
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.4195943816619761 0.40360264874556484
2023-08-04 18:52:49,895 - epoch:7, training loss:0.8090 validation loss:0.4196
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.41883521132609425 0.407020220423446
2023-08-04 18:53:08,583 - epoch:8, training loss:0.7884 validation loss:0.4188
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.415043929920477 0.41455170731334123
2023-08-04 18:53:27,126 - epoch:9, training loss:0.7698 validation loss:0.4150
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.4221850204117158 0.4118630574906574
2023-08-04 18:53:45,909 - epoch:10, training loss:0.7549 validation loss:0.4222
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.42301101106054645 0.4223189704558429
2023-08-04 18:54:04,577 - epoch:11, training loss:0.7470 validation loss:0.4230
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.4265359403455959 0.41980396167320366
2023-08-04 18:54:23,431 - epoch:12, training loss:0.7384 validation loss:0.4265
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.430700916577788 0.4265100342385909
2023-08-04 18:54:42,518 - epoch:13, training loss:0.7335 validation loss:0.4307
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.42835446140345407 0.42454071956522327
2023-08-04 18:55:01,895 - epoch:14, training loss:0.7251 validation loss:0.4284
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.4340484159834245 0.42854923711103554
2023-08-04 18:55:20,298 - epoch:15, training loss:0.7188 validation loss:0.4340
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.43384490293615 0.42989493117612954
2023-08-04 18:55:38,875 - epoch:16, training loss:0.7132 validation loss:0.4338
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.4355269801967284 0.4282279049648958
2023-08-04 18:55:57,656 - epoch:17, training loss:0.7091 validation loss:0.4355
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.4386898796348011 0.42694652781767006
2023-08-04 18:56:17,070 - epoch:18, training loss:0.7029 validation loss:0.4387
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.43613287280587587 0.4299646265366498
2023-08-04 18:56:35,899 - epoch:19, training loss:0.7002 validation loss:0.4361
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.4392763165866627 0.4264698997139931
2023-08-04 18:56:54,392 - epoch:20, training loss:0.6950 validation loss:0.4393
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.43877813833601337 0.4286661673994625
2023-08-04 18:57:12,751 - epoch:21, training loss:0.6939 validation loss:0.4388
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.44022563538130594 0.4274250339059269
2023-08-04 18:57:31,511 - epoch:22, training loss:0.6922 validation loss:0.4402
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.4408528217497994 0.4279148249941714
2023-08-04 18:57:50,799 - epoch:23, training loss:0.6894 validation loss:0.4409
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.441614328938372 0.42690516219419594
2023-08-04 18:58:09,243 - epoch:24, training loss:0.6892 validation loss:0.4416
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.4411744221168406 0.42566155116347704
2023-08-04 18:58:27,963 - epoch:25, training loss:0.6864 validation loss:0.4412
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.441922846085885 0.4276362517300774
2023-08-04 18:58:47,075 - epoch:26, training loss:0.6869 validation loss:0.4419
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.4424428676857668 0.4281509693931131
2023-08-04 18:59:05,956 - epoch:27, training loss:0.6868 validation loss:0.4424
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.44150467392276316 0.4279279296889025
2023-08-04 18:59:24,919 - epoch:28, training loss:0.6855 validation loss:0.4415
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.4417293834335664 0.42779047787189484
2023-08-04 18:59:43,077 - epoch:29, training loss:0.6846 validation loss:0.4417
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-18:50:17.283840/0/0.3903_epoch_4.pkl  &  0.39275239188881483
2023-08-04 18:59:44,569 - [*] loss:0.3903
2023-08-04 18:59:44,584 - [*] phase 0, testing
2023-08-04 18:59:45,462 - T:720	MAE	0.425032	RMSE	0.388635	MAPE	191.015804
2023-08-04 18:59:45,463 - 720	mae	0.4250	
2023-08-04 18:59:45,463 - 720	rmse	0.3886	
2023-08-04 18:59:45,463 - 720	mape	191.0158	
2023-08-04 18:59:46,813 - [*] loss:0.3928
2023-08-04 18:59:46,828 - [*] phase 0, testing
2023-08-04 18:59:47,742 - T:720	MAE	0.422307	RMSE	0.390927	MAPE	185.720277
2023-08-04 18:59:47,743 - 720	mae	0.4223	
2023-08-04 18:59:47,743 - 720	rmse	0.3909	
2023-08-04 18:59:47,743 - 720	mape	185.7203	
2023-08-04 18:59:49,974 - logger name:exp/ECL-PatchTST2023-08-04-18:59:49.974190/ECL-PatchTST.log
2023-08-04 18:59:49,974 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-18:59:49.974190', 'path': 'exp/ECL-PatchTST2023-08-04-18:59:49.974190', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 18:59:49,974 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 18:59:50,171 - [*] phase 0 Dataset load!
2023-08-04 18:59:51,161 - [*] phase 0 Training start
train 7585
2023-08-04 19:00:05,327 - epoch:0, training loss:0.9002 validation loss:0.4311
train 7585
vs, vt 0.43110248884733987 0.4455676157684887
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.40795590334078846 0.4226127477253185
2023-08-04 19:00:36,901 - epoch:1, training loss:8.7280 validation loss:0.4080
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.39601003422456627 0.4074413644916871
2023-08-04 19:01:01,103 - epoch:2, training loss:6.9083 validation loss:0.3960
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.39217620518277674 0.40534133639405756
2023-08-04 19:01:24,971 - epoch:3, training loss:4.7409 validation loss:0.3922
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.39088620902860866 0.39563780672409954
2023-08-04 19:01:49,797 - epoch:4, training loss:3.4058 validation loss:0.3909
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.39129040828522516 0.39443539258311777
2023-08-04 19:02:13,981 - epoch:5, training loss:3.0170 validation loss:0.3913
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.3971525847035296 0.39372660614111843
2023-08-04 19:02:37,949 - epoch:6, training loss:2.8913 validation loss:0.3972
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.398585504907019 0.4047045821652693
2023-08-04 19:03:02,669 - epoch:7, training loss:2.8471 validation loss:0.3986
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.39937911401776705 0.4046456651652561
2023-08-04 19:03:27,004 - epoch:8, training loss:2.7755 validation loss:0.3994
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.39956648560131297 0.4074224509737071
2023-08-04 19:03:50,529 - epoch:9, training loss:2.6277 validation loss:0.3996
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.40266665597172346 0.4112904632792753
2023-08-04 19:04:15,016 - epoch:10, training loss:2.5297 validation loss:0.4027
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.402994865880293 0.41809118407614093
2023-08-04 19:04:38,870 - epoch:11, training loss:2.4755 validation loss:0.4030
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.4061506793779485 0.41696229226448955
2023-08-04 19:05:02,965 - epoch:12, training loss:2.4182 validation loss:0.4062
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.40618279488647685 0.4231417284292333
2023-08-04 19:05:27,612 - epoch:13, training loss:2.3847 validation loss:0.4062
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.4073024129166323 0.42324038463480335
2023-08-04 19:05:51,156 - epoch:14, training loss:2.3652 validation loss:0.4073
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.4090462204288034 0.4267234328915091
2023-08-04 19:06:15,401 - epoch:15, training loss:2.3324 validation loss:0.4090
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.40722986968124614 0.4237365187967525
2023-08-04 19:06:39,853 - epoch:16, training loss:2.3118 validation loss:0.4072
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.40754324723692503 0.431983339435914
2023-08-04 19:07:03,987 - epoch:17, training loss:2.3106 validation loss:0.4075
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.40805039949276867 0.4305126921218984
2023-08-04 19:07:27,860 - epoch:18, training loss:2.3014 validation loss:0.4081
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.4105335947345285 0.42533543968901916
2023-08-04 19:07:51,911 - epoch:19, training loss:2.2978 validation loss:0.4105
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.41166816914782806 0.4299483027528314
2023-08-04 19:08:15,844 - epoch:20, training loss:2.2652 validation loss:0.4117
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.4121624301461613 0.4333989225766238
2023-08-04 19:08:39,532 - epoch:21, training loss:2.2862 validation loss:0.4122
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.4130033999681473 0.43211183127234964
2023-08-04 19:09:03,853 - epoch:22, training loss:2.2820 validation loss:0.4130
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.4132082260706845 0.4328873122439665
2023-08-04 19:09:27,847 - epoch:23, training loss:2.2978 validation loss:0.4132
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.4151346893871532 0.4328821625779657
2023-08-04 19:09:51,636 - epoch:24, training loss:2.2825 validation loss:0.4151
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.41239719005192027 0.4299522796097924
2023-08-04 19:10:15,907 - epoch:25, training loss:2.2766 validation loss:0.4124
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.41302325532716866 0.4315172801999485
2023-08-04 19:10:39,745 - epoch:26, training loss:2.2823 validation loss:0.4130
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.41287828631260814 0.43160728934933157
2023-08-04 19:11:04,247 - epoch:27, training loss:2.2966 validation loss:0.4129
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.413311680450159 0.4326070310438381
2023-08-04 19:11:28,305 - epoch:28, training loss:2.2796 validation loss:0.4133
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.41322926651029024 0.43193567412741046
2023-08-04 19:11:52,312 - epoch:29, training loss:2.2795 validation loss:0.4132
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-18:59:49.974190/0/0.3909_epoch_4.pkl  &  0.39372660614111843
2023-08-04 19:11:53,534 - [*] loss:0.3909
2023-08-04 19:11:53,553 - [*] phase 0, testing
2023-08-04 19:11:54,541 - T:720	MAE	0.426582	RMSE	0.389322	MAPE	196.403122
2023-08-04 19:11:54,541 - 720	mae	0.4266	
2023-08-04 19:11:54,541 - 720	rmse	0.3893	
2023-08-04 19:11:54,541 - 720	mape	196.4031	
2023-08-04 19:11:55,593 - [*] loss:0.3937
2023-08-04 19:11:55,613 - [*] phase 0, testing
2023-08-04 19:11:56,614 - T:720	MAE	0.420741	RMSE	0.390981	MAPE	187.070084
2023-08-04 19:11:56,615 - 720	mae	0.4207	
2023-08-04 19:11:56,615 - 720	rmse	0.3910	
2023-08-04 19:11:56,615 - 720	mape	187.0701	
2023-08-04 19:11:58,773 - logger name:exp/ECL-PatchTST2023-08-04-19:11:58.772735/ECL-PatchTST.log
2023-08-04 19:11:58,773 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-19:11:58.772735', 'path': 'exp/ECL-PatchTST2023-08-04-19:11:58.772735', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 19:11:58,773 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 19:11:58,982 - [*] phase 0 Dataset load!
2023-08-04 19:11:59,970 - [*] phase 0 Training start
train 7585
2023-08-04 19:12:12,754 - epoch:0, training loss:0.3117 validation loss:0.1925
train 7585
vs, vt 0.1925238690832082 0.19798029915374868
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.18252948364790747 0.18846567281905344
2023-08-04 19:12:44,386 - epoch:1, training loss:0.8299 validation loss:0.1825
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.17861140015370705 0.18212514392593326
2023-08-04 19:13:07,922 - epoch:2, training loss:0.7810 validation loss:0.1786
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.17553022898295345 0.17976396136424122
2023-08-04 19:13:32,145 - epoch:3, training loss:0.7274 validation loss:0.1755
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.1770443396971506 0.17698270192041116
2023-08-04 19:13:56,103 - epoch:4, training loss:0.6730 validation loss:0.1770
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.17953532715054119 0.17645058605600805
2023-08-04 19:14:19,850 - epoch:5, training loss:0.6478 validation loss:0.1795
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.18480540154611363 0.17584654206738753
2023-08-04 19:14:43,422 - epoch:6, training loss:0.6304 validation loss:0.1848
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.18326361258240306 0.17796140492838972
2023-08-04 19:15:07,796 - epoch:7, training loss:0.6216 validation loss:0.1833
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.18293142713168087 0.17777304570464528
2023-08-04 19:15:31,867 - epoch:8, training loss:0.6168 validation loss:0.1829
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.18035899650524645 0.1770056179341148
2023-08-04 19:15:55,599 - epoch:9, training loss:0.6107 validation loss:0.1804
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.18342918727327795 0.17410460202132955
2023-08-04 19:16:19,700 - epoch:10, training loss:0.5995 validation loss:0.1834
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.18359183279030464 0.17382371622849913
2023-08-04 19:16:43,537 - epoch:11, training loss:0.5917 validation loss:0.1836
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.185231291634195 0.17466773180400624
2023-08-04 19:17:07,185 - epoch:12, training loss:0.5829 validation loss:0.1852
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.18262436438132734 0.17541284924920866
2023-08-04 19:17:32,776 - epoch:13, training loss:0.5760 validation loss:0.1826
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.18504184922751257 0.17663952927379048
2023-08-04 19:17:57,828 - epoch:14, training loss:0.5702 validation loss:0.1850
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.1848674162345774 0.17624112240531864
2023-08-04 19:18:22,421 - epoch:15, training loss:0.5658 validation loss:0.1849
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.18439082757515066 0.17650018172229037
2023-08-04 19:18:46,225 - epoch:16, training loss:0.5614 validation loss:0.1844
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.18456336724407532 0.17744100970380447
2023-08-04 19:19:10,215 - epoch:17, training loss:0.5599 validation loss:0.1846
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.1853930634610793 0.17686382155208027
2023-08-04 19:19:34,435 - epoch:18, training loss:0.5565 validation loss:0.1854
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.18404381174375029 0.17678612996550166
2023-08-04 19:19:58,646 - epoch:19, training loss:0.5541 validation loss:0.1840
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.18487663479412303 0.17752159474527135
2023-08-04 19:20:22,795 - epoch:20, training loss:0.5513 validation loss:0.1849
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.18567867796210683 0.1772293720175238
2023-08-04 19:20:47,015 - epoch:21, training loss:0.5511 validation loss:0.1857
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.18484846590196385 0.17798317574402867
2023-08-04 19:21:11,686 - epoch:22, training loss:0.5505 validation loss:0.1848
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.18556079180801616 0.1776252840371693
2023-08-04 19:21:35,105 - epoch:23, training loss:0.5504 validation loss:0.1856
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.18547414462356007 0.17771069091909072
2023-08-04 19:21:59,095 - epoch:24, training loss:0.5503 validation loss:0.1855
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.18525484160465353 0.1777647440047825
2023-08-04 19:22:23,135 - epoch:25, training loss:0.5481 validation loss:0.1853
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.1851756857598529 0.1777783066910856
2023-08-04 19:22:47,396 - epoch:26, training loss:0.5472 validation loss:0.1852
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.18523607359212987 0.17745683123083675
2023-08-04 19:23:11,412 - epoch:27, training loss:0.5484 validation loss:0.1852
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.18531297191100962 0.17782795823672237
2023-08-04 19:23:35,556 - epoch:28, training loss:0.5478 validation loss:0.1853
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.18528681116945603 0.17781082803712173
2023-08-04 19:23:59,480 - epoch:29, training loss:0.5463 validation loss:0.1853
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-19:11:58.772735/0/0.1755_epoch_3.pkl  &  0.17382371622849913
2023-08-04 19:24:01,150 - [*] loss:0.3900
2023-08-04 19:24:01,263 - [*] phase 0, testing
2023-08-04 19:24:02,318 - T:720	MAE	0.423970	RMSE	0.388400	MAPE	192.866218
2023-08-04 19:24:02,323 - 720	mae	0.4240	
2023-08-04 19:24:02,323 - 720	rmse	0.3884	
2023-08-04 19:24:02,323 - 720	mape	192.8662	
2023-08-04 19:24:03,628 - [*] loss:0.3901
2023-08-04 19:24:03,770 - [*] phase 0, testing
2023-08-04 19:24:04,687 - T:720	MAE	0.418920	RMSE	0.387736	MAPE	180.128467
2023-08-04 19:24:04,692 - 720	mae	0.4189	
2023-08-04 19:24:04,692 - 720	rmse	0.3877	
2023-08-04 19:24:04,692 - 720	mape	180.1285	
2023-08-04 19:24:07,083 - logger name:exp/ECL-PatchTST2023-08-04-19:24:07.083618/ECL-PatchTST.log
2023-08-04 19:24:07,084 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-19:24:07.083618', 'path': 'exp/ECL-PatchTST2023-08-04-19:24:07.083618', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 19:24:07,084 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 19:24:07,293 - [*] phase 0 Dataset load!
2023-08-04 19:24:08,300 - [*] phase 0 Training start
train 7585
2023-08-04 19:24:20,755 - epoch:0, training loss:0.3280 validation loss:0.1974
train 7585
vs, vt 0.19740756820229924 0.1985697873374995
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.18574145129498312 0.18878784731907003
2023-08-04 19:24:47,709 - epoch:1, training loss:1.4947 validation loss:0.1857
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.18067758398897507 0.18456407811711817
2023-08-04 19:25:07,273 - epoch:2, training loss:1.2294 validation loss:0.1807
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.17757519619429812 0.18343823140158372
2023-08-04 19:25:26,684 - epoch:3, training loss:0.9803 validation loss:0.1776
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.17484978301560178 0.17910330786424525
2023-08-04 19:25:46,520 - epoch:4, training loss:0.8405 validation loss:0.1748
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.17307206878767295 0.17760423242169268
2023-08-04 19:26:05,848 - epoch:5, training loss:0.7906 validation loss:0.1731
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.17424632082967198 0.17489861631218126
2023-08-04 19:26:25,854 - epoch:6, training loss:0.7791 validation loss:0.1742
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.17395793230218046 0.17388217457953623
2023-08-04 19:26:45,265 - epoch:7, training loss:0.7623 validation loss:0.1740
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.17510741992908366 0.17621766469057867
2023-08-04 19:27:04,854 - epoch:8, training loss:0.7640 validation loss:0.1751
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.17423290868892388 0.17762918901794097
2023-08-04 19:27:23,435 - epoch:9, training loss:0.7685 validation loss:0.1742
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.17410371649791212 0.17719687302322948
2023-08-04 19:27:42,525 - epoch:10, training loss:0.7709 validation loss:0.1741
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.17461594632443259 0.17720875363139546
2023-08-04 19:28:02,764 - epoch:11, training loss:0.7698 validation loss:0.1746
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.17536329719073632 0.17861610848237486
2023-08-04 19:28:22,059 - epoch:12, training loss:0.7669 validation loss:0.1754
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.17625021167537747 0.1786902301451739
2023-08-04 19:28:40,711 - epoch:13, training loss:0.7667 validation loss:0.1763
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.1762644694570233 0.18108272662057595
2023-08-04 19:28:59,398 - epoch:14, training loss:0.7644 validation loss:0.1763
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.17740278756793806 0.1805622397976763
2023-08-04 19:29:18,824 - epoch:15, training loss:0.7623 validation loss:0.1774
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.17737767253728473 0.18269265355432734
2023-08-04 19:29:37,346 - epoch:16, training loss:0.7623 validation loss:0.1774
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.17709530802334056 0.18161870429621024
2023-08-04 19:29:56,616 - epoch:17, training loss:0.7606 validation loss:0.1771
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.17808539529933648 0.18124937375678735
2023-08-04 19:30:16,767 - epoch:18, training loss:0.7563 validation loss:0.1781
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.17798640009234934 0.18217717615120552
2023-08-04 19:30:35,721 - epoch:19, training loss:0.7587 validation loss:0.1780
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.1783535495838698 0.1819129178629202
2023-08-04 19:30:54,767 - epoch:20, training loss:0.7549 validation loss:0.1784
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.178047677173334 0.18248285177875967
2023-08-04 19:31:14,547 - epoch:21, training loss:0.7586 validation loss:0.1780
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.17828663745347192 0.18260475248098373
2023-08-04 19:31:33,838 - epoch:22, training loss:0.7561 validation loss:0.1783
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.17832238643484957 0.18262447088080294
2023-08-04 19:31:52,802 - epoch:23, training loss:0.7552 validation loss:0.1783
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.1782603680210955 0.1825396734125474
2023-08-04 19:32:12,453 - epoch:24, training loss:0.7555 validation loss:0.1783
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.1784373835167464 0.18251013164134586
2023-08-04 19:32:31,605 - epoch:25, training loss:0.7539 validation loss:0.1784
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.17839388045317986 0.18265989882981076
2023-08-04 19:32:52,200 - epoch:26, training loss:0.7549 validation loss:0.1784
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.17864828600602992 0.18282975146875663
2023-08-04 19:33:11,689 - epoch:27, training loss:0.7540 validation loss:0.1786
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.17848103690673323 0.182727156973937
2023-08-04 19:33:31,363 - epoch:28, training loss:0.7543 validation loss:0.1785
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.17861266894375577 0.1826663661529036
2023-08-04 19:33:50,690 - epoch:29, training loss:0.7558 validation loss:0.1786
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-19:24:07.083618/0/0.1731_epoch_5.pkl  &  0.17388217457953623
2023-08-04 19:33:52,104 - [*] loss:0.3846
2023-08-04 19:33:52,215 - [*] phase 0, testing
2023-08-04 19:33:53,112 - T:720	MAE	0.419293	RMSE	0.382772	MAPE	190.065789
2023-08-04 19:33:53,116 - 720	mae	0.4193	
2023-08-04 19:33:53,117 - 720	rmse	0.3828	
2023-08-04 19:33:53,117 - 720	mape	190.0658	
2023-08-04 19:33:54,609 - [*] loss:0.3901
2023-08-04 19:33:54,759 - [*] phase 0, testing
2023-08-04 19:33:55,757 - T:720	MAE	0.417360	RMSE	0.388073	MAPE	182.312381
2023-08-04 19:33:55,761 - 720	mae	0.4174	
2023-08-04 19:33:55,761 - 720	rmse	0.3881	
2023-08-04 19:33:55,761 - 720	mape	182.3124	
2023-08-04 19:33:58,088 - logger name:exp/ECL-PatchTST2023-08-04-19:33:58.088185/ECL-PatchTST.log
2023-08-04 19:33:58,088 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-19:33:58.088185', 'path': 'exp/ECL-PatchTST2023-08-04-19:33:58.088185', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 19:33:58,089 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 19:33:58,307 - [*] phase 0 Dataset load!
2023-08-04 19:33:59,404 - [*] phase 0 Training start
train 7585
2023-08-04 19:34:12,384 - epoch:0, training loss:0.3117 validation loss:0.1925
train 7585
vs, vt 0.1925238690832082 0.19798029915374868
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.1831387423855417 0.18846501322353587
2023-08-04 19:34:44,595 - epoch:1, training loss:3.2336 validation loss:0.1831
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.17828194950433338 0.18192535857943928
2023-08-04 19:35:09,227 - epoch:2, training loss:2.6368 validation loss:0.1783
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.1767021145890741 0.17883749788298325
2023-08-04 19:35:33,427 - epoch:3, training loss:1.8251 validation loss:0.1767
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.17615271381595554 0.17627016345367713
2023-08-04 19:35:57,960 - epoch:4, training loss:1.3029 validation loss:0.1762
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.1751632808762438 0.17621030461262255
2023-08-04 19:36:23,325 - epoch:5, training loss:1.1590 validation loss:0.1752
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.17706122074057074 0.1759501962977297
2023-08-04 19:36:47,987 - epoch:6, training loss:1.1177 validation loss:0.1771
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.17742935129824808 0.17773914315244732
2023-08-04 19:37:12,210 - epoch:7, training loss:1.0778 validation loss:0.1774
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.17643456182935657 0.1792875824167448
2023-08-04 19:37:37,153 - epoch:8, training loss:1.0332 validation loss:0.1764
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.17775196143809488 0.17982888287481139
2023-08-04 19:38:01,548 - epoch:9, training loss:0.9827 validation loss:0.1778
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.17722894908750758 0.17653548586017945
2023-08-04 19:38:25,963 - epoch:10, training loss:0.9481 validation loss:0.1772
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.17740007752881332 0.1783069306436707
2023-08-04 19:38:50,132 - epoch:11, training loss:0.9284 validation loss:0.1774
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.1783328772905995 0.17779167084132924
2023-08-04 19:39:14,995 - epoch:12, training loss:0.9307 validation loss:0.1783
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.17818514389150283 0.1792562187594526
2023-08-04 19:39:39,317 - epoch:13, training loss:0.9295 validation loss:0.1782
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.17813158736509435 0.18206892644657807
2023-08-04 19:40:03,859 - epoch:14, training loss:0.9321 validation loss:0.1781
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.17886480097385013 0.1809371911427554
2023-08-04 19:40:28,681 - epoch:15, training loss:0.9214 validation loss:0.1789
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.1785388753256377 0.18053825199604034
2023-08-04 19:40:53,409 - epoch:16, training loss:0.9214 validation loss:0.1785
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.177532027968589 0.18185321416924982
2023-08-04 19:41:17,959 - epoch:17, training loss:0.9252 validation loss:0.1775
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.17802779924343615 0.18219405719462564
2023-08-04 19:41:42,228 - epoch:18, training loss:0.9213 validation loss:0.1780
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.17853716497912125 0.18199625961920796
2023-08-04 19:42:06,910 - epoch:19, training loss:0.9223 validation loss:0.1785
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.17910282752093146 0.18301700318560882
2023-08-04 19:42:31,491 - epoch:20, training loss:0.9136 validation loss:0.1791
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.17891527471296928 0.18265759988742716
2023-08-04 19:42:55,730 - epoch:21, training loss:0.9256 validation loss:0.1789
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.17920531091444633 0.18303268797257366
2023-08-04 19:43:20,075 - epoch:22, training loss:0.9249 validation loss:0.1792
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.17918574525152936 0.18344686311834
2023-08-04 19:43:46,685 - epoch:23, training loss:0.9297 validation loss:0.1792
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.1795773002154687 0.18348343495060415
2023-08-04 19:44:11,932 - epoch:24, training loss:0.9270 validation loss:0.1796
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.17901604907477603 0.18330375017488704
2023-08-04 19:44:35,490 - epoch:25, training loss:0.9219 validation loss:0.1790
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.17921959411572008 0.18367217305828543
2023-08-04 19:45:11,290 - epoch:26, training loss:0.9231 validation loss:0.1792
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.17919299576212377 0.18372611044084325
2023-08-04 19:45:37,695 - epoch:27, training loss:0.9300 validation loss:0.1792
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.17918264581000104 0.18366611924241572
2023-08-04 19:46:05,055 - epoch:28, training loss:0.9262 validation loss:0.1792
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.17918888369903846 0.18368090426220612
2023-08-04 19:46:29,943 - epoch:29, training loss:0.9231 validation loss:0.1792
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-19:33:58.088185/0/0.1752_epoch_5.pkl  &  0.1759501962977297
2023-08-04 19:46:31,587 - [*] loss:0.3882
2023-08-04 19:46:31,884 - [*] phase 0, testing
2023-08-04 19:46:32,907 - T:720	MAE	0.423983	RMSE	0.386551	MAPE	195.041394
2023-08-04 19:46:32,911 - 720	mae	0.4240	
2023-08-04 19:46:32,911 - 720	rmse	0.3866	
2023-08-04 19:46:32,911 - 720	mape	195.0414	
2023-08-04 19:46:33,971 - [*] loss:0.3948
2023-08-04 19:46:34,061 - [*] phase 0, testing
2023-08-04 19:46:35,063 - T:720	MAE	0.421300	RMSE	0.392096	MAPE	188.064408
2023-08-04 19:46:35,067 - 720	mae	0.4213	
2023-08-04 19:46:35,067 - 720	rmse	0.3921	
2023-08-04 19:46:35,067 - 720	mape	188.0644	
2023-08-04 19:46:38,145 - logger name:exp/ECL-PatchTST2023-08-04-19:46:38.145185/ECL-PatchTST.log
2023-08-04 19:46:38,145 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-19:46:38.145185', 'path': 'exp/ECL-PatchTST2023-08-04-19:46:38.145185', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 19:46:38,145 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 19:46:38,357 - [*] phase 0 Dataset load!
2023-08-04 19:46:41,430 - [*] phase 0 Training start
train 7585
2023-08-04 19:46:56,400 - epoch:0, training loss:0.9569 validation loss:0.4437
train 7585
vs, vt 0.44365822655313153 0.4462026445304646
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.41332877734128165 0.4232623682302587
2023-08-04 19:47:25,512 - epoch:1, training loss:1.0107 validation loss:0.4133
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.4025687334292075 0.4135955598424463
2023-08-04 19:47:44,549 - epoch:2, training loss:0.9547 validation loss:0.4026
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.3930089464958976 0.4075771565822994
2023-08-04 19:48:03,677 - epoch:3, training loss:0.9172 validation loss:0.3930
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.39350760070716634 0.39780207767206077
2023-08-04 19:48:23,087 - epoch:4, training loss:0.8875 validation loss:0.3935
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.40540700844105554 0.3929182229673161
2023-08-04 19:48:42,049 - epoch:5, training loss:0.8610 validation loss:0.4054
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.4018411978202708 0.4014482905759531
2023-08-04 19:49:00,836 - epoch:6, training loss:0.8363 validation loss:0.4018
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.40828637340489554 0.40537093185326634
2023-08-04 19:49:19,851 - epoch:7, training loss:0.8127 validation loss:0.4083
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.4153684964951347 0.40951163979137645
2023-08-04 19:49:38,404 - epoch:8, training loss:0.7926 validation loss:0.4154
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.4136725138215458 0.41892531075898337
2023-08-04 19:49:56,844 - epoch:9, training loss:0.7748 validation loss:0.4137
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.4127034273217706 0.4245658131206737
2023-08-04 19:50:15,780 - epoch:10, training loss:0.7611 validation loss:0.4127
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.4124937110087451 0.4193582797751707
2023-08-04 19:50:34,510 - epoch:11, training loss:0.7477 validation loss:0.4125
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.41016388202414794 0.4228628536357599
2023-08-04 19:50:53,480 - epoch:12, training loss:0.7395 validation loss:0.4102
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.414853597388548 0.4248759119826205
2023-08-04 19:51:13,451 - epoch:13, training loss:0.7294 validation loss:0.4149
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.4154926003778682 0.43195659316637935
2023-08-04 19:51:31,978 - epoch:14, training loss:0.7224 validation loss:0.4155
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.41577493267900806 0.42588545557330637
2023-08-04 19:51:51,190 - epoch:15, training loss:0.7178 validation loss:0.4158
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.4169277575086145 0.4267611315145212
2023-08-04 19:52:09,860 - epoch:16, training loss:0.7147 validation loss:0.4169
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.41909436092657204 0.4263957537272397
2023-08-04 19:52:28,805 - epoch:17, training loss:0.7088 validation loss:0.4191
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.421818329130902 0.42833754113491845
2023-08-04 19:52:47,852 - epoch:18, training loss:0.7047 validation loss:0.4218
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.421060034457375 0.42925334634149775
2023-08-04 19:53:06,719 - epoch:19, training loss:0.6997 validation loss:0.4211
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.4180332159294802 0.42848505167400136
2023-08-04 19:53:25,496 - epoch:20, training loss:0.6966 validation loss:0.4180
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.4198263085940305 0.4265006754328223
2023-08-04 19:53:44,467 - epoch:21, training loss:0.6947 validation loss:0.4198
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.420734060161254 0.4281737300402978
2023-08-04 19:54:03,238 - epoch:22, training loss:0.6917 validation loss:0.4207
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.421651650001021 0.4269156442845569
2023-08-04 19:54:22,400 - epoch:23, training loss:0.6900 validation loss:0.4217
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.421912769184393 0.42832667249090534
2023-08-04 19:54:41,430 - epoch:24, training loss:0.6883 validation loss:0.4219
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.421760197948007 0.42787905782461166
2023-08-04 19:55:00,812 - epoch:25, training loss:0.6882 validation loss:0.4218
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.42118430751211505 0.4277920556419036
2023-08-04 19:55:19,401 - epoch:26, training loss:0.6852 validation loss:0.4212
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.4215174466371536 0.427492651869269
2023-08-04 19:55:38,319 - epoch:27, training loss:0.6849 validation loss:0.4215
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.42189542773891897 0.42832541421932335
2023-08-04 19:55:57,113 - epoch:28, training loss:0.6857 validation loss:0.4219
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.42150993469883413 0.4261980341637836
2023-08-04 19:56:16,360 - epoch:29, training loss:0.6866 validation loss:0.4215
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-19:46:38.145185/0/0.393_epoch_3.pkl  &  0.3929182229673161
2023-08-04 19:56:17,425 - [*] loss:0.3930
2023-08-04 19:56:17,464 - [*] phase 0, testing
2023-08-04 19:56:18,586 - T:720	MAE	0.426187	RMSE	0.391284	MAPE	192.380250
2023-08-04 19:56:18,588 - 720	mae	0.4262	
2023-08-04 19:56:18,588 - 720	rmse	0.3913	
2023-08-04 19:56:18,588 - 720	mape	192.3802	
2023-08-04 19:56:19,521 - [*] loss:0.3929
2023-08-04 19:56:19,608 - [*] phase 0, testing
2023-08-04 19:56:20,835 - T:720	MAE	0.423685	RMSE	0.391001	MAPE	187.547410
2023-08-04 19:56:20,837 - 720	mae	0.4237	
2023-08-04 19:56:20,837 - 720	rmse	0.3910	
2023-08-04 19:56:20,837 - 720	mape	187.5474	
2023-08-04 19:56:22,844 - logger name:exp/ECL-PatchTST2023-08-04-19:56:22.829472/ECL-PatchTST.log
2023-08-04 19:56:22,844 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-19:56:22.829472', 'path': 'exp/ECL-PatchTST2023-08-04-19:56:22.829472', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 19:56:22,844 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 19:56:23,039 - [*] phase 0 Dataset load!
2023-08-04 19:56:24,021 - [*] phase 0 Training start
train 7585
2023-08-04 19:56:36,707 - epoch:0, training loss:0.9012 validation loss:0.4313
train 7585
vs, vt 0.4312722078141044 0.4458739319268395
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.4083605110645294 0.42661163561484394
2023-08-04 19:57:08,480 - epoch:1, training loss:8.4451 validation loss:0.4084
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.39620175168794747 0.4179098834009731
2023-08-04 19:57:32,394 - epoch:2, training loss:6.6160 validation loss:0.3962
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.3942088879206601 0.41222504833165335
2023-08-04 19:57:56,590 - epoch:3, training loss:4.4416 validation loss:0.3942
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.39046404466909523 0.4049155045958126
2023-08-04 19:58:21,026 - epoch:4, training loss:3.2487 validation loss:0.3905
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.39513204027624693 0.41778600303565755
2023-08-04 19:58:45,346 - epoch:5, training loss:2.8908 validation loss:0.3951
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.3967862663900151 0.40735168755054474
2023-08-04 19:59:09,307 - epoch:6, training loss:2.7775 validation loss:0.3968
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.39529162338551355 0.4028998411753598
2023-08-04 19:59:33,884 - epoch:7, training loss:2.7366 validation loss:0.3953
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.3970714097513872 0.4017075396635953
2023-08-04 19:59:57,957 - epoch:8, training loss:2.6998 validation loss:0.3971
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.39888010831440196 0.40074144742068124
2023-08-04 20:00:28,641 - epoch:9, training loss:2.6482 validation loss:0.3989
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.40208835724522085 0.399668672505547
2023-08-04 20:00:52,991 - epoch:10, training loss:2.6434 validation loss:0.4021
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.4019712385009317 0.40304726449882283
2023-08-04 20:01:18,023 - epoch:11, training loss:2.6393 validation loss:0.4020
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.4024105843375711 0.403912068289869
2023-08-04 20:01:42,300 - epoch:12, training loss:2.6335 validation loss:0.4024
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.40458598645294414 0.40544973664424
2023-08-04 20:02:06,938 - epoch:13, training loss:2.6412 validation loss:0.4046
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.40520108359701495 0.40481560896424684
2023-08-04 20:02:31,405 - epoch:14, training loss:2.6097 validation loss:0.4052
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.4050268811338088 0.4102461863966549
2023-08-04 20:02:55,454 - epoch:15, training loss:2.6508 validation loss:0.4050
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.40672146134516773 0.4116399980643216
2023-08-04 20:03:19,738 - epoch:16, training loss:2.6685 validation loss:0.4067
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.40763086694128375 0.40901002638480244
2023-08-04 20:03:44,031 - epoch:17, training loss:2.6536 validation loss:0.4076
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.4067027419805527 0.40767182935686674
2023-08-04 20:04:07,624 - epoch:18, training loss:2.6459 validation loss:0.4067
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.4084431213491103 0.4085730103885426
2023-08-04 20:04:31,975 - epoch:19, training loss:2.6506 validation loss:0.4084
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.4082113197621177 0.40735241244820986
2023-08-04 20:04:56,096 - epoch:20, training loss:2.6600 validation loss:0.4082
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.40881863937658425 0.4065328748787151
2023-08-04 20:05:20,528 - epoch:21, training loss:2.6764 validation loss:0.4088
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.4086659647086087 0.40770772274802713
2023-08-04 20:05:44,310 - epoch:22, training loss:2.6713 validation loss:0.4087
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.40980200907763314 0.4084950317354763
2023-08-04 20:06:08,248 - epoch:23, training loss:2.6628 validation loss:0.4098
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.40910611082525816 0.40947068175848794
2023-08-04 20:06:33,222 - epoch:24, training loss:2.6759 validation loss:0.4091
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.41031047351220074 0.4079517424106598
2023-08-04 20:06:57,386 - epoch:25, training loss:2.6747 validation loss:0.4103
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.40922142565250397 0.407874155570479
2023-08-04 20:07:21,404 - epoch:26, training loss:2.6735 validation loss:0.4092
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.4093227824744056 0.4082853285705342
2023-08-04 20:07:46,009 - epoch:27, training loss:2.6906 validation loss:0.4093
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.40983348120661345 0.4075923237730475
2023-08-04 20:08:10,669 - epoch:28, training loss:2.6599 validation loss:0.4098
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.4096023282584022 0.40725909436450286
2023-08-04 20:08:34,655 - epoch:29, training loss:2.6812 validation loss:0.4096
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-19:56:22.829472/0/0.3905_epoch_4.pkl  &  0.399668672505547
2023-08-04 20:08:35,994 - [*] loss:0.3905
2023-08-04 20:08:36,011 - [*] phase 0, testing
2023-08-04 20:08:37,121 - T:720	MAE	0.428060	RMSE	0.388799	MAPE	198.227119
2023-08-04 20:08:37,125 - 720	mae	0.4281	
2023-08-04 20:08:37,125 - 720	rmse	0.3888	
2023-08-04 20:08:37,125 - 720	mape	198.2271	
2023-08-04 20:08:38,389 - [*] loss:0.3997
2023-08-04 20:08:38,404 - [*] phase 0, testing
2023-08-04 20:08:39,335 - T:720	MAE	0.426414	RMSE	0.396948	MAPE	187.443423
2023-08-04 20:08:39,335 - 720	mae	0.4264	
2023-08-04 20:08:39,335 - 720	rmse	0.3969	
2023-08-04 20:08:39,335 - 720	mape	187.4434	
2023-08-04 20:08:41,595 - logger name:exp/ECL-PatchTST2023-08-04-20:08:41.594719/ECL-PatchTST.log
2023-08-04 20:08:41,595 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-20:08:41.594719', 'path': 'exp/ECL-PatchTST2023-08-04-20:08:41.594719', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 20:08:41,595 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 20:08:41,895 - [*] phase 0 Dataset load!
2023-08-04 20:08:42,879 - [*] phase 0 Training start
train 7585
2023-08-04 20:08:56,012 - epoch:0, training loss:0.3119 validation loss:0.1926
train 7585
vs, vt 0.19264757808517008 0.19790074623682918
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.18274454093154738 0.18927319128723705
2023-08-04 20:09:27,908 - epoch:1, training loss:0.8456 validation loss:0.1827
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.17772646905744777 0.1846292860367719
2023-08-04 20:09:51,780 - epoch:2, training loss:0.7957 validation loss:0.1777
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.1769025097875034 0.18220801037900589
2023-08-04 20:10:16,408 - epoch:3, training loss:0.7349 validation loss:0.1769
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.1782893981127178 0.17873464305611217
2023-08-04 20:10:40,653 - epoch:4, training loss:0.6743 validation loss:0.1783
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.1807335549417664 0.17902765883242383
2023-08-04 20:11:04,642 - epoch:5, training loss:0.6490 validation loss:0.1807
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.18078511778046102 0.1770868866759188
2023-08-04 20:11:28,820 - epoch:6, training loss:0.6326 validation loss:0.1808
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.18194746182245367 0.1784509453265106
2023-08-04 20:11:53,051 - epoch:7, training loss:0.6202 validation loss:0.1819
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.18210240847924175 0.17726054765722332
2023-08-04 20:12:17,663 - epoch:8, training loss:0.6142 validation loss:0.1821
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.1813072036294376 0.17486718603793314
2023-08-04 20:12:42,338 - epoch:9, training loss:0.6065 validation loss:0.1813
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.18316661566495895 0.1767851944793673
2023-08-04 20:13:06,812 - epoch:10, training loss:0.5991 validation loss:0.1832
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.18274204783579884 0.17680560337270007
2023-08-04 20:13:30,413 - epoch:11, training loss:0.5905 validation loss:0.1827
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.18175220533328898 0.17821105372379809
2023-08-04 20:13:55,213 - epoch:12, training loss:0.5864 validation loss:0.1818
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.18277552329442082 0.17684881060438998
2023-08-04 20:14:19,075 - epoch:13, training loss:0.5791 validation loss:0.1828
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.18290050393518278 0.17759073175051632
2023-08-04 20:14:42,917 - epoch:14, training loss:0.5730 validation loss:0.1829
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.18322383919182947 0.17738407850265503
2023-08-04 20:15:06,999 - epoch:15, training loss:0.5707 validation loss:0.1832
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.1826681748909109 0.17755665016524932
2023-08-04 20:15:30,959 - epoch:16, training loss:0.5677 validation loss:0.1827
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.1832668299184126 0.17722056367818048
2023-08-04 20:15:55,524 - epoch:17, training loss:0.5658 validation loss:0.1833
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.18372591716401718 0.17759352067814155
2023-08-04 20:16:19,713 - epoch:18, training loss:0.5617 validation loss:0.1837
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.183545304571881 0.17842158587539897
2023-08-04 20:16:43,866 - epoch:19, training loss:0.5600 validation loss:0.1835
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.18381071704275467 0.17708558961749077
2023-08-04 20:17:07,919 - epoch:20, training loss:0.5591 validation loss:0.1838
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.18428764360792496 0.17730739471666954
2023-08-04 20:17:32,243 - epoch:21, training loss:0.5580 validation loss:0.1843
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.18380491873797247 0.17683164731544607
2023-08-04 20:17:55,898 - epoch:22, training loss:0.5571 validation loss:0.1838
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.1840534275945495 0.1771063861601493
2023-08-04 20:18:20,226 - epoch:23, training loss:0.5552 validation loss:0.1841
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.18415681272745132 0.17740063347360668
2023-08-04 20:18:44,543 - epoch:24, training loss:0.5545 validation loss:0.1842
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.1833964757183019 0.17695976092534907
2023-08-04 20:19:08,136 - epoch:25, training loss:0.5542 validation loss:0.1834
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.1836187208400053 0.17685279491193154
2023-08-04 20:19:32,694 - epoch:26, training loss:0.5534 validation loss:0.1836
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.1837745387764538 0.1769465078764102
2023-08-04 20:19:56,520 - epoch:27, training loss:0.5522 validation loss:0.1838
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.18407813678769505 0.17720222626538837
2023-08-04 20:20:20,913 - epoch:28, training loss:0.5530 validation loss:0.1841
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.18394480908618255 0.17678007154780276
2023-08-04 20:20:45,923 - epoch:29, training loss:0.5542 validation loss:0.1839
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-20:08:41.594719/0/0.1769_epoch_3.pkl  &  0.17486718603793314
2023-08-04 20:20:47,047 - [*] loss:0.3930
2023-08-04 20:20:47,061 - [*] phase 0, testing
2023-08-04 20:20:48,023 - T:720	MAE	0.426702	RMSE	0.391365	MAPE	194.277143
2023-08-04 20:20:48,023 - 720	mae	0.4267	
2023-08-04 20:20:48,023 - 720	rmse	0.3914	
2023-08-04 20:20:48,023 - 720	mape	194.2771	
2023-08-04 20:20:49,059 - [*] loss:0.3929
2023-08-04 20:20:49,073 - [*] phase 0, testing
2023-08-04 20:20:50,059 - T:720	MAE	0.419139	RMSE	0.390426	MAPE	182.177508
2023-08-04 20:20:50,059 - 720	mae	0.4191	
2023-08-04 20:20:50,059 - 720	rmse	0.3904	
2023-08-04 20:20:50,059 - 720	mape	182.1775	
2023-08-04 20:20:52,182 - logger name:exp/ECL-PatchTST2023-08-04-20:20:52.182291/ECL-PatchTST.log
2023-08-04 20:20:52,182 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-20:20:52.182291', 'path': 'exp/ECL-PatchTST2023-08-04-20:20:52.182291', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 20:20:52,183 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 20:20:52,379 - [*] phase 0 Dataset load!
2023-08-04 20:20:53,346 - [*] phase 0 Training start
train 7585
2023-08-04 20:21:06,610 - epoch:0, training loss:0.3287 validation loss:0.1973
train 7585
vs, vt 0.1973071887212641 0.19881752615465836
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.18620533145525875 0.189584881067276
2023-08-04 20:21:33,220 - epoch:1, training loss:1.4841 validation loss:0.1862
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.18138238086419947 0.18673189072047963
2023-08-04 20:21:52,462 - epoch:2, training loss:1.2056 validation loss:0.1814
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.17773588527651393 0.18374345298199093
2023-08-04 20:22:11,613 - epoch:3, training loss:0.9548 validation loss:0.1777
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.1752724640947931 0.18125176824191036
2023-08-04 20:22:30,074 - epoch:4, training loss:0.8328 validation loss:0.1753
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.1760719102971694 0.17779691232477918
2023-08-04 20:22:49,415 - epoch:5, training loss:0.7813 validation loss:0.1761
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.17498773599372192 0.17758650529910536
2023-08-04 20:23:08,464 - epoch:6, training loss:0.7714 validation loss:0.1750
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.17421703193994129 0.17689064429963336
2023-08-04 20:23:27,974 - epoch:7, training loss:0.7537 validation loss:0.1742
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.1749635430819848 0.1756916249937871
2023-08-04 20:23:46,899 - epoch:8, training loss:0.7487 validation loss:0.1750
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.17509913575999878 0.1755742908400648
2023-08-04 20:24:05,814 - epoch:9, training loss:0.7535 validation loss:0.1751
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.17548276199137464 0.17638060713515563
2023-08-04 20:24:24,441 - epoch:10, training loss:0.7618 validation loss:0.1755
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.17559394349946694 0.17654967855881243
2023-08-04 20:24:43,928 - epoch:11, training loss:0.7718 validation loss:0.1756
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.17652107249288 0.17812715733752532
2023-08-04 20:25:02,941 - epoch:12, training loss:0.7696 validation loss:0.1765
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.17710830249330578 0.18053950610406258
2023-08-04 20:25:21,345 - epoch:13, training loss:0.7772 validation loss:0.1771
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.17803260311484337 0.18055511770002983
2023-08-04 20:25:39,967 - epoch:14, training loss:0.7759 validation loss:0.1780
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.17751573179574573 0.18052940026802175
2023-08-04 20:25:58,977 - epoch:15, training loss:0.7770 validation loss:0.1775
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.17952057061826482 0.180432641550022
2023-08-04 20:26:18,213 - epoch:16, training loss:0.7747 validation loss:0.1795
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.1802193706526476 0.18292075855767026
2023-08-04 20:26:37,266 - epoch:17, training loss:0.7766 validation loss:0.1802
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.1804190784254495 0.18294866558383494
2023-08-04 20:26:56,039 - epoch:18, training loss:0.7743 validation loss:0.1804
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.18063544021809802 0.18414467485512004
2023-08-04 20:27:14,851 - epoch:19, training loss:0.7743 validation loss:0.1806
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.18072516252012813 0.18367506673230843
2023-08-04 20:27:33,735 - epoch:20, training loss:0.7745 validation loss:0.1807
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.18099069113240523 0.18447117967640653
2023-08-04 20:27:52,591 - epoch:21, training loss:0.7746 validation loss:0.1810
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.18093116340391777 0.18370013596380458
2023-08-04 20:28:12,024 - epoch:22, training loss:0.7692 validation loss:0.1809
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.18097058311104774 0.18434178259442835
2023-08-04 20:28:30,284 - epoch:23, training loss:0.7696 validation loss:0.1810
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.18116726266110644 0.1843434751910322
2023-08-04 20:28:48,883 - epoch:24, training loss:0.7698 validation loss:0.1812
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.18123164290890975 0.18460398474160364
2023-08-04 20:29:07,810 - epoch:25, training loss:0.7701 validation loss:0.1812
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.18078293322640307 0.1841076527009992
2023-08-04 20:29:26,760 - epoch:26, training loss:0.7687 validation loss:0.1808
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.18120130085769823 0.18410089020343387
2023-08-04 20:29:45,433 - epoch:27, training loss:0.7688 validation loss:0.1812
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.18140987372573683 0.18451142420663552
2023-08-04 20:30:03,967 - epoch:28, training loss:0.7698 validation loss:0.1814
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.18122081611962879 0.18410124739303307
2023-08-04 20:30:22,696 - epoch:29, training loss:0.7705 validation loss:0.1812
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-20:20:52.182291/0/0.1742_epoch_7.pkl  &  0.1755742908400648
2023-08-04 20:30:23,997 - [*] loss:0.3874
2023-08-04 20:30:24,012 - [*] phase 0, testing
2023-08-04 20:30:25,040 - T:720	MAE	0.420935	RMSE	0.385566	MAPE	191.143250
2023-08-04 20:30:25,040 - 720	mae	0.4209	
2023-08-04 20:30:25,040 - 720	rmse	0.3856	
2023-08-04 20:30:25,040 - 720	mape	191.1433	
2023-08-04 20:30:26,480 - [*] loss:0.3937
2023-08-04 20:30:26,495 - [*] phase 0, testing
2023-08-04 20:30:27,503 - T:720	MAE	0.421437	RMSE	0.391596	MAPE	188.994324
2023-08-04 20:30:27,504 - 720	mae	0.4214	
2023-08-04 20:30:27,504 - 720	rmse	0.3916	
2023-08-04 20:30:27,504 - 720	mape	188.9943	
2023-08-04 20:30:29,620 - logger name:exp/ECL-PatchTST2023-08-04-20:30:29.620594/ECL-PatchTST.log
2023-08-04 20:30:29,621 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-20:30:29.620594', 'path': 'exp/ECL-PatchTST2023-08-04-20:30:29.620594', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 20:30:29,621 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 20:30:29,821 - [*] phase 0 Dataset load!
2023-08-04 20:30:30,803 - [*] phase 0 Training start
train 7585
2023-08-04 20:30:43,630 - epoch:0, training loss:0.3119 validation loss:0.1926
train 7585
vs, vt 0.19264757808517008 0.19790074623682918
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.18354661631233551 0.1892732890213237
2023-08-04 20:31:15,963 - epoch:1, training loss:3.1443 validation loss:0.1835
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.17851231860763886 0.18489539184991052
2023-08-04 20:31:40,888 - epoch:2, training loss:2.5289 validation loss:0.1785
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.17746366319410942 0.1814390959985116
2023-08-04 20:32:05,187 - epoch:3, training loss:1.6967 validation loss:0.1775
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.1756666850517778 0.17975910753011703
2023-08-04 20:32:29,483 - epoch:4, training loss:1.2371 validation loss:0.1757
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.1773255338125369 0.18223540046635797
2023-08-04 20:32:54,097 - epoch:5, training loss:1.1058 validation loss:0.1773
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.17754094740923712 0.1832029545570121
2023-08-04 20:33:17,867 - epoch:6, training loss:1.0481 validation loss:0.1775
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.17721824391799815 0.18127755864578135
2023-08-04 20:33:42,914 - epoch:7, training loss:1.0193 validation loss:0.1772
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.1776729716974146 0.17920635246178684
2023-08-04 20:34:07,138 - epoch:8, training loss:0.9925 validation loss:0.1777
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.17759681734092095 0.17846882167984457
2023-08-04 20:34:31,127 - epoch:9, training loss:0.9736 validation loss:0.1776
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.17793985741103396 0.17812238414497936
2023-08-04 20:34:55,728 - epoch:10, training loss:0.9501 validation loss:0.1779
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.17711919305079124 0.17809835032505147
2023-08-04 20:35:20,443 - epoch:11, training loss:0.9505 validation loss:0.1771
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.17808853747213588 0.1813801671652233
2023-08-04 20:35:44,516 - epoch:12, training loss:0.9473 validation loss:0.1781
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.17989610486170826 0.18003959909957998
2023-08-04 20:36:08,577 - epoch:13, training loss:0.9403 validation loss:0.1799
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.18012160588713252 0.1810884068117422
2023-08-04 20:36:33,392 - epoch:14, training loss:0.9366 validation loss:0.1801
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.18080208143767187 0.18126732154804118
2023-08-04 20:36:57,252 - epoch:15, training loss:0.9297 validation loss:0.1808
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.18065906228388057 0.18114066956674352
2023-08-04 20:37:21,880 - epoch:16, training loss:0.9279 validation loss:0.1807
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.18153302502982757 0.1828088484266225
2023-08-04 20:37:46,236 - epoch:17, training loss:0.9201 validation loss:0.1815
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.18036367525072658 0.18071348921341054
2023-08-04 20:38:10,815 - epoch:18, training loss:0.9138 validation loss:0.1804
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.18080773116911159 0.1811939149218447
2023-08-04 20:38:34,926 - epoch:19, training loss:0.9124 validation loss:0.1808
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.1808290332555771 0.18156920680228403
2023-08-04 20:38:59,357 - epoch:20, training loss:0.9123 validation loss:0.1808
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.18159862607717514 0.18088165320017757
2023-08-04 20:39:22,988 - epoch:21, training loss:0.9140 validation loss:0.1816
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.18048158682444515 0.18143914070199518
2023-08-04 20:39:47,837 - epoch:22, training loss:0.9183 validation loss:0.1805
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.18122230514007456 0.18156237768776276
2023-08-04 20:40:12,634 - epoch:23, training loss:0.9162 validation loss:0.1812
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.18116093339288936 0.18219098098137798
2023-08-04 20:40:36,963 - epoch:24, training loss:0.9154 validation loss:0.1812
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.1816677880637786 0.18136892993660533
2023-08-04 20:41:01,377 - epoch:25, training loss:0.9133 validation loss:0.1817
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.18134701339637532 0.1814209091312745
2023-08-04 20:41:25,993 - epoch:26, training loss:0.9138 validation loss:0.1813
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.181364225990632 0.18149452086757212
2023-08-04 20:41:50,377 - epoch:27, training loss:0.9211 validation loss:0.1814
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.18175149971947951 0.18165913005085552
2023-08-04 20:42:15,114 - epoch:28, training loss:0.9131 validation loss:0.1818
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.18151890311171026 0.18129411164452047
2023-08-04 20:42:39,170 - epoch:29, training loss:0.9133 validation loss:0.1815
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-20:30:29.620594/0/0.1757_epoch_4.pkl  &  0.17809835032505147
2023-08-04 20:42:40,839 - [*] loss:0.3897
2023-08-04 20:42:40,854 - [*] phase 0, testing
2023-08-04 20:42:41,923 - T:720	MAE	0.424283	RMSE	0.387952	MAPE	191.635942
2023-08-04 20:42:41,924 - 720	mae	0.4243	
2023-08-04 20:42:41,924 - 720	rmse	0.3880	
2023-08-04 20:42:41,924 - 720	mape	191.6359	
2023-08-04 20:42:43,333 - [*] loss:0.3989
2023-08-04 20:42:43,347 - [*] phase 0, testing
2023-08-04 20:42:44,321 - T:720	MAE	0.426543	RMSE	0.396559	MAPE	191.360605
2023-08-04 20:42:44,322 - 720	mae	0.4265	
2023-08-04 20:42:44,322 - 720	rmse	0.3966	
2023-08-04 20:42:44,322 - 720	mape	191.3606	
2023-08-04 20:42:46,495 - logger name:exp/ECL-PatchTST2023-08-04-20:42:46.494689/ECL-PatchTST.log
2023-08-04 20:42:46,495 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-20:42:46.494689', 'path': 'exp/ECL-PatchTST2023-08-04-20:42:46.494689', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 20:42:46,495 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 20:42:46,718 - [*] phase 0 Dataset load!
2023-08-04 20:42:47,753 - [*] phase 0 Training start
train 7585
2023-08-04 20:43:00,069 - epoch:0, training loss:0.9546 validation loss:0.4437
train 7585
vs, vt 0.44365643227801604 0.4462485497488695
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.41229431155849905 0.42164936398758607
2023-08-04 20:43:27,518 - epoch:1, training loss:1.0067 validation loss:0.4123
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.402030307580443 0.4125487177687533
2023-08-04 20:43:47,212 - epoch:2, training loss:0.9509 validation loss:0.4020
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.39300675339558544 0.4054569759789635
2023-08-04 20:44:06,151 - epoch:3, training loss:0.9167 validation loss:0.3930
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.39933640772805495 0.401399367434137
2023-08-04 20:44:25,274 - epoch:4, training loss:0.8898 validation loss:0.3993
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.40787651784279766 0.3952628895640373
2023-08-04 20:44:44,952 - epoch:5, training loss:0.8554 validation loss:0.4079
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.4149671643972397 0.3992096737903707
2023-08-04 20:45:04,093 - epoch:6, training loss:0.8304 validation loss:0.4150
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.41817719796124625 0.40709847825414996
2023-08-04 20:45:22,895 - epoch:7, training loss:0.8109 validation loss:0.4182
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.4167787765755373 0.418482739259215
2023-08-04 20:45:41,936 - epoch:8, training loss:0.7924 validation loss:0.4168
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.42288198716500225 0.42182568723664565
2023-08-04 20:46:01,361 - epoch:9, training loss:0.7746 validation loss:0.4229
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.42772813579615426 0.43197423394988566
2023-08-04 20:46:20,746 - epoch:10, training loss:0.7612 validation loss:0.4277
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.42358152130070853 0.42663657665252686
2023-08-04 20:46:39,862 - epoch:11, training loss:0.7472 validation loss:0.4236
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.4321206615251653 0.4267443133627667
2023-08-04 20:46:59,196 - epoch:12, training loss:0.7383 validation loss:0.4321
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.43403560624403115 0.4294552544460577
2023-08-04 20:47:18,453 - epoch:13, training loss:0.7291 validation loss:0.4340
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.43454500331598167 0.43462445192477284
2023-08-04 20:47:37,772 - epoch:14, training loss:0.7230 validation loss:0.4345
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.43431928666198955 0.43512379158945647
2023-08-04 20:47:57,576 - epoch:15, training loss:0.7184 validation loss:0.4343
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.4441000608836903 0.4338842428782407
2023-08-04 20:48:16,607 - epoch:16, training loss:0.7097 validation loss:0.4441
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.4438491752919029 0.43867622666499195
2023-08-04 20:48:35,789 - epoch:17, training loss:0.7091 validation loss:0.4438
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.44410670329542723 0.43672578387400685
2023-08-04 20:48:55,769 - epoch:18, training loss:0.7024 validation loss:0.4441
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.44479400182471557 0.4365699589252472
2023-08-04 20:49:14,911 - epoch:19, training loss:0.6986 validation loss:0.4448
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.4461129895027946 0.43520703385857973
2023-08-04 20:49:34,015 - epoch:20, training loss:0.6955 validation loss:0.4461
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.44560441637740417 0.4359000031562412
2023-08-04 20:49:52,241 - epoch:21, training loss:0.6929 validation loss:0.4456
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.44548692422754627 0.43536149754243736
2023-08-04 20:50:12,291 - epoch:22, training loss:0.6904 validation loss:0.4455
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.4481517742661869 0.4377909311476876
2023-08-04 20:50:31,508 - epoch:23, training loss:0.6877 validation loss:0.4482
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.44806312988786134 0.4355858292649774
2023-08-04 20:50:50,531 - epoch:24, training loss:0.6877 validation loss:0.4481
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.4491227865219116 0.43730322753681855
2023-08-04 20:51:09,353 - epoch:25, training loss:0.6861 validation loss:0.4491
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.4499370999195996 0.4370852397645221
2023-08-04 20:51:29,630 - epoch:26, training loss:0.6854 validation loss:0.4499
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.44960616353680105 0.437213923124706
2023-08-04 20:51:48,628 - epoch:27, training loss:0.6845 validation loss:0.4496
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.44928239022984223 0.43786125630140305
2023-08-04 20:52:08,066 - epoch:28, training loss:0.6848 validation loss:0.4493
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.4491037691340727 0.43705104729708505
2023-08-04 20:52:27,702 - epoch:29, training loss:0.6835 validation loss:0.4491
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-20:42:46.494689/0/0.393_epoch_3.pkl  &  0.3952628895640373
2023-08-04 20:52:28,800 - [*] loss:0.3930
2023-08-04 20:52:28,814 - [*] phase 0, testing
2023-08-04 20:52:29,671 - T:720	MAE	0.427678	RMSE	0.391395	MAPE	195.586514
2023-08-04 20:52:29,672 - 720	mae	0.4277	
2023-08-04 20:52:29,672 - 720	rmse	0.3914	
2023-08-04 20:52:29,672 - 720	mape	195.5865	
2023-08-04 20:52:31,023 - [*] loss:0.3953
2023-08-04 20:52:31,037 - [*] phase 0, testing
2023-08-04 20:52:31,763 - T:720	MAE	0.423316	RMSE	0.393278	MAPE	184.295416
2023-08-04 20:52:31,763 - 720	mae	0.4233	
2023-08-04 20:52:31,763 - 720	rmse	0.3933	
2023-08-04 20:52:31,763 - 720	mape	184.2954	
2023-08-04 20:52:34,012 - logger name:exp/ECL-PatchTST2023-08-04-20:52:34.012101/ECL-PatchTST.log
2023-08-04 20:52:34,012 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-20:52:34.012101', 'path': 'exp/ECL-PatchTST2023-08-04-20:52:34.012101', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 20:52:34,013 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 20:52:34,222 - [*] phase 0 Dataset load!
2023-08-04 20:52:35,256 - [*] phase 0 Training start
train 7585
2023-08-04 20:52:47,578 - epoch:0, training loss:0.8991 validation loss:0.4299
train 7585
vs, vt 0.42991325872785907 0.44633544893825755
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.4080965755616917 0.42590181091252494
2023-08-04 20:53:20,108 - epoch:1, training loss:8.6387 validation loss:0.4081
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.39829836479004693 0.41446055910166574
2023-08-04 20:53:44,663 - epoch:2, training loss:6.6729 validation loss:0.3983
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.3925929788280936 0.4076796599170741
2023-08-04 20:54:08,757 - epoch:3, training loss:4.4862 validation loss:0.3926
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.39254676813588424 0.40106918399824815
2023-08-04 20:54:33,057 - epoch:4, training loss:3.3255 validation loss:0.3925
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.3936446900753414 0.4058968196896946
2023-08-04 20:54:57,613 - epoch:5, training loss:3.0074 validation loss:0.3936
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.39502418654806476 0.40341319538214626
2023-08-04 20:55:21,905 - epoch:6, training loss:2.9345 validation loss:0.3950
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.3962361229693188 0.4090443179011345
2023-08-04 20:55:46,423 - epoch:7, training loss:2.9134 validation loss:0.3962
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.4019727084566565 0.4273736511959749
2023-08-04 20:56:10,294 - epoch:8, training loss:2.8118 validation loss:0.4020
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.3970463341649841 0.4131281713352484
2023-08-04 20:56:34,613 - epoch:9, training loss:2.7430 validation loss:0.3970
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.40068239937810335 0.4120243042707443
2023-08-04 20:56:59,602 - epoch:10, training loss:2.7774 validation loss:0.4007
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.39905473502243266 0.4104195824440788
2023-08-04 20:57:23,730 - epoch:11, training loss:2.7686 validation loss:0.3991
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.40167448801152844 0.4178220305372687
2023-08-04 20:57:47,774 - epoch:12, training loss:2.7645 validation loss:0.4017
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.4017486217267373 0.42731206644983855
2023-08-04 20:58:12,149 - epoch:13, training loss:2.7321 validation loss:0.4017
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.4010981207384783 0.41917170321240144
2023-08-04 20:58:37,393 - epoch:14, training loss:2.8009 validation loss:0.4011
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.40260259193532605 0.4201008283040103
2023-08-04 20:59:01,117 - epoch:15, training loss:2.7541 validation loss:0.4026
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.40305181329741196 0.42122820808606987
2023-08-04 20:59:25,752 - epoch:16, training loss:2.7990 validation loss:0.4031
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.4034474768182811 0.42107493912472443
2023-08-04 20:59:49,623 - epoch:17, training loss:2.7960 validation loss:0.4034
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.406145577045048 0.421416196752997
2023-08-04 21:00:14,441 - epoch:18, training loss:2.7839 validation loss:0.4061
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.40579985695726734 0.41760889046332417
2023-08-04 21:00:38,812 - epoch:19, training loss:2.8018 validation loss:0.4058
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.4058617357822025 0.4192733300082824
2023-08-04 21:01:02,804 - epoch:20, training loss:2.7933 validation loss:0.4059
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.40609869317096825 0.4199307744993883
2023-08-04 21:01:26,041 - epoch:21, training loss:2.7817 validation loss:0.4061
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.4065774836960961 0.41914058082243977
2023-08-04 21:01:50,619 - epoch:22, training loss:2.7718 validation loss:0.4066
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.4070904811515528 0.4212828094468397
2023-08-04 21:02:14,982 - epoch:23, training loss:2.7813 validation loss:0.4071
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.40766190474524217 0.41905614207772646
2023-08-04 21:02:39,065 - epoch:24, training loss:2.7773 validation loss:0.4077
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.40790804124930324 0.4198158094111611
2023-08-04 21:03:03,541 - epoch:25, training loss:2.7900 validation loss:0.4079
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.40800707261351976 0.4206358974470812
2023-08-04 21:03:27,381 - epoch:26, training loss:2.7775 validation loss:0.4080
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.4080551529631895 0.42056451825534596
2023-08-04 21:03:51,249 - epoch:27, training loss:2.7734 validation loss:0.4081
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.40798699505188885 0.4198620389489567
2023-08-04 21:04:16,135 - epoch:28, training loss:2.7812 validation loss:0.4080
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.40812521953793135 0.4202466580797644
2023-08-04 21:04:40,433 - epoch:29, training loss:2.7679 validation loss:0.4081
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-20:52:34.012101/0/0.3925_epoch_4.pkl  &  0.40106918399824815
2023-08-04 21:04:42,124 - [*] loss:0.3925
2023-08-04 21:04:42,138 - [*] phase 0, testing
2023-08-04 21:04:42,873 - T:720	MAE	0.427021	RMSE	0.390978	MAPE	196.457458
2023-08-04 21:04:42,874 - 720	mae	0.4270	
2023-08-04 21:04:42,874 - 720	rmse	0.3910	
2023-08-04 21:04:42,874 - 720	mape	196.4575	
2023-08-04 21:04:45,029 - [*] loss:0.4011
2023-08-04 21:04:45,044 - [*] phase 0, testing
2023-08-04 21:04:45,784 - T:720	MAE	0.426605	RMSE	0.398398	MAPE	188.182962
2023-08-04 21:04:45,785 - 720	mae	0.4266	
2023-08-04 21:04:45,785 - 720	rmse	0.3984	
2023-08-04 21:04:45,785 - 720	mape	188.1830	
2023-08-04 21:04:47,977 - logger name:exp/ECL-PatchTST2023-08-04-21:04:47.977294/ECL-PatchTST.log
2023-08-04 21:04:47,977 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-21:04:47.977294', 'path': 'exp/ECL-PatchTST2023-08-04-21:04:47.977294', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 21:04:47,978 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 21:04:48,173 - [*] phase 0 Dataset load!
2023-08-04 21:04:49,170 - [*] phase 0 Training start
train 7585
2023-08-04 21:05:01,958 - epoch:0, training loss:0.3114 validation loss:0.1921
train 7585
vs, vt 0.19212167666238897 0.19794542649213007
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.18258382270441337 0.18897178944419413
2023-08-04 21:05:34,587 - epoch:1, training loss:0.8330 validation loss:0.1826
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.17941869554274223 0.18392394570743337
2023-08-04 21:05:58,473 - epoch:2, training loss:0.7842 validation loss:0.1794
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.1761008842903025 0.18156168763251865
2023-08-04 21:06:23,368 - epoch:3, training loss:0.7305 validation loss:0.1761
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.17758086279911153 0.17904582006089828
2023-08-04 21:06:47,278 - epoch:4, training loss:0.6759 validation loss:0.1776
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.18148470275542317 0.17962483470054233
2023-08-04 21:07:10,673 - epoch:5, training loss:0.6494 validation loss:0.1815
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.18194213816348245 0.17804797103299813
2023-08-04 21:07:35,008 - epoch:6, training loss:0.6333 validation loss:0.1819
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.18235713681753943 0.17877555945340326
2023-08-04 21:07:59,755 - epoch:7, training loss:0.6233 validation loss:0.1824
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.18325379315544577 0.18253725723308675
2023-08-04 21:08:24,508 - epoch:8, training loss:0.6147 validation loss:0.1833
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.18438124744331136 0.1776622713488691
2023-08-04 21:08:48,626 - epoch:9, training loss:0.6041 validation loss:0.1844
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.18528717098867192 0.17745323917445013
2023-08-04 21:09:13,380 - epoch:10, training loss:0.5963 validation loss:0.1853
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.18285090695409215 0.17765567210667274
2023-08-04 21:09:37,311 - epoch:11, training loss:0.5870 validation loss:0.1829
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.1844124890425626 0.1803126072182375
2023-08-04 21:10:01,362 - epoch:12, training loss:0.5835 validation loss:0.1844
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.18453269802472172 0.1797312962658265
2023-08-04 21:10:26,200 - epoch:13, training loss:0.5777 validation loss:0.1845
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.1830004484337919 0.18006233125925064
2023-08-04 21:10:50,930 - epoch:14, training loss:0.5734 validation loss:0.1830
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.18313720778507345 0.18012496697552063
2023-08-04 21:11:15,321 - epoch:15, training loss:0.5704 validation loss:0.1831
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.1850658532451181 0.18029404563062332
2023-08-04 21:11:39,278 - epoch:16, training loss:0.5669 validation loss:0.1851
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.18377404572332606 0.1800140992683523
2023-08-04 21:12:03,048 - epoch:17, training loss:0.5649 validation loss:0.1838
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.184760686229257 0.17997552892741034
2023-08-04 21:12:27,631 - epoch:18, training loss:0.5630 validation loss:0.1848
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.18390476002412684 0.18015589039115346
2023-08-04 21:12:51,444 - epoch:19, training loss:0.5627 validation loss:0.1839
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.1843409893267295 0.17983547291334936
2023-08-04 21:13:15,899 - epoch:20, training loss:0.5613 validation loss:0.1843
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.18462416222866843 0.1805979460477829
2023-08-04 21:13:39,633 - epoch:21, training loss:0.5599 validation loss:0.1846
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.1853461125317742 0.1798088046557763
2023-08-04 21:14:03,901 - epoch:22, training loss:0.5570 validation loss:0.1853
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.18481128356036017 0.1804039592252058
2023-08-04 21:14:28,761 - epoch:23, training loss:0.5563 validation loss:0.1848
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.18526523253496954 0.17957824687747395
2023-08-04 21:14:53,263 - epoch:24, training loss:0.5579 validation loss:0.1853
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.18549826565910787 0.17945371568202972
2023-08-04 21:15:17,583 - epoch:25, training loss:0.5549 validation loss:0.1855
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.18500673595596762 0.17967882533283794
2023-08-04 21:15:41,408 - epoch:26, training loss:0.5565 validation loss:0.1850
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.18512307019794688 0.1797405266586472
2023-08-04 21:16:05,844 - epoch:27, training loss:0.5545 validation loss:0.1851
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.18515177246402292 0.17963489714790792
2023-08-04 21:16:30,489 - epoch:28, training loss:0.5556 validation loss:0.1852
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.1852036640048027 0.1797093499232741
2023-08-04 21:16:54,744 - epoch:29, training loss:0.5552 validation loss:0.1852
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-21:04:47.977294/0/0.1761_epoch_3.pkl  &  0.17745323917445013
2023-08-04 21:16:56,868 - [*] loss:0.3910
2023-08-04 21:16:56,882 - [*] phase 0, testing
2023-08-04 21:16:57,569 - T:720	MAE	0.425116	RMSE	0.389486	MAPE	194.930792
2023-08-04 21:16:57,569 - 720	mae	0.4251	
2023-08-04 21:16:57,570 - 720	rmse	0.3895	
2023-08-04 21:16:57,570 - 720	mape	194.9308	
2023-08-04 21:16:59,289 - [*] loss:0.4005
2023-08-04 21:16:59,303 - [*] phase 0, testing
2023-08-04 21:17:00,156 - T:720	MAE	0.422849	RMSE	0.397570	MAPE	186.421943
2023-08-04 21:17:00,157 - 720	mae	0.4228	
2023-08-04 21:17:00,157 - 720	rmse	0.3976	
2023-08-04 21:17:00,157 - 720	mape	186.4219	
2023-08-04 21:17:02,402 - logger name:exp/ECL-PatchTST2023-08-04-21:17:02.402670/ECL-PatchTST.log
2023-08-04 21:17:02,403 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-21:17:02.402670', 'path': 'exp/ECL-PatchTST2023-08-04-21:17:02.402670', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 21:17:02,403 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 21:17:02,628 - [*] phase 0 Dataset load!
2023-08-04 21:17:03,655 - [*] phase 0 Training start
train 7585
2023-08-04 21:17:16,179 - epoch:0, training loss:0.3283 validation loss:0.1973
train 7585
vs, vt 0.197252816575415 0.1986305617234286
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.18577276257907643 0.18873493110432343
2023-08-04 21:17:44,674 - epoch:1, training loss:1.4832 validation loss:0.1858
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.1806817398790051 0.1866110168835696
2023-08-04 21:18:03,886 - epoch:2, training loss:1.2116 validation loss:0.1807
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.17754218069946065 0.18311628173379338
2023-08-04 21:18:22,708 - epoch:3, training loss:0.9590 validation loss:0.1775
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.17605303008766734 0.18100998068557067
2023-08-04 21:18:42,859 - epoch:4, training loss:0.8330 validation loss:0.1761
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.17464209917713613 0.17945275915896192
2023-08-04 21:19:02,231 - epoch:5, training loss:0.7896 validation loss:0.1746
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.17454864106634083 0.1762620705892058
2023-08-04 21:19:22,144 - epoch:6, training loss:0.7699 validation loss:0.1745
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.17628532363211408 0.17714935343931704
2023-08-04 21:19:41,529 - epoch:7, training loss:0.7657 validation loss:0.1763
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.17495148585123174 0.18038525257040472
2023-08-04 21:20:01,385 - epoch:8, training loss:0.7699 validation loss:0.1750
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.17608188289929838 0.17821749277851162
2023-08-04 21:20:20,795 - epoch:9, training loss:0.7686 validation loss:0.1761
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.17670220618738847 0.17966180316665592
2023-08-04 21:20:39,569 - epoch:10, training loss:0.7837 validation loss:0.1767
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.17507116496562958 0.17761854345307632
2023-08-04 21:20:58,606 - epoch:11, training loss:0.7735 validation loss:0.1751
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.17562491468646946 0.18004393862450824
2023-08-04 21:21:18,101 - epoch:12, training loss:0.7677 validation loss:0.1756
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.1785830969319624 0.18134242340045817
2023-08-04 21:21:37,303 - epoch:13, training loss:0.7695 validation loss:0.1786
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.17907455169102726 0.18184173939859166
2023-08-04 21:21:56,214 - epoch:14, training loss:0.7730 validation loss:0.1791
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.17712878567330978 0.1830556077115676
2023-08-04 21:22:15,380 - epoch:15, training loss:0.7774 validation loss:0.1771
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.17812770079163945 0.18340454097179806
2023-08-04 21:22:34,596 - epoch:16, training loss:0.7731 validation loss:0.1781
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.17864168742123773 0.1828142568907317
2023-08-04 21:22:53,523 - epoch:17, training loss:0.7788 validation loss:0.1786
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.17972603002015283 0.18512311380575686
2023-08-04 21:23:13,977 - epoch:18, training loss:0.7756 validation loss:0.1797
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.1796271726489067 0.18454468337928548
2023-08-04 21:23:33,054 - epoch:19, training loss:0.7770 validation loss:0.1796
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.17938688046791973 0.18484346656238332
2023-08-04 21:23:52,245 - epoch:20, training loss:0.7761 validation loss:0.1794
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.17965415163951762 0.18535331965369337
2023-08-04 21:24:11,739 - epoch:21, training loss:0.7773 validation loss:0.1797
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.17922511565334656 0.18480255082249641
2023-08-04 21:24:31,040 - epoch:22, training loss:0.7749 validation loss:0.1792
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.1795699285233722 0.18512994993258924
2023-08-04 21:24:50,542 - epoch:23, training loss:0.7776 validation loss:0.1796
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.18011702772449045 0.1852983878377606
2023-08-04 21:25:09,948 - epoch:24, training loss:0.7762 validation loss:0.1801
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.18051834579776316 0.18572464892092874
2023-08-04 21:25:29,569 - epoch:25, training loss:0.7755 validation loss:0.1805
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.1799108425483984 0.18545819512184927
2023-08-04 21:25:48,416 - epoch:26, training loss:0.7742 validation loss:0.1799
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.17996561176636638 0.18548955483471646
2023-08-04 21:26:07,918 - epoch:27, training loss:0.7733 validation loss:0.1800
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.1799897889004034 0.18571798612966256
2023-08-04 21:26:27,118 - epoch:28, training loss:0.7744 validation loss:0.1800
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.1799944091369124 0.18544116813470335
2023-08-04 21:26:47,172 - epoch:29, training loss:0.7761 validation loss:0.1800
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-21:17:02.402670/0/0.1745_epoch_6.pkl  &  0.1762620705892058
2023-08-04 21:26:48,238 - [*] loss:0.3886
2023-08-04 21:26:48,253 - [*] phase 0, testing
2023-08-04 21:26:49,097 - T:720	MAE	0.421247	RMSE	0.386743	MAPE	190.305448
2023-08-04 21:26:49,097 - 720	mae	0.4212	
2023-08-04 21:26:49,098 - 720	rmse	0.3867	
2023-08-04 21:26:49,098 - 720	mape	190.3054	
2023-08-04 21:26:50,617 - [*] loss:0.3942
2023-08-04 21:26:50,631 - [*] phase 0, testing
2023-08-04 21:26:51,456 - T:720	MAE	0.422170	RMSE	0.392321	MAPE	186.645126
2023-08-04 21:26:51,457 - 720	mae	0.4222	
2023-08-04 21:26:51,457 - 720	rmse	0.3923	
2023-08-04 21:26:51,457 - 720	mape	186.6451	
2023-08-04 21:26:53,605 - logger name:exp/ECL-PatchTST2023-08-04-21:26:53.605387/ECL-PatchTST.log
2023-08-04 21:26:53,606 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-04-21:26:53.605387', 'path': 'exp/ECL-PatchTST2023-08-04-21:26:53.605387', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-04 21:26:53,606 - [*] phase 0 start training
0 26304
train 7585
val 2161
test 2161
2023-08-04 21:26:53,819 - [*] phase 0 Dataset load!
2023-08-04 21:26:54,906 - [*] phase 0 Training start
train 7585
2023-08-04 21:27:07,524 - epoch:0, training loss:0.3114 validation loss:0.1921
train 7585
vs, vt 0.19212167666238897 0.19794542649213007
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.1832679705146481 0.1889719117213698
2023-08-04 21:27:40,394 - epoch:1, training loss:3.2028 validation loss:0.1833
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.1796336270430509 0.18390065957518184
2023-08-04 21:28:05,966 - epoch:2, training loss:2.5507 validation loss:0.1796
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.17769512468401125 0.18020879280041247
2023-08-04 21:28:30,495 - epoch:3, training loss:1.7225 validation loss:0.1777
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.1769865632933729 0.17765019636820345
2023-08-04 21:28:54,545 - epoch:4, training loss:1.2666 validation loss:0.1770
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.17686516988803358 0.18022364005446434
2023-08-04 21:29:19,410 - epoch:5, training loss:1.1296 validation loss:0.1769
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.17688915023908897 0.17616770789027214
2023-08-04 21:29:44,921 - epoch:6, training loss:1.0851 validation loss:0.1769
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.1766222886302892 0.17868351432330468
2023-08-04 21:30:09,439 - epoch:7, training loss:1.0333 validation loss:0.1766
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.17883379121913628 0.18296312606509993
2023-08-04 21:30:34,150 - epoch:8, training loss:0.9778 validation loss:0.1788
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.17730906846768715 0.1813932260607972
2023-08-04 21:30:58,423 - epoch:9, training loss:0.9584 validation loss:0.1773
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.1787726787083289 0.18030914664268494
2023-08-04 21:31:23,184 - epoch:10, training loss:0.9598 validation loss:0.1788
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.17813565910739057 0.17896846375044653
2023-08-04 21:31:47,648 - epoch:11, training loss:0.9607 validation loss:0.1781
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.17806323771091068 0.1789909022695878
2023-08-04 21:32:11,867 - epoch:12, training loss:0.9586 validation loss:0.1781
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.17796974410029018 0.18108385275391972
2023-08-04 21:32:35,818 - epoch:13, training loss:0.9615 validation loss:0.1780
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.17747845391140266 0.1801769917502123
2023-08-04 21:33:00,796 - epoch:14, training loss:0.9671 validation loss:0.1775
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.1772564684643465 0.18247154574183858
2023-08-04 21:33:25,289 - epoch:15, training loss:0.9651 validation loss:0.1773
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.1777832646142034 0.18135025045450995
2023-08-04 21:33:49,582 - epoch:16, training loss:0.9562 validation loss:0.1778
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.17790982587372556 0.1824360546820304
2023-08-04 21:34:14,766 - epoch:17, training loss:0.9627 validation loss:0.1779
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.1787544028723941 0.1816367086242227
2023-08-04 21:34:39,392 - epoch:18, training loss:0.9602 validation loss:0.1788
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.17807383440873203 0.18080277521820629
2023-08-04 21:35:03,952 - epoch:19, training loss:0.9693 validation loss:0.1781
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.1783061097649967 0.18093828068060033
2023-08-04 21:35:29,588 - epoch:20, training loss:0.9689 validation loss:0.1783
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.17803658457363353 0.18110299417201212
2023-08-04 21:35:53,975 - epoch:21, training loss:0.9627 validation loss:0.1780
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.17862860726959565 0.18138698795262506
2023-08-04 21:36:19,161 - epoch:22, training loss:0.9656 validation loss:0.1786
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.17853417598149357 0.18194879679118886
2023-08-04 21:36:44,024 - epoch:23, training loss:0.9601 validation loss:0.1785
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.1784891939776785 0.18107983513790019
2023-08-04 21:37:08,343 - epoch:24, training loss:0.9614 validation loss:0.1785
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
vs, vt 0.17860501544440494 0.18101492567973979
2023-08-04 21:37:32,278 - epoch:25, training loss:0.9624 validation loss:0.1786
Updating learning rate to 6.643743882952291e-06
Updating learning rate to 6.643743882952291e-06
train 7585
vs, vt 0.17839835386942415 0.18123020889127955
2023-08-04 21:37:56,998 - epoch:26, training loss:0.9637 validation loss:0.1784
Updating learning rate to 3.764070229049073e-06
Updating learning rate to 3.764070229049073e-06
train 7585
vs, vt 0.17842574395677624 0.18147240403820486
2023-08-04 21:38:21,274 - epoch:27, training loss:0.9627 validation loss:0.1784
Updating learning rate to 1.6755095746038162e-06
Updating learning rate to 1.6755095746038162e-06
train 7585
vs, vt 0.17838645594961502 0.18103664631352706
2023-08-04 21:38:46,377 - epoch:28, training loss:0.9614 validation loss:0.1784
Updating learning rate to 4.1379777147248194e-07
Updating learning rate to 4.1379777147248194e-07
train 7585
vs, vt 0.1784097845940029 0.18120063008630977
2023-08-04 21:39:11,586 - epoch:29, training loss:0.9609 validation loss:0.1784
Updating learning rate to 5.230584192419233e-10
Updating learning rate to 5.230584192419233e-10
check exp/ECL-PatchTST2023-08-04-21:26:53.605387/0/0.1766_epoch_7.pkl  &  0.17616770789027214
2023-08-04 21:39:12,784 - [*] loss:0.3932
2023-08-04 21:39:12,800 - [*] phase 0, testing
2023-08-04 21:39:13,479 - T:720	MAE	0.425737	RMSE	0.391474	MAPE	194.793487
2023-08-04 21:39:13,479 - 720	mae	0.4257	
2023-08-04 21:39:13,479 - 720	rmse	0.3915	
2023-08-04 21:39:13,479 - 720	mape	194.7935	
2023-08-04 21:39:14,752 - [*] loss:0.3950
2023-08-04 21:39:14,766 - [*] phase 0, testing
2023-08-04 21:39:15,415 - T:720	MAE	0.421589	RMSE	0.392219	MAPE	186.502826
2023-08-04 21:39:15,416 - 720	mae	0.4216	
2023-08-04 21:39:15,416 - 720	rmse	0.3922	
2023-08-04 21:39:15,416 - 720	mape	186.5028	
