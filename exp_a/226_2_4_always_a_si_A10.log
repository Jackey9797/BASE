2023-07-28 00:12:03,101 - logger name:exp/ECL-PatchTST2023-07-28-00:12:03.100432/ECL-PatchTST.log
2023-07-28 00:12:03,101 - params : {'conf': 'ECL-PatchTST', 'data_name': 'electricity', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.1, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': 226, 'aligner': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'seed': 2041, 'batch_size': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, 'loss': 'mse', '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-07-28-00:12:03.100432', 'path': 'exp/ECL-PatchTST2023-07-28-00:12:03.100432', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-07-28 00:12:03,101 - [*] phase 0 start training
0 26304
18412 2632 5260 0.7 0.2 26304
train 17981
18412 2632 5260 0.7 0.2 26304
val 2537
18412 2632 5260 0.7 0.2 26304
test 5165
2023-07-28 00:12:17,238 - [*] phase 0 Dataset load!
2023-07-28 00:12:18,422 - [*] phase 0 Training start
18412 2632 5260 0.7 0.2 26304
train 17981
2023-07-28 00:12:44,827 - epoch:0, training loss:0.6145 validation loss:0.1464
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.1464393340051174 0.14495970383286477
Updating learning rate to 1.0445766508069865e-05
Updating learning rate to 1.0445766508069865e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.07426774650812148 0.0706339618191123
2023-07-28 00:13:22,243 - epoch:1, training loss:1.7661 validation loss:0.0743
Updating learning rate to 2.805190328742299e-05
Updating learning rate to 2.805190328742299e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.05579506848007441 0.05313587700948119
2023-07-28 00:13:47,078 - epoch:2, training loss:1.1885 validation loss:0.0558
Updating learning rate to 5.208986672185724e-05
Updating learning rate to 5.208986672185724e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.05048167603090405 0.04797972086817026
2023-07-28 00:14:11,810 - epoch:3, training loss:0.8644 validation loss:0.0505
Updating learning rate to 7.610369432687743e-05
Updating learning rate to 7.610369432687743e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.0462709722109139 0.04515747893601656
2023-07-28 00:14:36,484 - epoch:4, training loss:0.7026 validation loss:0.0463
Updating learning rate to 9.364390586469177e-05
Updating learning rate to 9.364390586469177e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04555154992267489 0.0447482337243855
2023-07-28 00:15:00,991 - epoch:5, training loss:0.6155 validation loss:0.0456
Updating learning rate to 9.999997814456623e-05
Updating learning rate to 9.999997814456623e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04464224185794592 0.044020792748779056
2023-07-28 00:15:25,392 - epoch:6, training loss:0.5610 validation loss:0.0446
Updating learning rate to 9.956612105134122e-05
Updating learning rate to 9.956612105134122e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04677118249237537 0.045283083245158194
2023-07-28 00:15:50,245 - epoch:7, training loss:0.5226 validation loss:0.0468
Updating learning rate to 9.828417730665822e-05
Updating learning rate to 9.828417730665822e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.044333494082093236 0.044367034640163186
2023-07-28 00:16:14,221 - epoch:8, training loss:0.4967 validation loss:0.0443
Updating learning rate to 9.617608132341072e-05
Updating learning rate to 9.617608132341072e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04289670931175351 0.04290352277457714
2023-07-28 00:16:39,082 - epoch:9, training loss:0.4747 validation loss:0.0429
Updating learning rate to 9.327790320834669e-05
Updating learning rate to 9.327790320834669e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.044407643005251886 0.04213398117572069
2023-07-28 00:17:04,036 - epoch:10, training loss:0.4581 validation loss:0.0444
Updating learning rate to 8.963923159254168e-05
Updating learning rate to 8.963923159254168e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04358506659045815 0.04363856958225369
2023-07-28 00:17:29,135 - epoch:11, training loss:0.4457 validation loss:0.0436
Updating learning rate to 8.532232515617246e-05
Updating learning rate to 8.532232515617246e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.042657479830086234 0.04327241415157914
2023-07-28 00:17:55,845 - epoch:12, training loss:0.4344 validation loss:0.0427
Updating learning rate to 8.04010473652379e-05
Updating learning rate to 8.04010473652379e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04195219520479441 0.041237580869346854
2023-07-28 00:18:20,517 - epoch:13, training loss:0.4263 validation loss:0.0420
Updating learning rate to 7.495960264717686e-05
Updating learning rate to 7.495960264717686e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.040903907362371685 0.04160345857962966
2023-07-28 00:18:44,497 - epoch:14, training loss:0.4172 validation loss:0.0409
Updating learning rate to 6.909109562976884e-05
Updating learning rate to 6.909109562976884e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04226451106369496 0.0414585773833096
2023-07-28 00:19:08,318 - epoch:15, training loss:0.4102 validation loss:0.0423
Updating learning rate to 6.289593809513925e-05
Updating learning rate to 6.289593809513925e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.040434705093503 0.04043125370517373
2023-07-28 00:19:32,628 - epoch:16, training loss:0.4051 validation loss:0.0404
Updating learning rate to 5.6480130906327764e-05
Updating learning rate to 5.6480130906327764e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04073324026539922 0.040582336392253636
2023-07-28 00:19:57,694 - epoch:17, training loss:0.3998 validation loss:0.0407
Updating learning rate to 4.995345030313274e-05
Updating learning rate to 4.995345030313274e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.04073629816994071 0.040407382789999245
2023-07-28 00:20:22,210 - epoch:18, training loss:0.3962 validation loss:0.0407
Updating learning rate to 4.3427569600212594e-05
Updating learning rate to 4.3427569600212594e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.0402732577174902 0.04012569468468428
2023-07-28 00:20:47,535 - epoch:19, training loss:0.3924 validation loss:0.0403
Updating learning rate to 3.70141484257102e-05
Updating learning rate to 3.70141484257102e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03987791109830141 0.040119376685470345
2023-07-28 00:21:12,759 - epoch:20, training loss:0.3892 validation loss:0.0399
Updating learning rate to 3.0822922194057626e-05
Updating learning rate to 3.0822922194057626e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03936086790636182 0.0396969523280859
2023-07-28 00:21:38,292 - epoch:21, training loss:0.3865 validation loss:0.0394
Updating learning rate to 2.4959824502610635e-05
Updating learning rate to 2.4959824502610635e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03966020597144961 0.03968175258487463
2023-07-28 00:22:02,443 - epoch:22, training loss:0.3851 validation loss:0.0397
Updating learning rate to 1.9525174578427625e-05
Updating learning rate to 1.9525174578427625e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.039064378477633 0.03948929635807872
2023-07-28 00:22:30,266 - epoch:23, training loss:0.3829 validation loss:0.0391
Updating learning rate to 1.4611960788481038e-05
Updating learning rate to 1.4611960788481038e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.039186316169798376 0.03945705955848098
2023-07-28 00:23:01,626 - epoch:24, training loss:0.3817 validation loss:0.0392
Updating learning rate to 1.0304249582917059e-05
Updating learning rate to 1.0304249582917059e-05
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03874520640820265 0.03903410146012902
2023-07-28 00:23:32,320 - epoch:25, training loss:0.3806 validation loss:0.0387
Updating learning rate to 6.675747094786052e-06
Updating learning rate to 6.675747094786052e-06
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03870828589424491 0.039057761430740356
2023-07-28 00:23:56,039 - epoch:26, training loss:0.3797 validation loss:0.0387
Updating learning rate to 3.7885380076709116e-06
Updating learning rate to 3.7885380076709116e-06
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03865479175001383 0.03890484571456909
2023-07-28 00:24:18,757 - epoch:27, training loss:0.3794 validation loss:0.0387
Updating learning rate to 1.6920232695377565e-06
Updating learning rate to 1.6920232695377565e-06
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03859385885298252 0.03893851870670915
2023-07-28 00:24:42,435 - epoch:28, training loss:0.3790 validation loss:0.0386
Updating learning rate to 4.220748288197363e-07
Updating learning rate to 4.220748288197363e-07
18412 2632 5260 0.7 0.2 26304
train 17981
vs, vt 0.03861883450299501 0.03893351452425122
2023-07-28 00:25:05,572 - epoch:29, training loss:0.3787 validation loss:0.0386
Updating learning rate to 4.218554337707414e-10
Updating learning rate to 4.218554337707414e-10
check exp/ECL-PatchTST2023-07-28-00:12:03.100432/0/0.0386_epoch_28.pkl  &  0.03890484571456909
2023-07-28 00:25:07,535 - [*] loss:0.0717
2023-07-28 00:25:07,536 - [*] phase 0, testing
2023-07-28 00:25:07,545 - T:96	MAE	0.187048	RMSE	0.071636	MAPE	231.592178
2023-07-28 00:25:07,545 - 96	mae	0.1870	
2023-07-28 00:25:07,545 - 96	rmse	0.0716	
2023-07-28 00:25:07,545 - 96	mape	231.5922	
2023-07-28 00:25:09,792 - [*] loss:0.0713
2023-07-28 00:25:09,794 - [*] phase 0, testing
2023-07-28 00:25:09,802 - T:96	MAE	0.185862	RMSE	0.071165	MAPE	228.407478
2023-07-28 00:25:09,802 - 96	mae	0.1859	
2023-07-28 00:25:09,803 - 96	rmse	0.0712	
2023-07-28 00:25:09,803 - 96	mape	228.4075	
