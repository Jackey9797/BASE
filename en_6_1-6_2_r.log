2023-08-02 15:53:18,652 - logger name:exp/ECL-PatchTST2023-08-02-15:53:18.652145/ECL-PatchTST.log
2023-08-02 15:53:18,652 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 6, 'seed': 42033, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-02-15:53:18.652145', 'path': 'exp/ECL-PatchTST2023-08-02-15:53:18.652145', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-02 15:53:18,652 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-02 15:53:18,843 - [*] phase 0 Dataset load!
2023-08-02 15:53:19,672 - [*] phase 0 Training start
train 8209
2023-08-02 15:53:32,910 - epoch:0, training loss:0.2248 validation loss:0.1639
train 8209
vs, vt 0.1638776600699533 0.16768798807805235
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.1402419131587852 0.1353371598842469
2023-08-02 15:54:06,922 - epoch:1, training loss:2.6712 validation loss:0.1402
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12967548561705786 0.1257479857992042
2023-08-02 15:54:34,244 - epoch:2, training loss:2.0614 validation loss:0.1297
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.1257981142367829 0.12351063892922619
2023-08-02 15:55:00,898 - epoch:3, training loss:1.4275 validation loss:0.1258
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.1247311465611512 0.12269852742214095
2023-08-02 15:55:42,468 - epoch:4, training loss:1.0986 validation loss:0.1247
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.1234366378154267 0.12301681029864332
2023-08-02 15:56:09,337 - epoch:5, training loss:0.9554 validation loss:0.1234
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12502207920293917 0.12345297329805115
2023-08-02 15:57:00,908 - epoch:6, training loss:0.8885 validation loss:0.1250
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12402597653933546 0.12201429869640958
2023-08-02 15:57:58,624 - epoch:7, training loss:0.8474 validation loss:0.1240
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12476935542442581 0.12354048206047578
2023-08-02 15:58:49,523 - epoch:8, training loss:0.8117 validation loss:0.1248
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12434362298385664 0.12305061510679396
2023-08-02 15:59:41,603 - epoch:9, training loss:0.7834 validation loss:0.1243
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12445852812379599 0.12229676206003535
2023-08-02 16:00:34,688 - epoch:10, training loss:0.7583 validation loss:0.1245
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12667721128937873 0.12379434306851843
2023-08-02 16:01:26,572 - epoch:11, training loss:0.7507 validation loss:0.1267
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12522160989994352 0.12295743213458495
2023-08-02 16:02:17,138 - epoch:12, training loss:0.7563 validation loss:0.1252
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.1253738266161897 0.1233984525738792
2023-08-02 16:03:06,847 - epoch:13, training loss:0.7557 validation loss:0.1254
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12488284622403709 0.12412624518302354
2023-08-02 16:03:55,742 - epoch:14, training loss:0.7466 validation loss:0.1249
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.1259826815433123 0.12299902601675554
2023-08-02 16:04:44,271 - epoch:15, training loss:0.7452 validation loss:0.1260
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12544414587318897 0.12428439535539258
2023-08-02 16:05:33,168 - epoch:16, training loss:0.7241 validation loss:0.1254
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12423125696791844 0.12227368024601178
2023-08-02 16:06:28,664 - epoch:17, training loss:0.7318 validation loss:0.1242
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12438313231210817 0.12239988342943517
2023-08-02 16:07:18,109 - epoch:18, training loss:0.7342 validation loss:0.1244
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12405902477489276 0.12275369067422369
2023-08-02 16:08:07,395 - epoch:19, training loss:0.7344 validation loss:0.1241
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.1241705541244962 0.12234057215127078
2023-08-02 16:08:57,393 - epoch:20, training loss:0.7369 validation loss:0.1242
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.1248318212614818 0.12275225948542356
2023-08-02 16:09:47,041 - epoch:21, training loss:0.7327 validation loss:0.1248
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12487690904262391 0.12251448216424747
2023-08-02 16:10:38,413 - epoch:22, training loss:0.7264 validation loss:0.1249
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12468398616395214 0.12282963368025693
2023-08-02 16:11:30,774 - epoch:23, training loss:0.7259 validation loss:0.1247
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12463423906063492 0.12272507214749401
2023-08-02 16:12:23,828 - epoch:24, training loss:0.7220 validation loss:0.1246
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12457384660162708 0.12279813017018816
2023-08-02 16:13:16,904 - epoch:25, training loss:0.7246 validation loss:0.1246
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12451650938865813 0.12261529427699068
2023-08-02 16:14:09,131 - epoch:26, training loss:0.7211 validation loss:0.1245
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12461061801084063 0.12268848454749043
2023-08-02 16:15:06,072 - epoch:27, training loss:0.7232 validation loss:0.1246
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12450929079204798 0.12262977693568576
2023-08-02 16:16:01,363 - epoch:28, training loss:0.7228 validation loss:0.1245
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12448847132988951 0.12261411954056133
2023-08-02 16:16:51,559 - epoch:29, training loss:0.7214 validation loss:0.1245
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-02-15:53:18.652145/0/0.1234_epoch_5.pkl  &  0.12201429869640958
2023-08-02 16:16:56,410 - [*] loss:0.2733
2023-08-02 16:16:56,419 - [*] phase 0, testing
2023-08-02 16:16:56,491 - T:96	MAE	0.334825	RMSE	0.273446	MAPE	136.053538
2023-08-02 16:16:56,494 - 96	mae	0.3348	
2023-08-02 16:16:56,495 - 96	rmse	0.2734	
2023-08-02 16:16:56,495 - 96	mape	136.0535	
2023-08-02 16:17:00,908 - [*] loss:0.2700
2023-08-02 16:17:01,104 - [*] phase 0, testing
2023-08-02 16:17:01,162 - T:96	MAE	0.331778	RMSE	0.269883	MAPE	133.487523
2023-08-02 16:17:01,165 - 96	mae	0.3318	
2023-08-02 16:17:01,165 - 96	rmse	0.2699	
2023-08-02 16:17:01,165 - 96	mape	133.4875	
2023-08-02 16:17:04,089 - logger name:exp/ECL-PatchTST2023-08-02-16:17:04.088892/ECL-PatchTST.log
2023-08-02 16:17:04,090 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 6, 'seed': 42033, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 2.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-02-16:17:04.088892', 'path': 'exp/ECL-PatchTST2023-08-02-16:17:04.088892', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-02 16:17:04,090 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-02 16:17:04,356 - [*] phase 0 Dataset load!
2023-08-02 16:17:05,439 - [*] phase 0 Training start
train 8209
2023-08-02 16:17:38,542 - epoch:0, training loss:0.2248 validation loss:0.1639
train 8209
vs, vt 0.1638776600699533 0.16768798807805235
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.14024131880565124 0.13535167259926145
2023-08-02 16:18:43,386 - epoch:1, training loss:2.8869 validation loss:0.1402
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12967298535460775 0.1258320031179623
2023-08-02 16:19:31,544 - epoch:2, training loss:2.2441 validation loss:0.1297
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12579952502115208 0.12374088769270615
2023-08-02 16:20:22,452 - epoch:3, training loss:1.6006 validation loss:0.1258
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12475002667104657 0.12281010350720449
2023-08-02 16:20:55,126 - epoch:4, training loss:1.2727 validation loss:0.1248
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12349926070733504 0.12332709548486905
2023-08-02 16:21:21,871 - epoch:5, training loss:1.1307 validation loss:0.1235
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12503828497772868 0.12391204361549833
2023-08-02 16:21:48,085 - epoch:6, training loss:1.0618 validation loss:0.1250
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.1240627591244199 0.12229325088926336
2023-08-02 16:22:15,634 - epoch:7, training loss:1.0212 validation loss:0.1241
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12472262030298059 0.12379634744403037
2023-08-02 16:22:42,053 - epoch:8, training loss:0.9857 validation loss:0.1247
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12442564049904997 0.12321011019362645
2023-08-02 16:23:35,342 - epoch:9, training loss:0.9599 validation loss:0.1244
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12438504584133625 0.12226101011037827
2023-08-02 16:24:26,726 - epoch:10, training loss:0.9290 validation loss:0.1244
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.1265801058404825 0.12390247351405295
2023-08-02 16:25:20,067 - epoch:11, training loss:0.9254 validation loss:0.1266
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.1253541289744052 0.12298930690369823
2023-08-02 16:26:13,319 - epoch:12, training loss:0.9324 validation loss:0.1254
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.1253327357328751 0.12351461322131482
2023-08-02 16:27:06,228 - epoch:13, training loss:0.9304 validation loss:0.1253
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12497557953677395 0.12402561654082754
2023-08-02 16:27:57,740 - epoch:14, training loss:0.9169 validation loss:0.1250
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12587279106744312 0.12309464896944436
2023-08-02 16:28:47,203 - epoch:15, training loss:0.9095 validation loss:0.1259
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.1252439251508225 0.12427600634030321
2023-08-02 16:29:37,683 - epoch:16, training loss:0.8896 validation loss:0.1252
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12409614178944718 0.12242867315018718
2023-08-02 16:30:27,098 - epoch:17, training loss:0.9048 validation loss:0.1241
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12408938792280176 0.12259908867153255
2023-08-02 16:31:14,553 - epoch:18, training loss:0.9113 validation loss:0.1241
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12395296703008088 0.12301749439740722
2023-08-02 16:32:02,330 - epoch:19, training loss:0.9119 validation loss:0.1240
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12405894374982877 0.12259770760482008
2023-08-02 16:32:49,168 - epoch:20, training loss:0.9129 validation loss:0.1241
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12467418373985724 0.1228614446961067
2023-08-02 16:33:38,612 - epoch:21, training loss:0.9109 validation loss:0.1247
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12475215923041105 0.12266810695556077
2023-08-02 16:34:29,444 - epoch:22, training loss:0.9033 validation loss:0.1248
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12456349046392874 0.12299338948320258
2023-08-02 16:35:19,837 - epoch:23, training loss:0.9022 validation loss:0.1246
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12452890385280956 0.12281757728620009
2023-08-02 16:36:09,995 - epoch:24, training loss:0.8982 validation loss:0.1245
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12452683961865577 0.12295073321597143
2023-08-02 16:37:02,294 - epoch:25, training loss:0.9010 validation loss:0.1245
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12444584689695727 0.12276377901434898
2023-08-02 16:37:54,315 - epoch:26, training loss:0.8969 validation loss:0.1244
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12454489237544211 0.12283467603000728
2023-08-02 16:38:47,563 - epoch:27, training loss:0.8989 validation loss:0.1245
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12446019095792012 0.12275562723251907
2023-08-02 16:39:39,673 - epoch:28, training loss:0.8986 validation loss:0.1245
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12443550015715035 0.12274487342008135
2023-08-02 16:40:34,576 - epoch:29, training loss:0.8958 validation loss:0.1244
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-02-16:17:04.088892/0/0.1235_epoch_5.pkl  &  0.12226101011037827
2023-08-02 16:40:39,215 - [*] loss:0.2735
2023-08-02 16:40:39,219 - [*] phase 0, testing
2023-08-02 16:40:39,261 - T:96	MAE	0.334939	RMSE	0.273632	MAPE	136.104226
2023-08-02 16:40:39,262 - 96	mae	0.3349	
2023-08-02 16:40:39,262 - 96	rmse	0.2736	
2023-08-02 16:40:39,262 - 96	mape	136.1042	
2023-08-02 16:40:43,211 - [*] loss:0.2711
2023-08-02 16:40:43,214 - [*] phase 0, testing
2023-08-02 16:40:43,254 - T:96	MAE	0.331445	RMSE	0.271179	MAPE	133.195269
2023-08-02 16:40:43,255 - 96	mae	0.3314	
2023-08-02 16:40:43,255 - 96	rmse	0.2712	
2023-08-02 16:40:43,255 - 96	mape	133.1953	
