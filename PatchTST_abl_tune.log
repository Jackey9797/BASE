2023-08-30 15:36:43,156 - logger name:exp/ECL-PatchTST2023-08-30-15:36:43.156660/ECL-PatchTST.log
2023-08-30 15:36:43,157 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 1, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 100, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-30-15:36:43.156660', 'path': 'exp/ECL-PatchTST2023-08-30-15:36:43.156660', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-30 15:36:43,157 - [*] phase 0 start training
0 69680
train 34129
val 11425
test 11425
2023-08-30 15:36:43,954 - [*] phase 0 Dataset load!
2023-08-30 15:36:44,951 - [*] phase 0 Training start
train 34129
2023-08-30 15:38:17,859 - epoch:0, training loss:0.4354 validation loss:0.4376
train 34129
vs, vt 0.4376408346825176 0.4519533620940314
Updating learning rate to 1.0438661460315324e-05
Updating learning rate to 1.0438661460315324e-05
train 34129
vs, vt 0.40297588639789156 0.41022513930996257
need align? ->  False 0.41022513930996257
2023-08-30 15:44:11,789 - epoch:1, training loss:6.5025 validation loss:0.4030
Updating learning rate to 2.8027297449571736e-05
Updating learning rate to 2.8027297449571736e-05
train 34129
vs, vt 0.39857032837139234 0.399851470026705
need align? ->  False 0.399851470026705
2023-08-30 15:48:26,776 - epoch:2, training loss:2.8697 validation loss:0.3986
Updating learning rate to 5.204727160595503e-05
Updating learning rate to 5.204727160595503e-05
train 34129
vs, vt 0.3945493406719632 0.3991289000544283
need align? ->  False 0.3991289000544283
2023-08-30 15:52:39,911 - epoch:3, training loss:2.0660 validation loss:0.3945
Updating learning rate to 7.605456385119542e-05
Updating learning rate to 7.605456385119542e-05
train 34129
vs, vt 0.409616534329123 0.3986998135017024
need align? ->  True 0.3986998135017024
2023-08-30 15:57:14,460 - epoch:4, training loss:1.7494 validation loss:0.4096
Updating learning rate to 9.360855637921138e-05
Updating learning rate to 9.360855637921138e-05
train 34129
vs, vt 0.39779901653528216 0.3963172134425905
need align? ->  True 0.3963172134425905
2023-08-30 16:01:31,157 - epoch:5, training loss:1.5573 validation loss:0.3978
Updating learning rate to 9.99999939458629e-05
Updating learning rate to 9.99999939458629e-05
train 34129
vs, vt 0.38640329407321083 0.3962951319085227
need align? ->  False 0.3962951319085227
2023-08-30 16:05:52,176 - epoch:6, training loss:1.4112 validation loss:0.3864
Updating learning rate to 9.956902716655286e-05
Updating learning rate to 9.956902716655286e-05
train 34129
vs, vt 0.3897964412139522 0.375679562240839
need align? ->  True 0.375679562240839
2023-08-30 16:10:32,651 - epoch:7, training loss:1.3290 validation loss:0.3898
Updating learning rate to 9.828992401134782e-05
Updating learning rate to 9.828992401134782e-05
train 34129
vs, vt 0.3854985369576348 0.3870566364791658
need align? ->  True 0.375679562240839
2023-08-30 16:15:06,982 - epoch:8, training loss:1.2483 validation loss:0.3855
Updating learning rate to 9.618457028986775e-05
Updating learning rate to 9.618457028986775e-05
train 34129
vs, vt 0.3888833250436518 0.3793903915418519
need align? ->  True 0.375679562240839
2023-08-30 16:19:23,359 - epoch:9, training loss:1.1873 validation loss:0.3889
Updating learning rate to 9.32889891880015e-05
Updating learning rate to 9.32889891880015e-05
train 34129
vs, vt 0.38249639438258276 0.37735687866806983
need align? ->  True 0.375679562240839
2023-08-30 16:24:31,819 - epoch:10, training loss:1.1485 validation loss:0.3825
Updating learning rate to 8.965272490120875e-05
Updating learning rate to 8.965272490120875e-05
train 34129
vs, vt 0.3825057545469867 0.37618970428076054
need align? ->  True 0.375679562240839
2023-08-30 16:29:04,463 - epoch:11, training loss:1.1170 validation loss:0.3825
Updating learning rate to 8.533799491959945e-05
Updating learning rate to 8.533799491959945e-05
train 34129
vs, vt 0.3879530790779326 0.37391971515284644
need align? ->  True 0.37391971515284644
2023-08-30 16:33:32,415 - epoch:12, training loss:1.0930 validation loss:0.3880
Updating learning rate to 8.041862546942808e-05
Updating learning rate to 8.041862546942808e-05
train 34129
vs, vt 0.37714540304409133 0.3806597668263647
need align? ->  True 0.37391971515284644
2023-08-30 16:37:46,757 - epoch:13, training loss:1.1212 validation loss:0.3771
Updating learning rate to 7.497878832589398e-05
Updating learning rate to 7.497878832589398e-05
train 34129
vs, vt 0.37607127933038603 0.37267558756801816
need align? ->  True 0.37267558756801816
2023-08-30 16:42:17,224 - epoch:14, training loss:1.0742 validation loss:0.3761
Updating learning rate to 6.911156061073078e-05
Updating learning rate to 6.911156061073078e-05
train 34129
vs, vt 0.3731634953783618 0.36852906669179597
need align? ->  True 0.36852906669179597
2023-08-30 16:46:28,967 - epoch:15, training loss:1.0776 validation loss:0.3732
Updating learning rate to 6.291733221684778e-05
Updating learning rate to 6.291733221684778e-05
train 34129
vs, vt 0.37316116955545214 0.36834571361541746
need align? ->  True 0.36834571361541746
2023-08-30 16:50:43,353 - epoch:16, training loss:1.0634 validation loss:0.3732
Updating learning rate to 5.650208810942889e-05
Updating learning rate to 5.650208810942889e-05
train 34129
vs, vt 0.3725372037953801 0.36445557102561
need align? ->  True 0.36445557102561
2023-08-30 16:55:32,260 - epoch:17, training loss:1.0610 validation loss:0.3725
Updating learning rate to 4.9975594893793686e-05
Updating learning rate to 4.9975594893793686e-05
train 34129
vs, vt 0.37125810749000976 0.3660545935233434
need align? ->  True 0.36445557102561
2023-08-30 16:59:54,857 - epoch:18, training loss:1.0555 validation loss:0.3713
Updating learning rate to 4.3449522678347527e-05
Updating learning rate to 4.3449522678347527e-05
train 34129
vs, vt 0.3733773986498515 0.36498475405904984
need align? ->  True 0.36445557102561
2023-08-30 17:04:37,774 - epoch:19, training loss:1.0445 validation loss:0.3734
Updating learning rate to 3.7035534368065714e-05
Updating learning rate to 3.7035534368065714e-05
train 34129
vs, vt 0.369620263742076 0.36314535182383323
need align? ->  True 0.36314535182383323
2023-08-30 17:09:09,114 - epoch:20, training loss:1.0375 validation loss:0.3696
Updating learning rate to 3.084337508123067e-05
Updating learning rate to 3.084337508123067e-05
train 34129
vs, vt 0.37492102392845683 0.36255719140172005
need align? ->  True 0.36255719140172005
2023-08-30 17:13:50,791 - epoch:21, training loss:1.0476 validation loss:0.3749
Updating learning rate to 2.497899438003108e-05
Updating learning rate to 2.497899438003108e-05
train 34129
vs, vt 0.36967990224560104 0.3637225719789664
need align? ->  True 0.36255719140172005
2023-08-30 17:18:43,416 - epoch:22, training loss:1.0458 validation loss:0.3697
Updating learning rate to 1.9542733444177923e-05
Updating learning rate to 1.9542733444177923e-05
train 34129
vs, vt 0.37189353356758753 0.3623087066743109
need align? ->  True 0.3623087066743109
2023-08-30 17:23:42,109 - epoch:23, training loss:1.0401 validation loss:0.3719
Updating learning rate to 1.4627608205499963e-05
Updating learning rate to 1.4627608205499963e-05
train 34129
vs, vt 0.37318104431033133 0.36288008722994064
need align? ->  True 0.3623087066743109
2023-08-30 17:28:45,100 - epoch:24, training loss:1.0458 validation loss:0.3732
Updating learning rate to 1.0317717819561129e-05
Updating learning rate to 1.0317717819561129e-05
train 34129
vs, vt 0.3712159996231397 0.3632707703444693
need align? ->  True 0.3623087066743109
2023-08-30 17:33:57,547 - epoch:25, training loss:1.0429 validation loss:0.3712
Updating learning rate to 6.686805705792195e-06
Updating learning rate to 6.686805705792195e-06
train 34129
vs, vt 0.3721431346403228 0.3624740740491284
need align? ->  True 0.3623087066743109
2023-08-30 17:39:03,208 - epoch:26, training loss:1.0412 validation loss:0.3721
Updating learning rate to 3.79699777713878e-06
Updating learning rate to 3.79699777713878e-06
train 34129
vs, vt 0.37070932015776636 0.3628838896751404
need align? ->  True 0.3623087066743109
2023-08-30 17:43:49,809 - epoch:27, training loss:1.0395 validation loss:0.3707
Updating learning rate to 1.6977394484662593e-06
Updating learning rate to 1.6977394484662593e-06
train 34129
vs, vt 0.3708804287016392 0.36268522830473054
need align? ->  True 0.3623087066743109
2023-08-30 17:48:08,946 - epoch:28, training loss:1.0395 validation loss:0.3709
Updating learning rate to 4.2494961180259127e-07
Updating learning rate to 4.2494961180259127e-07
train 34129
vs, vt 0.37106689396831727 0.36277349582976764
need align? ->  True 0.3623087066743109
2023-08-30 17:52:27,271 - epoch:29, training loss:1.0386 validation loss:0.3711
Updating learning rate to 4.0605413710007e-10
Updating learning rate to 4.0605413710007e-10
check exp/ECL-PatchTST2023-08-30-15:36:43.156660/0/0.3696_epoch_20.pkl  &  0.3623087066743109
2023-08-30 17:53:05,321 - [*] loss:0.2909
2023-08-30 17:53:05,334 - [*] phase 0, testing
2023-08-30 17:53:05,538 - T:96	MAE	0.342710	RMSE	0.291147	MAPE	216.633010
2023-08-30 17:53:05,539 - 96	mae	0.3427	
2023-08-30 17:53:05,539 - 96	rmse	0.2911	
2023-08-30 17:53:05,539 - 96	mape	216.6330	
----*-----
2023-08-30 17:53:43,095 - [*] loss:0.2909
2023-08-30 17:53:43,105 - [*] phase 0, testing
2023-08-30 17:53:43,289 - T:96	MAE	0.342710	RMSE	0.291147	MAPE	216.633010
2023-08-30 17:54:21,190 - [*] loss:0.3077
2023-08-30 17:54:21,199 - [*] phase 0, testing
2023-08-30 17:54:21,380 - T:96	MAE	0.365513	RMSE	0.307896	MAPE	234.380651
2023-08-30 17:54:59,102 - [*] loss:0.4100
2023-08-30 17:54:59,113 - [*] phase 0, testing
2023-08-30 17:54:59,302 - T:96	MAE	0.426871	RMSE	0.411017	MAPE	280.571556
2023-08-30 17:55:44,232 - [*] loss:0.2952
2023-08-30 17:55:44,244 - [*] phase 0, testing
2023-08-30 17:55:44,462 - T:96	MAE	0.350365	RMSE	0.295543	MAPE	230.818510
2023-08-30 17:56:32,097 - [*] loss:0.4005
2023-08-30 17:56:32,107 - [*] phase 0, testing
2023-08-30 17:56:32,289 - T:96	MAE	0.433975	RMSE	0.401291	MAPE	246.855974
2023-08-30 17:57:17,801 - [*] loss:0.3443
2023-08-30 17:57:17,813 - [*] phase 0, testing
2023-08-30 17:57:18,002 - T:96	MAE	0.394524	RMSE	0.344879	MAPE	209.511042
2023-08-30 17:58:03,552 - [*] loss:0.2953
2023-08-30 17:58:03,562 - [*] phase 0, testing
2023-08-30 17:58:03,745 - T:96	MAE	0.349803	RMSE	0.295415	MAPE	222.304440
2023-08-30 17:58:49,710 - [*] loss:0.3061
2023-08-30 17:58:49,723 - [*] phase 0, testing
2023-08-30 17:58:49,972 - T:96	MAE	0.363104	RMSE	0.306012	MAPE	208.370566
----*-----
2023-08-30 17:59:16,123 - [*] loss:0.3011
2023-08-30 17:59:16,135 - [*] phase 0, testing
2023-08-30 17:59:16,333 - T:96	MAE	0.356273	RMSE	0.300771	MAPE	204.975128
2023-08-30 17:59:56,686 - [*] loss:0.3046
2023-08-30 17:59:56,697 - [*] phase 0, testing
2023-08-30 17:59:56,885 - T:96	MAE	0.356132	RMSE	0.305680	MAPE	201.412678
2023-08-30 18:00:17,444 - [*] loss:0.2981
2023-08-30 18:00:17,455 - [*] phase 0, testing
2023-08-30 18:00:17,645 - T:96	MAE	0.347227	RMSE	0.298832	MAPE	198.935771
2023-08-30 18:00:17,648 - 96	mae	0.3472	
2023-08-30 18:00:17,648 - 96	rmse	0.2988	
2023-08-30 18:00:17,648 - 96	mape	198.9358	
2023-08-30 18:00:19,883 - logger name:exp/ECL-PatchTST2023-08-30-18:00:19.882810/ECL-PatchTST.log
2023-08-30 18:00:19,883 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 1, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 100, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-30-18:00:19.882810', 'path': 'exp/ECL-PatchTST2023-08-30-18:00:19.882810', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-30 18:00:19,883 - [*] phase 0 start training
0 69680
train 34129
val 11425
test 11425
2023-08-30 18:00:20,713 - [*] phase 0 Dataset load!
2023-08-30 18:00:21,667 - [*] phase 0 Training start
train 34129
2023-08-30 18:01:56,539 - epoch:0, training loss:0.1877 validation loss:0.1806
train 34129
vs, vt 0.18059756143225564 0.1859618059462971
Updating learning rate to 1.0438661460315324e-05
Updating learning rate to 1.0438661460315324e-05
train 34129
vs, vt 0.16344440695312287 0.16918223653402592
need align? ->  False 0.16918223653402592
2023-08-30 18:06:58,089 - epoch:1, training loss:4.3775 validation loss:0.1634
Updating learning rate to 2.8027297449571736e-05
Updating learning rate to 2.8027297449571736e-05
train 34129
vs, vt 0.16291675033668676 0.16526186805632379
need align? ->  False 0.16526186805632379
2023-08-30 18:11:17,850 - epoch:2, training loss:1.9369 validation loss:0.1629
Updating learning rate to 5.204727160595503e-05
Updating learning rate to 5.204727160595503e-05
train 34129
vs, vt 0.1556461461302307 0.16253796857264308
need align? ->  False 0.16253796857264308
2023-08-30 18:15:40,609 - epoch:3, training loss:1.7212 validation loss:0.1556
Updating learning rate to 7.605456385119542e-05
Updating learning rate to 7.605456385119542e-05
train 34129
vs, vt 0.16442730178435644 0.15979512789183192
need align? ->  True 0.15979512789183192
2023-08-30 18:19:52,259 - epoch:4, training loss:1.4719 validation loss:0.1644
Updating learning rate to 9.360855637921138e-05
Updating learning rate to 9.360855637921138e-05
train 34129
vs, vt 0.16106430250737402 0.15708018292983372
need align? ->  True 0.15708018292983372
2023-08-30 18:24:04,258 - epoch:5, training loss:1.3603 validation loss:0.1611
Updating learning rate to 9.99999939458629e-05
Updating learning rate to 9.99999939458629e-05
train 34129
vs, vt 0.1566439541677634 0.1591229114267561
need align? ->  False 0.15708018292983372
2023-08-30 18:28:32,590 - epoch:6, training loss:1.3534 validation loss:0.1566
Updating learning rate to 9.956902716655286e-05
Updating learning rate to 9.956902716655286e-05
train 34129
vs, vt 0.15734529342088435 0.15338854462736182
need align? ->  True 0.15338854462736182
2023-08-30 18:32:59,009 - epoch:7, training loss:1.3672 validation loss:0.1573
Updating learning rate to 9.828992401134782e-05
Updating learning rate to 9.828992401134782e-05
train 34129
vs, vt 0.15868522967729304 0.1578312997602754
need align? ->  True 0.15338854462736182
2023-08-30 18:37:12,475 - epoch:8, training loss:1.3451 validation loss:0.1587
Updating learning rate to 9.618457028986775e-05
Updating learning rate to 9.618457028986775e-05
train 34129
vs, vt 0.15650723688304424 0.1522854605068763
need align? ->  True 0.1522854605068763
2023-08-30 18:42:18,742 - epoch:9, training loss:1.2917 validation loss:0.1565
Updating learning rate to 9.32889891880015e-05
Updating learning rate to 9.32889891880015e-05
train 34129
vs, vt 0.15816932117773427 0.15199276664190822
need align? ->  True 0.15199276664190822
2023-08-30 18:46:58,161 - epoch:10, training loss:1.2343 validation loss:0.1582
Updating learning rate to 8.965272490120875e-05
Updating learning rate to 8.965272490120875e-05
train 34129
vs, vt 0.15821850101153057 0.1528337733199199
need align? ->  True 0.15199276664190822
2023-08-30 18:51:18,727 - epoch:11, training loss:1.1509 validation loss:0.1582
Updating learning rate to 8.533799491959945e-05
Updating learning rate to 8.533799491959945e-05
train 34129
vs, vt 0.16207439655231104 0.15331549979746342
need align? ->  True 0.15199276664190822
2023-08-30 18:55:38,909 - epoch:12, training loss:1.0628 validation loss:0.1621
Updating learning rate to 8.041862546942808e-05
Updating learning rate to 8.041862546942808e-05
train 34129
vs, vt 0.1591914484484328 0.15517397742304537
need align? ->  True 0.15199276664190822
2023-08-30 19:00:35,186 - epoch:13, training loss:1.0189 validation loss:0.1592
Updating learning rate to 7.497878832589398e-05
Updating learning rate to 7.497878832589398e-05
train 34129
vs, vt 0.16206121999356482 0.1539145184473859
need align? ->  True 0.15199276664190822
2023-08-30 19:05:11,607 - epoch:14, training loss:0.9859 validation loss:0.1621
Updating learning rate to 6.911156061073078e-05
Updating learning rate to 6.911156061073078e-05
train 34129
vs, vt 0.1591314498335123 0.1564586634023322
need align? ->  True 0.15199276664190822
2023-08-30 19:09:49,622 - epoch:15, training loss:0.9949 validation loss:0.1591
Updating learning rate to 6.291733221684778e-05
Updating learning rate to 6.291733221684778e-05
train 34129
vs, vt 0.1596686068094439 0.15268145348462794
need align? ->  True 0.15199276664190822
2023-08-30 19:14:30,324 - epoch:16, training loss:1.0206 validation loss:0.1597
Updating learning rate to 5.650208810942889e-05
Updating learning rate to 5.650208810942889e-05
train 34129
vs, vt 0.15983406023846733 0.15380492181413705
need align? ->  True 0.15199276664190822
2023-08-30 19:19:02,499 - epoch:17, training loss:1.0844 validation loss:0.1598
Updating learning rate to 4.9975594893793686e-05
Updating learning rate to 4.9975594893793686e-05
train 34129
vs, vt 0.16064946237537597 0.15214663789504104
need align? ->  True 0.15199276664190822
2023-08-30 19:23:25,989 - epoch:18, training loss:1.1330 validation loss:0.1606
Updating learning rate to 4.3449522678347527e-05
Updating learning rate to 4.3449522678347527e-05
train 34129
vs, vt 0.15945865996181965 0.15224698405298923
need align? ->  True 0.15199276664190822
2023-08-30 19:27:38,534 - epoch:19, training loss:1.1948 validation loss:0.1595
Updating learning rate to 3.7035534368065714e-05
Updating learning rate to 3.7035534368065714e-05
train 34129
vs, vt 0.1589464189691676 0.15124590343071354
need align? ->  True 0.15124590343071354
2023-08-30 19:32:07,893 - epoch:20, training loss:1.2371 validation loss:0.1589
Updating learning rate to 3.084337508123067e-05
Updating learning rate to 3.084337508123067e-05
train 34129
vs, vt 0.16058692083590562 0.1521166475282775
need align? ->  True 0.15124590343071354
2023-08-30 19:36:21,329 - epoch:21, training loss:1.1511 validation loss:0.1606
Updating learning rate to 2.497899438003108e-05
Updating learning rate to 2.497899438003108e-05
train 34129
vs, vt 0.16046114563941954 0.1519658005485932
need align? ->  True 0.15124590343071354
2023-08-30 19:40:33,300 - epoch:22, training loss:1.1000 validation loss:0.1605
Updating learning rate to 1.9542733444177923e-05
Updating learning rate to 1.9542733444177923e-05
train 34129
vs, vt 0.16012061023049884 0.15157925710082054
need align? ->  True 0.15124590343071354
2023-08-30 19:45:09,224 - epoch:23, training loss:1.0876 validation loss:0.1601
Updating learning rate to 1.4627608205499963e-05
Updating learning rate to 1.4627608205499963e-05
train 34129
vs, vt 0.1600705491171943 0.1521341688103146
need align? ->  True 0.15124590343071354
2023-08-30 19:49:22,711 - epoch:24, training loss:1.0731 validation loss:0.1601
Updating learning rate to 1.0317717819561129e-05
Updating learning rate to 1.0317717819561129e-05
train 34129
vs, vt 0.1595978571722905 0.15215424887008136
need align? ->  True 0.15124590343071354
2023-08-30 19:53:34,507 - epoch:25, training loss:1.0671 validation loss:0.1596
Updating learning rate to 6.686805705792195e-06
Updating learning rate to 6.686805705792195e-06
train 34129
vs, vt 0.1600064659698142 0.15202821410364575
need align? ->  True 0.15124590343071354
2023-08-30 19:58:10,303 - epoch:26, training loss:1.0583 validation loss:0.1600
Updating learning rate to 3.79699777713878e-06
Updating learning rate to 3.79699777713878e-06
train 34129
vs, vt 0.1595371969872051 0.15214082106120055
need align? ->  True 0.15124590343071354
2023-08-30 20:02:27,305 - epoch:27, training loss:1.0601 validation loss:0.1595
Updating learning rate to 1.6977394484662593e-06
Updating learning rate to 1.6977394484662593e-06
train 34129
vs, vt 0.1597449845737881 0.15187777429819108
need align? ->  True 0.15124590343071354
2023-08-30 20:06:41,587 - epoch:28, training loss:1.0554 validation loss:0.1597
Updating learning rate to 4.2494961180259127e-07
Updating learning rate to 4.2494961180259127e-07
train 34129
vs, vt 0.15978596901728048 0.15192153279980022
need align? ->  True 0.15124590343071354
2023-08-30 20:11:11,124 - epoch:29, training loss:1.0578 validation loss:0.1598
Updating learning rate to 4.0605413710007e-10
Updating learning rate to 4.0605413710007e-10
check exp/ECL-PatchTST2023-08-30-18:00:19.882810/0/0.1556_epoch_3.pkl  &  0.15124590343071354
2023-08-30 20:11:49,587 - [*] loss:0.2961
2023-08-30 20:11:49,733 - [*] phase 0, testing
2023-08-30 20:11:50,591 - T:96	MAE	0.341227	RMSE	0.297805	MAPE	215.277791
2023-08-30 20:11:50,594 - 96	mae	0.3412	
2023-08-30 20:11:50,594 - 96	rmse	0.2978	
2023-08-30 20:11:50,595 - 96	mape	215.2778	
----*-----
2023-08-30 20:12:29,187 - [*] loss:0.2961
2023-08-30 20:12:29,302 - [*] phase 0, testing
2023-08-30 20:12:30,134 - T:96	MAE	0.341227	RMSE	0.297805	MAPE	215.277791
2023-08-30 20:13:05,812 - [*] loss:0.3124
2023-08-30 20:13:05,821 - [*] phase 0, testing
2023-08-30 20:13:06,352 - T:96	MAE	0.364438	RMSE	0.314035	MAPE	227.094817
2023-08-30 20:13:38,489 - [*] loss:0.3354
2023-08-30 20:13:38,499 - [*] phase 0, testing
2023-08-30 20:13:38,942 - T:96	MAE	0.373793	RMSE	0.337173	MAPE	232.694674
2023-08-30 20:14:12,318 - [*] loss:0.3038
2023-08-30 20:14:12,329 - [*] phase 0, testing
2023-08-30 20:14:12,753 - T:96	MAE	0.352102	RMSE	0.305629	MAPE	236.014986
2023-08-30 20:14:48,808 - [*] loss:0.3628
2023-08-30 20:14:48,818 - [*] phase 0, testing
2023-08-30 20:14:49,241 - T:96	MAE	0.401301	RMSE	0.364653	MAPE	229.297447
2023-08-30 20:15:21,336 - [*] loss:0.3441
2023-08-30 20:15:21,362 - [*] phase 0, testing
2023-08-30 20:15:21,702 - T:96	MAE	0.385174	RMSE	0.345829	MAPE	202.149534
2023-08-30 20:15:53,078 - [*] loss:0.3014
2023-08-30 20:15:53,089 - [*] phase 0, testing
2023-08-30 20:15:53,279 - T:96	MAE	0.349954	RMSE	0.303107	MAPE	219.435000
2023-08-30 20:16:24,971 - [*] loss:0.3094
2023-08-30 20:16:24,981 - [*] phase 0, testing
2023-08-30 20:16:25,156 - T:96	MAE	0.360538	RMSE	0.310980	MAPE	201.601601
----*-----
2023-08-30 20:16:40,795 - [*] loss:0.3037
2023-08-30 20:16:40,805 - [*] phase 0, testing
2023-08-30 20:16:40,986 - T:96	MAE	0.358419	RMSE	0.304572	MAPE	205.651569
2023-08-30 20:17:13,073 - [*] loss:0.3342
2023-08-30 20:17:13,083 - [*] phase 0, testing
2023-08-30 20:17:13,264 - T:96	MAE	0.375607	RMSE	0.336100	MAPE	210.974574
2023-08-30 20:17:29,743 - [*] loss:0.3010
2023-08-30 20:17:29,753 - [*] phase 0, testing
2023-08-30 20:17:29,946 - T:96	MAE	0.345869	RMSE	0.301438	MAPE	199.886620
2023-08-30 20:17:29,947 - 96	mae	0.3459	
2023-08-30 20:17:29,947 - 96	rmse	0.3014	
2023-08-30 20:17:29,947 - 96	mape	199.8866	
2023-08-30 20:17:32,609 - logger name:exp/ECL-PatchTST2023-08-30-20:17:32.601226/ECL-PatchTST.log
2023-08-30 20:17:32,610 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 1, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 100, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-30-20:17:32.601226', 'path': 'exp/ECL-PatchTST2023-08-30-20:17:32.601226', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-30 20:17:32,610 - [*] phase 0 start training
0 69680
train 34129
val 11425
test 11425
2023-08-30 20:17:33,388 - [*] phase 0 Dataset load!
2023-08-30 20:17:34,576 - [*] phase 0 Training start
train 34129
2023-08-30 20:18:54,587 - epoch:0, training loss:0.1877 validation loss:0.1806
train 34129
vs, vt 0.18059756143225564 0.1859618059462971
Updating learning rate to 1.0438661460315324e-05
Updating learning rate to 1.0438661460315324e-05
train 34129
vs, vt 0.16649549011554982 0.16918223653402592
need align? ->  False 0.16918223653402592
2023-08-30 20:23:55,743 - epoch:1, training loss:6.0548 validation loss:0.1665
Updating learning rate to 2.8027297449571736e-05
Updating learning rate to 2.8027297449571736e-05
train 34129
vs, vt 0.165627306410008 0.16534826929370564
need align? ->  True 0.16534826929370564
2023-08-30 20:28:23,707 - epoch:2, training loss:2.4752 validation loss:0.1656
Updating learning rate to 5.204727160595503e-05
Updating learning rate to 5.204727160595503e-05
train 34129
vs, vt 0.16564919307000106 0.16521665251089468
need align? ->  True 0.16521665251089468
2023-08-30 20:32:37,449 - epoch:3, training loss:1.7282 validation loss:0.1656
Updating learning rate to 7.605456385119542e-05
Updating learning rate to 7.605456385119542e-05
train 34129
vs, vt 0.1715865946892235 0.1643007665872574
need align? ->  True 0.1643007665872574
2023-08-30 20:36:51,207 - epoch:4, training loss:1.4129 validation loss:0.1716
Updating learning rate to 9.360855637921138e-05
Updating learning rate to 9.360855637921138e-05
train 34129
vs, vt 0.16590007812612587 0.16465181327528425
need align? ->  True 0.1643007665872574
2023-08-30 20:41:20,660 - epoch:5, training loss:1.2354 validation loss:0.1659
Updating learning rate to 9.99999939458629e-05
Updating learning rate to 9.99999939458629e-05
train 34129
vs, vt 0.16260676023860773 0.1638266077886025
need align? ->  False 0.1638266077886025
2023-08-30 20:45:34,853 - epoch:6, training loss:1.1083 validation loss:0.1626
Updating learning rate to 9.956902716655286e-05
Updating learning rate to 9.956902716655286e-05
train 34129
vs, vt 0.16232544999155735 0.15831966302875017
need align? ->  True 0.15831966302875017
2023-08-30 20:49:48,144 - epoch:7, training loss:1.0155 validation loss:0.1623
Updating learning rate to 9.828992401134782e-05
Updating learning rate to 9.828992401134782e-05
train 34129
vs, vt 0.16085500878592332 0.1604915279067225
need align? ->  True 0.15831966302875017
2023-08-30 20:54:24,279 - epoch:8, training loss:0.9298 validation loss:0.1609
Updating learning rate to 9.618457028986775e-05
Updating learning rate to 9.618457028986775e-05
train 34129
vs, vt 0.16257308849857913 0.1583636034693983
need align? ->  True 0.15831966302875017
2023-08-30 20:58:38,152 - epoch:9, training loss:0.8741 validation loss:0.1626
Updating learning rate to 9.32889891880015e-05
Updating learning rate to 9.32889891880015e-05
train 34129
vs, vt 0.1593877563873927 0.15763506934874588
need align? ->  True 0.15763506934874588
2023-08-30 21:02:50,574 - epoch:10, training loss:0.8366 validation loss:0.1594
Updating learning rate to 8.965272490120875e-05
Updating learning rate to 8.965272490120875e-05
train 34129
vs, vt 0.15901491435037718 0.15665835775434972
need align? ->  True 0.15665835775434972
2023-08-30 21:07:27,769 - epoch:11, training loss:0.8523 validation loss:0.1590
Updating learning rate to 8.533799491959945e-05
Updating learning rate to 8.533799491959945e-05
train 34129
vs, vt 0.15845958089662923 0.15604979772534636
need align? ->  True 0.15604979772534636
2023-08-30 21:11:43,810 - epoch:12, training loss:0.8108 validation loss:0.1585
Updating learning rate to 8.041862546942808e-05
Updating learning rate to 8.041862546942808e-05
train 34129
vs, vt 0.15728158768680361 0.15797383180922933
need align? ->  True 0.15604979772534636
2023-08-30 21:15:57,334 - epoch:13, training loss:0.8126 validation loss:0.1573
Updating learning rate to 7.497878832589398e-05
Updating learning rate to 7.497878832589398e-05
train 34129
vs, vt 0.15622572700182596 0.1559063534769747
need align? ->  True 0.1559063534769747
2023-08-30 21:20:29,431 - epoch:14, training loss:0.7879 validation loss:0.1562
Updating learning rate to 6.911156061073078e-05
Updating learning rate to 6.911156061073078e-05
train 34129
vs, vt 0.15560902162558501 0.15446094647049904
need align? ->  True 0.15446094647049904
2023-08-30 21:24:48,630 - epoch:15, training loss:0.7889 validation loss:0.1556
Updating learning rate to 6.291733221684778e-05
Updating learning rate to 6.291733221684778e-05
train 34129
vs, vt 0.15557142417463993 0.15359436182512176
need align? ->  True 0.15359436182512176
2023-08-30 21:29:02,433 - epoch:16, training loss:0.7822 validation loss:0.1556
Updating learning rate to 5.650208810942889e-05
Updating learning rate to 5.650208810942889e-05
train 34129
vs, vt 0.15442243578533332 0.15298853115075164
need align? ->  True 0.15298853115075164
2023-08-30 21:33:31,240 - epoch:17, training loss:0.7765 validation loss:0.1544
Updating learning rate to 4.9975594893793686e-05
Updating learning rate to 4.9975594893793686e-05
train 34129
vs, vt 0.15446168147027492 0.15273261182010173
need align? ->  True 0.15273261182010173
2023-08-30 21:37:54,941 - epoch:18, training loss:0.7760 validation loss:0.1545
Updating learning rate to 4.3449522678347527e-05
Updating learning rate to 4.3449522678347527e-05
train 34129
vs, vt 0.1552606265578005 0.15246966195603212
need align? ->  True 0.15246966195603212
2023-08-30 21:42:09,205 - epoch:19, training loss:0.7713 validation loss:0.1553
Updating learning rate to 3.7035534368065714e-05
Updating learning rate to 3.7035534368065714e-05
train 34129
vs, vt 0.15438625150256688 0.15225676372647284
need align? ->  True 0.15225676372647284
2023-08-30 21:46:27,219 - epoch:20, training loss:0.7717 validation loss:0.1544
Updating learning rate to 3.084337508123067e-05
Updating learning rate to 3.084337508123067e-05
train 34129
vs, vt 0.15561784857677088 0.15248566112584538
need align? ->  True 0.15225676372647284
2023-08-30 21:51:06,044 - epoch:21, training loss:0.7690 validation loss:0.1556
Updating learning rate to 2.497899438003108e-05
Updating learning rate to 2.497899438003108e-05
train 34129
vs, vt 0.15463443506095145 0.1525426516516341
need align? ->  True 0.15225676372647284
2023-08-30 21:55:22,659 - epoch:22, training loss:0.7642 validation loss:0.1546
Updating learning rate to 1.9542733444177923e-05
Updating learning rate to 1.9542733444177923e-05
train 34129
vs, vt 0.15518034415112603 0.15182533914016352
need align? ->  True 0.15182533914016352
2023-08-30 21:59:38,253 - epoch:23, training loss:0.7609 validation loss:0.1552
Updating learning rate to 1.4627608205499963e-05
Updating learning rate to 1.4627608205499963e-05
train 34129
vs, vt 0.1554641457895438 0.15228701113826698
need align? ->  True 0.15182533914016352
2023-08-30 22:04:06,472 - epoch:24, training loss:0.7711 validation loss:0.1555
Updating learning rate to 1.0317717819561129e-05
Updating learning rate to 1.0317717819561129e-05
train 34129
vs, vt 0.154888323860036 0.15230821607013542
need align? ->  True 0.15182533914016352
2023-08-30 22:08:19,172 - epoch:25, training loss:0.7684 validation loss:0.1549
Updating learning rate to 6.686805705792195e-06
Updating learning rate to 6.686805705792195e-06
train 34129
vs, vt 0.15513460520240996 0.15211912815769513
need align? ->  True 0.15182533914016352
2023-08-30 22:12:31,735 - epoch:26, training loss:0.7673 validation loss:0.1551
Updating learning rate to 3.79699777713878e-06
Updating learning rate to 3.79699777713878e-06
train 34129
vs, vt 0.15483340707918009 0.1521120898011658
need align? ->  True 0.15182533914016352
2023-08-30 22:17:05,016 - epoch:27, training loss:0.7666 validation loss:0.1548
Updating learning rate to 1.6977394484662593e-06
Updating learning rate to 1.6977394484662593e-06
train 34129
vs, vt 0.15481361899938847 0.15205116143657102
need align? ->  True 0.15182533914016352
2023-08-30 22:21:19,269 - epoch:28, training loss:0.7665 validation loss:0.1548
Updating learning rate to 4.2494961180259127e-07
Updating learning rate to 4.2494961180259127e-07
train 34129
vs, vt 0.15492149359650081 0.15210370897418923
need align? ->  True 0.15182533914016352
2023-08-30 22:25:32,343 - epoch:29, training loss:0.7662 validation loss:0.1549
Updating learning rate to 4.0605413710007e-10
Updating learning rate to 4.0605413710007e-10
check exp/ECL-PatchTST2023-08-30-20:17:32.601226/0/0.1544_epoch_20.pkl  &  0.15182533914016352
2023-08-30 22:26:03,423 - [*] loss:0.2897
2023-08-30 22:26:03,587 - [*] phase 0, testing
2023-08-30 22:26:04,361 - T:96	MAE	0.337059	RMSE	0.290272	MAPE	213.769817
2023-08-30 22:26:04,364 - 96	mae	0.3371	
2023-08-30 22:26:04,365 - 96	rmse	0.2903	
2023-08-30 22:26:04,365 - 96	mape	213.7698	
----*-----
2023-08-30 22:26:35,931 - [*] loss:0.2897
2023-08-30 22:26:35,941 - [*] phase 0, testing
2023-08-30 22:26:36,690 - T:96	MAE	0.337059	RMSE	0.290272	MAPE	213.769817
2023-08-30 22:27:09,551 - [*] loss:0.3091
2023-08-30 22:27:09,560 - [*] phase 0, testing
2023-08-30 22:27:10,094 - T:96	MAE	0.363061	RMSE	0.309794	MAPE	235.790205
2023-08-30 22:27:46,151 - [*] loss:0.4062
2023-08-30 22:27:46,160 - [*] phase 0, testing
2023-08-30 22:27:46,814 - T:96	MAE	0.420559	RMSE	0.407684	MAPE	279.469609
2023-08-30 22:28:26,835 - [*] loss:0.2939
2023-08-30 22:28:26,845 - [*] phase 0, testing
2023-08-30 22:28:27,509 - T:96	MAE	0.345190	RMSE	0.294632	MAPE	227.997184
2023-08-30 22:29:09,761 - [*] loss:0.3865
2023-08-30 22:29:09,771 - [*] phase 0, testing
2023-08-30 22:29:10,417 - T:96	MAE	0.420501	RMSE	0.387698	MAPE	241.342402
2023-08-30 22:29:50,106 - [*] loss:0.3374
2023-08-30 22:29:50,116 - [*] phase 0, testing
2023-08-30 22:29:50,805 - T:96	MAE	0.382606	RMSE	0.338338	MAPE	204.014754
2023-08-30 22:30:29,596 - [*] loss:0.2954
2023-08-30 22:30:29,607 - [*] phase 0, testing
2023-08-30 22:30:30,244 - T:96	MAE	0.345619	RMSE	0.295954	MAPE	221.037292
2023-08-30 22:31:05,254 - [*] loss:0.3039
2023-08-30 22:31:05,264 - [*] phase 0, testing
2023-08-30 22:31:05,787 - T:96	MAE	0.358401	RMSE	0.304473	MAPE	205.334759
----*-----
2023-08-30 22:31:21,859 - [*] loss:0.2997
2023-08-30 22:31:21,869 - [*] phase 0, testing
2023-08-30 22:31:22,306 - T:96	MAE	0.352831	RMSE	0.300234	MAPE	205.009866
2023-08-30 22:31:53,594 - [*] loss:0.3048
2023-08-30 22:31:53,604 - [*] phase 0, testing
2023-08-30 22:31:54,016 - T:96	MAE	0.354471	RMSE	0.305999	MAPE	201.039743
2023-08-30 22:32:09,910 - [*] loss:0.3007
2023-08-30 22:32:09,919 - [*] phase 0, testing
2023-08-30 22:32:10,256 - T:96	MAE	0.346995	RMSE	0.301659	MAPE	201.026368
2023-08-30 22:32:10,259 - 96	mae	0.3470	
2023-08-30 22:32:10,259 - 96	rmse	0.3017	
2023-08-30 22:32:10,259 - 96	mape	201.0264	
2023-08-30 22:32:12,727 - logger name:exp/ECL-PatchTST2023-08-30-22:32:12.724081/ECL-PatchTST.log
2023-08-30 22:32:12,727 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 1, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 100, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-30-22:32:12.724081', 'path': 'exp/ECL-PatchTST2023-08-30-22:32:12.724081', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-30 22:32:12,728 - [*] phase 0 start training
0 69680
train 33889
val 11185
test 11185
2023-08-30 22:32:13,518 - [*] phase 0 Dataset load!
2023-08-30 22:32:14,619 - [*] phase 0 Training start
train 33889
2023-08-30 22:33:35,834 - epoch:0, training loss:0.4930 validation loss:0.6868
train 33889
vs, vt 0.6867958413098346 0.6997498519379984
Updating learning rate to 1.0438721218484657e-05
Updating learning rate to 1.0438721218484657e-05
train 33889
vs, vt 0.6661991813982074 0.6687830971045927
need align? ->  False 0.6687830971045927
2023-08-30 22:38:34,978 - epoch:1, training loss:6.3198 validation loss:0.6662
Updating learning rate to 2.802750441854847e-05
Updating learning rate to 2.802750441854847e-05
train 33889
vs, vt 0.6701760903339494 0.665557973594828
need align? ->  True 0.665557973594828
2023-08-30 22:43:01,830 - epoch:2, training loss:2.8054 validation loss:0.6702
Updating learning rate to 5.2047629950292354e-05
Updating learning rate to 5.2047629950292354e-05
train 33889
vs, vt 0.6581206706098535 0.6643032323230397
need align? ->  False 0.6643032323230397
2023-08-30 22:47:23,326 - epoch:3, training loss:2.1428 validation loss:0.6581
Updating learning rate to 7.605497731655361e-05
Updating learning rate to 7.605497731655361e-05
train 33889
vs, vt 0.6766601583327759 0.6667080102488399
need align? ->  True 0.6643032323230397
2023-08-30 22:51:37,443 - epoch:4, training loss:1.8692 validation loss:0.6767
Updating learning rate to 9.3608854147054e-05
Updating learning rate to 9.3608854147054e-05
train 33889
vs, vt 0.659468591297892 0.6647290392694148
need align? ->  False 0.6643032323230397
2023-08-30 22:56:00,182 - epoch:5, training loss:1.7064 validation loss:0.6595
Updating learning rate to 9.99999938537861e-05
Updating learning rate to 9.99999938537861e-05
train 33889
vs, vt 0.6586020982570269 0.660835150883279
need align? ->  False 0.660835150883279
2023-08-30 23:00:24,795 - epoch:6, training loss:1.5903 validation loss:0.6586
Updating learning rate to 9.956900274488073e-05
Updating learning rate to 9.956900274488073e-05
train 33889
vs, vt 0.6709312336858023 0.6659671018238772
need align? ->  True 0.660835150883279
2023-08-30 23:04:36,308 - epoch:7, training loss:1.5240 validation loss:0.6709
Updating learning rate to 9.828987567794199e-05
Updating learning rate to 9.828987567794199e-05
train 33889
vs, vt 0.6552436899563129 0.6659579960290681
need align? ->  False 0.660835150883279
2023-08-30 23:08:49,289 - epoch:8, training loss:1.4187 validation loss:0.6552
Updating learning rate to 9.618449887172618e-05
Updating learning rate to 9.618449887172618e-05
train 33889
vs, vt 0.6610682822594588 0.662537646649236
need align? ->  True 0.660835150883279
2023-08-30 23:13:16,156 - epoch:9, training loss:1.3638 validation loss:0.6611
Updating learning rate to 9.328889590710838e-05
Updating learning rate to 9.328889590710838e-05
train 33889
vs, vt 0.6522400223396041 0.671440423804928
need align? ->  False 0.660835150883279
2023-08-30 23:17:28,975 - epoch:10, training loss:1.3244 validation loss:0.6522
Updating learning rate to 8.965261135362604e-05
Updating learning rate to 8.965261135362604e-05
train 33889
vs, vt 0.6533172275363044 0.6633598882366311
need align? ->  False 0.660835150883279
2023-08-30 23:21:42,345 - epoch:11, training loss:1.2949 validation loss:0.6533
Updating learning rate to 8.533786304815776e-05
Updating learning rate to 8.533786304815776e-05
train 33889
vs, vt 0.650941598313776 0.6591361651027744
need align? ->  False 0.6591361651027744
2023-08-30 23:26:12,985 - epoch:12, training loss:1.2728 validation loss:0.6509
Updating learning rate to 8.041847753048434e-05
Updating learning rate to 8.041847753048434e-05
train 33889
vs, vt 0.6564078048210252 0.6640905098650943
need align? ->  False 0.6591361651027744
2023-08-30 23:30:26,472 - epoch:13, training loss:1.4065 validation loss:0.6564
Updating learning rate to 7.497862685072454e-05
Updating learning rate to 7.497862685072454e-05
train 33889
vs, vt 0.6562929923053492 0.6631921970031478
need align? ->  False 0.6591361651027744
2023-08-30 23:34:39,221 - epoch:14, training loss:1.3195 validation loss:0.6563
Updating learning rate to 6.911138836222055e-05
Updating learning rate to 6.911138836222055e-05
train 33889
vs, vt 0.6485234904526309 0.6643367161635648
need align? ->  False 0.6591361651027744
2023-08-30 23:39:15,457 - epoch:15, training loss:1.2851 validation loss:0.6485
Updating learning rate to 6.291715214221653e-05
Updating learning rate to 6.291715214221653e-05
train 33889
vs, vt 0.6499644114889882 0.6605652599510822
need align? ->  False 0.6591361651027744
2023-08-30 23:43:30,366 - epoch:16, training loss:1.2647 validation loss:0.6500
Updating learning rate to 5.6501903289803477e-05
Updating learning rate to 5.6501903289803477e-05
train 33889
vs, vt 0.6486677621406588 0.660530912859196
need align? ->  False 0.6591361651027744
2023-08-30 23:47:43,204 - epoch:17, training loss:1.2489 validation loss:0.6487
Updating learning rate to 4.997540849148917e-05
Updating learning rate to 4.997540849148917e-05
train 33889
vs, vt 0.6621706462041899 0.660229701464149
need align? ->  True 0.6591361651027744
2023-08-30 23:52:17,088 - epoch:18, training loss:1.2359 validation loss:0.6622
Updating learning rate to 4.344933788275899e-05
Updating learning rate to 4.344933788275899e-05
train 33889
vs, vt 0.646407750858502 0.667623208581724
need align? ->  False 0.6591361651027744
2023-08-30 23:56:38,414 - epoch:19, training loss:1.2233 validation loss:0.6464
Updating learning rate to 3.703535434109692e-05
Updating learning rate to 3.703535434109692e-05
train 33889
vs, vt 0.6501692791723392 0.6607211493802342
need align? ->  False 0.6591361651027744
2023-08-31 00:00:56,447 - epoch:20, training loss:1.2163 validation loss:0.6502
Updating learning rate to 3.084320290319299e-05
Updating learning rate to 3.084320290319299e-05
train 33889
vs, vt 0.6508407122878865 0.6605225649916313
need align? ->  False 0.6591361651027744
2023-08-31 00:05:31,218 - epoch:21, training loss:1.2120 validation loss:0.6508
Updating learning rate to 2.4978832996938436e-05
Updating learning rate to 2.4978832996938436e-05
train 33889
vs, vt 0.6488743090832775 0.6596978545527566
need align? ->  False 0.6591361651027744
2023-08-31 00:09:52,076 - epoch:22, training loss:1.2057 validation loss:0.6489
Updating learning rate to 1.954258561733979e-05
Updating learning rate to 1.954258561733979e-05
train 33889
vs, vt 0.648082847029648 0.6575178074735132
need align? ->  False 0.6575178074735132
2023-08-31 00:14:06,565 - epoch:23, training loss:1.2015 validation loss:0.6481
Updating learning rate to 1.4627476464274535e-05
Updating learning rate to 1.4627476464274535e-05
train 33889
vs, vt 0.6485396731136874 0.6577051649378105
need align? ->  False 0.6575178074735132
2023-08-31 00:18:24,827 - epoch:24, training loss:1.3616 validation loss:0.6485
Updating learning rate to 1.0317604418077296e-05
Updating learning rate to 1.0317604418077296e-05
train 33889
vs, vt 0.6475284358655865 0.6564062081954696
need align? ->  False 0.6564062081954696
2023-08-31 00:22:48,853 - epoch:25, training loss:1.3362 validation loss:0.6475
Updating learning rate to 6.686712584380782e-06
Updating learning rate to 6.686712584380782e-06
train 33889
vs, vt 0.6477072594518011 0.6577145182104274
need align? ->  False 0.6564062081954696
2023-08-31 00:27:01,330 - epoch:26, training loss:1.3409 validation loss:0.6477
Updating learning rate to 3.7969265291329562e-06
Updating learning rate to 3.7969265291329562e-06
train 33889
vs, vt 0.6487853302705017 0.6580225705084476
need align? ->  False 0.6564062081954696
2023-08-31 00:31:12,319 - epoch:27, training loss:1.3366 validation loss:0.6488
Updating learning rate to 1.6976912929391648e-06
Updating learning rate to 1.6976912929391648e-06
train 33889
vs, vt 0.6488383578305895 0.6570080753246491
need align? ->  False 0.6564062081954696
2023-08-31 00:35:54,837 - epoch:28, training loss:1.3340 validation loss:0.6488
Updating learning rate to 4.2492537270863806e-07
Updating learning rate to 4.2492537270863806e-07
train 33889
vs, vt 0.6486920282583345 0.6571195016361095
need align? ->  False 0.6564062081954696
2023-08-31 00:40:31,085 - epoch:29, training loss:1.3335 validation loss:0.6487
Updating learning rate to 4.0614621390542476e-10
Updating learning rate to 4.0614621390542476e-10
check exp/ECL-PatchTST2023-08-30-22:32:12.724081/0/0.6464_epoch_19.pkl  &  0.6564062081954696
2023-08-31 00:41:08,251 - [*] loss:0.3733
2023-08-31 00:41:08,286 - [*] phase 0, testing
2023-08-31 00:41:10,837 - T:336	MAE	0.395880	RMSE	0.372938	MAPE	234.959245
2023-08-31 00:41:10,838 - 336	mae	0.3959	
2023-08-31 00:41:10,839 - 336	rmse	0.3729	
2023-08-31 00:41:10,839 - 336	mape	234.9592	
----*-----
2023-08-31 00:41:46,511 - [*] loss:0.3733
2023-08-31 00:41:46,547 - [*] phase 0, testing
2023-08-31 00:41:48,602 - T:336	MAE	0.395880	RMSE	0.372938	MAPE	234.959245
2023-08-31 00:42:26,882 - [*] loss:0.3812
2023-08-31 00:42:26,916 - [*] phase 0, testing
2023-08-31 00:42:28,732 - T:336	MAE	0.405933	RMSE	0.380811	MAPE	241.754413
2023-08-31 00:43:05,627 - [*] loss:0.4607
2023-08-31 00:43:05,660 - [*] phase 0, testing
2023-08-31 00:43:07,393 - T:336	MAE	0.455323	RMSE	0.460451	MAPE	294.620013
2023-08-31 00:43:45,501 - [*] loss:0.3812
2023-08-31 00:43:45,535 - [*] phase 0, testing
2023-08-31 00:43:46,679 - T:336	MAE	0.405257	RMSE	0.380836	MAPE	250.558853
2023-08-31 00:44:26,130 - [*] loss:0.4688
2023-08-31 00:44:26,164 - [*] phase 0, testing
2023-08-31 00:44:27,072 - T:336	MAE	0.475757	RMSE	0.468377	MAPE	257.549047
2023-08-31 00:45:04,726 - [*] loss:0.4136
2023-08-31 00:45:04,759 - [*] phase 0, testing
2023-08-31 00:45:05,445 - T:336	MAE	0.431782	RMSE	0.413046	MAPE	215.833187
2023-08-31 00:45:43,284 - [*] loss:0.3755
2023-08-31 00:45:43,331 - [*] phase 0, testing
2023-08-31 00:45:43,964 - T:336	MAE	0.399067	RMSE	0.375179	MAPE	237.078595
2023-08-31 00:46:21,228 - [*] loss:0.3773
2023-08-31 00:46:21,262 - [*] phase 0, testing
2023-08-31 00:46:21,950 - T:336	MAE	0.401714	RMSE	0.376791	MAPE	219.403744
----*-----
2023-08-31 00:46:43,005 - [*] loss:0.3728
2023-08-31 00:46:43,040 - [*] phase 0, testing
2023-08-31 00:46:43,667 - T:336	MAE	0.397010	RMSE	0.372256	MAPE	220.200014
2023-08-31 00:47:20,367 - [*] loss:0.3873
2023-08-31 00:47:20,403 - [*] phase 0, testing
2023-08-31 00:47:21,079 - T:336	MAE	0.400591	RMSE	0.387289	MAPE	226.721883
2023-08-31 00:47:41,951 - [*] loss:0.3906
2023-08-31 00:47:41,986 - [*] phase 0, testing
2023-08-31 00:47:42,643 - T:336	MAE	0.398767	RMSE	0.390629	MAPE	221.313357
2023-08-31 00:47:42,644 - 336	mae	0.3988	
2023-08-31 00:47:42,645 - 336	rmse	0.3906	
2023-08-31 00:47:42,645 - 336	mape	221.3134	
2023-08-31 00:47:45,242 - logger name:exp/ECL-PatchTST2023-08-31-00:47:45.241225/ECL-PatchTST.log
2023-08-31 00:47:45,242 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 1, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 100, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-31-00:47:45.241225', 'path': 'exp/ECL-PatchTST2023-08-31-00:47:45.241225', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-31 00:47:45,242 - [*] phase 0 start training
0 69680
train 33889
val 11185
test 11185
2023-08-31 00:47:46,111 - [*] phase 0 Dataset load!
2023-08-31 00:47:47,118 - [*] phase 0 Training start
train 33889
2023-08-31 00:49:22,688 - epoch:0, training loss:0.2098 validation loss:0.2611
train 33889
vs, vt 0.2611356626518748 0.26504589253189886
Updating learning rate to 1.0438721218484657e-05
Updating learning rate to 1.0438721218484657e-05
train 33889
vs, vt 0.25108757022429595 0.2533516910096461
need align? ->  False 0.2533516910096461
2023-08-31 00:54:52,707 - epoch:1, training loss:4.1317 validation loss:0.2511
Updating learning rate to 2.802750441854847e-05
Updating learning rate to 2.802750441854847e-05
train 33889
vs, vt 0.25608328539370134 0.25141219168224116
need align? ->  True 0.25141219168224116
2023-08-31 00:59:23,148 - epoch:2, training loss:1.7058 validation loss:0.2561
Updating learning rate to 5.2047629950292354e-05
Updating learning rate to 5.2047629950292354e-05
train 33889
vs, vt 0.24982610260221091 0.25195223689925944
need align? ->  False 0.25141219168224116
2023-08-31 01:03:32,527 - epoch:3, training loss:1.5660 validation loss:0.2498
Updating learning rate to 7.605497731655361e-05
Updating learning rate to 7.605497731655361e-05
train 33889
vs, vt 0.25371316392821347 0.2512431728907607
need align? ->  True 0.2512431728907607
2023-08-31 01:07:55,328 - epoch:4, training loss:1.7227 validation loss:0.2537
Updating learning rate to 9.3608854147054e-05
Updating learning rate to 9.3608854147054e-05
train 33889
vs, vt 0.2471198868429796 0.2506833358837122
need align? ->  False 0.2506833358837122
2023-08-31 01:12:25,428 - epoch:5, training loss:1.9766 validation loss:0.2471
Updating learning rate to 9.99999938537861e-05
Updating learning rate to 9.99999938537861e-05
train 33889
vs, vt 0.25188168341463263 0.25114378646354785
need align? ->  True 0.2506833358837122
2023-08-31 01:16:35,756 - epoch:6, training loss:2.6528 validation loss:0.2519
Updating learning rate to 9.956900274488073e-05
Updating learning rate to 9.956900274488073e-05
train 33889
vs, vt 0.2531343612565913 0.2541369424116882
need align? ->  True 0.2506833358837122
2023-08-31 01:20:57,117 - epoch:7, training loss:3.3326 validation loss:0.2531
Updating learning rate to 9.828987567794199e-05
Updating learning rate to 9.828987567794199e-05
train 33889
vs, vt 0.2536512421105396 0.2564901348457418
need align? ->  True 0.2506833358837122
2023-08-31 01:25:29,764 - epoch:8, training loss:3.2797 validation loss:0.2537
Updating learning rate to 9.618449887172618e-05
Updating learning rate to 9.618449887172618e-05
train 33889
vs, vt 0.2492249870080162 0.2522755096327852
need align? ->  False 0.2506833358837122
2023-08-31 01:29:38,110 - epoch:9, training loss:2.9786 validation loss:0.2492
Updating learning rate to 9.328889590710838e-05
Updating learning rate to 9.328889590710838e-05
train 33889
vs, vt 0.253953814718195 0.25211724850603123
need align? ->  True 0.2506833358837122
2023-08-31 01:33:58,612 - epoch:10, training loss:2.7950 validation loss:0.2540
Updating learning rate to 8.965261135362604e-05
Updating learning rate to 8.965261135362604e-05
train 33889
vs, vt 0.2587949007918889 0.2551145876588469
need align? ->  True 0.2506833358837122
2023-08-31 01:38:33,605 - epoch:11, training loss:2.7077 validation loss:0.2588
Updating learning rate to 8.533786304815776e-05
Updating learning rate to 8.533786304815776e-05
train 33889
vs, vt 0.2538244406354021 0.25281054471534764
need align? ->  True 0.2506833358837122
2023-08-31 01:42:53,097 - epoch:12, training loss:2.7016 validation loss:0.2538
Updating learning rate to 8.041847753048434e-05
Updating learning rate to 8.041847753048434e-05
train 33889
vs, vt 0.26180623056875035 0.25485254180702294
need align? ->  True 0.2506833358837122
2023-08-31 01:47:10,153 - epoch:13, training loss:2.7482 validation loss:0.2618
Updating learning rate to 7.497862685072454e-05
Updating learning rate to 7.497862685072454e-05
train 33889
vs, vt 0.26276751565323636 0.25463625352660363
need align? ->  True 0.2506833358837122
2023-08-31 01:51:39,949 - epoch:14, training loss:2.8010 validation loss:0.2628
Updating learning rate to 6.911138836222055e-05
Updating learning rate to 6.911138836222055e-05
train 33889
vs, vt 0.2625027029690417 0.26097300250760536
need align? ->  True 0.2506833358837122
2023-08-31 01:56:06,152 - epoch:15, training loss:2.7963 validation loss:0.2625
Updating learning rate to 6.291715214221653e-05
Updating learning rate to 6.291715214221653e-05
train 33889
vs, vt 0.2648258315483955 0.2583889341489835
need align? ->  True 0.2506833358837122
2023-08-31 02:00:20,926 - epoch:16, training loss:2.8213 validation loss:0.2648
Updating learning rate to 5.6501903289803477e-05
Updating learning rate to 5.6501903289803477e-05
train 33889
vs, vt 0.26716273570094595 0.25840395092117513
need align? ->  True 0.2506833358837122
2023-08-31 02:04:43,766 - epoch:17, training loss:2.8267 validation loss:0.2672
Updating learning rate to 4.997540849148917e-05
Updating learning rate to 4.997540849148917e-05
train 33889
vs, vt 0.26893623173236847 0.2583177684890953
need align? ->  True 0.2506833358837122
2023-08-31 02:09:23,015 - epoch:18, training loss:2.8321 validation loss:0.2689
Updating learning rate to 4.344933788275899e-05
Updating learning rate to 4.344933788275899e-05
train 33889
vs, vt 0.2624627789905803 0.2573638375509869
need align? ->  True 0.2506833358837122
2023-08-31 02:13:38,102 - epoch:19, training loss:2.8273 validation loss:0.2625
Updating learning rate to 3.703535434109692e-05
Updating learning rate to 3.703535434109692e-05
train 33889
vs, vt 0.2675040319392627 0.2553242594850334
need align? ->  True 0.2506833358837122
2023-08-31 02:17:58,922 - epoch:20, training loss:2.8040 validation loss:0.2675
Updating learning rate to 3.084320290319299e-05
Updating learning rate to 3.084320290319299e-05
train 33889
vs, vt 0.26796464164826 0.2573495874266056
need align? ->  True 0.2506833358837122
2023-08-31 02:22:29,230 - epoch:21, training loss:2.7835 validation loss:0.2680
Updating learning rate to 2.4978832996938436e-05
Updating learning rate to 2.4978832996938436e-05
train 33889
vs, vt 0.26694421825761144 0.25715563839978794
need align? ->  True 0.2506833358837122
2023-08-31 02:26:40,231 - epoch:22, training loss:2.7709 validation loss:0.2669
Updating learning rate to 1.954258561733979e-05
Updating learning rate to 1.954258561733979e-05
train 33889
vs, vt 0.2674557525579902 0.25619469316337595
need align? ->  True 0.2506833358837122
2023-08-31 02:30:59,732 - epoch:23, training loss:2.7389 validation loss:0.2675
Updating learning rate to 1.4627476464274535e-05
Updating learning rate to 1.4627476464274535e-05
train 33889
vs, vt 0.2674296072006903 0.2568885880437764
need align? ->  True 0.2506833358837122
2023-08-31 02:35:34,464 - epoch:24, training loss:2.7349 validation loss:0.2674
Updating learning rate to 1.0317604418077296e-05
Updating learning rate to 1.0317604418077296e-05
train 33889
vs, vt 0.26863580535758624 0.2576552229002118
need align? ->  True 0.2506833358837122
2023-08-31 02:39:45,311 - epoch:25, training loss:2.7264 validation loss:0.2686
Updating learning rate to 6.686712584380782e-06
Updating learning rate to 6.686712584380782e-06
train 33889
vs, vt 0.26866650356995786 0.2567773678509349
need align? ->  True 0.2506833358837122
2023-08-31 02:44:04,452 - epoch:26, training loss:2.7129 validation loss:0.2687
Updating learning rate to 3.7969265291329562e-06
Updating learning rate to 3.7969265291329562e-06
train 33889
vs, vt 0.2691122615201907 0.2578508090651171
need align? ->  True 0.2506833358837122
2023-08-31 02:48:37,739 - epoch:27, training loss:2.7102 validation loss:0.2691
Updating learning rate to 1.6976912929391648e-06
Updating learning rate to 1.6976912929391648e-06
train 33889
vs, vt 0.26846137968823314 0.2573109505359422
need align? ->  True 0.2506833358837122
2023-08-31 02:52:55,497 - epoch:28, training loss:2.7007 validation loss:0.2685
Updating learning rate to 4.2492537270863806e-07
Updating learning rate to 4.2492537270863806e-07
train 33889
vs, vt 0.26867727444253187 0.25759612231261353
need align? ->  True 0.2506833358837122
2023-08-31 02:57:14,092 - epoch:29, training loss:2.6978 validation loss:0.2687
Updating learning rate to 4.0614621390542476e-10
Updating learning rate to 4.0614621390542476e-10
check exp/ECL-PatchTST2023-08-31-00:47:45.241225/0/0.2471_epoch_5.pkl  &  0.2506833358837122
2023-08-31 02:57:42,837 - [*] loss:0.3788
2023-08-31 02:57:42,873 - [*] phase 0, testing
2023-08-31 02:57:43,777 - T:336	MAE	0.392968	RMSE	0.378674	MAPE	240.477371
2023-08-31 02:57:43,778 - 336	mae	0.3930	
2023-08-31 02:57:43,778 - 336	rmse	0.3787	
2023-08-31 02:57:43,778 - 336	mape	240.4774	
----*-----
2023-08-31 02:58:12,511 - [*] loss:0.3788
2023-08-31 02:58:12,545 - [*] phase 0, testing
2023-08-31 02:58:13,110 - T:336	MAE	0.392968	RMSE	0.378674	MAPE	240.477371
2023-08-31 02:58:49,142 - [*] loss:0.3868
2023-08-31 02:58:49,177 - [*] phase 0, testing
2023-08-31 02:58:50,126 - T:336	MAE	0.403927	RMSE	0.386580	MAPE	251.527238
2023-08-31 02:59:28,902 - [*] loss:0.4475
2023-08-31 02:59:28,938 - [*] phase 0, testing
2023-08-31 02:59:29,830 - T:336	MAE	0.437455	RMSE	0.447569	MAPE	278.884435
2023-08-31 03:00:11,147 - [*] loss:0.3828
2023-08-31 03:00:11,182 - [*] phase 0, testing
2023-08-31 03:00:11,819 - T:336	MAE	0.398598	RMSE	0.382587	MAPE	250.831723
2023-08-31 03:00:53,450 - [*] loss:0.4745
2023-08-31 03:00:53,485 - [*] phase 0, testing
2023-08-31 03:00:54,124 - T:336	MAE	0.474842	RMSE	0.474109	MAPE	257.886314
2023-08-31 03:01:32,722 - [*] loss:0.4222
2023-08-31 03:01:32,757 - [*] phase 0, testing
2023-08-31 03:01:33,329 - T:336	MAE	0.437106	RMSE	0.421773	MAPE	218.990111
2023-08-31 03:02:01,601 - [*] loss:0.3809
2023-08-31 03:02:01,635 - [*] phase 0, testing
2023-08-31 03:02:02,468 - T:336	MAE	0.396552	RMSE	0.380739	MAPE	244.990063
2023-08-31 03:02:31,838 - [*] loss:0.3825
2023-08-31 03:02:31,873 - [*] phase 0, testing
2023-08-31 03:02:32,490 - T:336	MAE	0.400685	RMSE	0.382205	MAPE	228.012466
----*-----
2023-08-31 03:02:46,725 - [*] loss:0.3760
2023-08-31 03:02:46,759 - [*] phase 0, testing
2023-08-31 03:02:47,374 - T:336	MAE	0.397098	RMSE	0.375259	MAPE	231.538081
2023-08-31 03:03:17,192 - [*] loss:0.4641
2023-08-31 03:03:17,227 - [*] phase 0, testing
2023-08-31 03:03:17,804 - T:336	MAE	0.446292	RMSE	0.463936	MAPE	222.911215
2023-08-31 03:03:33,827 - [*] loss:0.3812
2023-08-31 03:03:33,860 - [*] phase 0, testing
2023-08-31 03:03:34,584 - T:336	MAE	0.396564	RMSE	0.381114	MAPE	216.319013
2023-08-31 03:03:34,585 - 336	mae	0.3966	
2023-08-31 03:03:34,585 - 336	rmse	0.3811	
2023-08-31 03:03:34,585 - 336	mape	216.3190	
2023-08-31 03:03:36,751 - logger name:exp/ECL-PatchTST2023-08-31-03:03:36.748353/ECL-PatchTST.log
2023-08-31 03:03:36,751 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 1, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 100, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-31-03:03:36.748353', 'path': 'exp/ECL-PatchTST2023-08-31-03:03:36.748353', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-31 03:03:36,751 - [*] phase 0 start training
0 69680
train 33889
val 11185
test 11185
2023-08-31 03:03:37,552 - [*] phase 0 Dataset load!
2023-08-31 03:03:38,499 - [*] phase 0 Training start
train 33889
2023-08-31 03:04:57,432 - epoch:0, training loss:0.2098 validation loss:0.2611
train 33889
vs, vt 0.2611356626518748 0.26504589253189886
Updating learning rate to 1.0438721218484657e-05
Updating learning rate to 1.0438721218484657e-05
train 33889
vs, vt 0.2534156257540665 0.2533516910096461
need align? ->  True 0.2533516910096461
2023-08-31 03:10:08,961 - epoch:1, training loss:5.7807 validation loss:0.2534
Updating learning rate to 2.802750441854847e-05
Updating learning rate to 2.802750441854847e-05
train 33889
vs, vt 0.25452636885033414 0.25182016659528017
need align? ->  True 0.25182016659528017
2023-08-31 03:14:38,823 - epoch:2, training loss:2.3517 validation loss:0.2545
Updating learning rate to 5.2047629950292354e-05
Updating learning rate to 5.2047629950292354e-05
train 33889
vs, vt 0.2512584132650359 0.2520659087141129
need align? ->  False 0.25182016659528017
2023-08-31 03:19:05,389 - epoch:3, training loss:1.7067 validation loss:0.2513
Updating learning rate to 7.605497731655361e-05
Updating learning rate to 7.605497731655361e-05
train 33889
vs, vt 0.2569060246036811 0.25230683775788004
need align? ->  True 0.25182016659528017
2023-08-31 03:23:19,136 - epoch:4, training loss:1.5019 validation loss:0.2569
Updating learning rate to 9.3608854147054e-05
Updating learning rate to 9.3608854147054e-05
train 33889
vs, vt 0.25193825508044526 0.2520200386220081
need align? ->  True 0.25182016659528017
2023-08-31 03:27:43,872 - epoch:5, training loss:1.3443 validation loss:0.2519
Updating learning rate to 9.99999938537861e-05
Updating learning rate to 9.99999938537861e-05
train 33889
vs, vt 0.25115187415345147 0.2506780738688328
need align? ->  True 0.2506780738688328
2023-08-31 03:32:12,639 - epoch:6, training loss:1.2259 validation loss:0.2512
Updating learning rate to 9.956900274488073e-05
Updating learning rate to 9.956900274488073e-05
train 33889
vs, vt 0.2548907079564577 0.25253048598427663
need align? ->  True 0.2506780738688328
2023-08-31 03:36:23,922 - epoch:7, training loss:1.1214 validation loss:0.2549
Updating learning rate to 9.828987567794199e-05
Updating learning rate to 9.828987567794199e-05
train 33889
vs, vt 0.24999013996090402 0.2514918750067326
need align? ->  False 0.2506780738688328
2023-08-31 03:40:45,106 - epoch:8, training loss:0.9919 validation loss:0.2500
Updating learning rate to 9.618449887172618e-05
Updating learning rate to 9.618449887172618e-05
train 33889
vs, vt 0.25120734944092954 0.2520559713329104
need align? ->  True 0.2506780738688328
2023-08-31 03:45:18,753 - epoch:9, training loss:0.9384 validation loss:0.2512
Updating learning rate to 9.328889590710838e-05
Updating learning rate to 9.328889590710838e-05
train 33889
vs, vt 0.24837546444244005 0.25249872665682976
need align? ->  False 0.2506780738688328
2023-08-31 03:49:28,741 - epoch:10, training loss:0.9033 validation loss:0.2484
Updating learning rate to 8.965261135362604e-05
Updating learning rate to 8.965261135362604e-05
train 33889
vs, vt 0.24933099454607477 0.25252143932845106
need align? ->  False 0.2506780738688328
2023-08-31 03:53:50,754 - epoch:11, training loss:0.8768 validation loss:0.2493
Updating learning rate to 8.533786304815776e-05
Updating learning rate to 8.533786304815776e-05
train 33889
vs, vt 0.24835077753629198 0.25000809332017193
need align? ->  False 0.25000809332017193
2023-08-31 03:58:27,908 - epoch:12, training loss:0.8548 validation loss:0.2484
Updating learning rate to 8.041847753048434e-05
Updating learning rate to 8.041847753048434e-05
train 33889
vs, vt 0.24859371849081732 0.2520119132982059
need align? ->  False 0.25000809332017193
2023-08-31 04:02:44,215 - epoch:13, training loss:0.9857 validation loss:0.2486
Updating learning rate to 7.497862685072454e-05
Updating learning rate to 7.497862685072454e-05
train 33889
vs, vt 0.2481970181265338 0.24985309724103322
need align? ->  False 0.24985309724103322
2023-08-31 04:07:38,366 - epoch:14, training loss:0.9069 validation loss:0.2482
Updating learning rate to 6.911138836222055e-05
Updating learning rate to 6.911138836222055e-05
train 33889
vs, vt 0.24766673880036583 0.25098621286451817
need align? ->  False 0.24985309724103322
2023-08-31 04:12:34,882 - epoch:15, training loss:0.9223 validation loss:0.2477
Updating learning rate to 6.291715214221653e-05
Updating learning rate to 6.291715214221653e-05
train 33889
vs, vt 0.2481298861940476 0.2501168533397669
need align? ->  False 0.24985309724103322
2023-08-31 04:16:40,357 - epoch:16, training loss:0.8952 validation loss:0.2481
Updating learning rate to 5.6501903289803477e-05
Updating learning rate to 5.6501903289803477e-05
train 33889
vs, vt 0.248209552670067 0.24985223763029685
need align? ->  False 0.24985223763029685
2023-08-31 04:19:48,016 - epoch:17, training loss:0.8787 validation loss:0.2482
Updating learning rate to 4.997540849148917e-05
Updating learning rate to 4.997540849148917e-05
train 33889
vs, vt 0.2514052593403242 0.25024473980407824
need align? ->  True 0.24985223763029685
2023-08-31 04:22:51,838 - epoch:18, training loss:0.9121 validation loss:0.2514
Updating learning rate to 4.344933788275899e-05
Updating learning rate to 4.344933788275899e-05
train 33889
vs, vt 0.24707429627464575 0.2523619173602624
need align? ->  False 0.24985223763029685
2023-08-31 04:25:55,027 - epoch:19, training loss:0.8923 validation loss:0.2471
Updating learning rate to 3.703535434109692e-05
Updating learning rate to 3.703535434109692e-05
train 33889
vs, vt 0.2477646609344943 0.2505713658068668
need align? ->  False 0.24985223763029685
2023-08-31 04:28:59,374 - epoch:20, training loss:0.8860 validation loss:0.2478
Updating learning rate to 3.084320290319299e-05
Updating learning rate to 3.084320290319299e-05
train 33889
vs, vt 0.24818441267548638 0.250754494601014
need align? ->  False 0.24985223763029685
2023-08-31 04:32:03,029 - epoch:21, training loss:0.8797 validation loss:0.2482
Updating learning rate to 2.4978832996938436e-05
Updating learning rate to 2.4978832996938436e-05
train 33889
vs, vt 0.2470261315730485 0.2502223511057144
need align? ->  False 0.24985223763029685
2023-08-31 04:35:06,679 - epoch:22, training loss:0.8738 validation loss:0.2470
Updating learning rate to 1.954258561733979e-05
Updating learning rate to 1.954258561733979e-05
train 33889
vs, vt 0.2470879714343358 0.24941466181454333
need align? ->  False 0.24941466181454333
2023-08-31 04:38:10,162 - epoch:23, training loss:0.8685 validation loss:0.2471
Updating learning rate to 1.4627476464274535e-05
Updating learning rate to 1.4627476464274535e-05
train 33889
vs, vt 0.2467733257534829 0.24936867873607713
need align? ->  False 0.24936867873607713
2023-08-31 04:41:13,653 - epoch:24, training loss:0.9219 validation loss:0.2468
Updating learning rate to 1.0317604418077296e-05
Updating learning rate to 1.0317604418077296e-05
train 33889
vs, vt 0.24711431736465206 0.24940651782195677
need align? ->  False 0.24936867873607713
2023-08-31 04:44:17,287 - epoch:25, training loss:0.9183 validation loss:0.2471
Updating learning rate to 6.686712584380782e-06
Updating learning rate to 6.686712584380782e-06
train 33889
vs, vt 0.24682155869562517 0.2492247783070938
need align? ->  False 0.2492247783070938
2023-08-31 04:47:21,166 - epoch:26, training loss:0.9144 validation loss:0.2468
Updating learning rate to 3.7969265291329562e-06
Updating learning rate to 3.7969265291329562e-06
train 33889
vs, vt 0.24705098679458554 0.2493658826940439
need align? ->  False 0.2492247783070938
2023-08-31 04:50:24,233 - epoch:27, training loss:0.9214 validation loss:0.2471
Updating learning rate to 1.6976912929391648e-06
Updating learning rate to 1.6976912929391648e-06
train 33889
vs, vt 0.24701198952441866 0.249247348528694
need align? ->  False 0.2492247783070938
2023-08-31 04:53:27,247 - epoch:28, training loss:0.9197 validation loss:0.2470
Updating learning rate to 4.2492537270863806e-07
Updating learning rate to 4.2492537270863806e-07
train 33889
vs, vt 0.24707518932832914 0.2492765601809052
need align? ->  False 0.2492247783070938
2023-08-31 04:56:31,367 - epoch:29, training loss:0.9201 validation loss:0.2471
Updating learning rate to 4.0614621390542476e-10
Updating learning rate to 4.0614621390542476e-10
check exp/ECL-PatchTST2023-08-31-03:03:36.748353/0/0.2468_epoch_24.pkl  &  0.2492247783070938
2023-08-31 04:56:46,721 - [*] loss:0.3705
2023-08-31 04:56:46,764 - [*] phase 0, testing
2023-08-31 04:56:47,339 - T:336	MAE	0.388665	RMSE	0.370324	MAPE	235.863638
2023-08-31 04:56:47,339 - 336	mae	0.3887	
2023-08-31 04:56:47,339 - 336	rmse	0.3703	
2023-08-31 04:56:47,339 - 336	mape	235.8636	
----*-----
2023-08-31 04:57:02,761 - [*] loss:0.3705
2023-08-31 04:57:02,802 - [*] phase 0, testing
2023-08-31 04:57:03,377 - T:336	MAE	0.388665	RMSE	0.370324	MAPE	235.863638
2023-08-31 04:57:18,955 - [*] loss:0.3843
2023-08-31 04:57:18,995 - [*] phase 0, testing
2023-08-31 04:57:19,583 - T:336	MAE	0.403885	RMSE	0.384101	MAPE	254.101372
2023-08-31 04:57:34,819 - [*] loss:0.4897
2023-08-31 04:57:34,862 - [*] phase 0, testing
2023-08-31 04:57:35,492 - T:336	MAE	0.464515	RMSE	0.489876	MAPE	306.241679
2023-08-31 04:57:50,951 - [*] loss:0.3770
2023-08-31 04:57:50,992 - [*] phase 0, testing
2023-08-31 04:57:51,568 - T:336	MAE	0.398597	RMSE	0.376639	MAPE	251.630998
2023-08-31 04:58:09,930 - [*] loss:0.4604
2023-08-31 04:58:09,971 - [*] phase 0, testing
2023-08-31 04:58:10,567 - T:336	MAE	0.468381	RMSE	0.460245	MAPE	261.877084
2023-08-31 04:58:26,603 - [*] loss:0.4062
2023-08-31 04:58:26,643 - [*] phase 0, testing
2023-08-31 04:58:27,226 - T:336	MAE	0.426486	RMSE	0.405844	MAPE	215.202308
2023-08-31 04:58:42,672 - [*] loss:0.3745
2023-08-31 04:58:42,712 - [*] phase 0, testing
2023-08-31 04:58:43,284 - T:336	MAE	0.393227	RMSE	0.374356	MAPE	242.242432
2023-08-31 04:58:59,317 - [*] loss:0.3780
2023-08-31 04:58:59,358 - [*] phase 0, testing
2023-08-31 04:58:59,944 - T:336	MAE	0.399626	RMSE	0.377653	MAPE	227.121854
----*-----
2023-08-31 04:59:04,229 - [*] loss:0.3767
2023-08-31 04:59:04,269 - [*] phase 0, testing
2023-08-31 04:59:04,859 - T:336	MAE	0.395998	RMSE	0.376362	MAPE	228.120923
2023-08-31 04:59:20,322 - [*] loss:0.3870
2023-08-31 04:59:20,362 - [*] phase 0, testing
2023-08-31 04:59:20,937 - T:336	MAE	0.399549	RMSE	0.386920	MAPE	226.493454
2023-08-31 04:59:26,649 - [*] loss:0.4057
2023-08-31 04:59:26,691 - [*] phase 0, testing
2023-08-31 04:59:27,298 - T:336	MAE	0.402285	RMSE	0.405757	MAPE	232.172704
2023-08-31 04:59:27,299 - 336	mae	0.4023	
2023-08-31 04:59:27,299 - 336	rmse	0.4058	
2023-08-31 04:59:27,299 - 336	mape	232.1727	
