2023-08-28 18:23:33,162 - logger name:exp/ECL-PatchTST2023-08-28-18:23:33.161944/ECL-PatchTST.log
2023-08-28 18:23:33,162 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'weather', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 720, 'noise_rate': 0.8, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 32, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-18:23:33.161944', 'path': 'exp/ECL-PatchTST2023-08-28-18:23:33.161944', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 18:23:33,162 - [*] phase 0 start training
0 52696
36887 5270 10539 0.7 0.2 52696
train 35832
36887 5270 10539 0.7 0.2 52696
val 4551
36887 5270 10539 0.7 0.2 52696
test 9820
2023-08-28 18:23:33,965 - [*] phase 0 Dataset load!
2023-08-28 18:23:34,984 - [*] phase 0 Training start
36887 5270 10539 0.7 0.2 52696
train 35832
2023-08-28 18:25:47,997 - epoch:0, training loss:0.8072 validation loss:0.7135
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.7134613677129878 0.6934825658172994
Updating learning rate to 1.0432652690361703e-05
Updating learning rate to 1.0432652690361703e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6816100602174973 0.6848054916917027
need align? ->  False 0.6848054916917027
2023-08-28 18:31:24,490 - epoch:1, training loss:0.6797 validation loss:0.6816
Updating learning rate to 2.800648490166273e-05
Updating learning rate to 2.800648490166273e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6675118009944062 0.6711854236526089
need align? ->  False 0.6711854236526089
2023-08-28 18:36:47,329 - epoch:2, training loss:0.6484 validation loss:0.6675
Updating learning rate to 5.2011231673320136e-05
Updating learning rate to 5.2011231673320136e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6617625182742005 0.6691399531347768
need align? ->  False 0.6691399531347768
2023-08-28 18:40:49,742 - epoch:3, training loss:0.6369 validation loss:0.6618
Updating learning rate to 7.601296805107756e-05
Updating learning rate to 7.601296805107756e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6697490044407077 0.6661176153204658
need align? ->  True 0.6661176153204658
2023-08-28 18:44:51,025 - epoch:4, training loss:0.6302 validation loss:0.6697
Updating learning rate to 9.357857594811273e-05
Updating learning rate to 9.357857594811273e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6603822208993084 0.6677645002003316
need align? ->  False 0.6661176153204658
2023-08-28 18:48:54,481 - epoch:5, training loss:0.6232 validation loss:0.6604
Updating learning rate to 9.999999965789821e-05
Updating learning rate to 9.999999965789821e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6621380247227795 0.6633048705704563
need align? ->  False 0.6633048705704563
2023-08-28 18:52:57,015 - epoch:6, training loss:0.6149 validation loss:0.6621
Updating learning rate to 9.957148100130167e-05
Updating learning rate to 9.957148100130167e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6595617685909871 0.6656056843645923
need align? ->  False 0.6633048705704563
2023-08-28 18:57:01,755 - epoch:7, training loss:0.6096 validation loss:0.6596
Updating learning rate to 9.829478398301725e-05
Updating learning rate to 9.829478398301725e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6611199668654195 0.6807495406666002
need align? ->  False 0.6633048705704563
2023-08-28 19:01:05,781 - epoch:8, training loss:0.6050 validation loss:0.6611
Updating learning rate to 9.61917532429951e-05
Updating learning rate to 9.61917532429951e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6581747735088522 0.6808467774124413
need align? ->  False 0.6633048705704563
2023-08-28 19:05:08,605 - epoch:9, training loss:0.6001 validation loss:0.6582
Updating learning rate to 9.329837222026726e-05
Updating learning rate to 9.329837222026726e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6534202450013661 0.6828444188499784
need align? ->  False 0.6633048705704563
2023-08-28 19:12:38,032 - epoch:10, training loss:0.5957 validation loss:0.6534
Updating learning rate to 8.966414746632942e-05
Updating learning rate to 8.966414746632942e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6521071615544233 0.6768148640444228
need align? ->  False 0.6633048705704563
2023-08-28 19:20:52,496 - epoch:11, training loss:0.5917 validation loss:0.6521
Updating learning rate to 8.535126157431886e-05
Updating learning rate to 8.535126157431886e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6609853963543485 0.6816389367922203
need align? ->  False 0.6633048705704563
2023-08-28 19:28:52,558 - epoch:12, training loss:0.5872 validation loss:0.6610
Updating learning rate to 8.043350921760577e-05
Updating learning rate to 8.043350921760577e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6628485681502135 0.6887662482636792
need align? ->  False 0.6633048705704563
2023-08-28 19:36:53,856 - epoch:13, training loss:0.5838 validation loss:0.6628
Updating learning rate to 7.499503450247206e-05
Updating learning rate to 7.499503450247206e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6674633858712403 0.6829719491355069
need align? ->  True 0.6633048705704563
2023-08-28 19:45:10,613 - epoch:14, training loss:0.5803 validation loss:0.6675
Updating learning rate to 6.912889123912373e-05
Updating learning rate to 6.912889123912373e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6664386875145919 0.684162789604047
need align? ->  True 0.6633048705704563
2023-08-28 19:53:08,507 - epoch:15, training loss:0.5768 validation loss:0.6664
Updating learning rate to 6.293545076519881e-05
Updating learning rate to 6.293545076519881e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6655856605384733 0.6869813437228436
need align? ->  True 0.6633048705704563
2023-08-28 20:01:14,677 - epoch:16, training loss:0.5741 validation loss:0.6656
Updating learning rate to 5.652068456435232e-05
Updating learning rate to 5.652068456435232e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6776865043423392 0.6918923312967474
need align? ->  True 0.6633048705704563
2023-08-28 20:09:31,767 - epoch:17, training loss:0.5709 validation loss:0.6777
Updating learning rate to 4.99943510647899e-05
Updating learning rate to 4.99943510647899e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6716227821120015 0.6949470595463173
need align? ->  True 0.6633048705704563
2023-08-28 20:17:18,144 - epoch:18, training loss:0.5691 validation loss:0.6716
Updating learning rate to 4.346811764213053e-05
Updating learning rate to 4.346811764213053e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6707252018101566 0.6896567818793383
need align? ->  True 0.6633048705704563
2023-08-28 20:25:33,864 - epoch:19, training loss:0.5684 validation loss:0.6707
Updating learning rate to 3.705364995964971e-05
Updating learning rate to 3.705364995964971e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6728528381644429 0.6915125122645518
need align? ->  True 0.6633048705704563
2023-08-28 20:33:56,805 - epoch:20, training loss:0.5650 validation loss:0.6729
Updating learning rate to 3.0860701337821077e-05
Updating learning rate to 3.0860701337821077e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6755619100132189 0.6966794485395605
need align? ->  True 0.6633048705704563
2023-08-28 20:41:48,688 - epoch:21, training loss:0.5636 validation loss:0.6756
Updating learning rate to 2.4995234844573852e-05
Updating learning rate to 2.4995234844573852e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6728514377679025 0.6953041724391751
need align? ->  True 0.6633048705704563
2023-08-28 20:50:03,807 - epoch:22, training loss:0.5624 validation loss:0.6729
Updating learning rate to 1.9557610237822065e-05
Updating learning rate to 1.9557610237822065e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6768388263620697 0.6999544980642679
need align? ->  True 0.6633048705704563
2023-08-28 20:58:27,434 - epoch:23, training loss:0.5609 validation loss:0.6768
Updating learning rate to 1.4640866782181578e-05
Updating learning rate to 1.4640866782181578e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6723206925850648 0.697290633525048
need align? ->  True 0.6633048705704563
2023-08-28 21:06:18,852 - epoch:24, training loss:0.5601 validation loss:0.6723
Updating learning rate to 1.0329131321357218e-05
Updating learning rate to 1.0329131321357218e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6706047696875526 0.6962810160813632
need align? ->  True 0.6633048705704563
2023-08-28 21:14:35,584 - epoch:25, training loss:0.5594 validation loss:0.6706
Updating learning rate to 6.696178844522622e-06
Updating learning rate to 6.696178844522622e-06
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6736021131485492 0.6973570606091639
need align? ->  True 0.6633048705704563
2023-08-28 21:22:46,677 - epoch:26, training loss:0.5585 validation loss:0.6736
Updating learning rate to 3.8041701758011355e-06
Updating learning rate to 3.8041701758011355e-06
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.674323780865936 0.6995838692971876
need align? ->  True 0.6633048705704563
2023-08-28 21:30:45,616 - epoch:27, training loss:0.5583 validation loss:0.6743
Updating learning rate to 1.7025883853308567e-06
Updating learning rate to 1.7025883853308567e-06
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.6752715402549797 0.6998708707677734
need align? ->  True 0.6633048705704563
2023-08-28 21:39:12,517 - epoch:28, training loss:0.5580 validation loss:0.6753
Updating learning rate to 4.273921202153066e-07
Updating learning rate to 4.273921202153066e-07
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.67353188470527 0.699035808756635
need align? ->  True 0.6633048705704563
2023-08-28 21:47:52,014 - epoch:29, training loss:0.5580 validation loss:0.6735
Updating learning rate to 4.0034210179960347e-10
Updating learning rate to 4.0034210179960347e-10
check exp/ECL-PatchTST2023-08-28-18:23:33.161944/0/0.6521_epoch_11.pkl  &  0.6633048705704563
2023-08-28 21:48:48,974 - [*] loss:0.3238
2023-08-28 21:48:49,200 - [*] phase 0, testing
2023-08-28 21:49:05,737 - T:720	MAE	0.334027	RMSE	0.323738	MAPE	1491.044044
2023-08-28 21:49:05,739 - 720	mae	0.3340	
2023-08-28 21:49:05,739 - 720	rmse	0.3237	
2023-08-28 21:49:05,739 - 720	mape	1491.0440	
2023-08-28 21:50:04,441 - [*] loss:0.3238
2023-08-28 21:50:04,693 - [*] phase 0, testing
2023-08-28 21:50:19,182 - T:720	MAE	0.334027	RMSE	0.323738	MAPE	1491.044044
2023-08-28 21:51:17,495 - [*] loss:0.3456
2023-08-28 21:51:17,681 - [*] phase 0, testing
2023-08-28 21:51:21,405 - T:720	MAE	0.357495	RMSE	0.345549	MAPE	1498.527336
2023-08-28 21:52:11,440 - [*] loss:0.3222
2023-08-28 21:52:11,628 - [*] phase 0, testing
2023-08-28 21:52:18,421 - T:720	MAE	0.329809	RMSE	0.322150	MAPE	1424.500656
2023-08-28 21:52:18,423 - 720	mae	0.3298	
2023-08-28 21:52:18,424 - 720	rmse	0.3221	
2023-08-28 21:52:18,424 - 720	mape	1424.5007	
2023-08-28 21:52:21,043 - logger name:exp/ECL-PatchTST2023-08-28-21:52:21.036414/ECL-PatchTST.log
2023-08-28 21:52:21,043 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'weather', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 720, 'noise_rate': 0.8, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 32, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-28-21:52:21.036414', 'path': 'exp/ECL-PatchTST2023-08-28-21:52:21.036414', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-28 21:52:21,043 - [*] phase 0 start training
0 52696
36887 5270 10539 0.7 0.2 52696
train 35832
36887 5270 10539 0.7 0.2 52696
val 4551
36887 5270 10539 0.7 0.2 52696
test 9820
2023-08-28 21:52:22,003 - [*] phase 0 Dataset load!
2023-08-28 21:52:23,082 - [*] phase 0 Training start
36887 5270 10539 0.7 0.2 52696
train 35832
2023-08-28 21:58:49,645 - epoch:0, training loss:0.2292 validation loss:0.2123
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.21225117350166495 0.21355234122984892
Updating learning rate to 1.0432652690361703e-05
Updating learning rate to 1.0432652690361703e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.2112864900406424 0.21206357111255605
need align? ->  False 0.21206357111255605
2023-08-28 22:17:54,266 - epoch:1, training loss:2.0636 validation loss:0.2113
Updating learning rate to 2.800648490166273e-05
Updating learning rate to 2.800648490166273e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20878003329247027 0.2092388759959828
need align? ->  False 0.2092388759959828
2023-08-28 22:34:04,225 - epoch:2, training loss:1.2395 validation loss:0.2088
Updating learning rate to 5.2011231673320136e-05
Updating learning rate to 5.2011231673320136e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20687472992218459 0.2091108073408787
need align? ->  False 0.2091108073408787
2023-08-28 22:50:26,459 - epoch:3, training loss:1.0101 validation loss:0.2069
Updating learning rate to 7.601296805107756e-05
Updating learning rate to 7.601296805107756e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20787969116981214 0.20814222899767068
need align? ->  False 0.20814222899767068
2023-08-28 23:07:08,384 - epoch:4, training loss:0.8998 validation loss:0.2079
Updating learning rate to 9.357857594811273e-05
Updating learning rate to 9.357857594811273e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20479273045813287 0.20665509680976402
need align? ->  False 0.20665509680976402
2023-08-28 23:23:14,951 - epoch:5, training loss:0.8743 validation loss:0.2048
Updating learning rate to 9.999999965789821e-05
Updating learning rate to 9.999999965789821e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20549025537667576 0.20455434157714977
need align? ->  True 0.20455434157714977
2023-08-28 23:39:17,730 - epoch:6, training loss:0.8628 validation loss:0.2055
Updating learning rate to 9.957148100130167e-05
Updating learning rate to 9.957148100130167e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20507445665714624 0.20352853225036102
need align? ->  True 0.20352853225036102
2023-08-28 23:55:38,088 - epoch:7, training loss:0.9186 validation loss:0.2051
Updating learning rate to 9.829478398301725e-05
Updating learning rate to 9.829478398301725e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.2035380780592665 0.20643591557766175
need align? ->  True 0.20352853225036102
2023-08-29 00:11:59,077 - epoch:8, training loss:0.9823 validation loss:0.2035
Updating learning rate to 9.61917532429951e-05
Updating learning rate to 9.61917532429951e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20363319144173936 0.2043956955308681
need align? ->  True 0.20352853225036102
2023-08-29 00:28:08,679 - epoch:9, training loss:0.9365 validation loss:0.2036
Updating learning rate to 9.329837222026726e-05
Updating learning rate to 9.329837222026726e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20242041797487886 0.20490706586962812
need align? ->  False 0.20352853225036102
2023-08-29 00:44:05,530 - epoch:10, training loss:0.9141 validation loss:0.2024
Updating learning rate to 8.966414746632942e-05
Updating learning rate to 8.966414746632942e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20146407780947384 0.20387986948440126
need align? ->  False 0.20352853225036102
2023-08-29 01:00:18,234 - epoch:11, training loss:0.8968 validation loss:0.2015
Updating learning rate to 8.535126157431886e-05
Updating learning rate to 8.535126157431886e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.2030162128743592 0.20416015881550062
need align? ->  False 0.20352853225036102
2023-08-29 01:16:58,366 - epoch:12, training loss:0.8865 validation loss:0.2030
Updating learning rate to 8.043350921760577e-05
Updating learning rate to 8.043350921760577e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20320609290074634 0.20737328317823944
need align? ->  False 0.20352853225036102
2023-08-29 01:33:27,190 - epoch:13, training loss:0.8764 validation loss:0.2032
Updating learning rate to 7.499503450247206e-05
Updating learning rate to 7.499503450247206e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.203314210099357 0.20624858041951707
need align? ->  False 0.20352853225036102
2023-08-29 01:49:30,075 - epoch:14, training loss:0.8689 validation loss:0.2033
Updating learning rate to 6.912889123912373e-05
Updating learning rate to 6.912889123912373e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20195646297473174 0.20821121595539413
need align? ->  False 0.20352853225036102
2023-08-29 02:05:26,469 - epoch:15, training loss:0.8611 validation loss:0.2020
Updating learning rate to 6.293545076519881e-05
Updating learning rate to 6.293545076519881e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20236103288777224 0.20890154533244512
need align? ->  False 0.20352853225036102
2023-08-29 02:21:39,208 - epoch:16, training loss:0.8559 validation loss:0.2024
Updating learning rate to 5.652068456435232e-05
Updating learning rate to 5.652068456435232e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20316771657525243 0.20694863462781574
need align? ->  False 0.20352853225036102
2023-08-29 02:37:50,798 - epoch:17, training loss:0.8505 validation loss:0.2032
Updating learning rate to 4.99943510647899e-05
Updating learning rate to 4.99943510647899e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20161650012006294 0.20758869238458313
need align? ->  False 0.20352853225036102
2023-08-29 02:53:41,960 - epoch:18, training loss:0.8465 validation loss:0.2016
Updating learning rate to 4.346811764213053e-05
Updating learning rate to 4.346811764213053e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20249871940879555 0.20622012857999
need align? ->  False 0.20352853225036102
2023-08-29 03:09:00,792 - epoch:19, training loss:0.8438 validation loss:0.2025
Updating learning rate to 3.705364995964971e-05
Updating learning rate to 3.705364995964971e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20363941236392602 0.20811796099781157
need align? ->  True 0.20352853225036102
2023-08-29 03:25:07,487 - epoch:20, training loss:0.8401 validation loss:0.2036
Updating learning rate to 3.0860701337821077e-05
Updating learning rate to 3.0860701337821077e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.2045158529198253 0.20848071497011852
need align? ->  True 0.20352853225036102
2023-08-29 03:40:48,679 - epoch:21, training loss:0.8371 validation loss:0.2045
Updating learning rate to 2.4995234844573852e-05
Updating learning rate to 2.4995234844573852e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20219904783513995 0.20840096327808352
need align? ->  False 0.20352853225036102
2023-08-29 03:56:19,321 - epoch:22, training loss:0.8337 validation loss:0.2022
Updating learning rate to 1.9557610237822065e-05
Updating learning rate to 1.9557610237822065e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20307959736018746 0.20810169502571746
need align? ->  False 0.20352853225036102
2023-08-29 04:11:43,644 - epoch:23, training loss:0.8317 validation loss:0.2031
Updating learning rate to 1.4640866782181578e-05
Updating learning rate to 1.4640866782181578e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20309770425091256 0.20756985393020658
need align? ->  False 0.20352853225036102
2023-08-29 04:27:22,420 - epoch:24, training loss:0.8307 validation loss:0.2031
Updating learning rate to 1.0329131321357218e-05
Updating learning rate to 1.0329131321357218e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20379946089082665 0.20788168094374918
need align? ->  True 0.20352853225036102
2023-08-29 04:43:05,946 - epoch:25, training loss:0.8292 validation loss:0.2038
Updating learning rate to 6.696178844522622e-06
Updating learning rate to 6.696178844522622e-06
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20389919754091676 0.20803529920277897
need align? ->  True 0.20352853225036102
2023-08-29 04:58:46,407 - epoch:26, training loss:0.8286 validation loss:0.2039
Updating learning rate to 3.8041701758011355e-06
Updating learning rate to 3.8041701758011355e-06
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20369995182210748 0.20835896300060766
need align? ->  True 0.20352853225036102
2023-08-29 05:15:00,822 - epoch:27, training loss:0.8277 validation loss:0.2037
Updating learning rate to 1.7025883853308567e-06
Updating learning rate to 1.7025883853308567e-06
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20365701600179806 0.20803557930292782
need align? ->  True 0.20352853225036102
2023-08-29 05:31:00,144 - epoch:28, training loss:0.8274 validation loss:0.2037
Updating learning rate to 4.273921202153066e-07
Updating learning rate to 4.273921202153066e-07
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.2037706836224436 0.2080522482315977
need align? ->  True 0.20352853225036102
2023-08-29 05:46:42,092 - epoch:29, training loss:0.8270 validation loss:0.2038
Updating learning rate to 4.0034210179960347e-10
Updating learning rate to 4.0034210179960347e-10
check exp/ECL-PatchTST2023-08-28-21:52:21.036414/0/0.2015_epoch_11.pkl  &  0.20352853225036102
2023-08-29 05:48:18,047 - [*] loss:0.3210
2023-08-29 05:48:19,489 - [*] phase 0, testing
2023-08-29 05:48:31,431 - T:720	MAE	0.329212	RMSE	0.320936	MAPE	1468.759918
2023-08-29 05:48:31,486 - 720	mae	0.3292	
2023-08-29 05:48:31,487 - 720	rmse	0.3209	
2023-08-29 05:48:31,487 - 720	mape	1468.7599	
2023-08-29 05:49:32,612 - [*] loss:0.3201
2023-08-29 05:49:32,812 - [*] phase 0, testing
2023-08-29 05:49:56,759 - T:720	MAE	0.329015	RMSE	0.320036	MAPE	1447.248840
2023-08-29 05:51:37,280 - [*] loss:0.3231
2023-08-29 05:51:37,464 - [*] phase 0, testing
2023-08-29 05:51:52,244 - T:720	MAE	0.330082	RMSE	0.322990	MAPE	1519.211197
2023-08-29 05:52:40,471 - [*] loss:0.3233
2023-08-29 05:52:40,657 - [*] phase 0, testing
2023-08-29 05:52:51,973 - T:720	MAE	0.331011	RMSE	0.323235	MAPE	1554.806042
2023-08-29 05:52:51,975 - 720	mae	0.3310	
2023-08-29 05:52:51,976 - 720	rmse	0.3232	
2023-08-29 05:52:51,976 - 720	mape	1554.8060	
2023-08-29 05:52:54,251 - logger name:exp/ECL-PatchTST2023-08-29-05:52:54.251501/ECL-PatchTST.log
2023-08-29 05:52:54,252 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'weather', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 96, 'noise_rate': 0.8, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 32, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-29-05:52:54.251501', 'path': 'exp/ECL-PatchTST2023-08-29-05:52:54.251501', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-29 05:52:54,252 - [*] phase 0 start training
0 52696
36887 5270 10539 0.7 0.2 52696
train 36456
36887 5270 10539 0.7 0.2 52696
val 5175
36887 5270 10539 0.7 0.2 52696
test 10444
2023-08-29 05:52:55,055 - [*] phase 0 Dataset load!
2023-08-29 05:52:56,105 - [*] phase 0 Training start
36887 5270 10539 0.7 0.2 52696
train 36456
2023-08-29 05:58:05,380 - epoch:0, training loss:0.6506 validation loss:0.4700
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.4699552993255633 0.45695046505626336
Updating learning rate to 1.0432619811165389e-05
Updating learning rate to 1.0432619811165389e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.4263731064159929 0.4359560674171389
need align? ->  False 0.4359560674171389
2023-08-29 06:07:42,094 - epoch:1, training loss:0.5005 validation loss:0.4264
Updating learning rate to 2.800637100986999e-05
Updating learning rate to 2.800637100986999e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.40568838891699727 0.42439302592825745
need align? ->  False 0.42439302592825745
2023-08-29 06:15:10,904 - epoch:2, training loss:0.4568 validation loss:0.4057
Updating learning rate to 5.2011034424560533e-05
Updating learning rate to 5.2011034424560533e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3976523253928732 0.41261626016578556
need align? ->  False 0.41261626016578556
2023-08-29 06:22:24,821 - epoch:3, training loss:0.4447 validation loss:0.3977
Updating learning rate to 7.60127403284997e-05
Updating learning rate to 7.60127403284997e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.39714883481738744 0.40849857391030703
need align? ->  False 0.40849857391030703
2023-08-29 06:29:53,363 - epoch:4, training loss:0.4363 validation loss:0.3971
Updating learning rate to 9.357841168421065e-05
Updating learning rate to 9.357841168421065e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.399538825277561 0.4192842937959933
need align? ->  False 0.40849857391030703
2023-08-29 06:37:08,696 - epoch:5, training loss:0.4287 validation loss:0.3995
Updating learning rate to 9.999999966980683e-05
Updating learning rate to 9.999999966980683e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.39687473363714454 0.5353124551015136
need align? ->  False 0.40849857391030703
2023-08-29 06:44:33,520 - epoch:6, training loss:0.4235 validation loss:0.3969
Updating learning rate to 9.95714944185384e-05
Updating learning rate to 9.95714944185384e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3925471683435234 0.5241135976242798
need align? ->  False 0.40849857391030703
2023-08-29 06:51:54,350 - epoch:7, training loss:0.4167 validation loss:0.3925
Updating learning rate to 9.829481057600946e-05
Updating learning rate to 9.829481057600946e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.38961010299806975 0.8220130204234595
need align? ->  False 0.40849857391030703
2023-08-29 06:59:13,488 - epoch:8, training loss:0.4145 validation loss:0.3896
Updating learning rate to 9.619179255672933e-05
Updating learning rate to 9.619179255672933e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.4043127432879474 0.7391069436468828
need align? ->  False 0.40849857391030703
2023-08-29 07:06:39,723 - epoch:9, training loss:0.4128 validation loss:0.4043
Updating learning rate to 9.329842358207461e-05
Updating learning rate to 9.329842358207461e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3920752209507757 0.6917869551682178
need align? ->  False 0.40849857391030703
2023-08-29 07:13:52,397 - epoch:10, training loss:0.4091 validation loss:0.3921
Updating learning rate to 8.966420999739508e-05
Updating learning rate to 8.966420999739508e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.39411509115202925 0.6429079455082063
need align? ->  False 0.40849857391030703
2023-08-29 07:21:20,793 - epoch:11, training loss:0.4066 validation loss:0.3941
Updating learning rate to 8.535133420471901e-05
Updating learning rate to 8.535133420471901e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.39495031176893797 0.5734987861626311
need align? ->  False 0.40849857391030703
2023-08-29 07:28:30,020 - epoch:12, training loss:0.4039 validation loss:0.3950
Updating learning rate to 8.043359070461411e-05
Updating learning rate to 8.043359070461411e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.39666620248721707 0.5318093923966826
need align? ->  False 0.40849857391030703
2023-08-29 07:36:03,314 - epoch:13, training loss:0.4017 validation loss:0.3967
Updating learning rate to 7.499512345182329e-05
Updating learning rate to 7.499512345182329e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3927037927241605 0.5540482329871551
need align? ->  False 0.40849857391030703
2023-08-29 07:44:03,352 - epoch:14, training loss:0.3993 validation loss:0.3927
Updating learning rate to 6.912898612886978e-05
Updating learning rate to 6.912898612886978e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3989515806099883 0.5411206597209345
need align? ->  False 0.40849857391030703
2023-08-29 07:51:27,657 - epoch:15, training loss:0.3977 validation loss:0.3990
Updating learning rate to 6.293554997174984e-05
Updating learning rate to 6.293554997174984e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.398779835322021 0.6273645238467941
need align? ->  False 0.40849857391030703
2023-08-29 07:58:45,322 - epoch:16, training loss:0.3957 validation loss:0.3988
Updating learning rate to 5.652078639025672e-05
Updating learning rate to 5.652078639025672e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.4033121474455168 0.568793982313371
need align? ->  False 0.40849857391030703
2023-08-29 08:06:12,062 - epoch:17, training loss:0.3943 validation loss:0.4033
Updating learning rate to 4.999445376777819e-05
Updating learning rate to 4.999445376777819e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.39907770617692556 0.6539299674562098
need align? ->  False 0.40849857391030703
2023-08-29 08:13:26,239 - epoch:18, training loss:0.3924 validation loss:0.3991
Updating learning rate to 4.3468219464926156e-05
Updating learning rate to 4.3468219464926156e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.4028328865490578 0.5286078642547867
need align? ->  False 0.40849857391030703
2023-08-29 08:20:53,690 - epoch:19, training loss:0.3913 validation loss:0.4028
Updating learning rate to 3.7053749160036374e-05
Updating learning rate to 3.7053749160036374e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3990612351738008 0.5987531044922861
need align? ->  False 0.40849857391030703
2023-08-29 08:28:07,774 - epoch:20, training loss:0.3898 validation loss:0.3991
Updating learning rate to 3.0860796218452653e-05
Updating learning rate to 3.0860796218452653e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3985250163363454 0.5276896716957843
need align? ->  False 0.40849857391030703
2023-08-29 08:35:36,117 - epoch:21, training loss:0.3890 validation loss:0.3985
Updating learning rate to 2.499532378201647e-05
Updating learning rate to 2.499532378201647e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3982274832640901 0.5214848449154769
need align? ->  False 0.40849857391030703
2023-08-29 08:42:45,605 - epoch:22, training loss:0.3879 validation loss:0.3982
Updating learning rate to 1.9557691710331397e-05
Updating learning rate to 1.9557691710331397e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.4014467305653257 0.4961389338160739
need align? ->  False 0.40849857391030703
2023-08-29 08:50:13,979 - epoch:23, training loss:0.3869 validation loss:0.4014
Updating learning rate to 1.4640939395740385e-05
Updating learning rate to 1.4640939395740385e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.40227078156614743 0.47555443952664916
need align? ->  False 0.40849857391030703
2023-08-29 08:57:22,569 - epoch:24, training loss:0.3865 validation loss:0.4023
Updating learning rate to 1.0329193833527397e-05
Updating learning rate to 1.0329193833527397e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.40062556772228375 0.48542232707970673
need align? ->  False 0.40849857391030703
2023-08-29 09:04:51,936 - epoch:25, training loss:0.3859 validation loss:0.4006
Updating learning rate to 6.696230185703604e-06
Updating learning rate to 6.696230185703604e-06
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.400676565658715 0.4827482083145483
need align? ->  False 0.40849857391030703
2023-08-29 09:11:57,816 - epoch:26, training loss:0.3858 validation loss:0.4007
Updating learning rate to 3.804209467531073e-06
Updating learning rate to 3.804209467531073e-06
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.40107119317959855 0.47598738922381106
need align? ->  False 0.40849857391030703
2023-08-29 09:19:26,043 - epoch:27, training loss:0.3852 validation loss:0.4011
Updating learning rate to 1.7026149553173745e-06
Updating learning rate to 1.7026149553173745e-06
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.4009791667584652 0.4768797829287288
need align? ->  False 0.40849857391030703
2023-08-29 09:26:29,719 - epoch:28, training loss:0.3851 validation loss:0.4010
Updating learning rate to 4.2740551383855506e-07
Updating learning rate to 4.2740551383855506e-07
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.4012203143886578 0.4842106962553513
need align? ->  False 0.40849857391030703
2023-08-29 09:34:00,476 - epoch:29, training loss:0.3852 validation loss:0.4012
Updating learning rate to 4.003301931676402e-10
Updating learning rate to 4.003301931676402e-10
check exp/ECL-PatchTST2023-08-29-05:52:54.251501/0/0.3896_epoch_8.pkl  &  0.40849857391030703
2023-08-29 09:34:46,621 - [*] loss:0.1505
2023-08-29 09:34:46,652 - [*] phase 0, testing
2023-08-29 09:34:47,113 - T:96	MAE	0.196955	RMSE	0.150669	MAPE	1307.700443
2023-08-29 09:34:47,114 - 96	mae	0.1970	
2023-08-29 09:34:47,114 - 96	rmse	0.1507	
2023-08-29 09:34:47,114 - 96	mape	1307.7004	
2023-08-29 09:35:32,446 - [*] loss:0.1505
2023-08-29 09:35:32,476 - [*] phase 0, testing
2023-08-29 09:35:32,941 - T:96	MAE	0.196955	RMSE	0.150669	MAPE	1307.700443
2023-08-29 09:36:22,239 - [*] loss:0.1679
2023-08-29 09:36:22,276 - [*] phase 0, testing
2023-08-29 09:36:22,760 - T:96	MAE	0.223174	RMSE	0.168177	MAPE	1105.031776
2023-08-29 09:37:24,419 - [*] loss:0.1611
2023-08-29 09:37:24,450 - [*] phase 0, testing
2023-08-29 09:37:24,936 - T:96	MAE	0.205973	RMSE	0.161317	MAPE	1170.329666
2023-08-29 09:37:24,937 - 96	mae	0.2060	
2023-08-29 09:37:24,937 - 96	rmse	0.1613	
2023-08-29 09:37:24,938 - 96	mape	1170.3297	
2023-08-29 09:37:27,215 - logger name:exp/ECL-PatchTST2023-08-29-09:37:27.215093/ECL-PatchTST.log
2023-08-29 09:37:27,216 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'weather', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 96, 'noise_rate': 0.8, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 32, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-29-09:37:27.215093', 'path': 'exp/ECL-PatchTST2023-08-29-09:37:27.215093', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-29 09:37:27,216 - [*] phase 0 start training
0 52696
36887 5270 10539 0.7 0.2 52696
train 36456
36887 5270 10539 0.7 0.2 52696
val 5175
36887 5270 10539 0.7 0.2 52696
test 10444
2023-08-29 09:37:28,220 - [*] phase 0 Dataset load!
2023-08-29 09:37:29,305 - [*] phase 0 Training start
36887 5270 10539 0.7 0.2 52696
train 36456
2023-08-29 09:42:51,098 - epoch:0, training loss:0.1672 validation loss:0.1146
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.11455518721292417 0.11850660002249995
Updating learning rate to 1.0432619811165389e-05
Updating learning rate to 1.0432619811165389e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.11240985778010922 0.11329427693775039
need align? ->  False 0.11329427693775039
2023-08-29 10:00:05,741 - epoch:1, training loss:2.1021 validation loss:0.1124
Updating learning rate to 2.800637100986999e-05
Updating learning rate to 2.800637100986999e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.11202384966115157 0.11265388833657827
need align? ->  False 0.11265388833657827
2023-08-29 10:11:19,748 - epoch:2, training loss:1.1075 validation loss:0.1120
Updating learning rate to 5.2011034424560533e-05
Updating learning rate to 5.2011034424560533e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.11029445635223831 0.10979764344009721
need align? ->  True 0.10979764344009721
2023-08-29 10:20:41,318 - epoch:3, training loss:0.8419 validation loss:0.1103
Updating learning rate to 7.60127403284997e-05
Updating learning rate to 7.60127403284997e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.1071286870074677 0.10835909635334472
need align? ->  False 0.10835909635334472
2023-08-29 10:35:42,175 - epoch:4, training loss:0.7176 validation loss:0.1071
Updating learning rate to 9.357841168421065e-05
Updating learning rate to 9.357841168421065e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.10788642743855346 0.11236273931960265
need align? ->  False 0.10835909635334472
2023-08-29 10:51:28,564 - epoch:5, training loss:0.6837 validation loss:0.1079
Updating learning rate to 9.999999966980683e-05
Updating learning rate to 9.999999966980683e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.1070279246426107 0.11432317048771146
need align? ->  False 0.10835909635334472
2023-08-29 11:01:12,075 - epoch:6, training loss:0.6359 validation loss:0.1070
Updating learning rate to 9.95714944185384e-05
Updating learning rate to 9.95714944185384e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.1062678116845128 0.11808164047514215
need align? ->  False 0.10835909635334472
2023-08-29 11:10:30,522 - epoch:7, training loss:0.6141 validation loss:0.1063
Updating learning rate to 9.829481057600946e-05
Updating learning rate to 9.829481057600946e-05
36887 5270 10539 0.7 0.2 52696
train 36456
