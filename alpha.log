2023-08-05 09:21:41,871 - logger name:exp/ECL-PatchTST2023-08-05-09:21:41.871226/ECL-PatchTST.log
2023-08-05 09:21:41,871 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-09:21:41.871226', 'path': 'exp/ECL-PatchTST2023-08-05-09:21:41.871226', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 09:21:41,871 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 09:21:42,061 - [*] phase 0 Dataset load!
2023-08-05 09:21:42,932 - [*] phase 0 Training start
train 8281
2023-08-05 09:22:05,074 - epoch:0, training loss:0.2000 validation loss:0.1367
train 8281
vs, vt 0.13673778509964113 0.1439287234907565
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.09809132754478765 0.11183329979362695
need align? ->  False 0.11183329979362695
2023-08-05 09:22:50,126 - epoch:1, training loss:0.2288 validation loss:0.0981
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08231674296700436 0.09016589091523834
need align? ->  False 0.09016589091523834
2023-08-05 09:23:21,860 - epoch:2, training loss:0.1813 validation loss:0.0823
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07862483305127724 0.08016122383591921
need align? ->  False 0.08016122383591921
2023-08-05 09:23:54,569 - epoch:3, training loss:0.1547 validation loss:0.0786
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07678687637266905 0.0769107855208542
need align? ->  False 0.0769107855208542
2023-08-05 09:24:26,699 - epoch:4, training loss:0.1383 validation loss:0.0768
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07624680054900439 0.07584473531207313
need align? ->  True 0.07584473531207313
2023-08-05 09:24:59,638 - epoch:5, training loss:0.1292 validation loss:0.0762
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07695104991612227 0.07642956515369208
need align? ->  True 0.07584473531207313
2023-08-05 09:25:31,066 - epoch:6, training loss:0.1234 validation loss:0.0770
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07578716564761556 0.07640314636671025
need align? ->  False 0.07584473531207313
2023-08-05 09:26:03,590 - epoch:7, training loss:0.1214 validation loss:0.0758
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07602984895524771 0.07533294092053952
need align? ->  True 0.07533294092053952
2023-08-05 09:26:37,221 - epoch:8, training loss:0.1199 validation loss:0.0760
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07772207405904065 0.07531486475921195
need align? ->  True 0.07531486475921195
2023-08-05 09:27:11,329 - epoch:9, training loss:0.1162 validation loss:0.0777
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.0764292536870293 0.07536501558902471
need align? ->  True 0.07531486475921195
2023-08-05 09:27:45,427 - epoch:10, training loss:0.1154 validation loss:0.0764
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07606669442485207 0.07551304866438327
need align? ->  True 0.07531486475921195
2023-08-05 09:28:18,603 - epoch:11, training loss:0.1145 validation loss:0.0761
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07579082938963952 0.07492133213774017
need align? ->  True 0.07492133213774017
2023-08-05 09:28:52,353 - epoch:12, training loss:0.1128 validation loss:0.0758
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.0754846390336752 0.07469721349037212
need align? ->  True 0.07469721349037212
2023-08-05 09:29:26,001 - epoch:13, training loss:0.1121 validation loss:0.0755
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07587177403595137 0.07472249549692092
need align? ->  True 0.07469721349037212
2023-08-05 09:29:59,245 - epoch:14, training loss:0.1111 validation loss:0.0759
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07615047668957192 0.07494600578818632
need align? ->  True 0.07469721349037212
2023-08-05 09:30:32,337 - epoch:15, training loss:0.1106 validation loss:0.0762
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07624012158940667 0.07514410797992478
need align? ->  True 0.07469721349037212
2023-08-05 09:31:06,299 - epoch:16, training loss:0.1099 validation loss:0.0762
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07558909285327663 0.07457887891518034
need align? ->  True 0.07457887891518034
2023-08-05 09:31:40,279 - epoch:17, training loss:0.1093 validation loss:0.0756
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07597008776729522 0.07438055263913196
need align? ->  True 0.07438055263913196
2023-08-05 09:32:15,070 - epoch:18, training loss:0.1090 validation loss:0.0760
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07560176734367142 0.0744338408112526
need align? ->  True 0.07438055263913196
2023-08-05 09:32:48,232 - epoch:19, training loss:0.1086 validation loss:0.0756
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07586518059606137 0.07429936390532099
need align? ->  True 0.07429936390532099
2023-08-05 09:33:21,766 - epoch:20, training loss:0.1081 validation loss:0.0759
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.0754679897395165 0.07435699851940507
need align? ->  True 0.07429936390532099
2023-08-05 09:33:56,257 - epoch:21, training loss:0.1079 validation loss:0.0755
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07574714169554088 0.07405797797052757
need align? ->  True 0.07405797797052757
2023-08-05 09:34:30,137 - epoch:22, training loss:0.1074 validation loss:0.0757
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07555657171684763 0.07422994052910287
need align? ->  True 0.07405797797052757
2023-08-05 09:35:04,277 - epoch:23, training loss:0.1073 validation loss:0.0756
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07555129468117071 0.07418533608965251
need align? ->  True 0.07405797797052757
2023-08-05 09:35:38,188 - epoch:24, training loss:0.1072 validation loss:0.0756
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07561661878033825 0.0741114301364059
need align? ->  True 0.07405797797052757
2023-08-05 09:36:12,105 - epoch:25, training loss:0.1070 validation loss:0.0756
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07561255011545576 0.07410518201472967
need align? ->  True 0.07405797797052757
2023-08-05 09:36:45,869 - epoch:26, training loss:0.1070 validation loss:0.0756
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07559316743003286 0.07415994920808336
need align? ->  True 0.07405797797052757
2023-08-05 09:37:19,301 - epoch:27, training loss:0.1067 validation loss:0.0756
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.075559354994608 0.07413678308543951
need align? ->  True 0.07405797797052757
2023-08-05 09:37:52,976 - epoch:28, training loss:0.1065 validation loss:0.0756
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07557231859992379 0.07414436494202717
need align? ->  True 0.07405797797052757
2023-08-05 09:38:27,248 - epoch:29, training loss:0.1067 validation loss:0.0756
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-09:21:41.871226/0/0.0755_epoch_21.pkl  &  0.07405797797052757
2023-08-05 09:38:30,726 - [*] loss:0.1607
2023-08-05 09:38:30,727 - [*] phase 0, testing
2023-08-05 09:38:30,746 - T:24	MAE	0.256063	RMSE	0.162999	MAPE	113.251770
2023-08-05 09:38:30,747 - 24	mae	0.2561	
2023-08-05 09:38:30,747 - 24	rmse	0.1630	
2023-08-05 09:38:30,747 - 24	mape	113.2518	
2023-08-05 09:38:34,578 - [*] loss:0.1577
2023-08-05 09:38:34,579 - [*] phase 0, testing
2023-08-05 09:38:34,588 - T:24	MAE	0.252458	RMSE	0.160017	MAPE	112.830234
2023-08-05 09:38:34,588 - 24	mae	0.2525	
2023-08-05 09:38:34,588 - 24	rmse	0.1600	
2023-08-05 09:38:34,588 - 24	mape	112.8302	
2023-08-05 09:38:37,113 - logger name:exp/ECL-PatchTST2023-08-05-09:38:37.112783/ECL-PatchTST.log
2023-08-05 09:38:37,113 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-09:38:37.112783', 'path': 'exp/ECL-PatchTST2023-08-05-09:38:37.112783', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 09:38:37,113 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 09:38:37,313 - [*] phase 0 Dataset load!
2023-08-05 09:38:38,259 - [*] phase 0 Training start
train 2522
2023-08-05 09:38:54,047 - epoch:0, training loss:0.3305 validation loss:0.1704
train 8281
vs, vt 0.17041075683158377 0.14846370774118797
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 4.581756728986143e-06
train 8281
vs, vt 0.1231363738036674 0.11626972824982974
need align? ->  True 0.11626972824982974
2023-08-05 09:39:38,049 - epoch:1, training loss:0.2673 validation loss:0.1231
Updating learning rate to 1.470112384525715e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.09409640939987224 0.09188767449687356
need align? ->  True 0.09188767449687356
2023-08-05 09:40:10,113 - epoch:2, training loss:0.2080 validation loss:0.0941
Updating learning rate to 3.486574630308983e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.08136553325406883 0.08323706273475419
need align? ->  False 0.08323706273475419
2023-08-05 09:40:42,545 - epoch:3, training loss:0.1675 validation loss:0.0814
Updating learning rate to 5.964493014230274e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07846945038308269 0.07849078682129798
need align? ->  False 0.07849078682129798
2023-08-05 09:41:15,319 - epoch:4, training loss:0.1448 validation loss:0.0785
Updating learning rate to 8.236519737959902e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07680916583732418 0.07685264370039753
need align? ->  False 0.07685264370039753
2023-08-05 09:41:48,343 - epoch:5, training loss:0.1327 validation loss:0.0768
Updating learning rate to 9.690757314613029e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07606946150569813 0.07578095291619716
need align? ->  True 0.07578095291619716
2023-08-05 09:42:20,801 - epoch:6, training loss:0.1261 validation loss:0.0761
Updating learning rate to 9.995817316369045e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07565848594126494 0.07546862311985182
need align? ->  True 0.07546862311985182
2023-08-05 09:42:51,793 - epoch:7, training loss:0.1226 validation loss:0.0757
Updating learning rate to 9.926388506391859e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07657827274954837 0.07511776766699294
need align? ->  True 0.07511776766699294
2023-08-05 09:43:24,310 - epoch:8, training loss:0.1199 validation loss:0.0766
Updating learning rate to 9.772668165422919e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07545787549537161 0.07550697222999904
need align? ->  True 0.07511776766699294
2023-08-05 09:43:55,026 - epoch:9, training loss:0.1180 validation loss:0.0755
Updating learning rate to 9.537286491115533e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.0751479450289322 0.07448133785763512
need align? ->  True 0.07448133785763512
2023-08-05 09:44:28,311 - epoch:10, training loss:0.1169 validation loss:0.0751
Updating learning rate to 9.22427092917723e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07548543503103049 0.07496680343604606
need align? ->  True 0.07448133785763512
2023-08-05 09:45:01,109 - epoch:11, training loss:0.1150 validation loss:0.0755
Updating learning rate to 8.838977262657086e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07600660071424815 0.0745197718396135
need align? ->  True 0.07448133785763512
2023-08-05 09:45:32,627 - epoch:12, training loss:0.1144 validation loss:0.0760
Updating learning rate to 8.387997973012845e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07523739297428857 0.07484642254269641
need align? ->  True 0.07448133785763512
2023-08-05 09:46:03,377 - epoch:13, training loss:0.1134 validation loss:0.0752
Updating learning rate to 7.8790494409254e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07569158263504505 0.07462494585501112
need align? ->  True 0.07448133785763512
2023-08-05 09:46:35,042 - epoch:14, training loss:0.1131 validation loss:0.0757
Updating learning rate to 7.320839916885957e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.075004065166349 0.07459555558212426
need align? ->  True 0.07448133785763512
2023-08-05 09:47:07,249 - epoch:15, training loss:0.1117 validation loss:0.0750
Updating learning rate to 6.722920520615754e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07540892898712469 0.07405057436098224
need align? ->  True 0.07405057436098224
2023-08-05 09:47:40,956 - epoch:16, training loss:0.1115 validation loss:0.0754
Updating learning rate to 6.0955218187595454e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07564143957975118 0.07407819309636303
need align? ->  True 0.07405057436098224
2023-08-05 09:48:12,097 - epoch:17, training loss:0.1103 validation loss:0.0756
Updating learning rate to 5.449378777053877e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07547153508209664 0.07420446205398311
need align? ->  True 0.07405057436098224
2023-08-05 09:48:43,745 - epoch:18, training loss:0.1099 validation loss:0.0755
Updating learning rate to 4.7955470820870264e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.0755455831632666 0.07404719576563525
need align? ->  True 0.07404719576563525
2023-08-05 09:49:15,681 - epoch:19, training loss:0.1091 validation loss:0.0755
Updating learning rate to 4.1452139754362694e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.0751432672790859 0.07424228354964567
need align? ->  True 0.07404719576563525
2023-08-05 09:49:47,489 - epoch:20, training loss:0.1086 validation loss:0.0751
Updating learning rate to 3.509506836862803e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07515803109044614 0.07411576767006646
need align? ->  True 0.07404719576563525
2023-08-05 09:50:20,742 - epoch:21, training loss:0.1085 validation loss:0.0752
Updating learning rate to 2.899302791758933e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07497257067133552 0.07391694455367068
need align? ->  True 0.07391694455367068
2023-08-05 09:50:54,639 - epoch:22, training loss:0.1080 validation loss:0.0750
Updating learning rate to 2.325042600516913e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07518172555643579 0.07368496060371399
need align? ->  True 0.07368496060371399
2023-08-05 09:51:26,159 - epoch:23, training loss:0.1076 validation loss:0.0752
Updating learning rate to 1.7965520142238964e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.0751533786887708 0.07377997056945511
need align? ->  True 0.07368496060371399
2023-08-05 09:51:57,228 - epoch:24, training loss:0.1074 validation loss:0.0752
Updating learning rate to 1.3228736533366284e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07527467908094758 0.07378708781755489
need align? ->  True 0.07368496060371399
2023-08-05 09:52:28,554 - epoch:25, training loss:0.1072 validation loss:0.0753
Updating learning rate to 9.12112285938345e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07508253079393636 0.07371791768009248
need align? ->  True 0.07368496060371399
2023-08-05 09:53:00,222 - epoch:26, training loss:0.1073 validation loss:0.0751
Updating learning rate to 5.7129615290979635e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07519865060306113 0.0736505317947139
need align? ->  True 0.0736505317947139
2023-08-05 09:53:34,193 - epoch:27, training loss:0.1072 validation loss:0.0752
Updating learning rate to 3.062567127791838e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07510319570808308 0.07370223428892053
need align? ->  True 0.0736505317947139
2023-08-05 09:54:07,170 - epoch:28, training loss:0.1069 validation loss:0.0751
Updating learning rate to 1.2152886384995848e-06
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07509944320696851 0.07368401405603989
need align? ->  True 0.0736505317947139
2023-08-05 09:54:39,558 - epoch:29, training loss:0.1069 validation loss:0.0751
Updating learning rate to 2.0273350833534377e-07
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-09:38:37.112783/0/0.075_epoch_22.pkl  &  0.0736505317947139
2023-08-05 09:54:43,057 - [*] loss:0.1598
2023-08-05 09:54:43,058 - [*] phase 0, testing
2023-08-05 09:54:43,069 - T:24	MAE	0.254377	RMSE	0.162122	MAPE	113.029003
2023-08-05 09:54:43,069 - 24	mae	0.2544	
2023-08-05 09:54:43,069 - 24	rmse	0.1621	
2023-08-05 09:54:43,070 - 24	mape	113.0290	
2023-08-05 09:54:46,884 - [*] loss:0.1567
2023-08-05 09:54:46,885 - [*] phase 0, testing
2023-08-05 09:54:46,894 - T:24	MAE	0.251440	RMSE	0.158996	MAPE	112.478328
2023-08-05 09:54:46,894 - 24	mae	0.2514	
2023-08-05 09:54:46,894 - 24	rmse	0.1590	
2023-08-05 09:54:46,894 - 24	mape	112.4783	
2023-08-05 09:54:49,339 - logger name:exp/ECL-PatchTST2023-08-05-09:54:49.338886/ECL-PatchTST.log
2023-08-05 09:54:49,339 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-09:54:49.338886', 'path': 'exp/ECL-PatchTST2023-08-05-09:54:49.338886', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 09:54:49,339 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 09:54:49,555 - [*] phase 0 Dataset load!
2023-08-05 09:54:50,489 - [*] phase 0 Training start
train 8281
2023-08-05 09:55:10,932 - epoch:0, training loss:0.1973 validation loss:0.1336
train 8281
vs, vt 0.13359046646434328 0.13925113528966904
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.09950085162468579 0.1105099884874147
need align? ->  False 0.1105099884874147
2023-08-05 09:55:58,016 - epoch:1, training loss:0.2265 validation loss:0.0995
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08277826699549737 0.09059893574727618
need align? ->  False 0.09059893574727618
2023-08-05 09:56:29,187 - epoch:2, training loss:0.1810 validation loss:0.0828
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07870525801959245 0.08055051132712675
need align? ->  False 0.08055051132712675
2023-08-05 09:57:00,931 - epoch:3, training loss:0.1538 validation loss:0.0787
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07620238869086556 0.0773807677399853
need align? ->  False 0.0773807677399853
2023-08-05 09:57:33,798 - epoch:4, training loss:0.1369 validation loss:0.0762
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07673089109037233 0.07630534445785958
need align? ->  True 0.07630534445785958
2023-08-05 09:58:07,869 - epoch:5, training loss:0.1283 validation loss:0.0767
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07595383060043273 0.07598701192309028
need align? ->  False 0.07598701192309028
2023-08-05 09:58:41,329 - epoch:6, training loss:0.1233 validation loss:0.0760
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.0770966149866581 0.07493639194771
need align? ->  True 0.07493639194771
2023-08-05 09:59:12,236 - epoch:7, training loss:0.1205 validation loss:0.0771
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.0758783166985149 0.07607729020326034
need align? ->  True 0.07493639194771
2023-08-05 09:59:44,089 - epoch:8, training loss:0.1184 validation loss:0.0759
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07602908235529195 0.07551009313243887
need align? ->  True 0.07493639194771
2023-08-05 10:00:16,530 - epoch:9, training loss:0.1167 validation loss:0.0760
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.0764406155794859 0.07568390116743419
need align? ->  True 0.07493639194771
2023-08-05 10:00:50,067 - epoch:10, training loss:0.1157 validation loss:0.0764
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07603925555620504 0.07534671194203522
need align? ->  True 0.07493639194771
2023-08-05 10:01:23,969 - epoch:11, training loss:0.1149 validation loss:0.0760
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07609745989675107 0.0748782587926025
need align? ->  True 0.0748782587926025
2023-08-05 10:01:57,002 - epoch:12, training loss:0.1143 validation loss:0.0761
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07587354662625687 0.07524750559874203
need align? ->  True 0.0748782587926025
2023-08-05 10:02:28,511 - epoch:13, training loss:0.1126 validation loss:0.0759
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07657948419775652 0.07470980715816436
need align? ->  True 0.07470980715816436
2023-08-05 10:03:00,680 - epoch:14, training loss:0.1114 validation loss:0.0766
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07566109000016814 0.07523302328975304
need align? ->  True 0.07470980715816436
2023-08-05 10:03:32,491 - epoch:15, training loss:0.1107 validation loss:0.0757
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07574316924032957 0.07458105780508208
need align? ->  True 0.07458105780508208
2023-08-05 10:04:05,009 - epoch:16, training loss:0.1100 validation loss:0.0757
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07585359364748001 0.07469354067807613
need align? ->  True 0.07458105780508208
2023-08-05 10:04:39,116 - epoch:17, training loss:0.1094 validation loss:0.0759
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07581093210888945 0.07439222599825134
need align? ->  True 0.07439222599825134
2023-08-05 10:05:11,901 - epoch:18, training loss:0.1089 validation loss:0.0758
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.0757942944765091 0.07456406618913879
need align? ->  True 0.07439222599825134
2023-08-05 10:05:43,838 - epoch:19, training loss:0.1085 validation loss:0.0758
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07585180056807787 0.07440109348491482
need align? ->  True 0.07439222599825134
2023-08-05 10:06:15,477 - epoch:20, training loss:0.1082 validation loss:0.0759
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07564559149677338 0.07436278745855975
need align? ->  True 0.07436278745855975
2023-08-05 10:06:47,503 - epoch:21, training loss:0.1077 validation loss:0.0756
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07555248272483764 0.07417839247247447
need align? ->  True 0.07417839247247447
2023-08-05 10:07:20,727 - epoch:22, training loss:0.1071 validation loss:0.0756
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07543050546361052 0.07405704786272152
need align? ->  True 0.07405704786272152
2023-08-05 10:07:54,741 - epoch:23, training loss:0.1068 validation loss:0.0754
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07565636284973311 0.0739385190865268
need align? ->  True 0.0739385190865268
2023-08-05 10:08:26,920 - epoch:24, training loss:0.1068 validation loss:0.0757
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07549652287169643 0.07407245704013368
need align? ->  True 0.0739385190865268
2023-08-05 10:08:59,185 - epoch:25, training loss:0.1067 validation loss:0.0755
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.0756338755397693 0.07406862063900284
need align? ->  True 0.0739385190865268
2023-08-05 10:09:31,105 - epoch:26, training loss:0.1066 validation loss:0.0756
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07557587244588396 0.0740647021195163
need align? ->  True 0.0739385190865268
2023-08-05 10:10:02,896 - epoch:27, training loss:0.1065 validation loss:0.0756
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07559916206999966 0.07406772179124148
need align? ->  True 0.0739385190865268
2023-08-05 10:10:34,944 - epoch:28, training loss:0.1065 validation loss:0.0756
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07559551298618317 0.07405826935301656
need align? ->  True 0.0739385190865268
2023-08-05 10:11:07,760 - epoch:29, training loss:0.1060 validation loss:0.0756
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-09:54:49.338886/0/0.0754_epoch_23.pkl  &  0.0739385190865268
2023-08-05 10:11:10,725 - [*] loss:0.1605
2023-08-05 10:11:10,726 - [*] phase 0, testing
2023-08-05 10:11:10,735 - T:24	MAE	0.256055	RMSE	0.162837	MAPE	113.093126
2023-08-05 10:11:10,735 - 24	mae	0.2561	
2023-08-05 10:11:10,735 - 24	rmse	0.1628	
2023-08-05 10:11:10,735 - 24	mape	113.0931	
2023-08-05 10:11:13,810 - [*] loss:0.1575
2023-08-05 10:11:13,812 - [*] phase 0, testing
2023-08-05 10:11:13,822 - T:24	MAE	0.251813	RMSE	0.159903	MAPE	113.303924
2023-08-05 10:11:13,822 - 24	mae	0.2518	
2023-08-05 10:11:13,823 - 24	rmse	0.1599	
2023-08-05 10:11:13,823 - 24	mape	113.3039	
2023-08-05 10:11:16,348 - logger name:exp/ECL-PatchTST2023-08-05-10:11:16.347531/ECL-PatchTST.log
2023-08-05 10:11:16,348 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 1.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-10:11:16.347531', 'path': 'exp/ECL-PatchTST2023-08-05-10:11:16.347531', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 10:11:16,348 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 10:11:16,551 - [*] phase 0 Dataset load!
2023-08-05 10:11:17,554 - [*] phase 0 Training start
train 8281
2023-08-05 10:11:40,975 - epoch:0, training loss:0.2000 validation loss:0.1367
train 8281
vs, vt 0.13673778509964113 0.1439287234907565
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.09825160516344983 0.11183329979362695
need align? ->  False 0.11183329979362695
2023-08-05 10:12:27,620 - epoch:1, training loss:0.2873 validation loss:0.0983
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08252453682539256 0.09020119257595229
need align? ->  False 0.09020119257595229
2023-08-05 10:13:01,103 - epoch:2, training loss:0.2323 validation loss:0.0825
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.0787103543786899 0.08016256404959637
need align? ->  False 0.08016256404959637
2023-08-05 10:13:33,441 - epoch:3, training loss:0.1959 validation loss:0.0787
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07688787279893523 0.07698169650266999
need align? ->  False 0.07698169650266999
2023-08-05 10:14:05,309 - epoch:4, training loss:0.1704 validation loss:0.0769
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07628436216517635 0.07584441315544688
need align? ->  True 0.07584441315544688
2023-08-05 10:14:36,746 - epoch:5, training loss:0.1556 validation loss:0.0763
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07697017158827056 0.0764810744996952
need align? ->  True 0.07584441315544688
2023-08-05 10:15:08,740 - epoch:6, training loss:0.1465 validation loss:0.0770
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07589097053784391 0.07630544957583366
need align? ->  True 0.07584441315544688
2023-08-05 10:15:40,890 - epoch:7, training loss:0.1439 validation loss:0.0759
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07610126471389896 0.07543403104595515
need align? ->  True 0.07543403104595515
2023-08-05 10:16:14,491 - epoch:8, training loss:0.1420 validation loss:0.0761
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07771610582004422 0.07536831085124741
need align? ->  True 0.07536831085124741
2023-08-05 10:16:46,278 - epoch:9, training loss:0.1360 validation loss:0.0777
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07646774095685585 0.0753219669441814
need align? ->  True 0.0753219669441814
2023-08-05 10:17:17,610 - epoch:10, training loss:0.1346 validation loss:0.0765
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07602528782318467 0.07541794848182927
need align? ->  True 0.0753219669441814
2023-08-05 10:17:50,644 - epoch:11, training loss:0.1330 validation loss:0.0760
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07577457449034504 0.07495148188394049
need align? ->  True 0.07495148188394049
2023-08-05 10:18:24,180 - epoch:12, training loss:0.1311 validation loss:0.0758
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07535357676122499 0.07468131929636002
need align? ->  True 0.07468131929636002
2023-08-05 10:18:57,376 - epoch:13, training loss:0.1302 validation loss:0.0754
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07575051073470841 0.07473344979402811
need align? ->  True 0.07468131929636002
2023-08-05 10:19:30,402 - epoch:14, training loss:0.1290 validation loss:0.0758
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07596306138388488 0.07482496138824068
need align? ->  True 0.07468131929636002
2023-08-05 10:20:04,039 - epoch:15, training loss:0.1285 validation loss:0.0760
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07616548418350842 0.07496674941933673
need align? ->  True 0.07468131929636002
2023-08-05 10:20:37,196 - epoch:16, training loss:0.1276 validation loss:0.0762
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07548198468335297 0.07454304753438286
need align? ->  True 0.07454304753438286
2023-08-05 10:21:11,262 - epoch:17, training loss:0.1271 validation loss:0.0755
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07574763643028944 0.07432884998295618
need align? ->  True 0.07432884998295618
2023-08-05 10:21:45,092 - epoch:18, training loss:0.1265 validation loss:0.0757
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07535476018877132 0.07439665544940077
need align? ->  True 0.07432884998295618
2023-08-05 10:22:18,950 - epoch:19, training loss:0.1260 validation loss:0.0754
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07558864573745624 0.07424563647288343
need align? ->  True 0.07424563647288343
2023-08-05 10:22:52,066 - epoch:20, training loss:0.1254 validation loss:0.0756
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07525823807910732 0.07435641356784364
need align? ->  True 0.07424563647288343
2023-08-05 10:23:25,313 - epoch:21, training loss:0.1250 validation loss:0.0753
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07550370353071587 0.073995609484289
need align? ->  True 0.073995609484289
2023-08-05 10:23:57,979 - epoch:22, training loss:0.1245 validation loss:0.0755
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07534799506158932 0.07413995954329552
need align? ->  True 0.073995609484289
2023-08-05 10:24:31,095 - epoch:23, training loss:0.1244 validation loss:0.0753
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07533597670819449 0.07413882025234077
need align? ->  True 0.073995609484289
2023-08-05 10:25:04,403 - epoch:24, training loss:0.1243 validation loss:0.0753
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07537672620104707 0.07404524233678113
need align? ->  True 0.073995609484289
2023-08-05 10:25:37,918 - epoch:25, training loss:0.1240 validation loss:0.0754
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07538489462888759 0.0740288023877403
need align? ->  True 0.073995609484289
2023-08-05 10:26:11,280 - epoch:26, training loss:0.1240 validation loss:0.0754
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07535837501611399 0.07409352023640405
need align? ->  True 0.073995609484289
2023-08-05 10:26:45,185 - epoch:27, training loss:0.1237 validation loss:0.0754
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.0753340950316709 0.07406662355946458
need align? ->  True 0.073995609484289
2023-08-05 10:27:18,196 - epoch:28, training loss:0.1235 validation loss:0.0753
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07534672020246154 0.07407140075836492
need align? ->  True 0.073995609484289
2023-08-05 10:27:51,322 - epoch:29, training loss:0.1237 validation loss:0.0753
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-10:11:16.347531/0/0.0753_epoch_21.pkl  &  0.073995609484289
2023-08-05 10:27:54,313 - [*] loss:0.1602
2023-08-05 10:27:54,314 - [*] phase 0, testing
2023-08-05 10:27:54,322 - T:24	MAE	0.255632	RMSE	0.162469	MAPE	113.342345
2023-08-05 10:27:54,322 - 24	mae	0.2556	
2023-08-05 10:27:54,322 - 24	rmse	0.1625	
2023-08-05 10:27:54,322 - 24	mape	113.3423	
2023-08-05 10:27:57,454 - [*] loss:0.1575
2023-08-05 10:27:57,455 - [*] phase 0, testing
2023-08-05 10:27:57,464 - T:24	MAE	0.252319	RMSE	0.159859	MAPE	112.868261
2023-08-05 10:27:57,464 - 24	mae	0.2523	
2023-08-05 10:27:57,464 - 24	rmse	0.1599	
2023-08-05 10:27:57,464 - 24	mape	112.8683	
2023-08-05 10:27:59,769 - logger name:exp/ECL-PatchTST2023-08-05-10:27:59.769546/ECL-PatchTST.log
2023-08-05 10:27:59,770 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 1.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-10:27:59.769546', 'path': 'exp/ECL-PatchTST2023-08-05-10:27:59.769546', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 10:27:59,770 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 10:28:00,044 - [*] phase 0 Dataset load!
2023-08-05 10:28:01,141 - [*] phase 0 Training start
train 8281
2023-08-05 10:28:24,333 - epoch:0, training loss:0.1993 validation loss:0.1351
train 8281
vs, vt 0.13509844648449318 0.140629466785037
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.09988894240687722 0.11028115418942078
need align? ->  False 0.11028115418942078
2023-08-05 10:29:09,782 - epoch:1, training loss:0.2845 validation loss:0.0999
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08318659962843293 0.09048100982023322
need align? ->  False 0.09048100982023322
2023-08-05 10:29:42,393 - epoch:2, training loss:0.2321 validation loss:0.0832
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07853990291123805 0.08060683391016463
need align? ->  False 0.08060683391016463
2023-08-05 10:30:14,941 - epoch:3, training loss:0.1953 validation loss:0.0785
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07737395758538143 0.07713717432773631
need align? ->  True 0.07713717432773631
2023-08-05 10:30:47,447 - epoch:4, training loss:0.1705 validation loss:0.0774
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07640542332892833 0.07661457764713661
need align? ->  False 0.07661457764713661
2023-08-05 10:31:19,972 - epoch:5, training loss:0.1565 validation loss:0.0764
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07573862171367458 0.0757001371163389
need align? ->  True 0.0757001371163389
2023-08-05 10:31:52,910 - epoch:6, training loss:0.1476 validation loss:0.0757
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07582712505498658 0.07489377211617387
need align? ->  True 0.07489377211617387
2023-08-05 10:32:25,480 - epoch:7, training loss:0.1426 validation loss:0.0758
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07639990617399631 0.07504112451620724
need align? ->  True 0.07489377211617387
2023-08-05 10:32:57,406 - epoch:8, training loss:0.1396 validation loss:0.0764
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07539486739298572 0.07556042642049167
need align? ->  True 0.07489377211617387
2023-08-05 10:33:29,507 - epoch:9, training loss:0.1382 validation loss:0.0754
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07508411485215892 0.07450477584548619
need align? ->  True 0.07450477584548619
2023-08-05 10:34:02,259 - epoch:10, training loss:0.1369 validation loss:0.0751
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07535125971164393 0.07479544801880485
need align? ->  True 0.07450477584548619
2023-08-05 10:34:35,810 - epoch:11, training loss:0.1335 validation loss:0.0754
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.0757806021720171 0.07450997424514397
need align? ->  True 0.07450477584548619
2023-08-05 10:35:09,516 - epoch:12, training loss:0.1321 validation loss:0.0758
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07521642987494884 0.0749371896457413
need align? ->  True 0.07450477584548619
2023-08-05 10:35:43,447 - epoch:13, training loss:0.1315 validation loss:0.0752
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07543553573929745 0.07466224224671074
need align? ->  True 0.07450477584548619
2023-08-05 10:36:15,948 - epoch:14, training loss:0.1309 validation loss:0.0754
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07500649486546931 0.07447785365840663
need align? ->  True 0.07447785365840663
2023-08-05 10:36:47,426 - epoch:15, training loss:0.1296 validation loss:0.0750
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07530363495259182 0.07422133284094541
need align? ->  True 0.07422133284094541
2023-08-05 10:37:18,896 - epoch:16, training loss:0.1282 validation loss:0.0753
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07525979193008464 0.07406688999870549
need align? ->  True 0.07406688999870549
2023-08-05 10:37:51,886 - epoch:17, training loss:0.1270 validation loss:0.0753
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07535677532786908 0.07428187472016914
need align? ->  True 0.07406688999870549
2023-08-05 10:38:25,113 - epoch:18, training loss:0.1267 validation loss:0.0754
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07529792529733284 0.07426257723051569
need align? ->  True 0.07406688999870549
2023-08-05 10:38:58,635 - epoch:19, training loss:0.1258 validation loss:0.0753
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07502569833203503 0.07424436663479908
need align? ->  True 0.07406688999870549
2023-08-05 10:39:30,904 - epoch:20, training loss:0.1255 validation loss:0.0750
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07496294605991115 0.07400400847520518
need align? ->  True 0.07400400847520518
2023-08-05 10:40:02,716 - epoch:21, training loss:0.1253 validation loss:0.0750
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07485966060472571 0.07401775309573048
need align? ->  True 0.07400400847520518
2023-08-05 10:40:33,966 - epoch:22, training loss:0.1247 validation loss:0.0749
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.074863877552359 0.07375074688183225
need align? ->  True 0.07375074688183225
2023-08-05 10:41:06,355 - epoch:23, training loss:0.1241 validation loss:0.0749
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.0749926191309224 0.07379620805706667
need align? ->  True 0.07375074688183225
2023-08-05 10:41:39,782 - epoch:24, training loss:0.1243 validation loss:0.0750
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07503607674785283 0.07389242866117021
need align? ->  True 0.07375074688183225
2023-08-05 10:42:13,134 - epoch:25, training loss:0.1242 validation loss:0.0750
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07487927871229856 0.07378586486953756
need align? ->  True 0.07375074688183225
2023-08-05 10:42:45,845 - epoch:26, training loss:0.1240 validation loss:0.0749
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07493949835391148 0.0737134181772885
need align? ->  True 0.0737134181772885
2023-08-05 10:43:17,494 - epoch:27, training loss:0.1239 validation loss:0.0749
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.0749282758359028 0.07373467444077782
need align? ->  True 0.0737134181772885
2023-08-05 10:43:51,951 - epoch:28, training loss:0.1238 validation loss:0.0749
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.0749185565535141 0.07372316426557043
need align? ->  True 0.0737134181772885
2023-08-05 10:44:23,910 - epoch:29, training loss:0.1238 validation loss:0.0749
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-10:27:59.769546/0/0.0749_epoch_22.pkl  &  0.0737134181772885
2023-08-05 10:44:26,576 - [*] loss:0.1595
2023-08-05 10:44:26,577 - [*] phase 0, testing
2023-08-05 10:44:26,588 - T:24	MAE	0.254139	RMSE	0.161831	MAPE	113.135076
2023-08-05 10:44:26,589 - 24	mae	0.2541	
2023-08-05 10:44:26,589 - 24	rmse	0.1618	
2023-08-05 10:44:26,589 - 24	mape	113.1351	
2023-08-05 10:44:29,319 - [*] loss:0.1568
2023-08-05 10:44:29,320 - [*] phase 0, testing
2023-08-05 10:44:29,330 - T:24	MAE	0.251488	RMSE	0.159120	MAPE	112.702668
2023-08-05 10:44:29,331 - 24	mae	0.2515	
2023-08-05 10:44:29,331 - 24	rmse	0.1591	
2023-08-05 10:44:29,331 - 24	mape	112.7027	
2023-08-05 10:44:31,794 - logger name:exp/ECL-PatchTST2023-08-05-10:44:31.794275/ECL-PatchTST.log
2023-08-05 10:44:31,794 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 1.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-10:44:31.794275', 'path': 'exp/ECL-PatchTST2023-08-05-10:44:31.794275', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 10:44:31,795 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 10:44:31,998 - [*] phase 0 Dataset load!
2023-08-05 10:44:32,940 - [*] phase 0 Training start
train 8281
2023-08-05 10:44:56,306 - epoch:0, training loss:0.1973 validation loss:0.1336
train 8281
vs, vt 0.13359046646434328 0.13925113528966904
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.0995921635919291 0.1105099884874147
need align? ->  False 0.1105099884874147
2023-08-05 10:45:42,773 - epoch:1, training loss:0.2844 validation loss:0.0996
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08297534173597461 0.09061295482451501
need align? ->  False 0.09061295482451501
2023-08-05 10:46:20,473 - epoch:2, training loss:0.2312 validation loss:0.0830
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07883883429610211 0.08064678335643333
need align? ->  False 0.08064678335643333
2023-08-05 10:46:57,206 - epoch:3, training loss:0.1945 validation loss:0.0788
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07628954023770664 0.07738988036694734
need align? ->  False 0.07738988036694734
2023-08-05 10:47:29,202 - epoch:4, training loss:0.1680 validation loss:0.0763
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07683815402181252 0.07638093788662682
need align? ->  True 0.07638093788662682
2023-08-05 10:48:04,578 - epoch:5, training loss:0.1540 validation loss:0.0768
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07596025389173756 0.07578432956791442
need align? ->  True 0.07578432956791442
2023-08-05 10:48:36,842 - epoch:6, training loss:0.1461 validation loss:0.0760
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07721917379809462 0.07498412012406018
need align? ->  True 0.07498412012406018
2023-08-05 10:49:09,478 - epoch:7, training loss:0.1418 validation loss:0.0772
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.075971071810826 0.07600815075895061
need align? ->  True 0.07498412012406018
2023-08-05 10:49:42,925 - epoch:8, training loss:0.1384 validation loss:0.0760
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07606638579264931 0.07552160298370797
need align? ->  True 0.07498412012406018
2023-08-05 10:50:16,410 - epoch:9, training loss:0.1365 validation loss:0.0761
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07648533469308978 0.07575276623601498
need align? ->  True 0.07498412012406018
2023-08-05 10:50:48,390 - epoch:10, training loss:0.1354 validation loss:0.0765
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.0759534319943708 0.07532067200087983
need align? ->  True 0.07498412012406018
2023-08-05 10:51:20,544 - epoch:11, training loss:0.1344 validation loss:0.0760
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07604843046030273 0.07478882566742275
need align? ->  True 0.07478882566742275
2023-08-05 10:51:52,400 - epoch:12, training loss:0.1336 validation loss:0.0760
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.0757773091773624 0.07518469226425109
need align? ->  True 0.07478882566742275
2023-08-05 10:52:25,373 - epoch:13, training loss:0.1308 validation loss:0.0758
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07643040812209896 0.07473094008215096
need align? ->  True 0.07473094008215096
2023-08-05 10:52:59,672 - epoch:14, training loss:0.1292 validation loss:0.0764
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.0754789397444414 0.07524289401329082
need align? ->  True 0.07473094008215096
2023-08-05 10:53:33,217 - epoch:15, training loss:0.1283 validation loss:0.0755
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07555717173153939 0.07457458835257136
need align? ->  True 0.07457458835257136
2023-08-05 10:54:05,283 - epoch:16, training loss:0.1275 validation loss:0.0756
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07560366659384707 0.07457254009078378
need align? ->  True 0.07457254009078378
2023-08-05 10:54:37,671 - epoch:17, training loss:0.1266 validation loss:0.0756
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07563855320863101 0.0744456603475239
need align? ->  True 0.0744456603475239
2023-08-05 10:55:11,436 - epoch:18, training loss:0.1259 validation loss:0.0756
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07559336155005124 0.07450506528434546
need align? ->  True 0.0744456603475239
2023-08-05 10:55:43,838 - epoch:19, training loss:0.1253 validation loss:0.0756
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07567705149235933 0.07443492325103801
need align? ->  True 0.07443492325103801
2023-08-05 10:56:17,023 - epoch:20, training loss:0.1249 validation loss:0.0757
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07538028417722038 0.07437287408696569
need align? ->  True 0.07437287408696569
2023-08-05 10:56:49,919 - epoch:21, training loss:0.1245 validation loss:0.0754
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07532014414344144 0.07412649828778661
need align? ->  True 0.07412649828778661
2023-08-05 10:57:22,447 - epoch:22, training loss:0.1237 validation loss:0.0753
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07517399906140307 0.07401789651940698
need align? ->  True 0.07401789651940698
2023-08-05 10:57:55,253 - epoch:23, training loss:0.1233 validation loss:0.0752
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07538983541662278 0.07389828020139881
need align? ->  True 0.07389828020139881
2023-08-05 10:58:27,934 - epoch:24, training loss:0.1234 validation loss:0.0754
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07525556008129017 0.07401052603255147
need align? ->  True 0.07389828020139881
2023-08-05 10:59:01,067 - epoch:25, training loss:0.1232 validation loss:0.0753
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07538692969018998 0.07398190997217012
need align? ->  True 0.07389828020139881
2023-08-05 10:59:32,854 - epoch:26, training loss:0.1231 validation loss:0.0754
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07533324817600458 0.07399714171238568
need align? ->  True 0.07389828020139881
2023-08-05 11:00:06,011 - epoch:27, training loss:0.1231 validation loss:0.0753
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07535314438459666 0.07400281460064909
need align? ->  True 0.07389828020139881
2023-08-05 11:00:38,506 - epoch:28, training loss:0.1231 validation loss:0.0754
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07534159784731657 0.07399060407086559
need align? ->  True 0.07389828020139881
2023-08-05 11:01:11,676 - epoch:29, training loss:0.1225 validation loss:0.0753
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-10:44:31.794275/0/0.0752_epoch_23.pkl  &  0.07389828020139881
2023-08-05 11:01:14,880 - [*] loss:0.1599
2023-08-05 11:01:14,882 - [*] phase 0, testing
2023-08-05 11:01:14,891 - T:24	MAE	0.255490	RMSE	0.162172	MAPE	113.299751
2023-08-05 11:01:14,891 - 24	mae	0.2555	
2023-08-05 11:01:14,891 - 24	rmse	0.1622	
2023-08-05 11:01:14,891 - 24	mape	113.2998	
2023-08-05 11:01:18,094 - [*] loss:0.1574
2023-08-05 11:01:18,095 - [*] phase 0, testing
2023-08-05 11:01:18,103 - T:24	MAE	0.251712	RMSE	0.159788	MAPE	113.477790
2023-08-05 11:01:18,104 - 24	mae	0.2517	
2023-08-05 11:01:18,104 - 24	rmse	0.1598	
2023-08-05 11:01:18,104 - 24	mape	113.4778	
2023-08-05 11:01:20,595 - logger name:exp/ECL-PatchTST2023-08-05-11:01:20.595325/ECL-PatchTST.log
2023-08-05 11:01:20,596 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 3.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-11:01:20.595325', 'path': 'exp/ECL-PatchTST2023-08-05-11:01:20.595325', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 11:01:20,596 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 11:01:20,830 - [*] phase 0 Dataset load!
2023-08-05 11:01:21,906 - [*] phase 0 Training start
train 8281
2023-08-05 11:01:44,131 - epoch:0, training loss:0.2000 validation loss:0.1367
train 8281
vs, vt 0.13673778509964113 0.1439287234907565
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.09875915855493235 0.11183329979362695
need align? ->  False 0.11183329979362695
2023-08-05 11:02:31,761 - epoch:1, training loss:0.5196 validation loss:0.0988
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08296027326065561 0.09036092071429543
need align? ->  False 0.09036092071429543
2023-08-05 11:03:05,307 - epoch:2, training loss:0.4323 validation loss:0.0830
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07898631959181765 0.0805347380430802
need align? ->  False 0.0805347380430802
2023-08-05 11:03:38,680 - epoch:3, training loss:0.3580 validation loss:0.0790
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07713781726425109 0.07712104973261771
need align? ->  True 0.07712104973261771
2023-08-05 11:04:11,377 - epoch:4, training loss:0.2962 validation loss:0.0771
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07633329474407693 0.07593204348307589
need align? ->  True 0.07593204348307589
2023-08-05 11:04:44,990 - epoch:5, training loss:0.2585 validation loss:0.0763
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07708046949752 0.07630483409308869
need align? ->  True 0.07593204348307589
2023-08-05 11:05:18,005 - epoch:6, training loss:0.2361 validation loss:0.0771
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07606939663705618 0.07646401866298655
need align? ->  True 0.07593204348307589
2023-08-05 11:05:51,828 - epoch:7, training loss:0.2316 validation loss:0.0761
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07621131788777269 0.07555672412981158
need align? ->  True 0.07555672412981158
2023-08-05 11:06:25,476 - epoch:8, training loss:0.2283 validation loss:0.0762
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07799669371350952 0.07529918784680574
need align? ->  True 0.07529918784680574
2023-08-05 11:06:58,308 - epoch:9, training loss:0.2119 validation loss:0.0780
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07669128315604251 0.07562915446317714
need align? ->  True 0.07529918784680574
2023-08-05 11:07:31,374 - epoch:10, training loss:0.2086 validation loss:0.0767
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07599818325884965 0.07563455581017163
need align? ->  True 0.07529918784680574
2023-08-05 11:08:04,391 - epoch:11, training loss:0.2063 validation loss:0.0760
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07568470534423123 0.07494162079756675
need align? ->  True 0.07494162079756675
2023-08-05 11:08:38,598 - epoch:12, training loss:0.2039 validation loss:0.0757
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07535748410484065 0.07461969805476458
need align? ->  True 0.07461969805476458
2023-08-05 11:09:11,927 - epoch:13, training loss:0.1994 validation loss:0.0754
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07575680760909682 0.07473103199963985
need align? ->  True 0.07461969805476458
2023-08-05 11:09:44,850 - epoch:14, training loss:0.1976 validation loss:0.0758
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07572645378177581 0.07471332666666611
need align? ->  True 0.07461969805476458
2023-08-05 11:10:17,576 - epoch:15, training loss:0.1964 validation loss:0.0757
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07617434632519017 0.0747540048930956
need align? ->  True 0.07461969805476458
2023-08-05 11:10:50,970 - epoch:16, training loss:0.1950 validation loss:0.0762
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.075385222049511 0.07453434995335081
need align? ->  True 0.07453434995335081
2023-08-05 11:11:25,181 - epoch:17, training loss:0.1943 validation loss:0.0754
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.0756082909748606 0.07431810252044511
need align? ->  True 0.07431810252044511
2023-08-05 11:11:58,268 - epoch:18, training loss:0.1928 validation loss:0.0756
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07519848369385886 0.07444397797403128
need align? ->  True 0.07431810252044511
2023-08-05 11:12:31,570 - epoch:19, training loss:0.1917 validation loss:0.0752
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07539727579316367 0.07423761122576568
need align? ->  True 0.07423761122576568
2023-08-05 11:13:05,975 - epoch:20, training loss:0.1907 validation loss:0.0754
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07505172296710637 0.07432750774466473
need align? ->  True 0.07423761122576568
2023-08-05 11:13:38,973 - epoch:21, training loss:0.1899 validation loss:0.0751
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07529203107823497 0.07406805731032205
need align? ->  True 0.07406805731032205
2023-08-05 11:14:10,562 - epoch:22, training loss:0.1888 validation loss:0.0753
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07514576314260131 0.07407259293224501
need align? ->  True 0.07406805731032205
2023-08-05 11:14:42,295 - epoch:23, training loss:0.1887 validation loss:0.0751
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07510911519436733 0.07407406077760717
need align? ->  True 0.07406805731032205
2023-08-05 11:15:14,537 - epoch:24, training loss:0.1887 validation loss:0.0751
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07511240090041058 0.0740316480236209
need align? ->  True 0.0740316480236209
2023-08-05 11:15:48,058 - epoch:25, training loss:0.1882 validation loss:0.0751
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07514060150993906 0.07398742801793244
need align? ->  True 0.07398742801793244
2023-08-05 11:16:22,428 - epoch:26, training loss:0.1881 validation loss:0.0751
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07510607195613177 0.07404898155642592
need align? ->  True 0.07398742801793244
2023-08-05 11:16:58,554 - epoch:27, training loss:0.1876 validation loss:0.0751
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07509313742427723 0.07402600024057471
need align? ->  True 0.07398742801793244
2023-08-05 11:17:30,876 - epoch:28, training loss:0.1874 validation loss:0.0751
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.0751018724201814 0.07402646355330944
need align? ->  True 0.07398742801793244
2023-08-05 11:18:02,086 - epoch:29, training loss:0.1875 validation loss:0.0751
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-11:01:20.595325/0/0.0751_epoch_21.pkl  &  0.07398742801793244
2023-08-05 11:18:04,963 - [*] loss:0.1596
2023-08-05 11:18:04,964 - [*] phase 0, testing
2023-08-05 11:18:04,973 - T:24	MAE	0.255248	RMSE	0.161856	MAPE	113.839889
2023-08-05 11:18:04,973 - 24	mae	0.2552	
2023-08-05 11:18:04,973 - 24	rmse	0.1619	
2023-08-05 11:18:04,974 - 24	mape	113.8399	
2023-08-05 11:18:08,006 - [*] loss:0.1577
2023-08-05 11:18:08,007 - [*] phase 0, testing
2023-08-05 11:18:08,018 - T:24	MAE	0.251478	RMSE	0.160077	MAPE	113.271129
2023-08-05 11:18:08,019 - 24	mae	0.2515	
2023-08-05 11:18:08,019 - 24	rmse	0.1601	
2023-08-05 11:18:08,019 - 24	mape	113.2711	
2023-08-05 11:18:11,021 - logger name:exp/ECL-PatchTST2023-08-05-11:18:11.020738/ECL-PatchTST.log
2023-08-05 11:18:11,022 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 3.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-11:18:11.020738', 'path': 'exp/ECL-PatchTST2023-08-05-11:18:11.020738', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 11:18:11,022 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 11:18:11,332 - [*] phase 0 Dataset load!
2023-08-05 11:18:12,505 - [*] phase 0 Training start
train 8281
2023-08-05 11:18:34,330 - epoch:0, training loss:0.1993 validation loss:0.1351
train 8281
vs, vt 0.13509844648449318 0.140629466785037
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10031289688271025 0.11028115418942078
need align? ->  False 0.11028115418942078
2023-08-05 11:19:18,516 - epoch:1, training loss:0.5121 validation loss:0.1003
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08365587703883648 0.09051147718792377
need align? ->  False 0.09051147718792377
2023-08-05 11:19:51,007 - epoch:2, training loss:0.4277 validation loss:0.0837
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07883594444264537 0.0808354250114897
need align? ->  False 0.0808354250114897
2023-08-05 11:20:24,291 - epoch:3, training loss:0.3538 validation loss:0.0788
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07755125103437382 0.07711957872885725
need align? ->  True 0.07711957872885725
2023-08-05 11:20:56,713 - epoch:4, training loss:0.2942 validation loss:0.0776
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07648822989152826 0.07673610973617305
need align? ->  False 0.07673610973617305
2023-08-05 11:21:28,804 - epoch:5, training loss:0.2598 validation loss:0.0765
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07589705911991389 0.07583270529689996
need align? ->  True 0.07583270529689996
2023-08-05 11:22:00,526 - epoch:6, training loss:0.2387 validation loss:0.0759
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07611270119314609 0.07504708425182363
need align? ->  True 0.07504708425182363
2023-08-05 11:22:32,117 - epoch:7, training loss:0.2265 validation loss:0.0761
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.0766222238054742 0.0751295288943726
need align? ->  True 0.07504708425182363
2023-08-05 11:23:04,856 - epoch:8, training loss:0.2201 validation loss:0.0766
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.075596132842095 0.07576622409017189
need align? ->  True 0.07504708425182363
2023-08-05 11:23:39,166 - epoch:9, training loss:0.2174 validation loss:0.0756
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.0754121343564728 0.07485901313307493
need align? ->  True 0.07485901313307493
2023-08-05 11:24:12,118 - epoch:10, training loss:0.2152 validation loss:0.0754
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.0754448956121569 0.07500629677720692
need align? ->  True 0.07485901313307493
2023-08-05 11:24:43,823 - epoch:11, training loss:0.2066 validation loss:0.0754
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07585899790991908 0.07458107481184213
need align? ->  True 0.07458107481184213
2023-08-05 11:25:15,137 - epoch:12, training loss:0.2043 validation loss:0.0759
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07532052671455819 0.07507592164303946
need align? ->  True 0.07458107481184213
2023-08-05 11:25:46,702 - epoch:13, training loss:0.2006 validation loss:0.0753
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07551079259618469 0.07485671584372935
need align? ->  True 0.07458107481184213
2023-08-05 11:26:19,812 - epoch:14, training loss:0.1990 validation loss:0.0755
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07503647859329762 0.07462366427416386
need align? ->  True 0.07458107481184213
2023-08-05 11:26:52,800 - epoch:15, training loss:0.1972 validation loss:0.0750
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07534690930143646 0.0743153256404659
need align? ->  True 0.0743153256404659
2023-08-05 11:27:25,435 - epoch:16, training loss:0.1963 validation loss:0.0753
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07519711286801359 0.07424925777899183
need align? ->  True 0.07424925777899183
2023-08-05 11:27:57,889 - epoch:17, training loss:0.1943 validation loss:0.0752
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07520992891943973 0.07435464964288732
need align? ->  True 0.07424925777899183
2023-08-05 11:28:29,855 - epoch:18, training loss:0.1925 validation loss:0.0752
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07510821205442367 0.07430352456867695
need align? ->  True 0.07424925777899183
2023-08-05 11:29:01,552 - epoch:19, training loss:0.1913 validation loss:0.0751
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07489815054704314 0.07429063028615454
need align? ->  True 0.07424925777899183
2023-08-05 11:29:34,962 - epoch:20, training loss:0.1908 validation loss:0.0749
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07481111197367958 0.07408813200891018
need align? ->  True 0.07408813200891018
2023-08-05 11:30:09,661 - epoch:21, training loss:0.1904 validation loss:0.0748
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07476955125837223 0.07400410565669122
need align? ->  True 0.07400410565669122
2023-08-05 11:30:42,828 - epoch:22, training loss:0.1892 validation loss:0.0748
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.0747361778403106 0.07384014809909074
need align? ->  True 0.07384014809909074
2023-08-05 11:31:16,811 - epoch:23, training loss:0.1890 validation loss:0.0747
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07481515593826771 0.07386309208105439
need align? ->  True 0.07384014809909074
2023-08-05 11:31:48,480 - epoch:24, training loss:0.1887 validation loss:0.0748
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07484976607172386 0.07392419639812864
need align? ->  True 0.07384014809909074
2023-08-05 11:32:21,873 - epoch:25, training loss:0.1886 validation loss:0.0748
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07470675306799619 0.07385922952190689
need align? ->  True 0.07384014809909074
2023-08-05 11:32:55,278 - epoch:26, training loss:0.1884 validation loss:0.0747
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07476737082976362 0.07379349377816138
need align? ->  True 0.07379349377816138
2023-08-05 11:33:28,997 - epoch:27, training loss:0.1882 validation loss:0.0748
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07475868677315504 0.07381241606629413
need align? ->  True 0.07379349377816138
2023-08-05 11:34:01,062 - epoch:28, training loss:0.1880 validation loss:0.0748
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07474403771693292 0.07380441398076389
need align? ->  True 0.07379349377816138
2023-08-05 11:34:32,640 - epoch:29, training loss:0.1880 validation loss:0.0747
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-11:18:11.020738/0/0.0747_epoch_26.pkl  &  0.07379349377816138
2023-08-05 11:34:36,307 - [*] loss:0.1588
2023-08-05 11:34:36,308 - [*] phase 0, testing
2023-08-05 11:34:36,319 - T:24	MAE	0.254435	RMSE	0.161055	MAPE	113.583446
2023-08-05 11:34:36,319 - 24	mae	0.2544	
2023-08-05 11:34:36,319 - 24	rmse	0.1611	
2023-08-05 11:34:36,319 - 24	mape	113.5834	
2023-08-05 11:34:39,711 - [*] loss:0.1571
2023-08-05 11:34:39,712 - [*] phase 0, testing
2023-08-05 11:34:39,723 - T:24	MAE	0.251341	RMSE	0.159378	MAPE	113.303912
2023-08-05 11:34:39,723 - 24	mae	0.2513	
2023-08-05 11:34:39,724 - 24	rmse	0.1594	
2023-08-05 11:34:39,724 - 24	mape	113.3039	
2023-08-05 11:34:42,334 - logger name:exp/ECL-PatchTST2023-08-05-11:34:42.334287/ECL-PatchTST.log
2023-08-05 11:34:42,335 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 3.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-11:34:42.334287', 'path': 'exp/ECL-PatchTST2023-08-05-11:34:42.334287', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 11:34:42,335 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 11:34:42,636 - [*] phase 0 Dataset load!
2023-08-05 11:34:43,661 - [*] phase 0 Training start
train 8281
2023-08-05 11:35:05,492 - epoch:0, training loss:0.1973 validation loss:0.1336
train 8281
vs, vt 0.13359046646434328 0.13925113528966904
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.09992994376174781 0.1105099884874147
need align? ->  False 0.1105099884874147
2023-08-05 11:35:50,784 - epoch:1, training loss:0.5139 validation loss:0.0999
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.0834781258650448 0.09064886718988419
need align? ->  False 0.09064886718988419
2023-08-05 11:36:23,108 - epoch:2, training loss:0.4280 validation loss:0.0835
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07919813133776188 0.08070914228649242
need align? ->  False 0.08070914228649242
2023-08-05 11:36:55,759 - epoch:3, training loss:0.3540 validation loss:0.0792
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.0765355963907812 0.07727801532525083
need align? ->  False 0.07727801532525083
2023-08-05 11:37:29,170 - epoch:4, training loss:0.2901 validation loss:0.0765
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07704820929338103 0.07645169831812382
need align? ->  True 0.07645169831812382
2023-08-05 11:38:03,821 - epoch:5, training loss:0.2543 validation loss:0.0770
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.0758917986050896 0.07579682896966519
need align? ->  True 0.07579682896966519
2023-08-05 11:38:35,638 - epoch:6, training loss:0.2348 validation loss:0.0759
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07744418837777946 0.07489288322951483
need align? ->  True 0.07489288322951483
2023-08-05 11:39:07,266 - epoch:7, training loss:0.2241 validation loss:0.0774
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07620377198833486 0.07586014546130014
need align? ->  True 0.07489288322951483
2023-08-05 11:39:39,043 - epoch:8, training loss:0.2159 validation loss:0.0762
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07623146134226219 0.0754573780881322
need align? ->  True 0.07489288322951483
2023-08-05 11:40:11,089 - epoch:9, training loss:0.2131 validation loss:0.0762
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07658971020061037 0.07614678913808387
need align? ->  True 0.07489288322951483
2023-08-05 11:40:45,152 - epoch:10, training loss:0.2113 validation loss:0.0766
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07596882967197377 0.07539906781976638
need align? ->  True 0.07489288322951483
2023-08-05 11:41:18,965 - epoch:11, training loss:0.2091 validation loss:0.0760
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.0761482590890449 0.07487327961818031
need align? ->  True 0.07487327961818031
2023-08-05 11:41:51,753 - epoch:12, training loss:0.2080 validation loss:0.0761
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07578610920387766 0.07535280578810236
need align? ->  True 0.07487327961818031
2023-08-05 11:42:23,576 - epoch:13, training loss:0.1998 validation loss:0.0758
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07616754694153434 0.07491408822977025
need align? ->  True 0.07487327961818031
2023-08-05 11:42:55,428 - epoch:14, training loss:0.1970 validation loss:0.0762
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07542162736796815 0.07491603574675063
need align? ->  True 0.07487327961818031
2023-08-05 11:43:28,136 - epoch:15, training loss:0.1955 validation loss:0.0754
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07545007182204205 0.07458089988516725
need align? ->  True 0.07458089988516725
2023-08-05 11:44:01,574 - epoch:16, training loss:0.1944 validation loss:0.0755
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.0754655554242756 0.07450418749257275
need align? ->  True 0.07450418749257275
2023-08-05 11:44:34,730 - epoch:17, training loss:0.1924 validation loss:0.0755
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07549506231494572 0.07433902666620586
need align? ->  True 0.07433902666620586
2023-08-05 11:45:07,634 - epoch:18, training loss:0.1908 validation loss:0.0755
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07542483767737514 0.07440969185984653
need align? ->  True 0.07433902666620586
2023-08-05 11:45:40,605 - epoch:19, training loss:0.1898 validation loss:0.0754
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07546634810126346 0.07444320772976978
need align? ->  True 0.07433902666620586
2023-08-05 11:46:12,144 - epoch:20, training loss:0.1893 validation loss:0.0755
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07511094925196274 0.07434955530840417
need align? ->  True 0.07433902666620586
2023-08-05 11:46:44,042 - epoch:21, training loss:0.1881 validation loss:0.0751
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.0751207719516495 0.0739822126115146
need align? ->  True 0.0739822126115146
2023-08-05 11:47:17,263 - epoch:22, training loss:0.1872 validation loss:0.0751
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07496274886247904 0.07394344335340935
need align? ->  True 0.07394344335340935
2023-08-05 11:47:51,821 - epoch:23, training loss:0.1867 validation loss:0.0750
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07512513418560443 0.07392218684696633
need align? ->  True 0.07392218684696633
2023-08-05 11:48:25,722 - epoch:24, training loss:0.1867 validation loss:0.0751
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07500887176264888 0.07398520320977854
need align? ->  True 0.07392218684696633
2023-08-05 11:48:57,803 - epoch:25, training loss:0.1860 validation loss:0.0750
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07511313842690509 0.07393052145514799
need align? ->  True 0.07392218684696633
2023-08-05 11:49:29,558 - epoch:26, training loss:0.1854 validation loss:0.0751
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07508134509882201 0.07393902208170165
need align? ->  True 0.07392218684696633
2023-08-05 11:50:01,999 - epoch:27, training loss:0.1860 validation loss:0.0751
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.0750956837411808 0.07394631563321404
need align? ->  True 0.07392218684696633
2023-08-05 11:50:35,475 - epoch:28, training loss:0.1857 validation loss:0.0751
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07506855172307594 0.07393142192260078
need align? ->  True 0.07392218684696633
2023-08-05 11:51:09,934 - epoch:29, training loss:0.1850 validation loss:0.0751
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-11:34:42.334287/0/0.075_epoch_23.pkl  &  0.07392218684696633
2023-08-05 11:51:12,989 - [*] loss:0.1593
2023-08-05 11:51:12,990 - [*] phase 0, testing
2023-08-05 11:51:13,000 - T:24	MAE	0.255054	RMSE	0.161522	MAPE	113.792372
2023-08-05 11:51:13,001 - 24	mae	0.2551	
2023-08-05 11:51:13,001 - 24	rmse	0.1615	
2023-08-05 11:51:13,001 - 24	mape	113.7924	
2023-08-05 11:51:16,239 - [*] loss:0.1574
2023-08-05 11:51:16,241 - [*] phase 0, testing
2023-08-05 11:51:16,252 - T:24	MAE	0.251604	RMSE	0.159716	MAPE	113.677394
2023-08-05 11:51:16,252 - 24	mae	0.2516	
2023-08-05 11:51:16,252 - 24	rmse	0.1597	
2023-08-05 11:51:16,252 - 24	mape	113.6774	
2023-08-05 11:51:18,668 - logger name:exp/ECL-PatchTST2023-08-05-11:51:18.667961/ECL-PatchTST.log
2023-08-05 11:51:18,668 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 5.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-11:51:18.667961', 'path': 'exp/ECL-PatchTST2023-08-05-11:51:18.667961', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 11:51:18,668 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 11:51:18,939 - [*] phase 0 Dataset load!
2023-08-05 11:51:20,041 - [*] phase 0 Training start
train 8281
2023-08-05 11:51:43,765 - epoch:0, training loss:0.2000 validation loss:0.1367
train 8281
vs, vt 0.13673778509964113 0.1439287234907565
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.09904912486672401 0.11183329979362695
need align? ->  False 0.11183329979362695
2023-08-05 11:52:29,405 - epoch:1, training loss:0.7508 validation loss:0.0990
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08314881749127222 0.09045706333025642
need align? ->  False 0.09045706333025642
2023-08-05 11:53:02,240 - epoch:2, training loss:0.6310 validation loss:0.0831
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07915081909817198 0.08058798345534698
need align? ->  False 0.08058798345534698
2023-08-05 11:53:34,448 - epoch:3, training loss:0.5191 validation loss:0.0792
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07727670750540236 0.07702324409847675
need align? ->  True 0.07702324409847675
2023-08-05 11:54:08,280 - epoch:4, training loss:0.4212 validation loss:0.0773
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.0763759940214779 0.07600233347519585
need align? ->  True 0.07600233347519585
2023-08-05 11:54:41,514 - epoch:5, training loss:0.3607 validation loss:0.0764
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07716997558979885 0.07635145261883736
need align? ->  True 0.07600233347519585
2023-08-05 11:55:15,012 - epoch:6, training loss:0.3251 validation loss:0.0772
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07624677965498489 0.07642407877289731
need align? ->  True 0.07600233347519585
2023-08-05 11:55:47,820 - epoch:7, training loss:0.3185 validation loss:0.0762
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07640271706749564 0.07561212033033371
need align? ->  True 0.07561212033033371
2023-08-05 11:56:21,022 - epoch:8, training loss:0.3137 validation loss:0.0764
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07800961318223373 0.075469304928961
need align? ->  True 0.075469304928961
2023-08-05 11:56:54,055 - epoch:9, training loss:0.2868 validation loss:0.0780
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07689149442898191 0.07584973448968452
need align? ->  True 0.075469304928961
2023-08-05 11:57:27,065 - epoch:10, training loss:0.2814 validation loss:0.0769
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.0760535336221042 0.07597480916782566
need align? ->  True 0.075469304928961
2023-08-05 11:58:00,237 - epoch:11, training loss:0.2781 validation loss:0.0761
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.0757489457078602 0.0749668528366348
need align? ->  True 0.0749668528366348
2023-08-05 11:58:33,279 - epoch:12, training loss:0.2747 validation loss:0.0757
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07542444336349549 0.07470930408200492
need align? ->  True 0.07470930408200492
2023-08-05 11:59:06,544 - epoch:13, training loss:0.2677 validation loss:0.0754
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07582742283525674 0.07477649171715198
need align? ->  True 0.07470930408200492
2023-08-05 11:59:40,136 - epoch:14, training loss:0.2647 validation loss:0.0758
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07577005190693814 0.07473205289115077
need align? ->  True 0.07470930408200492
2023-08-05 12:00:13,279 - epoch:15, training loss:0.2630 validation loss:0.0758
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07621749150364296 0.07464243665985439
need align? ->  True 0.07464243665985439
2023-08-05 12:00:46,898 - epoch:16, training loss:0.2608 validation loss:0.0762
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07551936003501 0.07456587716613126
need align? ->  True 0.07456587716613126
2023-08-05 12:01:20,575 - epoch:17, training loss:0.2580 validation loss:0.0755
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07566486355727134 0.07439504021211811
need align? ->  True 0.07439504021211811
2023-08-05 12:01:53,737 - epoch:18, training loss:0.2571 validation loss:0.0757
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07523754352460736 0.07448519288521746
need align? ->  True 0.07439504021211811
2023-08-05 12:02:26,778 - epoch:19, training loss:0.2554 validation loss:0.0752
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07542997436678928 0.07426388156802757
need align? ->  True 0.07426388156802757
2023-08-05 12:03:00,264 - epoch:20, training loss:0.2541 validation loss:0.0754
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07508134469389915 0.07433633676365665
need align? ->  True 0.07426388156802757
2023-08-05 12:03:33,138 - epoch:21, training loss:0.2531 validation loss:0.0751
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07529935759046803 0.07408002043223899
need align? ->  True 0.07408002043223899
2023-08-05 12:04:06,892 - epoch:22, training loss:0.2514 validation loss:0.0753
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07514047889929751 0.07408756212047908
need align? ->  True 0.07408002043223899
2023-08-05 12:04:40,123 - epoch:23, training loss:0.2511 validation loss:0.0751
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07510560580893703 0.07409755861305672
need align? ->  True 0.07408002043223899
2023-08-05 12:05:12,889 - epoch:24, training loss:0.2512 validation loss:0.0751
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07510843443805756 0.07403702199783015
need align? ->  True 0.07403702199783015
2023-08-05 12:05:45,316 - epoch:25, training loss:0.2505 validation loss:0.0751
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07514769179017647 0.07399751197384752
need align? ->  True 0.07399751197384752
2023-08-05 12:06:18,166 - epoch:26, training loss:0.2503 validation loss:0.0751
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.0751004999746447 0.07405093296066574
need align? ->  True 0.07399751197384752
2023-08-05 12:06:50,543 - epoch:27, training loss:0.2498 validation loss:0.0751
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07509652913912483 0.07403363902931628
need align? ->  True 0.07399751197384752
2023-08-05 12:07:23,224 - epoch:28, training loss:0.2494 validation loss:0.0751
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07510587532559167 0.07403228682992251
need align? ->  True 0.07399751197384752
2023-08-05 12:07:56,704 - epoch:29, training loss:0.2495 validation loss:0.0751
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-11:51:18.667961/0/0.0751_epoch_21.pkl  &  0.07399751197384752
2023-08-05 12:08:00,383 - [*] loss:0.1596
2023-08-05 12:08:00,384 - [*] phase 0, testing
2023-08-05 12:08:00,393 - T:24	MAE	0.255339	RMSE	0.161877	MAPE	114.102125
2023-08-05 12:08:00,393 - 24	mae	0.2553	
2023-08-05 12:08:00,394 - 24	rmse	0.1619	
2023-08-05 12:08:00,394 - 24	mape	114.1021	
2023-08-05 12:08:03,493 - [*] loss:0.1577
2023-08-05 12:08:03,495 - [*] phase 0, testing
2023-08-05 12:08:03,506 - T:24	MAE	0.251548	RMSE	0.160032	MAPE	113.270533
2023-08-05 12:08:03,507 - 24	mae	0.2515	
2023-08-05 12:08:03,507 - 24	rmse	0.1600	
2023-08-05 12:08:03,507 - 24	mape	113.2705	
2023-08-05 12:08:05,816 - logger name:exp/ECL-PatchTST2023-08-05-12:08:05.816099/ECL-PatchTST.log
2023-08-05 12:08:05,816 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 5.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-12:08:05.816099', 'path': 'exp/ECL-PatchTST2023-08-05-12:08:05.816099', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 12:08:05,817 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 12:08:06,046 - [*] phase 0 Dataset load!
2023-08-05 12:08:07,031 - [*] phase 0 Training start
train 8281
2023-08-05 12:08:28,366 - epoch:0, training loss:0.1993 validation loss:0.1351
train 8281
vs, vt 0.13509844648449318 0.140629466785037
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10054555130393608 0.11028115418942078
need align? ->  False 0.11028115418942078
2023-08-05 12:09:16,133 - epoch:1, training loss:0.7387 validation loss:0.1005
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08386429091510565 0.09054064556308415
need align? ->  False 0.09054064556308415
2023-08-05 12:09:47,371 - epoch:2, training loss:0.6219 validation loss:0.0839
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07897100451847781 0.08089204221639944
need align? ->  False 0.08089204221639944
2023-08-05 12:10:19,495 - epoch:3, training loss:0.5112 validation loss:0.0790
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07762376494381738 0.0771632205856883
need align? ->  True 0.0771632205856883
2023-08-05 12:10:51,112 - epoch:4, training loss:0.4167 validation loss:0.0776
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07659159503553224 0.07677788838096287
need align? ->  False 0.07677788838096287
2023-08-05 12:11:24,987 - epoch:5, training loss:0.3618 validation loss:0.0766
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07603766145589559 0.0758667275633501
need align? ->  True 0.0758667275633501
2023-08-05 12:11:58,961 - epoch:6, training loss:0.3288 validation loss:0.0760
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.0762869986988928 0.0750439299027557
need align? ->  True 0.0750439299027557
2023-08-05 12:12:31,233 - epoch:7, training loss:0.3099 validation loss:0.0763
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.0767100061411443 0.07511310813867528
need align? ->  True 0.0750439299027557
2023-08-05 12:13:02,979 - epoch:8, training loss:0.2999 validation loss:0.0767
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07571854474751846 0.0757733175288076
need align? ->  True 0.0750439299027557
2023-08-05 12:13:35,172 - epoch:9, training loss:0.2960 validation loss:0.0757
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07553977365403072 0.07492805652968261
need align? ->  True 0.07492805652968261
2023-08-05 12:14:07,379 - epoch:10, training loss:0.2925 validation loss:0.0755
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07558347673519798 0.07502614674360855
need align? ->  True 0.07492805652968261
2023-08-05 12:14:40,857 - epoch:11, training loss:0.2790 validation loss:0.0756
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07594209310153256 0.07463025180218012
need align? ->  True 0.07463025180218012
2023-08-05 12:15:14,987 - epoch:12, training loss:0.2753 validation loss:0.0759
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07544104244721972 0.07518290825512099
need align? ->  True 0.07463025180218012
2023-08-05 12:15:48,633 - epoch:13, training loss:0.2691 validation loss:0.0754
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07562907331663629 0.07477051384099152
need align? ->  True 0.07463025180218012
2023-08-05 12:16:21,213 - epoch:14, training loss:0.2665 validation loss:0.0756
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07514927576741447 0.0746178426982268
need align? ->  True 0.0746178426982268
2023-08-05 12:16:52,989 - epoch:15, training loss:0.2640 validation loss:0.0751
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07538090233245621 0.07426966937339824
need align? ->  True 0.07426966937339824
2023-08-05 12:17:26,161 - epoch:16, training loss:0.2599 validation loss:0.0754
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07532304663049139 0.07423943386453649
need align? ->  True 0.07423943386453649
2023-08-05 12:18:00,084 - epoch:17, training loss:0.2586 validation loss:0.0753
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07524562902424646 0.07446099817752838
need align? ->  True 0.07423943386453649
2023-08-05 12:18:34,423 - epoch:18, training loss:0.2569 validation loss:0.0752
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07514406238561092 0.07433993523211582
need align? ->  True 0.07423943386453649
2023-08-05 12:19:07,547 - epoch:19, training loss:0.2552 validation loss:0.0751
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07493159620334273 0.07430500626240087
need align? ->  True 0.07423943386453649
2023-08-05 12:19:39,829 - epoch:20, training loss:0.2545 validation loss:0.0749
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07483884789373564 0.07409586749323037
need align? ->  True 0.07409586749323037
2023-08-05 12:20:11,643 - epoch:21, training loss:0.2537 validation loss:0.0748
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07479501110704048 0.07397094053094802
need align? ->  True 0.07397094053094802
2023-08-05 12:20:44,038 - epoch:22, training loss:0.2515 validation loss:0.0748
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07477802596986294 0.07381179199918457
need align? ->  True 0.07381179199918457
2023-08-05 12:21:17,622 - epoch:23, training loss:0.2510 validation loss:0.0748
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07484933993090755 0.07382885480056638
need align? ->  True 0.07381179199918457
2023-08-05 12:21:51,094 - epoch:24, training loss:0.2507 validation loss:0.0748
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07487353050838345 0.07391219245998756
need align? ->  True 0.07381179199918457
2023-08-05 12:22:24,496 - epoch:25, training loss:0.2510 validation loss:0.0749
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07474020568896896 0.07385014248606951
need align? ->  True 0.07381179199918457
2023-08-05 12:22:56,368 - epoch:26, training loss:0.2502 validation loss:0.0747
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07479151095385136 0.07379352649592835
need align? ->  True 0.07379352649592835
2023-08-05 12:23:28,248 - epoch:27, training loss:0.2499 validation loss:0.0748
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07478533085921536 0.07380804014594658
need align? ->  True 0.07379352649592835
2023-08-05 12:24:00,873 - epoch:28, training loss:0.2496 validation loss:0.0748
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.0747690109293098 0.07379909831544627
need align? ->  True 0.07379352649592835
2023-08-05 12:24:33,462 - epoch:29, training loss:0.2496 validation loss:0.0748
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-12:08:05.816099/0/0.0747_epoch_26.pkl  &  0.07379352649592835
2023-08-05 12:24:36,578 - [*] loss:0.1588
2023-08-05 12:24:36,579 - [*] phase 0, testing
2023-08-05 12:24:36,589 - T:24	MAE	0.254533	RMSE	0.161079	MAPE	113.969326
2023-08-05 12:24:36,589 - 24	mae	0.2545	
2023-08-05 12:24:36,589 - 24	rmse	0.1611	
2023-08-05 12:24:36,589 - 24	mape	113.9693	
2023-08-05 12:24:39,780 - [*] loss:0.1571
2023-08-05 12:24:39,781 - [*] phase 0, testing
2023-08-05 12:24:39,790 - T:24	MAE	0.251308	RMSE	0.159378	MAPE	113.454628
2023-08-05 12:24:39,790 - 24	mae	0.2513	
2023-08-05 12:24:39,791 - 24	rmse	0.1594	
2023-08-05 12:24:39,791 - 24	mape	113.4546	
2023-08-05 12:24:42,122 - logger name:exp/ECL-PatchTST2023-08-05-12:24:42.122434/ECL-PatchTST.log
2023-08-05 12:24:42,123 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 5.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-12:24:42.122434', 'path': 'exp/ECL-PatchTST2023-08-05-12:24:42.122434', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 12:24:42,123 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 12:24:42,326 - [*] phase 0 Dataset load!
2023-08-05 12:24:43,307 - [*] phase 0 Training start
train 8281
2023-08-05 12:25:06,539 - epoch:0, training loss:0.1973 validation loss:0.1336
train 8281
vs, vt 0.13359046646434328 0.13925113528966904
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10014858906683714 0.1105099884874147
need align? ->  False 0.1105099884874147
2023-08-05 12:25:51,264 - epoch:1, training loss:0.7424 validation loss:0.1001
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08370126127872778 0.09070893547133259
need align? ->  False 0.09070893547133259
2023-08-05 12:26:25,166 - epoch:2, training loss:0.6238 validation loss:0.0837
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07938078718016976 0.08077213265325713
need align? ->  False 0.08077213265325713
2023-08-05 12:26:57,818 - epoch:3, training loss:0.5125 validation loss:0.0794
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07664997713721317 0.07733152134586936
need align? ->  False 0.07733152134586936
2023-08-05 12:27:30,936 - epoch:4, training loss:0.4116 validation loss:0.0766
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07719367264729479 0.07639125677878442
need align? ->  True 0.07639125677878442
2023-08-05 12:28:04,272 - epoch:5, training loss:0.3537 validation loss:0.0772
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07598473706647106 0.0759428137022516
need align? ->  True 0.0759428137022516
2023-08-05 12:28:37,116 - epoch:6, training loss:0.3225 validation loss:0.0760
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.0777209825489832 0.0748538033955771
need align? ->  True 0.0748538033955771
2023-08-05 12:29:09,911 - epoch:7, training loss:0.3060 validation loss:0.0777
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07640075335360091 0.07601527375695498
need align? ->  True 0.0748538033955771
2023-08-05 12:29:42,115 - epoch:8, training loss:0.2925 validation loss:0.0764
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07634815201163292 0.07566220807316511
need align? ->  True 0.0748538033955771
2023-08-05 12:30:15,249 - epoch:9, training loss:0.2892 validation loss:0.0763
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.0767048841909222 0.07619952791087005
need align? ->  True 0.0748538033955771
2023-08-05 12:30:47,902 - epoch:10, training loss:0.2858 validation loss:0.0767
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07605586954109046 0.07552499194508014
need align? ->  True 0.0748538033955771
2023-08-05 12:31:21,310 - epoch:11, training loss:0.2832 validation loss:0.0761
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.0762678844773251 0.07492391562656216
need align? ->  True 0.0748538033955771
2023-08-05 12:31:53,231 - epoch:12, training loss:0.2810 validation loss:0.0763
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07574542563246645 0.07539352280614169
need align? ->  True 0.0748538033955771
2023-08-05 12:32:26,131 - epoch:13, training loss:0.2788 validation loss:0.0757
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07625793633253677 0.07481810086123321
need align? ->  True 0.07481810086123321
2023-08-05 12:32:59,021 - epoch:14, training loss:0.2770 validation loss:0.0763
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07547711200364259 0.07475930233688458
need align? ->  True 0.07475930233688458
2023-08-05 12:33:31,296 - epoch:15, training loss:0.2634 validation loss:0.0755
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07547744185380313 0.07453922919281152
need align? ->  True 0.07453922919281152
2023-08-05 12:34:03,141 - epoch:16, training loss:0.2572 validation loss:0.0755
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07542693614959717 0.07445312064626942
need align? ->  True 0.07445312064626942
2023-08-05 12:34:35,970 - epoch:17, training loss:0.2562 validation loss:0.0754
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.0754934929959152 0.07430941894974398
need align? ->  True 0.07430941894974398
2023-08-05 12:35:09,022 - epoch:18, training loss:0.2533 validation loss:0.0755
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07546510482611864 0.07443903176032979
need align? ->  True 0.07430941894974398
2023-08-05 12:35:41,705 - epoch:19, training loss:0.2523 validation loss:0.0755
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07547734110899594 0.07445453820021256
need align? ->  True 0.07430941894974398
2023-08-05 12:36:13,957 - epoch:20, training loss:0.2514 validation loss:0.0755
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07508444154392117 0.07433967737723952
need align? ->  True 0.07430941894974398
2023-08-05 12:36:46,331 - epoch:21, training loss:0.2500 validation loss:0.0751
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07512927031063515 0.07397501899496368
need align? ->  True 0.07397501899496368
2023-08-05 12:37:19,726 - epoch:22, training loss:0.2492 validation loss:0.0751
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07495180679404218 0.07396416965386142
need align? ->  True 0.07396416965386142
2023-08-05 12:37:52,976 - epoch:23, training loss:0.2488 validation loss:0.0750
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07511898996713369 0.07388905419603638
need align? ->  True 0.07388905419603638
2023-08-05 12:38:25,148 - epoch:24, training loss:0.2488 validation loss:0.0751
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07500160987610402 0.07396374059760052
need align? ->  True 0.07388905419603638
2023-08-05 12:38:58,177 - epoch:25, training loss:0.2474 validation loss:0.0750
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07511116936802864 0.07390226861057074
need align? ->  True 0.07388905419603638
2023-08-05 12:39:30,907 - epoch:26, training loss:0.2465 validation loss:0.0751
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07508480451677156 0.07390920137581618
need align? ->  True 0.07388905419603638
2023-08-05 12:40:04,052 - epoch:27, training loss:0.2472 validation loss:0.0751
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07509637599729974 0.07391895094643468
need align? ->  True 0.07388905419603638
2023-08-05 12:40:37,006 - epoch:28, training loss:0.2469 validation loss:0.0751
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07506250120375467 0.07390431816811147
need align? ->  True 0.07388905419603638
2023-08-05 12:41:09,636 - epoch:29, training loss:0.2460 validation loss:0.0751
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-12:24:42.122434/0/0.075_epoch_23.pkl  &  0.07388905419603638
2023-08-05 12:41:12,969 - [*] loss:0.1592
2023-08-05 12:41:12,970 - [*] phase 0, testing
2023-08-05 12:41:12,979 - T:24	MAE	0.255092	RMSE	0.161421	MAPE	113.994670
2023-08-05 12:41:12,980 - 24	mae	0.2551	
2023-08-05 12:41:12,980 - 24	rmse	0.1614	
2023-08-05 12:41:12,980 - 24	mape	113.9947	
2023-08-05 12:41:16,740 - [*] loss:0.1573
2023-08-05 12:41:16,741 - [*] phase 0, testing
2023-08-05 12:41:16,751 - T:24	MAE	0.251528	RMSE	0.159636	MAPE	113.636744
2023-08-05 12:41:16,751 - 24	mae	0.2515	
2023-08-05 12:41:16,751 - 24	rmse	0.1596	
2023-08-05 12:41:16,751 - 24	mape	113.6367	
2023-08-05 12:41:19,465 - logger name:exp/ECL-PatchTST2023-08-05-12:41:19.465276/ECL-PatchTST.log
2023-08-05 12:41:19,466 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 7.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-12:41:19.465276', 'path': 'exp/ECL-PatchTST2023-08-05-12:41:19.465276', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 12:41:19,466 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 12:41:19,713 - [*] phase 0 Dataset load!
2023-08-05 12:41:20,806 - [*] phase 0 Training start
train 8281
2023-08-05 12:41:42,524 - epoch:0, training loss:0.2000 validation loss:0.1367
train 8281
vs, vt 0.13673778509964113 0.1439287234907565
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.09924128319582214 0.11183329979362695
need align? ->  False 0.11183329979362695
2023-08-05 12:42:30,404 - epoch:1, training loss:0.9818 validation loss:0.0992
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08328156262312246 0.09047920443117619
need align? ->  False 0.09047920443117619
2023-08-05 12:43:04,638 - epoch:2, training loss:0.8296 validation loss:0.0833
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07922452048439046 0.0807161868914314
need align? ->  False 0.0807161868914314
2023-08-05 12:43:39,046 - epoch:3, training loss:0.6803 validation loss:0.0792
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07733964911945489 0.07701778338979119
need align? ->  True 0.07701778338979119
2023-08-05 12:44:11,999 - epoch:4, training loss:0.5461 validation loss:0.0773
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.0764128757721704 0.07602585973623006
need align? ->  True 0.07602585973623006
2023-08-05 12:44:43,770 - epoch:5, training loss:0.4631 validation loss:0.0764
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07724573592776837 0.07642136473694573
need align? ->  True 0.07602585973623006
2023-08-05 12:45:16,940 - epoch:6, training loss:0.4137 validation loss:0.0772
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07626833742403466 0.07651998661458492
need align? ->  True 0.07602585973623006
2023-08-05 12:45:51,172 - epoch:7, training loss:0.4057 validation loss:0.0763
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.0765253733843565 0.07565966191823068
need align? ->  True 0.07565966191823068
2023-08-05 12:46:24,773 - epoch:8, training loss:0.3991 validation loss:0.0765
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07798918174660724 0.0755630887882865
need align? ->  True 0.0755630887882865
2023-08-05 12:46:56,609 - epoch:9, training loss:0.3618 validation loss:0.0780
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07698184778184994 0.07576595611222413
need align? ->  True 0.0755630887882865
2023-08-05 12:47:27,870 - epoch:10, training loss:0.3542 validation loss:0.0770
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07609084191853585 0.07595011421843716
need align? ->  True 0.0755630887882865
2023-08-05 12:47:59,774 - epoch:11, training loss:0.3500 validation loss:0.0761
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07581751388700111 0.07497816233207351
need align? ->  True 0.07497816233207351
2023-08-05 12:48:33,725 - epoch:12, training loss:0.3459 validation loss:0.0758
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07545178343096505 0.07478576849984087
need align? ->  True 0.07478576849984087
2023-08-05 12:49:10,144 - epoch:13, training loss:0.3357 validation loss:0.0755
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07587538983510889 0.07479724182706812
need align? ->  True 0.07478576849984087
2023-08-05 12:49:45,362 - epoch:14, training loss:0.3319 validation loss:0.0759
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.0758292146998903 0.07485121367094309
need align? ->  True 0.07478576849984087
2023-08-05 12:50:17,636 - epoch:15, training loss:0.3295 validation loss:0.0758
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07628834976450256 0.07465688244480154
need align? ->  True 0.07465688244480154
2023-08-05 12:50:50,292 - epoch:16, training loss:0.3267 validation loss:0.0763
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07557488289540229 0.07454140573416067
need align? ->  True 0.07454140573416067
2023-08-05 12:51:25,153 - epoch:17, training loss:0.3225 validation loss:0.0756
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07574182562530041 0.07443751378551773
need align? ->  True 0.07443751378551773
2023-08-05 12:52:00,034 - epoch:18, training loss:0.3214 validation loss:0.0757
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07526648214653782 0.07451967141874459
need align? ->  True 0.07443751378551773
2023-08-05 12:52:39,846 - epoch:19, training loss:0.3193 validation loss:0.0753
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07543604691391406 0.07429604710120222
need align? ->  True 0.07429604710120222
2023-08-05 12:53:16,501 - epoch:20, training loss:0.3176 validation loss:0.0754
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07511848543325196 0.07435050323281599
need align? ->  True 0.07429604710120222
2023-08-05 12:53:49,626 - epoch:21, training loss:0.3162 validation loss:0.0751
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07533244011194809 0.07410351729587368
need align? ->  True 0.07410351729587368
2023-08-05 12:54:21,844 - epoch:22, training loss:0.3141 validation loss:0.0753
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07515087845208852 0.0741094618063906
need align? ->  True 0.07410351729587368
2023-08-05 12:54:54,352 - epoch:23, training loss:0.3133 validation loss:0.0752
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07511694591654383 0.07412947009762992
need align? ->  True 0.07410351729587368
2023-08-05 12:55:27,863 - epoch:24, training loss:0.3134 validation loss:0.0751
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07510514371097088 0.07405546526222126
need align? ->  True 0.07405546526222126
2023-08-05 12:56:01,719 - epoch:25, training loss:0.3127 validation loss:0.0751
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07516714234067046 0.07402366594127986
need align? ->  True 0.07402366594127986
2023-08-05 12:56:35,573 - epoch:26, training loss:0.3126 validation loss:0.0752
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07510454612581627 0.07407658215126266
need align? ->  True 0.07402366594127986
2023-08-05 12:57:08,448 - epoch:27, training loss:0.3119 validation loss:0.0751
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07511367956581323 0.07406085259888483
need align? ->  True 0.07402366594127986
2023-08-05 12:57:39,906 - epoch:28, training loss:0.3113 validation loss:0.0751
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07512571079575497 0.07405732563980248
need align? ->  True 0.07402366594127986
2023-08-05 12:58:11,701 - epoch:29, training loss:0.3114 validation loss:0.0751
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-12:41:19.465276/0/0.0751_epoch_27.pkl  &  0.07402366594127986
2023-08-05 12:58:14,762 - [*] loss:0.1597
2023-08-05 12:58:14,763 - [*] phase 0, testing
2023-08-05 12:58:14,772 - T:24	MAE	0.255080	RMSE	0.161961	MAPE	114.143515
2023-08-05 12:58:14,772 - 24	mae	0.2551	
2023-08-05 12:58:14,772 - 24	rmse	0.1620	
2023-08-05 12:58:14,772 - 24	mape	114.1435	
2023-08-05 12:58:17,824 - [*] loss:0.1577
2023-08-05 12:58:17,826 - [*] phase 0, testing
2023-08-05 12:58:17,836 - T:24	MAE	0.251604	RMSE	0.160065	MAPE	113.283563
2023-08-05 12:58:17,837 - 24	mae	0.2516	
2023-08-05 12:58:17,837 - 24	rmse	0.1601	
2023-08-05 12:58:17,837 - 24	mape	113.2836	
2023-08-05 12:58:20,423 - logger name:exp/ECL-PatchTST2023-08-05-12:58:20.422692/ECL-PatchTST.log
2023-08-05 12:58:20,423 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 7.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-12:58:20.422692', 'path': 'exp/ECL-PatchTST2023-08-05-12:58:20.422692', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 12:58:20,423 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 12:58:20,653 - [*] phase 0 Dataset load!
2023-08-05 12:58:21,663 - [*] phase 0 Training start
train 8281
2023-08-05 12:58:44,380 - epoch:0, training loss:0.1993 validation loss:0.1351
train 8281
vs, vt 0.13509844648449318 0.140629466785037
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10069494046594786 0.11028115418942078
need align? ->  False 0.11028115418942078
2023-08-05 12:59:29,029 - epoch:1, training loss:0.9650 validation loss:0.1007
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08398831708599692 0.09056909155586491
need align? ->  False 0.09056909155586491
2023-08-05 13:00:02,110 - epoch:2, training loss:0.8161 validation loss:0.0840
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07905761857071648 0.08090070910427881
need align? ->  False 0.08090070910427881
2023-08-05 13:00:36,042 - epoch:3, training loss:0.6686 validation loss:0.0791
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07769421164108359 0.07717077191109242
need align? ->  True 0.07717077191109242
2023-08-05 13:01:09,413 - epoch:4, training loss:0.5392 validation loss:0.0777
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07668347033145635 0.07684479961576669
need align? ->  False 0.07684479961576669
2023-08-05 13:01:41,002 - epoch:5, training loss:0.4637 validation loss:0.0767
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07612933736780415 0.07595654439343058
need align? ->  True 0.07595654439343058
2023-08-05 13:02:13,939 - epoch:6, training loss:0.4191 validation loss:0.0761
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07642108623100363 0.07500075710856396
need align? ->  True 0.07500075710856396
2023-08-05 13:02:46,417 - epoch:7, training loss:0.3939 validation loss:0.0764
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07679811824599038 0.07514632315091464
need align? ->  True 0.07500075710856396
2023-08-05 13:03:19,043 - epoch:8, training loss:0.3789 validation loss:0.0768
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07584051917428555 0.07566653240634047
need align? ->  True 0.07500075710856396
2023-08-05 13:03:53,736 - epoch:9, training loss:0.3737 validation loss:0.0758
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07562706012116827 0.07509072197844154
need align? ->  True 0.07500075710856396
2023-08-05 13:04:27,368 - epoch:10, training loss:0.3692 validation loss:0.0756
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07583348655506321 0.07509116261549618
need align? ->  True 0.07500075710856396
2023-08-05 13:04:59,526 - epoch:11, training loss:0.3652 validation loss:0.0758
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07590695041353288 0.07468120267857677
need align? ->  True 0.07468120267857677
2023-08-05 13:05:32,027 - epoch:12, training loss:0.3621 validation loss:0.0759
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07554248258795428 0.07505903522605481
need align? ->  True 0.07468120267857677
2023-08-05 13:06:05,171 - epoch:13, training loss:0.3410 validation loss:0.0755
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07572442673794601 0.07479004569999549
need align? ->  True 0.07468120267857677
2023-08-05 13:06:39,101 - epoch:14, training loss:0.3347 validation loss:0.0757
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.0752242149218269 0.07465905161655467
need align? ->  True 0.07465905161655467
2023-08-05 13:07:13,539 - epoch:15, training loss:0.3309 validation loss:0.0752
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07541945916802986 0.07428558867262758
need align? ->  True 0.07428558867262758
2023-08-05 13:07:46,587 - epoch:16, training loss:0.3257 validation loss:0.0754
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07536374720866265 0.07427498479576214
need align? ->  True 0.07427498479576214
2023-08-05 13:08:18,560 - epoch:17, training loss:0.3236 validation loss:0.0754
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07530752156415711 0.07452687953153382
need align? ->  True 0.07427498479576214
2023-08-05 13:08:51,223 - epoch:18, training loss:0.3214 validation loss:0.0753
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07516205602365991 0.07434284346907036
need align? ->  True 0.07427498479576214
2023-08-05 13:09:23,488 - epoch:19, training loss:0.3192 validation loss:0.0752
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.0749695902771276 0.0743006061276664
need align? ->  True 0.07427498479576214
2023-08-05 13:09:57,185 - epoch:20, training loss:0.3183 validation loss:0.0750
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07487969643071941 0.07410344740618831
need align? ->  True 0.07410344740618831
2023-08-05 13:10:31,315 - epoch:21, training loss:0.3164 validation loss:0.0749
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07482007332146168 0.07399901820589667
need align? ->  True 0.07399901820589667
2023-08-05 13:11:05,947 - epoch:22, training loss:0.3142 validation loss:0.0748
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07480138005769771 0.07382489465500998
need align? ->  True 0.07382489465500998
2023-08-05 13:11:38,895 - epoch:23, training loss:0.3134 validation loss:0.0748
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07488717235948729 0.07385278582248998
need align? ->  True 0.07382489465500998
2023-08-05 13:12:11,268 - epoch:24, training loss:0.3128 validation loss:0.0749
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07491037711177183 0.07393225582073563
need align? ->  True 0.07382489465500998
2023-08-05 13:12:43,420 - epoch:25, training loss:0.3133 validation loss:0.0749
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.0747708939825711 0.07386024174806864
need align? ->  True 0.07382489465500998
2023-08-05 13:13:15,701 - epoch:26, training loss:0.3128 validation loss:0.0748
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07481843905280465 0.07380664437685323
need align? ->  True 0.07380664437685323
2023-08-05 13:13:48,671 - epoch:27, training loss:0.3118 validation loss:0.0748
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.0748159857061894 0.07382276775720327
need align? ->  True 0.07380664437685323
2023-08-05 13:14:23,781 - epoch:28, training loss:0.3114 validation loss:0.0748
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07479960617163907 0.07381242756610332
need align? ->  True 0.07380664437685323
2023-08-05 13:14:57,060 - epoch:29, training loss:0.3113 validation loss:0.0748
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-12:58:20.422692/0/0.0748_epoch_26.pkl  &  0.07380664437685323
2023-08-05 13:15:01,001 - [*] loss:0.1589
2023-08-05 13:15:01,002 - [*] phase 0, testing
2023-08-05 13:15:01,012 - T:24	MAE	0.254622	RMSE	0.161106	MAPE	114.140999
2023-08-05 13:15:01,013 - 24	mae	0.2546	
2023-08-05 13:15:01,013 - 24	rmse	0.1611	
2023-08-05 13:15:01,013 - 24	mape	114.1410	
2023-08-05 13:15:04,329 - [*] loss:0.1571
2023-08-05 13:15:04,331 - [*] phase 0, testing
2023-08-05 13:15:04,341 - T:24	MAE	0.251306	RMSE	0.159406	MAPE	113.525164
2023-08-05 13:15:04,341 - 24	mae	0.2513	
2023-08-05 13:15:04,341 - 24	rmse	0.1594	
2023-08-05 13:15:04,341 - 24	mape	113.5252	
2023-08-05 13:15:06,666 - logger name:exp/ECL-PatchTST2023-08-05-13:15:06.666301/ECL-PatchTST.log
2023-08-05 13:15:06,666 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 7.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-13:15:06.666301', 'path': 'exp/ECL-PatchTST2023-08-05-13:15:06.666301', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 13:15:06,667 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 13:15:06,893 - [*] phase 0 Dataset load!
2023-08-05 13:15:07,998 - [*] phase 0 Training start
train 8281
2023-08-05 13:15:29,407 - epoch:0, training loss:0.1973 validation loss:0.1336
train 8281
vs, vt 0.13359046646434328 0.13925113528966904
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10029392365528189 0.1105099884874147
need align? ->  False 0.1105099884874147
2023-08-05 13:16:16,660 - epoch:1, training loss:0.9707 validation loss:0.1003
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08381932747104893 0.09074774813716827
need align? ->  False 0.09074774813716827
2023-08-05 13:16:48,892 - epoch:2, training loss:0.8195 validation loss:0.0838
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07946933741154878 0.08072198676350324
need align? ->  False 0.08072198676350324
2023-08-05 13:17:20,658 - epoch:3, training loss:0.6710 validation loss:0.0795
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07673970690887907 0.07733467763856701
need align? ->  False 0.07733467763856701
2023-08-05 13:17:53,221 - epoch:4, training loss:0.5332 validation loss:0.0767
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07732320609300034 0.07641165565861308
need align? ->  True 0.07641165565861308
2023-08-05 13:18:26,168 - epoch:5, training loss:0.4530 validation loss:0.0773
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07607019806037778 0.07596672674560029
need align? ->  True 0.07596672674560029
2023-08-05 13:18:59,884 - epoch:6, training loss:0.4106 validation loss:0.0761
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07788385207886281 0.07484323669063009
need align? ->  True 0.07484323669063009
2023-08-05 13:19:34,127 - epoch:7, training loss:0.3872 validation loss:0.0779
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07648274780291578 0.0761813031428534
need align? ->  True 0.07484323669063009
2023-08-05 13:20:07,139 - epoch:8, training loss:0.3694 validation loss:0.0765
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07644677218859611 0.0756811568432528
need align? ->  True 0.07484323669063009
2023-08-05 13:20:39,583 - epoch:9, training loss:0.3647 validation loss:0.0764
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07680609935651654 0.07629088968362498
need align? ->  True 0.07484323669063009
2023-08-05 13:21:11,781 - epoch:10, training loss:0.3613 validation loss:0.0768
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07608905361722344 0.07560123316943645
need align? ->  True 0.07484323669063009
2023-08-05 13:21:44,265 - epoch:11, training loss:0.3572 validation loss:0.0761
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07635024481493494 0.07487515886516674
need align? ->  True 0.07484323669063009
2023-08-05 13:22:17,496 - epoch:12, training loss:0.3546 validation loss:0.0764
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07581421360373497 0.07555692464761112
need align? ->  True 0.07484323669063009
2023-08-05 13:22:51,228 - epoch:13, training loss:0.3516 validation loss:0.0758
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07624609379664711 0.07493256494078947
need align? ->  True 0.07484323669063009
2023-08-05 13:23:24,756 - epoch:14, training loss:0.3495 validation loss:0.0762
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.0756402239203453 0.07474648417985957
need align? ->  True 0.07474648417985957
2023-08-05 13:24:00,402 - epoch:15, training loss:0.3466 validation loss:0.0756
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07556120354844176 0.07450899878597778
need align? ->  True 0.07450899878597778
2023-08-05 13:24:32,474 - epoch:16, training loss:0.3268 validation loss:0.0756
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07546779545752899 0.07448340822821078
need align? ->  True 0.07448340822821078
2023-08-05 13:25:05,768 - epoch:17, training loss:0.3215 validation loss:0.0755
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07553305873728317 0.07427358424857906
need align? ->  True 0.07427358424857906
2023-08-05 13:25:38,513 - epoch:18, training loss:0.3166 validation loss:0.0755
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07549235961683419 0.07440150464358537
need align? ->  True 0.07427358424857906
2023-08-05 13:26:10,702 - epoch:19, training loss:0.3145 validation loss:0.0755
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.0754920942303927 0.07445125095546246
need align? ->  True 0.07427358424857906
2023-08-05 13:26:43,955 - epoch:20, training loss:0.3134 validation loss:0.0755
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07509586830501971 0.07432772154393404
need align? ->  True 0.07427358424857906
2023-08-05 13:27:16,149 - epoch:21, training loss:0.3115 validation loss:0.0751
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07517062686383724 0.07399823249358198
need align? ->  True 0.07399823249358198
2023-08-05 13:27:49,088 - epoch:22, training loss:0.3108 validation loss:0.0752
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07499999374799106 0.07402073429978412
need align? ->  True 0.07399823249358198
2023-08-05 13:28:21,885 - epoch:23, training loss:0.3101 validation loss:0.0750
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07514025829732418 0.07390941258357919
need align? ->  True 0.07390941258357919
2023-08-05 13:28:55,153 - epoch:24, training loss:0.3090 validation loss:0.0751
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07502843285708324 0.07397311950183433
need align? ->  True 0.07390941258357919
2023-08-05 13:29:28,166 - epoch:25, training loss:0.3094 validation loss:0.0750
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07512994628885518 0.07389191983510619
need align? ->  True 0.07389191983510619
2023-08-05 13:30:01,079 - epoch:26, training loss:0.3072 validation loss:0.0751
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.0750981512600961 0.07391004207665505
need align? ->  True 0.07389191983510619
2023-08-05 13:30:33,670 - epoch:27, training loss:0.3082 validation loss:0.0751
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07510638844383799 0.07392045150956382
need align? ->  True 0.07389191983510619
2023-08-05 13:31:06,162 - epoch:28, training loss:0.3080 validation loss:0.0751
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07506603415569534 0.07390464000079942
need align? ->  True 0.07389191983510619
2023-08-05 13:31:39,541 - epoch:29, training loss:0.3071 validation loss:0.0751
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-13:15:06.666301/0/0.075_epoch_23.pkl  &  0.07389191983510619
2023-08-05 13:31:42,683 - [*] loss:0.1593
2023-08-05 13:31:42,684 - [*] phase 0, testing
2023-08-05 13:31:42,695 - T:24	MAE	0.255268	RMSE	0.161458	MAPE	114.046252
2023-08-05 13:31:42,695 - 24	mae	0.2553	
2023-08-05 13:31:42,695 - 24	rmse	0.1615	
2023-08-05 13:31:42,696 - 24	mape	114.0463	
2023-08-05 13:31:46,575 - [*] loss:0.1574
2023-08-05 13:31:46,577 - [*] phase 0, testing
2023-08-05 13:31:46,586 - T:24	MAE	0.251544	RMSE	0.159691	MAPE	113.518322
2023-08-05 13:31:46,586 - 24	mae	0.2515	
2023-08-05 13:31:46,586 - 24	rmse	0.1597	
2023-08-05 13:31:46,587 - 24	mape	113.5183	
2023-08-05 13:31:49,309 - logger name:exp/ECL-PatchTST2023-08-05-13:31:49.309140/ECL-PatchTST.log
2023-08-05 13:31:49,309 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 12.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-13:31:49.309140', 'path': 'exp/ECL-PatchTST2023-08-05-13:31:49.309140', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 13:31:49,309 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 13:31:49,563 - [*] phase 0 Dataset load!
2023-08-05 13:31:50,633 - [*] phase 0 Training start
train 8281
2023-08-05 13:32:12,256 - epoch:0, training loss:0.2000 validation loss:0.1367
train 8281
vs, vt 0.13673778509964113 0.1439287234907565
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.09951254280041093 0.11183329979362695
need align? ->  False 0.11183329979362695
2023-08-05 13:33:00,365 - epoch:1, training loss:1.5592 validation loss:0.0995
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08343916078624518 0.0905877320986727
need align? ->  False 0.0905877320986727
2023-08-05 13:33:34,897 - epoch:2, training loss:1.3266 validation loss:0.0834
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07931338501689227 0.08078563472499019
need align? ->  False 0.08078563472499019
2023-08-05 13:34:08,859 - epoch:3, training loss:1.0836 validation loss:0.0793
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07741366091953672 0.07704984995981921
need align? ->  True 0.07704984995981921
2023-08-05 13:34:44,013 - epoch:4, training loss:0.8594 validation loss:0.0774
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.0764831138531799 0.07597673384715682
need align? ->  True 0.07597673384715682
2023-08-05 13:35:18,744 - epoch:5, training loss:0.7193 validation loss:0.0765
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.0773554488044718 0.07648601204804752
need align? ->  True 0.07597673384715682
2023-08-05 13:35:52,881 - epoch:6, training loss:0.6376 validation loss:0.0774
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07645575677895028 0.0765673698776442
need align? ->  True 0.07597673384715682
2023-08-05 13:36:26,882 - epoch:7, training loss:0.6245 validation loss:0.0765
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07667624245843162 0.0757541088792293
need align? ->  True 0.0757541088792293
2023-08-05 13:37:00,913 - epoch:8, training loss:0.6139 validation loss:0.0767
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.0780662989810757 0.07568639703094959
need align? ->  True 0.07568639703094959
2023-08-05 13:37:35,311 - epoch:9, training loss:0.5492 validation loss:0.0781
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07719949446618557 0.07581362014879352
need align? ->  True 0.07568639703094959
2023-08-05 13:38:09,702 - epoch:10, training loss:0.5357 validation loss:0.0772
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07624138098048128 0.07620593994531942
need align? ->  True 0.07568639703094959
2023-08-05 13:38:42,416 - epoch:11, training loss:0.5287 validation loss:0.0762
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07597039405094541 0.07504466192229935
need align? ->  True 0.07504466192229935
2023-08-05 13:39:16,968 - epoch:12, training loss:0.5227 validation loss:0.0760
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07555742610407912 0.0748713336399068
need align? ->  True 0.0748713336399068
2023-08-05 13:39:50,527 - epoch:13, training loss:0.5053 validation loss:0.0756
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07599742281372132 0.07482004789230616
need align? ->  True 0.07482004789230616
2023-08-05 13:40:24,411 - epoch:14, training loss:0.4985 validation loss:0.0760
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07585801921137002 0.07493276740221874
need align? ->  True 0.07482004789230616
2023-08-05 13:40:57,956 - epoch:15, training loss:0.4938 validation loss:0.0759
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07623093656223753 0.0746717844158411
need align? ->  True 0.0746717844158411
2023-08-05 13:41:31,439 - epoch:16, training loss:0.4891 validation loss:0.0762
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07564965230615242 0.07458912897045197
need align? ->  True 0.07458912897045197
2023-08-05 13:42:05,112 - epoch:17, training loss:0.4848 validation loss:0.0756
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07590375573414823 0.07445803811044796
need align? ->  True 0.07445803811044796
2023-08-05 13:42:39,739 - epoch:18, training loss:0.4826 validation loss:0.0759
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07533961364432522 0.07459997086097366
need align? ->  True 0.07445803811044796
2023-08-05 13:43:13,731 - epoch:19, training loss:0.4806 validation loss:0.0753
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07544556932280892 0.07431667111814022
need align? ->  True 0.07431667111814022
2023-08-05 13:43:46,131 - epoch:20, training loss:0.4776 validation loss:0.0754
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.0751771250658709 0.07436697687143864
need align? ->  True 0.07431667111814022
2023-08-05 13:44:17,870 - epoch:21, training loss:0.4744 validation loss:0.0752
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07540227237926878 0.07418216319511765
need align? ->  True 0.07418216319511765
2023-08-05 13:44:49,934 - epoch:22, training loss:0.4711 validation loss:0.0754
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07524801418185234 0.07420135062673817
need align? ->  True 0.07418216319511765
2023-08-05 13:45:22,242 - epoch:23, training loss:0.4705 validation loss:0.0752
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07517054944258669 0.07419977620567965
need align? ->  True 0.07418216319511765
2023-08-05 13:45:56,354 - epoch:24, training loss:0.4707 validation loss:0.0752
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.0751660142255866 0.07412596322272134
need align? ->  True 0.07412596322272134
2023-08-05 13:46:29,800 - epoch:25, training loss:0.4693 validation loss:0.0752
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.0752734288411296 0.0740752081670191
need align? ->  True 0.0740752081670191
2023-08-05 13:47:02,071 - epoch:26, training loss:0.4687 validation loss:0.0753
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.075160162118466 0.07413175864064175
need align? ->  True 0.0740752081670191
2023-08-05 13:47:33,328 - epoch:27, training loss:0.4669 validation loss:0.0752
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07520179563890332 0.07412233203649521
need align? ->  True 0.0740752081670191
2023-08-05 13:48:04,776 - epoch:28, training loss:0.4673 validation loss:0.0752
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.0752096364841513 0.07411519867246566
need align? ->  True 0.0740752081670191
2023-08-05 13:48:38,252 - epoch:29, training loss:0.4670 validation loss:0.0752
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-13:31:49.309140/0/0.0752_epoch_27.pkl  &  0.0740752081670191
2023-08-05 13:48:41,425 - [*] loss:0.1597
2023-08-05 13:48:41,426 - [*] phase 0, testing
2023-08-05 13:48:41,435 - T:24	MAE	0.255278	RMSE	0.161978	MAPE	114.302647
2023-08-05 13:48:41,435 - 24	mae	0.2553	
2023-08-05 13:48:41,435 - 24	rmse	0.1620	
2023-08-05 13:48:41,435 - 24	mape	114.3026	
2023-08-05 13:48:44,579 - [*] loss:0.1578
2023-08-05 13:48:44,580 - [*] phase 0, testing
2023-08-05 13:48:44,589 - T:24	MAE	0.251671	RMSE	0.160165	MAPE	113.416612
2023-08-05 13:48:44,589 - 24	mae	0.2517	
2023-08-05 13:48:44,589 - 24	rmse	0.1602	
2023-08-05 13:48:44,589 - 24	mape	113.4166	
2023-08-05 13:48:47,142 - logger name:exp/ECL-PatchTST2023-08-05-13:48:47.141710/ECL-PatchTST.log
2023-08-05 13:48:47,142 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 12.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-13:48:47.141710', 'path': 'exp/ECL-PatchTST2023-08-05-13:48:47.141710', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 13:48:47,142 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 13:48:47,419 - [*] phase 0 Dataset load!
2023-08-05 13:48:48,486 - [*] phase 0 Training start
train 8281
2023-08-05 13:49:12,029 - epoch:0, training loss:0.1993 validation loss:0.1351
train 8281
vs, vt 0.13509844648449318 0.140629466785037
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.1009033243779255 0.11028115418942078
need align? ->  False 0.11028115418942078
2023-08-05 13:49:56,955 - epoch:1, training loss:1.5307 validation loss:0.1009
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08415110125813795 0.09061686192517696
need align? ->  False 0.09061686192517696
2023-08-05 13:50:31,098 - epoch:2, training loss:1.3017 validation loss:0.0842
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07917150239581647 0.08096313006851984
need align? ->  False 0.08096313006851984
2023-08-05 13:51:03,924 - epoch:3, training loss:1.0620 validation loss:0.0792
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07778376454244489 0.07715504154886889
need align? ->  True 0.07715504154886889
2023-08-05 13:51:34,820 - epoch:4, training loss:0.8460 validation loss:0.0778
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07683415014458739 0.07686088072217029
need align? ->  False 0.07686088072217029
2023-08-05 13:52:06,838 - epoch:5, training loss:0.7193 validation loss:0.0768
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07628839616866215 0.0760821947099074
need align? ->  True 0.0760821947099074
2023-08-05 13:52:40,184 - epoch:6, training loss:0.6450 validation loss:0.0763
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07663367170354594 0.07499992450618226
need align? ->  True 0.07499992450618226
2023-08-05 13:53:13,498 - epoch:7, training loss:0.6026 validation loss:0.0766
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07697662613962007 0.07527444511651993
need align? ->  True 0.07499992450618226
2023-08-05 13:53:47,241 - epoch:8, training loss:0.5762 validation loss:0.0770
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07609675330636294 0.07565223385134469
need align? ->  True 0.07499992450618226
2023-08-05 13:54:19,358 - epoch:9, training loss:0.5678 validation loss:0.0761
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07580466706143774 0.07529858013857967
need align? ->  True 0.07499992450618226
2023-08-05 13:54:50,786 - epoch:10, training loss:0.5609 validation loss:0.0758
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07602873711806277 0.07524570093854614
need align? ->  True 0.07499992450618226
2023-08-05 13:55:23,206 - epoch:11, training loss:0.5538 validation loss:0.0760
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07610850295294887 0.07476580248254797
need align? ->  True 0.07476580248254797
2023-08-05 13:55:55,026 - epoch:12, training loss:0.5492 validation loss:0.0761
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.0757767358875793 0.07518904267445854
need align? ->  True 0.07476580248254797
2023-08-05 13:56:29,539 - epoch:13, training loss:0.5141 validation loss:0.0758
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07592040081710918 0.07492131027190582
need align? ->  True 0.07476580248254797
2023-08-05 13:57:01,959 - epoch:14, training loss:0.5033 validation loss:0.0759
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07543698916940586 0.07476644396134045
need align? ->  True 0.07476580248254797
2023-08-05 13:57:33,169 - epoch:15, training loss:0.4984 validation loss:0.0754
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07578893216407817 0.0743993676227072
need align? ->  True 0.0743993676227072
2023-08-05 13:58:05,605 - epoch:16, training loss:0.4949 validation loss:0.0758
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07557188282194345 0.0744472652187814
need align? ->  True 0.0743993676227072
2023-08-05 13:58:38,057 - epoch:17, training loss:0.4874 validation loss:0.0756
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.0756255624734837 0.07473163047562474
need align? ->  True 0.0743993676227072
2023-08-05 13:59:11,364 - epoch:18, training loss:0.4831 validation loss:0.0756
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07547539472579956 0.07448782196835331
need align? ->  True 0.0743993676227072
2023-08-05 13:59:45,874 - epoch:19, training loss:0.4801 validation loss:0.0755
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07522452614553597 0.07446578938675963
need align? ->  True 0.0743993676227072
2023-08-05 14:00:19,316 - epoch:20, training loss:0.4787 validation loss:0.0752
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07513184456721596 0.0741927255268978
need align? ->  True 0.0741927255268978
2023-08-05 14:00:42,938 - epoch:21, training loss:0.4763 validation loss:0.0751
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07506098735915578 0.07411033635878045
need align? ->  True 0.07411033635878045
2023-08-05 14:01:03,643 - epoch:22, training loss:0.4714 validation loss:0.0751
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07504052037130231 0.07396298606434594
need align? ->  True 0.07396298606434594
2023-08-05 14:01:24,161 - epoch:23, training loss:0.4702 validation loss:0.0750
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07512561936417352 0.07399250575057838
need align? ->  True 0.07396298606434594
2023-08-05 14:01:45,253 - epoch:24, training loss:0.4691 validation loss:0.0751
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.0751215562062419 0.07408135578684184
need align? ->  True 0.07396298606434594
2023-08-05 14:02:05,611 - epoch:25, training loss:0.4689 validation loss:0.0751
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07498174063537431 0.07396012164004472
need align? ->  True 0.07396012164004472
2023-08-05 14:02:26,415 - epoch:26, training loss:0.4682 validation loss:0.0750
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07502394955119361 0.07391208993351978
need align? ->  True 0.07391208993351978
2023-08-05 14:02:47,372 - epoch:27, training loss:0.4674 validation loss:0.0750
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.0750295102758252 0.07393761035864768
need align? ->  True 0.07391208993351978
2023-08-05 14:03:07,690 - epoch:28, training loss:0.4666 validation loss:0.0750
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07500787897278434 0.07392568610932516
need align? ->  True 0.07391208993351978
2023-08-05 14:03:27,833 - epoch:29, training loss:0.4668 validation loss:0.0750
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-13:48:47.141710/0/0.075_epoch_26.pkl  &  0.07391208993351978
2023-08-05 14:03:29,471 - [*] loss:0.1593
2023-08-05 14:03:29,473 - [*] phase 0, testing
2023-08-05 14:03:29,483 - T:24	MAE	0.255127	RMSE	0.161499	MAPE	114.370978
2023-08-05 14:03:29,483 - 24	mae	0.2551	
2023-08-05 14:03:29,483 - 24	rmse	0.1615	
2023-08-05 14:03:29,483 - 24	mape	114.3710	
2023-08-05 14:03:30,597 - [*] loss:0.1573
2023-08-05 14:03:30,599 - [*] phase 0, testing
2023-08-05 14:03:30,607 - T:24	MAE	0.251546	RMSE	0.159593	MAPE	113.481331
2023-08-05 14:03:30,608 - 24	mae	0.2515	
2023-08-05 14:03:30,608 - 24	rmse	0.1596	
2023-08-05 14:03:30,608 - 24	mape	113.4813	
2023-08-05 14:03:32,763 - logger name:exp/ECL-PatchTST2023-08-05-14:03:32.762695/ECL-PatchTST.log
2023-08-05 14:03:32,763 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 12.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-14:03:32.762695', 'path': 'exp/ECL-PatchTST2023-08-05-14:03:32.762695', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 14:03:32,763 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 14:03:32,966 - [*] phase 0 Dataset load!
2023-08-05 14:03:33,906 - [*] phase 0 Training start
train 8281
2023-08-05 14:03:45,315 - epoch:0, training loss:0.1973 validation loss:0.1336
train 8281
vs, vt 0.13359046646434328 0.13925113528966904
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10049723113036674 0.1105099884874147
need align? ->  False 0.1105099884874147
2023-08-05 14:04:14,579 - epoch:1, training loss:1.5414 validation loss:0.1005
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.083971390824603 0.09080028420557147
need align? ->  False 0.09080028420557147
2023-08-05 14:04:35,076 - epoch:2, training loss:1.3089 validation loss:0.0840
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07957441343561462 0.08067387682588203
need align? ->  False 0.08067387682588203
2023-08-05 14:04:55,853 - epoch:3, training loss:1.0677 validation loss:0.0796
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07686916819733122 0.07730279077330361
need align? ->  False 0.07730279077330361
2023-08-05 14:05:16,143 - epoch:4, training loss:0.8371 validation loss:0.0769
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07752783742287885 0.07636886389683122
need align? ->  True 0.07636886389683122
2023-08-05 14:05:36,106 - epoch:5, training loss:0.7016 validation loss:0.0775
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07626193462182647 0.07598557865814022
need align? ->  True 0.07598557865814022
2023-08-05 14:05:56,739 - epoch:6, training loss:0.6302 validation loss:0.0763
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07832126781020475 0.07506294912942078
need align? ->  True 0.07506294912942078
2023-08-05 14:06:17,446 - epoch:7, training loss:0.5901 validation loss:0.0783
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07678489532807599 0.0762809055816868
need align? ->  True 0.07506294912942078
2023-08-05 14:06:37,673 - epoch:8, training loss:0.5632 validation loss:0.0768
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07662415504455566 0.07581795436208663
need align? ->  True 0.07506294912942078
2023-08-05 14:06:57,925 - epoch:9, training loss:0.5548 validation loss:0.0766
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.0770298124331495 0.0762038342654705
need align? ->  True 0.07506294912942078
2023-08-05 14:07:18,401 - epoch:10, training loss:0.5501 validation loss:0.0770
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07626899493777234 0.07580116731317146
need align? ->  True 0.07506294912942078
2023-08-05 14:07:38,956 - epoch:11, training loss:0.5432 validation loss:0.0763
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07660007687366527 0.07497983223394207
need align? ->  True 0.07497983223394207
2023-08-05 14:07:59,476 - epoch:12, training loss:0.5381 validation loss:0.0766
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07604736656598422 0.07572094448234724
need align? ->  True 0.07497983223394207
2023-08-05 14:08:20,053 - epoch:13, training loss:0.5045 validation loss:0.0760
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07615606462501961 0.07499275097380513
need align? ->  True 0.07497983223394207
2023-08-05 14:08:40,894 - epoch:14, training loss:0.4966 validation loss:0.0762
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07552428600256858 0.0747374850122825
need align? ->  True 0.0747374850122825
2023-08-05 14:09:01,387 - epoch:15, training loss:0.4900 validation loss:0.0755
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07563167664667834 0.07457329575782237
need align? ->  True 0.07457329575782237
2023-08-05 14:09:21,369 - epoch:16, training loss:0.4825 validation loss:0.0756
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07550799376938654 0.07453878110517627
need align? ->  True 0.07453878110517627
2023-08-05 14:09:42,295 - epoch:17, training loss:0.4798 validation loss:0.0755
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.0756204359881256 0.07434391206049401
need align? ->  True 0.07434391206049401
2023-08-05 14:10:02,497 - epoch:18, training loss:0.4702 validation loss:0.0756
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07554230839014053 0.07451764421294564
need align? ->  True 0.07434391206049401
2023-08-05 14:10:22,603 - epoch:19, training loss:0.4687 validation loss:0.0755
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.0755277844550817 0.07450887131626191
need align? ->  True 0.07434391206049401
2023-08-05 14:10:43,652 - epoch:20, training loss:0.4684 validation loss:0.0755
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07510966135431892 0.07441233195688414
need align? ->  True 0.07434391206049401
2023-08-05 14:11:04,451 - epoch:21, training loss:0.4620 validation loss:0.0751
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07523931903035744 0.07412458316463491
need align? ->  True 0.07412458316463491
2023-08-05 14:11:24,666 - epoch:22, training loss:0.4634 validation loss:0.0752
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07503697721530563 0.07411767404688441
need align? ->  True 0.07411767404688441
2023-08-05 14:11:45,614 - epoch:23, training loss:0.4644 validation loss:0.0750
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07517219172871631 0.07398980945024801
need align? ->  True 0.07398980945024801
2023-08-05 14:12:05,698 - epoch:24, training loss:0.4612 validation loss:0.0752
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07503560517469178 0.07404930193139159
need align? ->  True 0.07398980945024801
2023-08-05 14:12:26,602 - epoch:25, training loss:0.4584 validation loss:0.0750
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07511625754768433 0.07394569083724332
need align? ->  True 0.07394569083724332
2023-08-05 14:12:47,119 - epoch:26, training loss:0.4562 validation loss:0.0751
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.0751035187555396 0.07396862915028697
need align? ->  True 0.07394569083724332
2023-08-05 14:13:07,436 - epoch:27, training loss:0.4577 validation loss:0.0751
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07510740415233633 0.073979784856024
need align? ->  True 0.07394569083724332
2023-08-05 14:13:27,795 - epoch:28, training loss:0.4574 validation loss:0.0751
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07506570778787136 0.07395977040995723
need align? ->  True 0.07394569083724332
2023-08-05 14:13:48,025 - epoch:29, training loss:0.4557 validation loss:0.0751
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-14:03:32.762695/0/0.075_epoch_25.pkl  &  0.07394569083724332
2023-08-05 14:13:49,571 - [*] loss:0.1594
2023-08-05 14:13:49,572 - [*] phase 0, testing
2023-08-05 14:13:49,581 - T:24	MAE	0.255117	RMSE	0.161651	MAPE	114.523053
2023-08-05 14:13:49,581 - 24	mae	0.2551	
2023-08-05 14:13:49,581 - 24	rmse	0.1617	
2023-08-05 14:13:49,581 - 24	mape	114.5231	
2023-08-05 14:13:50,882 - [*] loss:0.1575
2023-08-05 14:13:50,883 - [*] phase 0, testing
2023-08-05 14:13:50,893 - T:24	MAE	0.251749	RMSE	0.159787	MAPE	113.433707
2023-08-05 14:13:50,893 - 24	mae	0.2517	
2023-08-05 14:13:50,893 - 24	rmse	0.1598	
2023-08-05 14:13:50,893 - 24	mape	113.4337	
2023-08-05 14:13:52,989 - logger name:exp/ECL-PatchTST2023-08-05-14:13:52.989141/ECL-PatchTST.log
2023-08-05 14:13:52,989 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 15.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-14:13:52.989141', 'path': 'exp/ECL-PatchTST2023-08-05-14:13:52.989141', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 14:13:52,989 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 14:13:53,203 - [*] phase 0 Dataset load!
2023-08-05 14:13:54,162 - [*] phase 0 Training start
train 8281
2023-08-05 14:14:05,737 - epoch:0, training loss:0.2000 validation loss:0.1367
train 8281
vs, vt 0.13673778509964113 0.1439287234907565
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.09960663480603177 0.11183329979362695
need align? ->  False 0.11183329979362695
2023-08-05 14:14:34,669 - epoch:1, training loss:1.9058 validation loss:0.0996
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08349408225520798 0.09059916793004326
need align? ->  False 0.09059916793004326
2023-08-05 14:14:54,691 - epoch:2, training loss:1.6249 validation loss:0.0835
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07933374243262022 0.08079820936140807
need align? ->  False 0.08079820936140807
2023-08-05 14:15:15,616 - epoch:3, training loss:1.3259 validation loss:0.0793
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.0774325848273609 0.07705512853420299
need align? ->  True 0.07705512853420299
2023-08-05 14:15:36,064 - epoch:4, training loss:1.0471 validation loss:0.0774
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07650696680597636 0.07593345245265443
need align? ->  True 0.07593345245265443
2023-08-05 14:15:56,251 - epoch:5, training loss:0.8726 validation loss:0.0765
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.0773880037114672 0.07649518881479035
need align? ->  True 0.07593345245265443
2023-08-05 14:16:16,925 - epoch:6, training loss:0.7714 validation loss:0.0774
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07651228579166143 0.07658953624575035
need align? ->  True 0.07593345245265443
2023-08-05 14:16:37,070 - epoch:7, training loss:0.7554 validation loss:0.0765
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07675495784243812 0.07569205113079237
need align? ->  True 0.07569205113079237
2023-08-05 14:16:58,510 - epoch:8, training loss:0.7423 validation loss:0.0768
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.0782218893746967 0.07570345764574797
need align? ->  True 0.07569205113079237
2023-08-05 14:17:18,322 - epoch:9, training loss:0.6608 validation loss:0.0782
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07754590799627097 0.076025798106971
need align? ->  True 0.07569205113079237
2023-08-05 14:17:38,510 - epoch:10, training loss:0.6477 validation loss:0.0775
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07639679822908796 0.07627465397767398
need align? ->  True 0.07569205113079237
2023-08-05 14:17:59,357 - epoch:11, training loss:0.6402 validation loss:0.0764
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07610428527645442 0.07500517951405566
need align? ->  True 0.07500517951405566
2023-08-05 14:18:20,044 - epoch:12, training loss:0.6334 validation loss:0.0761
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07570329008866912 0.0748567471847586
need align? ->  True 0.0748567471847586
2023-08-05 14:18:40,505 - epoch:13, training loss:0.6103 validation loss:0.0757
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07616592716911565 0.07486087245785672
need align? ->  True 0.0748567471847586
2023-08-05 14:19:01,691 - epoch:14, training loss:0.5994 validation loss:0.0762
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07603107635741649 0.07495267559652743
need align? ->  True 0.0748567471847586
2023-08-05 14:19:21,968 - epoch:15, training loss:0.5966 validation loss:0.0760
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07636349923584772 0.07473311767629955
need align? ->  True 0.07473311767629955
2023-08-05 14:19:42,562 - epoch:16, training loss:0.5881 validation loss:0.0764
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.0757476144834705 0.07463396534971568
need align? ->  True 0.07463396534971568
2023-08-05 14:20:03,787 - epoch:17, training loss:0.5829 validation loss:0.0757
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07600416437439296 0.07450805264322655
need align? ->  True 0.07450805264322655
2023-08-05 14:20:24,058 - epoch:18, training loss:0.5797 validation loss:0.0760
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.0754090941146664 0.07468898513394853
need align? ->  True 0.07450805264322655
2023-08-05 14:20:44,508 - epoch:19, training loss:0.5764 validation loss:0.0754
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07552865212378294 0.07434368659944637
need align? ->  True 0.07434368659944637
2023-08-05 14:21:05,695 - epoch:20, training loss:0.5744 validation loss:0.0755
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07528835373080295 0.07443332866482112
need align? ->  True 0.07434368659944637
2023-08-05 14:21:25,863 - epoch:21, training loss:0.5704 validation loss:0.0753
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07547980652231237 0.07420955177234567
need align? ->  True 0.07420955177234567
2023-08-05 14:21:46,133 - epoch:22, training loss:0.5647 validation loss:0.0755
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07531755109844 0.07426240635306938
need align? ->  True 0.07420955177234567
2023-08-05 14:22:07,137 - epoch:23, training loss:0.5645 validation loss:0.0753
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.0752388319891432 0.07427408270861792
need align? ->  True 0.07420955177234567
2023-08-05 14:22:27,335 - epoch:24, training loss:0.5648 validation loss:0.0752
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07521796169812264 0.07417104903446592
need align? ->  True 0.07417104903446592
2023-08-05 14:22:48,155 - epoch:25, training loss:0.5630 validation loss:0.0752
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07535287276234316 0.07411530281862487
need align? ->  True 0.07411530281862487
2023-08-05 14:23:08,813 - epoch:26, training loss:0.5622 validation loss:0.0754
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07522052429292513 0.07418033051425996
need align? ->  True 0.07411530281862487
2023-08-05 14:23:29,263 - epoch:27, training loss:0.5599 validation loss:0.0752
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07527006919617238 0.07416887330296247
need align? ->  True 0.07411530281862487
2023-08-05 14:23:50,004 - epoch:28, training loss:0.5602 validation loss:0.0753
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07527690623765407 0.07416058242644953
need align? ->  True 0.07411530281862487
2023-08-05 14:24:10,708 - epoch:29, training loss:0.5601 validation loss:0.0753
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-14:13:52.989141/0/0.0752_epoch_25.pkl  &  0.07411530281862487
2023-08-05 14:24:11,950 - [*] loss:0.1599
2023-08-05 14:24:11,951 - [*] phase 0, testing
2023-08-05 14:24:11,960 - T:24	MAE	0.255259	RMSE	0.162232	MAPE	114.484072
2023-08-05 14:24:11,961 - 24	mae	0.2553	
2023-08-05 14:24:11,961 - 24	rmse	0.1622	
2023-08-05 14:24:11,961 - 24	mape	114.4841	
2023-08-05 14:24:13,372 - [*] loss:0.1579
2023-08-05 14:24:13,373 - [*] phase 0, testing
2023-08-05 14:24:13,382 - T:24	MAE	0.251748	RMSE	0.160244	MAPE	113.433480
2023-08-05 14:24:13,382 - 24	mae	0.2517	
2023-08-05 14:24:13,382 - 24	rmse	0.1602	
2023-08-05 14:24:13,382 - 24	mape	113.4335	
2023-08-05 14:24:15,565 - logger name:exp/ECL-PatchTST2023-08-05-14:24:15.564843/ECL-PatchTST.log
2023-08-05 14:24:15,565 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 15.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-14:24:15.564843', 'path': 'exp/ECL-PatchTST2023-08-05-14:24:15.564843', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 14:24:15,565 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 14:24:15,770 - [*] phase 0 Dataset load!
2023-08-05 14:24:16,656 - [*] phase 0 Training start
train 8281
2023-08-05 14:24:28,609 - epoch:0, training loss:0.1993 validation loss:0.1351
train 8281
vs, vt 0.13509844648449318 0.140629466785037
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10097550770834736 0.11028115418942078
need align? ->  False 0.11028115418942078
2023-08-05 14:24:57,007 - epoch:1, training loss:1.8702 validation loss:0.1010
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08420597737574059 0.0906363343415053
need align? ->  False 0.0906363343415053
2023-08-05 14:25:17,367 - epoch:2, training loss:1.5934 validation loss:0.0842
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07919822187851304 0.08097971133563829
need align? ->  False 0.08097971133563829
2023-08-05 14:25:37,584 - epoch:3, training loss:1.2984 validation loss:0.0792
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07781160023549329 0.07715409888845423
need align? ->  True 0.07715409888845423
2023-08-05 14:25:57,582 - epoch:4, training loss:1.0303 validation loss:0.0778
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07688263738932817 0.07690599957562011
need align? ->  False 0.07690599957562011
2023-08-05 14:26:18,755 - epoch:5, training loss:0.8729 validation loss:0.0769
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.0763455706284098 0.07608704058372456
need align? ->  True 0.07608704058372456
2023-08-05 14:26:39,299 - epoch:6, training loss:0.7804 validation loss:0.0763
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07670920245025468 0.07500257610302904
need align? ->  True 0.07500257610302904
2023-08-05 14:27:00,616 - epoch:7, training loss:0.7293 validation loss:0.0767
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07704982890383057 0.0752524412844492
need align? ->  True 0.07500257610302904
2023-08-05 14:27:21,039 - epoch:8, training loss:0.6952 validation loss:0.0770
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07621250115334988 0.07570408786768498
need align? ->  True 0.07500257610302904
2023-08-05 14:27:41,261 - epoch:9, training loss:0.6848 validation loss:0.0762
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07588752261970354 0.07536718597554642
need align? ->  True 0.07500257610302904
2023-08-05 14:28:02,730 - epoch:10, training loss:0.6762 validation loss:0.0759
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07610599056858083 0.07538813911378384
need align? ->  True 0.07500257610302904
2023-08-05 14:28:23,044 - epoch:11, training loss:0.6678 validation loss:0.0761
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07619724017770393 0.07482542704952799
need align? ->  True 0.07482542704952799
2023-08-05 14:28:43,497 - epoch:12, training loss:0.6622 validation loss:0.0762
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07582203347397887 0.07517352902694889
need align? ->  True 0.07482542704952799
2023-08-05 14:29:04,086 - epoch:13, training loss:0.6170 validation loss:0.0758
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07607703939404177 0.07485450439802978
need align? ->  True 0.07482542704952799
2023-08-05 14:29:24,386 - epoch:14, training loss:0.6040 validation loss:0.0761
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07551891089457533 0.07480960501276929
need align? ->  True 0.07480960501276929
2023-08-05 14:29:45,043 - epoch:15, training loss:0.5966 validation loss:0.0755
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07581952716345372 0.07446301663699358
need align? ->  True 0.07446301663699358
2023-08-05 14:30:05,642 - epoch:16, training loss:0.5827 validation loss:0.0758
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07560491075982219 0.07451488604040249
need align? ->  True 0.07446301663699358
2023-08-05 14:30:25,708 - epoch:17, training loss:0.5821 validation loss:0.0756
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07572758262572081 0.0748459322620993
need align? ->  True 0.07446301663699358
2023-08-05 14:30:45,960 - epoch:18, training loss:0.5787 validation loss:0.0757
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07553178112467994 0.07454200088977814
need align? ->  True 0.07446301663699358
2023-08-05 14:31:07,248 - epoch:19, training loss:0.5747 validation loss:0.0755
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07530431101179641 0.07449841993334501
need align? ->  True 0.07446301663699358
2023-08-05 14:31:27,637 - epoch:20, training loss:0.5734 validation loss:0.0753
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07523402158656846 0.07424738930295342
need align? ->  True 0.07424738930295342
2023-08-05 14:31:47,667 - epoch:21, training loss:0.5701 validation loss:0.0752
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07514860586303732 0.07418657895987449
need align? ->  True 0.07418657895987449
2023-08-05 14:32:08,588 - epoch:22, training loss:0.5653 validation loss:0.0751
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.0751243666138338 0.07403184643582157
need align? ->  True 0.07403184643582157
2023-08-05 14:32:29,340 - epoch:23, training loss:0.5638 validation loss:0.0751
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07521770570589148 0.07408146778850452
need align? ->  True 0.07403184643582157
2023-08-05 14:32:50,392 - epoch:24, training loss:0.5623 validation loss:0.0752
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07519503172648989 0.07414670839257863
need align? ->  True 0.07403184643582157
2023-08-05 14:33:10,696 - epoch:25, training loss:0.5619 validation loss:0.0752
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07506801042219867 0.07401220306106236
need align? ->  True 0.07401220306106236
2023-08-05 14:33:31,905 - epoch:26, training loss:0.5610 validation loss:0.0751
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07511120532517848 0.07397504709661007
need align? ->  True 0.07397504709661007
2023-08-05 14:33:52,387 - epoch:27, training loss:0.5597 validation loss:0.0751
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07511891529935857 0.07400201131468234
need align? ->  True 0.07397504709661007
2023-08-05 14:34:12,700 - epoch:28, training loss:0.5587 validation loss:0.0751
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07509628885790058 0.07398912091941937
need align? ->  True 0.07397504709661007
2023-08-05 14:34:33,538 - epoch:29, training loss:0.5589 validation loss:0.0751
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-14:24:15.564843/0/0.0751_epoch_26.pkl  &  0.07397504709661007
2023-08-05 14:34:34,539 - [*] loss:0.1594
2023-08-05 14:34:34,540 - [*] phase 0, testing
2023-08-05 14:34:34,549 - T:24	MAE	0.255297	RMSE	0.161677	MAPE	114.466858
2023-08-05 14:34:34,549 - 24	mae	0.2553	
2023-08-05 14:34:34,549 - 24	rmse	0.1617	
2023-08-05 14:34:34,549 - 24	mape	114.4669	
2023-08-05 14:34:36,215 - [*] loss:0.1574
2023-08-05 14:34:36,217 - [*] phase 0, testing
2023-08-05 14:34:36,225 - T:24	MAE	0.251651	RMSE	0.159719	MAPE	113.502526
2023-08-05 14:34:36,225 - 24	mae	0.2517	
2023-08-05 14:34:36,225 - 24	rmse	0.1597	
2023-08-05 14:34:36,225 - 24	mape	113.5025	
2023-08-05 14:34:38,507 - logger name:exp/ECL-PatchTST2023-08-05-14:34:38.507347/ECL-PatchTST.log
2023-08-05 14:34:38,507 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 15.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-14:34:38.507347', 'path': 'exp/ECL-PatchTST2023-08-05-14:34:38.507347', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 14:34:38,508 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 14:34:38,701 - [*] phase 0 Dataset load!
2023-08-05 14:34:39,565 - [*] phase 0 Training start
train 8281
2023-08-05 14:34:51,587 - epoch:0, training loss:0.1973 validation loss:0.1336
train 8281
vs, vt 0.13359046646434328 0.13925113528966904
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10057095575915731 0.1105099884874147
need align? ->  False 0.1105099884874147
2023-08-05 14:35:19,138 - epoch:1, training loss:1.8838 validation loss:0.1006
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08402001388047052 0.09082179395077021
need align? ->  False 0.09082179395077021
2023-08-05 14:35:39,798 - epoch:2, training loss:1.6028 validation loss:0.0840
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07959867155422336 0.08067215890016245
need align? ->  False 0.08067215890016245
2023-08-05 14:36:00,549 - epoch:3, training loss:1.3060 validation loss:0.0796
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.0769160225501527 0.07734010255207187
need align? ->  False 0.07734010255207187
2023-08-05 14:36:20,646 - epoch:4, training loss:1.0197 validation loss:0.0769
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07760673348346482 0.07639219417520192
need align? ->  True 0.07639219417520192
2023-08-05 14:36:40,955 - epoch:5, training loss:0.8510 validation loss:0.0776
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.0763347571634728 0.07595607565472955
need align? ->  True 0.07595607565472955
2023-08-05 14:37:01,587 - epoch:6, training loss:0.7629 validation loss:0.0763
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.0784810612221127 0.07509786376486653
need align? ->  True 0.07509786376486653
2023-08-05 14:37:21,647 - epoch:7, training loss:0.7125 validation loss:0.0785
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07688521464233813 0.0763172435209803
need align? ->  True 0.07509786376486653
2023-08-05 14:37:42,062 - epoch:8, training loss:0.6776 validation loss:0.0769
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07672939927357694 0.07586045789977779
need align? ->  True 0.07509786376486653
2023-08-05 14:38:02,765 - epoch:9, training loss:0.6715 validation loss:0.0767
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.0771687286703483 0.07652381046310715
need align? ->  True 0.07509786376486653
2023-08-05 14:38:23,036 - epoch:10, training loss:0.6618 validation loss:0.0772
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07633759081363678 0.07588159357723982
need align? ->  True 0.07509786376486653
2023-08-05 14:38:43,242 - epoch:11, training loss:0.6562 validation loss:0.0763
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07667364803669245 0.07503466227132341
need align? ->  True 0.07503466227132341
2023-08-05 14:39:04,451 - epoch:12, training loss:0.6484 validation loss:0.0767
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07610076852142811 0.07575724759827489
need align? ->  True 0.07503466227132341
2023-08-05 14:39:24,584 - epoch:13, training loss:0.6054 validation loss:0.0761
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07614616571885088 0.07500344555339088
need align? ->  True 0.07500344555339088
2023-08-05 14:39:44,903 - epoch:14, training loss:0.5999 validation loss:0.0761
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.0755140082505734 0.07469136049242123
need align? ->  True 0.07469136049242123
2023-08-05 14:40:05,915 - epoch:15, training loss:0.5785 validation loss:0.0755
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07567852735519409 0.07468142004116722
need align? ->  True 0.07468142004116722
2023-08-05 14:40:26,113 - epoch:16, training loss:0.5766 validation loss:0.0757
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07547701352640339 0.07453531464156897
need align? ->  True 0.07453531464156897
2023-08-05 14:40:47,477 - epoch:17, training loss:0.5769 validation loss:0.0755
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07561907569027465 0.07428306762291037
need align? ->  True 0.07428306762291037
2023-08-05 14:41:07,896 - epoch:18, training loss:0.5609 validation loss:0.0756
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07549743159957555 0.07450803612237392
need align? ->  True 0.07428306762291037
2023-08-05 14:41:28,837 - epoch:19, training loss:0.5626 validation loss:0.0755
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.0755277326249558 0.07457067064293053
need align? ->  True 0.07428306762291037
2023-08-05 14:41:48,963 - epoch:20, training loss:0.5625 validation loss:0.0755
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.0751241227692884 0.0744423593353966
need align? ->  True 0.07428306762291037
2023-08-05 14:42:09,273 - epoch:21, training loss:0.5546 validation loss:0.0751
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07527116961453272 0.07415745026715424
need align? ->  True 0.07415745026715424
2023-08-05 14:42:30,362 - epoch:22, training loss:0.5572 validation loss:0.0753
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.0750497693762831 0.07414754229071348
need align? ->  True 0.07414754229071348
2023-08-05 14:42:50,864 - epoch:23, training loss:0.5549 validation loss:0.0750
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07520738867637904 0.07400519546607266
need align? ->  True 0.07400519546607266
2023-08-05 14:43:11,079 - epoch:24, training loss:0.5508 validation loss:0.0752
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07508052213360435 0.0741233338156472
need align? ->  True 0.07400519546607266
2023-08-05 14:43:31,935 - epoch:25, training loss:0.5474 validation loss:0.0751
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07515251660800498 0.07400608321894771
need align? ->  True 0.07400519546607266
2023-08-05 14:43:52,249 - epoch:26, training loss:0.5444 validation loss:0.0752
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07515292711879897 0.07403956191695255
need align? ->  True 0.07400519546607266
2023-08-05 14:44:12,553 - epoch:27, training loss:0.5455 validation loss:0.0752
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07515743569187495 0.07404623885193597
need align? ->  True 0.07400519546607266
2023-08-05 14:44:32,290 - epoch:28, training loss:0.5451 validation loss:0.0752
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07511029449170051 0.0740283313004867
need align? ->  True 0.07400519546607266
2023-08-05 14:44:53,085 - epoch:29, training loss:0.5440 validation loss:0.0751
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-14:34:38.507347/0/0.075_epoch_23.pkl  &  0.07400519546607266
2023-08-05 14:44:54,406 - [*] loss:0.1593
2023-08-05 14:44:54,407 - [*] phase 0, testing
2023-08-05 14:44:54,416 - T:24	MAE	0.255623	RMSE	0.161438	MAPE	114.080858
2023-08-05 14:44:54,416 - 24	mae	0.2556	
2023-08-05 14:44:54,416 - 24	rmse	0.1614	
2023-08-05 14:44:54,416 - 24	mape	114.0809	
2023-08-05 14:44:55,751 - [*] loss:0.1574
2023-08-05 14:44:55,752 - [*] phase 0, testing
2023-08-05 14:44:55,761 - T:24	MAE	0.251830	RMSE	0.159728	MAPE	113.446581
2023-08-05 14:44:55,761 - 24	mae	0.2518	
2023-08-05 14:44:55,761 - 24	rmse	0.1597	
2023-08-05 14:44:55,762 - 24	mape	113.4466	
2023-08-05 14:44:57,802 - logger name:exp/ECL-PatchTST2023-08-05-14:44:57.801825/ECL-PatchTST.log
2023-08-05 14:44:57,802 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 33, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 20.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-14:44:57.801825', 'path': 'exp/ECL-PatchTST2023-08-05-14:44:57.801825', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 14:44:57,802 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 14:44:57,999 - [*] phase 0 Dataset load!
2023-08-05 14:44:58,880 - [*] phase 0 Training start
train 8281
2023-08-05 14:45:10,829 - epoch:0, training loss:0.2000 validation loss:0.1367
train 8281
vs, vt 0.13673778509964113 0.1439287234907565
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.09971094382521899 0.11183329979362695
need align? ->  False 0.11183329979362695
2023-08-05 14:45:39,058 - epoch:1, training loss:2.4834 validation loss:0.0997
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08355213045749975 0.0905998038209003
need align? ->  False 0.0905998038209003
2023-08-05 14:46:00,222 - epoch:2, training loss:2.1224 validation loss:0.0836
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07935694718490476 0.0808125446833994
need align? ->  False 0.0808125446833994
2023-08-05 14:46:20,801 - epoch:3, training loss:1.7296 validation loss:0.0794
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07745898126260094 0.07705563104347042
need align? ->  True 0.07705563104347042
2023-08-05 14:46:41,013 - epoch:4, training loss:1.3607 validation loss:0.0775
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07652909559724123 0.07594425320301366
need align? ->  True 0.07594425320301366
2023-08-05 14:47:01,174 - epoch:5, training loss:1.1300 validation loss:0.0765
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07746737806693367 0.07649669553274693
need align? ->  True 0.07594425320301366
2023-08-05 14:47:22,240 - epoch:6, training loss:0.9941 validation loss:0.0775
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07655964467836462 0.07660598186371119
need align? ->  True 0.07594425320301366
2023-08-05 14:47:42,133 - epoch:7, training loss:0.9732 validation loss:0.0766
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07681468485490135 0.0756955166225848
need align? ->  True 0.0756955166225848
2023-08-05 14:48:03,385 - epoch:8, training loss:0.9564 validation loss:0.0768
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.0781251591346834 0.07575079596236996
need align? ->  True 0.0756955166225848
2023-08-05 14:48:24,209 - epoch:9, training loss:0.8490 validation loss:0.0781
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07759734154071497 0.07590775742478993
need align? ->  True 0.0756955166225848
2023-08-05 14:48:44,163 - epoch:10, training loss:0.8315 validation loss:0.0776
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07646077263938345 0.07622625573497752
need align? ->  True 0.0756955166225848
2023-08-05 14:49:04,878 - epoch:11, training loss:0.8218 validation loss:0.0765
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.0761969523585361 0.07498480606338252
need align? ->  True 0.07498480606338252
2023-08-05 14:49:26,180 - epoch:12, training loss:0.8132 validation loss:0.0762
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07571524527409802 0.07485771762288135
need align? ->  True 0.07485771762288135
2023-08-05 14:49:46,507 - epoch:13, training loss:0.7819 validation loss:0.0757
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07620660239911597 0.07479968254009019
need align? ->  True 0.07479968254009019
2023-08-05 14:50:06,770 - epoch:14, training loss:0.7643 validation loss:0.0762
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07608619364707367 0.07498863371817963
need align? ->  True 0.07479968254009019
2023-08-05 14:50:28,139 - epoch:15, training loss:0.7518 validation loss:0.0761
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07639620145377905 0.07478665155561073
need align? ->  True 0.07478665155561073
2023-08-05 14:50:48,610 - epoch:16, training loss:0.7439 validation loss:0.0764
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07580852281788121 0.07471241388955842
need align? ->  True 0.07471241388955842
2023-08-05 14:51:10,080 - epoch:17, training loss:0.7400 validation loss:0.0758
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07615945617789807 0.07451961715908154
need align? ->  True 0.07451961715908154
2023-08-05 14:51:30,471 - epoch:18, training loss:0.7383 validation loss:0.0762
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07546039014730764 0.07472899386092373
need align? ->  True 0.07451961715908154
2023-08-05 14:51:50,873 - epoch:19, training loss:0.7361 validation loss:0.0755
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.0755407179341368 0.07443394860171754
need align? ->  True 0.07443394860171754
2023-08-05 14:52:12,057 - epoch:20, training loss:0.7335 validation loss:0.0755
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07535990967374781 0.07445870550430339
need align? ->  True 0.07443394860171754
2023-08-05 14:52:32,176 - epoch:21, training loss:0.7294 validation loss:0.0754
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.0755323124644549 0.07427646187336548
need align? ->  True 0.07427646187336548
2023-08-05 14:52:52,461 - epoch:22, training loss:0.7212 validation loss:0.0755
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07540271576979886 0.07428817062274269
need align? ->  True 0.07427646187336548
2023-08-05 14:53:12,701 - epoch:23, training loss:0.7208 validation loss:0.0754
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07532302687025588 0.07430348877349625
need align? ->  True 0.07427646187336548
2023-08-05 14:53:33,440 - epoch:24, training loss:0.7215 validation loss:0.0753
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07527946178679881 0.07420590617086577
need align? ->  True 0.07420590617086577
2023-08-05 14:53:53,851 - epoch:25, training loss:0.7197 validation loss:0.0753
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07543859736103078 0.0741640449218128
need align? ->  True 0.0741640449218128
2023-08-05 14:54:15,069 - epoch:26, training loss:0.7181 validation loss:0.0754
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07527207202561524 0.07422272010665873
need align? ->  True 0.0741640449218128
2023-08-05 14:54:35,378 - epoch:27, training loss:0.7153 validation loss:0.0753
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07533728906317898 0.07421304487987705
need align? ->  True 0.0741640449218128
2023-08-05 14:54:57,024 - epoch:28, training loss:0.7156 validation loss:0.0753
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07535186563820942 0.0742022046090468
need align? ->  True 0.0741640449218128
2023-08-05 14:55:21,410 - epoch:29, training loss:0.7152 validation loss:0.0754
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-14:44:57.801825/0/0.0753_epoch_27.pkl  &  0.0741640449218128
2023-08-05 14:55:23,207 - [*] loss:0.1599
2023-08-05 14:55:23,208 - [*] phase 0, testing
2023-08-05 14:55:23,217 - T:24	MAE	0.255583	RMSE	0.162146	MAPE	114.367461
2023-08-05 14:55:23,217 - 24	mae	0.2556	
2023-08-05 14:55:23,217 - 24	rmse	0.1621	
2023-08-05 14:55:23,217 - 24	mape	114.3675	
2023-08-05 14:55:24,386 - [*] loss:0.1580
2023-08-05 14:55:24,387 - [*] phase 0, testing
2023-08-05 14:55:24,396 - T:24	MAE	0.251862	RMSE	0.160328	MAPE	113.457191
2023-08-05 14:55:24,396 - 24	mae	0.2519	
2023-08-05 14:55:24,396 - 24	rmse	0.1603	
2023-08-05 14:55:24,397 - 24	mape	113.4572	
2023-08-05 14:55:26,654 - logger name:exp/ECL-PatchTST2023-08-05-14:55:26.653832/ECL-PatchTST.log
2023-08-05 14:55:26,654 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 20.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-14:55:26.653832', 'path': 'exp/ECL-PatchTST2023-08-05-14:55:26.653832', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 14:55:26,654 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 14:55:26,848 - [*] phase 0 Dataset load!
2023-08-05 14:55:27,731 - [*] phase 0 Training start
train 8281
2023-08-05 14:55:39,164 - epoch:0, training loss:0.1993 validation loss:0.1351
train 8281
vs, vt 0.13509844648449318 0.140629466785037
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10105629986071069 0.11028115418942078
need align? ->  False 0.11028115418942078
2023-08-05 14:56:08,679 - epoch:1, training loss:2.4361 validation loss:0.1011
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08426524779718855 0.09066928785456263
need align? ->  False 0.09066928785456263
2023-08-05 14:56:29,179 - epoch:2, training loss:2.0797 validation loss:0.0843
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07923501227860866 0.08100101457017919
need align? ->  False 0.08100101457017919
2023-08-05 14:56:57,315 - epoch:3, training loss:1.6931 validation loss:0.0792
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07784027897793314 0.07717222558415454
need align? ->  True 0.07717222558415454
2023-08-05 14:57:18,080 - epoch:4, training loss:1.3377 validation loss:0.0778
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07693565580184045 0.07690982974093893
need align? ->  True 0.07690982974093893
2023-08-05 14:57:40,386 - epoch:5, training loss:1.1290 validation loss:0.0769
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07642383743887363 0.07611387279694495
need align? ->  True 0.07611387279694495
2023-08-05 14:58:09,983 - epoch:6, training loss:1.0067 validation loss:0.0764
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07683274749180545 0.07505306083223094
need align? ->  True 0.07505306083223094
2023-08-05 14:58:33,011 - epoch:7, training loss:0.9382 validation loss:0.0768
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07725309240429298 0.07535155433351579
need align? ->  True 0.07505306083223094
2023-08-05 14:58:53,136 - epoch:8, training loss:0.8930 validation loss:0.0773
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07653899466537911 0.07590245342125064
need align? ->  True 0.07505306083223094
2023-08-05 14:59:22,281 - epoch:9, training loss:0.8782 validation loss:0.0765
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07613701762064644 0.0754814344741728
need align? ->  True 0.07505306083223094
2023-08-05 14:59:42,952 - epoch:10, training loss:0.8667 validation loss:0.0761
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07634824676358182 0.07542249252614767
need align? ->  True 0.07505306083223094
2023-08-05 15:00:03,126 - epoch:11, training loss:0.8561 validation loss:0.0763
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07648893024610437 0.07496528392252715
need align? ->  True 0.07496528392252715
2023-08-05 15:00:24,396 - epoch:12, training loss:0.8488 validation loss:0.0765
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.0760423157202161 0.07535506039857864
need align? ->  True 0.07496528392252715
2023-08-05 15:00:44,851 - epoch:13, training loss:0.7846 validation loss:0.0760
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07627362126241559 0.07502887794828933
need align? ->  True 0.07496528392252715
2023-08-05 15:01:05,365 - epoch:14, training loss:0.7684 validation loss:0.0763
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07567007256590802 0.07497834973037243
need align? ->  True 0.07496528392252715
2023-08-05 15:01:25,934 - epoch:15, training loss:0.7630 validation loss:0.0757
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07593670422616212 0.07456078072604926
need align? ->  True 0.07456078072604926
2023-08-05 15:01:46,420 - epoch:16, training loss:0.7612 validation loss:0.0759
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07579169616751048 0.07458908159447752
need align? ->  True 0.07456078072604926
2023-08-05 15:02:06,536 - epoch:17, training loss:0.7457 validation loss:0.0758
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07589536257412123 0.07491874492362789
need align? ->  True 0.07456078072604926
2023-08-05 15:02:27,589 - epoch:18, training loss:0.7388 validation loss:0.0759
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.0756855801395748 0.07465255171384501
need align? ->  True 0.07456078072604926
2023-08-05 15:02:47,867 - epoch:19, training loss:0.7341 validation loss:0.0757
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07544220870603686 0.07457082977761394
need align? ->  True 0.07456078072604926
2023-08-05 15:03:08,099 - epoch:20, training loss:0.7323 validation loss:0.0754
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07532631468189799 0.07434990086957165
need align? ->  True 0.07434990086957165
2023-08-05 15:03:28,412 - epoch:21, training loss:0.7283 validation loss:0.0753
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.0752636268734932 0.07424452666031278
need align? ->  True 0.07424452666031278
2023-08-05 15:03:49,458 - epoch:22, training loss:0.7220 validation loss:0.0753
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07521119713783264 0.07411360019898933
need align? ->  True 0.07411360019898933
2023-08-05 15:04:10,303 - epoch:23, training loss:0.7197 validation loss:0.0752
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07533526436790176 0.07414746632718522
need align? ->  True 0.07411360019898933
2023-08-05 15:04:31,682 - epoch:24, training loss:0.7188 validation loss:0.0753
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.0752920604756345 0.07420549914240837
need align? ->  True 0.07411360019898933
2023-08-05 15:04:51,726 - epoch:25, training loss:0.7182 validation loss:0.0753
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07516770907070326 0.07409131964263709
need align? ->  True 0.07409131964263709
2023-08-05 15:05:12,252 - epoch:26, training loss:0.7173 validation loss:0.0752
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07520532365078511 0.0740462058102307
need align? ->  True 0.0740462058102307
2023-08-05 15:05:33,261 - epoch:27, training loss:0.7147 validation loss:0.0752
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.07522085398111654 0.07407683044995951
need align? ->  True 0.0740462058102307
2023-08-05 15:05:53,412 - epoch:28, training loss:0.7136 validation loss:0.0752
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07519475484023923 0.07406362915492576
need align? ->  True 0.0740462058102307
2023-08-05 15:06:13,618 - epoch:29, training loss:0.7140 validation loss:0.0752
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-14:55:26.653832/0/0.0752_epoch_26.pkl  &  0.0740462058102307
2023-08-05 15:06:15,637 - [*] loss:0.1596
2023-08-05 15:06:15,638 - [*] phase 0, testing
2023-08-05 15:06:15,647 - T:24	MAE	0.255511	RMSE	0.161875	MAPE	114.481914
2023-08-05 15:06:15,647 - 24	mae	0.2555	
2023-08-05 15:06:15,648 - 24	rmse	0.1619	
2023-08-05 15:06:15,648 - 24	mape	114.4819	
2023-08-05 15:06:16,671 - [*] loss:0.1576
2023-08-05 15:06:16,672 - [*] phase 0, testing
2023-08-05 15:06:16,681 - T:24	MAE	0.251819	RMSE	0.159842	MAPE	113.486648
2023-08-05 15:06:16,681 - 24	mae	0.2518	
2023-08-05 15:06:16,681 - 24	rmse	0.1598	
2023-08-05 15:06:16,681 - 24	mape	113.4866	
2023-08-05 15:06:18,915 - logger name:exp/ECL-PatchTST2023-08-05-15:06:18.914736/ECL-PatchTST.log
2023-08-05 15:06:18,915 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 24, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 0, 'refiner_block_num': 1, 'enhance': 0, 'enhance_type': 0, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 20.0, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-05-15:06:18.914736', 'path': 'exp/ECL-PatchTST2023-08-05-15:06:18.914736', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-05 15:06:18,915 - [*] phase 0 start training
0 26304
train 8281
val 2857
test 2857
2023-08-05 15:06:19,114 - [*] phase 0 Dataset load!
2023-08-05 15:06:20,054 - [*] phase 0 Training start
train 8281
2023-08-05 15:06:31,483 - epoch:0, training loss:0.1973 validation loss:0.1336
train 8281
vs, vt 0.13359046646434328 0.13925113528966904
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8281
vs, vt 0.10065305095327937 0.1105099884874147
need align? ->  False 0.1105099884874147
2023-08-05 15:07:00,169 - epoch:1, training loss:2.4547 validation loss:0.1007
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8281
vs, vt 0.08407433695443299 0.09082914370557536
need align? ->  False 0.09082914370557536
2023-08-05 15:07:20,181 - epoch:2, training loss:2.0928 validation loss:0.0841
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8281
vs, vt 0.07962418323301751 0.08072216711614442
need align? ->  False 0.08072216711614442
2023-08-05 15:07:41,154 - epoch:3, training loss:1.7034 validation loss:0.0796
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8281
vs, vt 0.07698230493975722 0.07733793014093586
need align? ->  False 0.07733793014093586
2023-08-05 15:08:01,971 - epoch:4, training loss:1.3239 validation loss:0.0770
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8281
vs, vt 0.07772243719386018 0.07645375751282858
need align? ->  True 0.07645375751282858
2023-08-05 15:08:23,151 - epoch:5, training loss:1.0997 validation loss:0.0777
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8281
vs, vt 0.07643289880260178 0.07602313055616358
need align? ->  True 0.07602313055616358
2023-08-05 15:08:43,584 - epoch:6, training loss:0.9830 validation loss:0.0764
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8281
vs, vt 0.07870730749614861 0.07520740568313909
need align? ->  True 0.07520740568313909
2023-08-05 15:09:04,614 - epoch:7, training loss:0.9171 validation loss:0.0787
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8281
vs, vt 0.07706436101833115 0.07635837275048961
need align? ->  True 0.07520740568313909
2023-08-05 15:09:24,741 - epoch:8, training loss:0.8694 validation loss:0.0771
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8281
vs, vt 0.07685219188747199 0.07596228530873424
need align? ->  True 0.07520740568313909
2023-08-05 15:09:44,895 - epoch:9, training loss:0.8576 validation loss:0.0769
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8281
vs, vt 0.07719970413524171 0.07638499821009843
need align? ->  True 0.07520740568313909
2023-08-05 15:10:05,289 - epoch:10, training loss:0.8518 validation loss:0.0772
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8281
vs, vt 0.07641380911935931 0.07588688454226307
need align? ->  True 0.07520740568313909
2023-08-05 15:10:26,122 - epoch:11, training loss:0.8378 validation loss:0.0764
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8281
vs, vt 0.07676947068261064 0.07506446516060311
need align? ->  True 0.07506446516060311
2023-08-05 15:10:46,716 - epoch:12, training loss:0.8313 validation loss:0.0768
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8281
vs, vt 0.07613507893098437 0.0758485947938069
need align? ->  True 0.07506446516060311
2023-08-05 15:11:06,803 - epoch:13, training loss:0.7744 validation loss:0.0761
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8281
vs, vt 0.07620357738240906 0.07503367029130459
need align? ->  True 0.07503367029130459
2023-08-05 15:11:27,762 - epoch:14, training loss:0.7670 validation loss:0.0762
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8281
vs, vt 0.07554011346529359 0.07474330472557442
need align? ->  True 0.07474330472557442
2023-08-05 15:11:48,397 - epoch:15, training loss:0.7369 validation loss:0.0755
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8281
vs, vt 0.07575593621510526 0.07469284639734289
need align? ->  True 0.07469284639734289
2023-08-05 15:12:08,493 - epoch:16, training loss:0.7370 validation loss:0.0758
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8281
vs, vt 0.07552127191877883 0.07458994383721249
need align? ->  True 0.07458994383721249
2023-08-05 15:12:29,440 - epoch:17, training loss:0.7352 validation loss:0.0755
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8281
vs, vt 0.07566387538352738 0.07430822863850904
need align? ->  True 0.07430822863850904
2023-08-05 15:12:49,926 - epoch:18, training loss:0.7172 validation loss:0.0757
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8281
vs, vt 0.07555325904294201 0.0745304642326158
need align? ->  True 0.07430822863850904
2023-08-05 15:13:10,134 - epoch:19, training loss:0.7219 validation loss:0.0756
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8281
vs, vt 0.07556172338840754 0.07460608228069285
need align? ->  True 0.07430822863850904
2023-08-05 15:13:31,423 - epoch:20, training loss:0.7194 validation loss:0.0756
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8281
vs, vt 0.07516295381862184 0.07446164483933346
need align? ->  True 0.07430822863850904
2023-08-05 15:13:51,679 - epoch:21, training loss:0.7064 validation loss:0.0752
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8281
vs, vt 0.07531801133376101 0.07421064117680425
need align? ->  True 0.07421064117680425
2023-08-05 15:14:12,428 - epoch:22, training loss:0.7129 validation loss:0.0753
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8281
vs, vt 0.07510320299669453 0.07416976648180382
need align? ->  True 0.07416976648180382
2023-08-05 15:14:33,795 - epoch:23, training loss:0.7089 validation loss:0.0751
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8281
vs, vt 0.07525100251254828 0.07404505412863649
need align? ->  True 0.07404505412863649
2023-08-05 15:14:54,421 - epoch:24, training loss:0.7042 validation loss:0.0753
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8281
vs, vt 0.07512617078812225 0.07412254769840966
need align? ->  True 0.07404505412863649
2023-08-05 15:15:14,488 - epoch:25, training loss:0.6999 validation loss:0.0751
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8281
vs, vt 0.07517837457682776 0.07399913862995479
need align? ->  True 0.07399913862995479
2023-08-05 15:15:35,395 - epoch:26, training loss:0.6937 validation loss:0.0752
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8281
vs, vt 0.07518975161339926 0.07403191770224468
need align? ->  True 0.07399913862995479
2023-08-05 15:15:55,728 - epoch:27, training loss:0.6970 validation loss:0.0752
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8281
vs, vt 0.075189260037049 0.0740438098816768
need align? ->  True 0.07399913862995479
2023-08-05 15:16:16,291 - epoch:28, training loss:0.6962 validation loss:0.0752
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8281
vs, vt 0.07514024542077728 0.07402246510205061
need align? ->  True 0.07399913862995479
2023-08-05 15:16:36,861 - epoch:29, training loss:0.6944 validation loss:0.0751
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-05-15:06:18.914736/0/0.0751_epoch_23.pkl  &  0.07399913862995479
2023-08-05 15:16:37,889 - [*] loss:0.1594
2023-08-05 15:16:37,890 - [*] phase 0, testing
2023-08-05 15:16:37,900 - T:24	MAE	0.255750	RMSE	0.161544	MAPE	114.098740
2023-08-05 15:16:37,900 - 24	mae	0.2557	
2023-08-05 15:16:37,900 - 24	rmse	0.1615	
2023-08-05 15:16:37,900 - 24	mape	114.0987	
2023-08-05 15:16:39,538 - [*] loss:0.1576
2023-08-05 15:16:39,539 - [*] phase 0, testing
2023-08-05 15:16:39,547 - T:24	MAE	0.251920	RMSE	0.159886	MAPE	113.418829
2023-08-05 15:16:39,547 - 24	mae	0.2519	
2023-08-05 15:16:39,547 - 24	rmse	0.1599	
2023-08-05 15:16:39,547 - 24	mape	113.4188	
