2023-08-23 20:03:07,229 - logger name:exp/ECL-PatchTST2023-08-23-20:03:07.229538/ECL-PatchTST.log
2023-08-23 20:03:07,230 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-22-20:19:56.785959/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 2.0, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'omega': 1.0, 'theta': 1.1, 'mask_border': 1, 'sup_weight': 20.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-23-20:03:07.229538', 'path': 'exp/ECL-PatchTST2023-08-23-20:03:07.229538', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-23 20:03:07,230 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-23 20:03:07,449 - [*] phase 0 Dataset load!
2023-08-23 20:03:08,534 - [*] phase 0 Training start
train 8209
2023-08-23 20:03:18,848 - epoch:0, training loss:0.2223 validation loss:0.1619
train 8209
vs, vt 0.1619003186510368 0.1647822740064426
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.1371006742119789 0.14328568784350698
need align? ->  False 0.14328568784350698
2023-08-23 20:04:12,606 - epoch:1, training loss:5.7222 validation loss:0.1371
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12924362388862806 0.13119312545115297
need align? ->  False 0.13119312545115297
2023-08-23 20:05:00,070 - epoch:2, training loss:4.0228 validation loss:0.1292
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12533152306621725 0.12725666021420198
need align? ->  False 0.12725666021420198
2023-08-23 20:05:48,683 - epoch:3, training loss:2.9471 validation loss:0.1253
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12571652750061316 0.12433195495131341
need align? ->  True 0.12433195495131341
2023-08-23 20:06:35,842 - epoch:4, training loss:2.2131 validation loss:0.1257
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12406234375455162 0.12411789756945589
need align? ->  False 0.12411789756945589
2023-08-23 20:07:24,991 - epoch:5, training loss:1.8530 validation loss:0.1241
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.1242703538896008 0.12326497931710699
need align? ->  True 0.12326497931710699
2023-08-23 20:08:11,523 - epoch:6, training loss:1.7160 validation loss:0.1243
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12213831513442776 0.12326864728873427
need align? ->  False 0.12326497931710699
2023-08-23 20:08:58,724 - epoch:7, training loss:1.6265 validation loss:0.1221
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12440567776899446 0.12294025880030611
need align? ->  True 0.12294025880030611
2023-08-23 20:09:46,208 - epoch:8, training loss:1.5851 validation loss:0.1244
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12329202365468848 0.12409988121891563
need align? ->  True 0.12294025880030611
2023-08-23 20:10:33,176 - epoch:9, training loss:1.5580 validation loss:0.1233
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12470060934058645 0.12410801945423539
need align? ->  True 0.12294025880030611
2023-08-23 20:11:20,458 - epoch:10, training loss:1.5269 validation loss:0.1247
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12412793248553168 0.12520843210884117
need align? ->  True 0.12294025880030611
2023-08-23 20:12:07,404 - epoch:11, training loss:1.4920 validation loss:0.1241
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12353116282346574 0.12506259165026926
need align? ->  True 0.12294025880030611
2023-08-23 20:12:54,453 - epoch:12, training loss:1.4646 validation loss:0.1235
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12418684143234383 0.12361035707660696
need align? ->  True 0.12294025880030611
2023-08-23 20:13:41,314 - epoch:13, training loss:1.4510 validation loss:0.1242
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12356712228872559 0.12481565824286504
need align? ->  True 0.12294025880030611
2023-08-23 20:14:29,372 - epoch:14, training loss:1.4267 validation loss:0.1236
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12404067814350128 0.12451182906939225
need align? ->  True 0.12294025880030611
2023-08-23 20:15:16,432 - epoch:15, training loss:1.4186 validation loss:0.1240
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12384502073241906 0.1246715404770591
need align? ->  True 0.12294025880030611
2023-08-23 20:16:03,714 - epoch:16, training loss:1.3986 validation loss:0.1238
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12389195092361081 0.1249391409483823
need align? ->  True 0.12294025880030611
2023-08-23 20:16:51,086 - epoch:17, training loss:1.3834 validation loss:0.1239
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.1237279563634233 0.12566426032307473
need align? ->  True 0.12294025880030611
2023-08-23 20:17:38,444 - epoch:18, training loss:1.3768 validation loss:0.1237
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12343311047350819 0.12497573252767324
need align? ->  True 0.12294025880030611
2023-08-23 20:18:27,016 - epoch:19, training loss:1.3708 validation loss:0.1234
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12347423268312757 0.12524400380524722
need align? ->  True 0.12294025880030611
2023-08-23 20:19:29,554 - epoch:20, training loss:1.3515 validation loss:0.1235
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12336525083942847 0.12520647260614418
need align? ->  True 0.12294025880030611
2023-08-23 20:20:16,153 - epoch:21, training loss:1.3559 validation loss:0.1234
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.1238258309154348 0.12520434707403183
need align? ->  True 0.12294025880030611
2023-08-23 20:21:13,339 - epoch:22, training loss:1.3511 validation loss:0.1238
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12384415392509916 0.12541299567303874
need align? ->  True 0.12294025880030611
2023-08-23 20:21:59,993 - epoch:23, training loss:1.3456 validation loss:0.1238
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.1233375094492327 0.12525737302547152
need align? ->  True 0.12294025880030611
2023-08-23 20:22:47,752 - epoch:24, training loss:1.3380 validation loss:0.1233
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12343183236027305 0.12485896398059347
need align? ->  True 0.12294025880030611
2023-08-23 20:23:34,585 - epoch:25, training loss:1.3348 validation loss:0.1234
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12350718202916058 0.12508475653488527
need align? ->  True 0.12294025880030611
2023-08-23 20:24:21,050 - epoch:26, training loss:1.3352 validation loss:0.1235
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12344251750883731 0.12512175264683636
need align? ->  True 0.12294025880030611
2023-08-23 20:25:07,450 - epoch:27, training loss:1.3325 validation loss:0.1234
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12348430324345827 0.12509489914571698
need align? ->  True 0.12294025880030611
2023-08-23 20:25:54,486 - epoch:28, training loss:1.3324 validation loss:0.1235
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12346887554634702 0.12504567959430543
need align? ->  True 0.12294025880030611
2023-08-23 20:26:41,087 - epoch:29, training loss:1.3307 validation loss:0.1235
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-23-20:03:07.229538/0/0.1221_epoch_7.pkl  &  0.12294025880030611
2023-08-23 20:26:45,256 - [*] loss:0.2703
2023-08-23 20:26:45,260 - [*] phase 0, testing
2023-08-23 20:26:45,298 - T:96	MAE	0.331309	RMSE	0.270385	MAPE	134.451973
2023-08-23 20:26:45,299 - 96	mae	0.3313	
2023-08-23 20:26:45,299 - 96	rmse	0.2704	
2023-08-23 20:26:45,299 - 96	mape	134.4520	
2023-08-23 20:26:46,372 - [*] loss:0.2713
2023-08-23 20:26:46,375 - [*] phase 0, testing
2023-08-23 20:26:46,413 - T:96	MAE	0.331986	RMSE	0.271379	MAPE	135.507953
2023-08-23 20:26:47,390 - [*] loss:0.2731
2023-08-23 20:26:47,394 - [*] phase 0, testing
2023-08-23 20:26:47,437 - T:96	MAE	0.332186	RMSE	0.273091	MAPE	132.560015
2023-08-23 20:26:47,438 - 96	mae	0.3322	
2023-08-23 20:26:47,438 - 96	rmse	0.2731	
2023-08-23 20:26:47,438 - 96	mape	132.5600	
