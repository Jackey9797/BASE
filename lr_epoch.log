2023-08-08 18:33:37,357 - logger name:exp/ECL-PatchTST2023-08-08-18:33:37.357624/ECL-PatchTST.log
2023-08-08 18:33:37,358 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-08-10:33:05.890873/0/0.1629_epoch_11.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 1, 'rec_ori': 1, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 50, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-18:33:37.357624', 'path': 'exp/ECL-PatchTST2023-08-08-18:33:37.357624', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 18:33:37,358 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 18:33:37,549 - [*] phase 0 Dataset load!
2023-08-08 18:33:38,395 - [*] phase 0 Training start
train 8209
2023-08-08 18:34:16,917 - epoch:0, training loss:0.2046 validation loss:0.1576
train 8209
vs, vt 0.1576495085927573 0.16912354139441793
Updating learning rate to 1.2713170341050318e-05
Updating learning rate to 6.356585170525159e-06
train 8209
vs, vt 0.14116295460950246 0.144423804838549
need align? ->  False 0.144423804838549
2023-08-08 18:35:43,568 - epoch:1, training loss:2.0279 validation loss:0.1412
Updating learning rate to 2.6389890225373026e-05
Updating learning rate to 1.3194945112686513e-05
train 8209
vs, vt 0.13212019586088983 0.13281789057972757
need align? ->  False 0.13281789057972757
2023-08-08 18:37:04,630 - epoch:2, training loss:1.7639 validation loss:0.1321
Updating learning rate to 4.7687228183767216e-05
Updating learning rate to 2.3843614091883608e-05
train 8209
vs, vt 0.12638088739053768 0.1267097694799304
need align? ->  False 0.1267097694799304
2023-08-08 18:38:25,923 - epoch:3, training loss:1.5239 validation loss:0.1264
Updating learning rate to 7.451397626604897e-05
Updating learning rate to 3.7256988133024484e-05
train 8209
vs, vt 0.12504953061315147 0.12444141905077478
need align? ->  True 0.12444141905077478
2023-08-08 18:39:47,423 - epoch:4, training loss:1.2866 validation loss:0.1250
Updating learning rate to 0.00010423598794287996
Updating learning rate to 5.211799397143998e-05
train 8209
vs, vt 0.12641402359374546 0.12305273115634918
need align? ->  True 0.12305273115634918
2023-08-08 18:41:10,826 - epoch:5, training loss:1.1042 validation loss:0.1264
Updating learning rate to 0.00013393482771610203
Updating learning rate to 6.696741385805102e-05
train 8209
vs, vt 0.12446253441951492 0.12288502070375464
need align? ->  True 0.12288502070375464
2023-08-08 18:42:32,468 - epoch:6, training loss:0.9931 validation loss:0.1245
Updating learning rate to 0.00016069433536107449
Updating learning rate to 8.034716768053724e-05
train 8209
vs, vt 0.1243249595334584 0.12243310285901482
need align? ->  True 0.12243310285901482
2023-08-08 18:43:54,305 - epoch:7, training loss:0.9308 validation loss:0.1243
Updating learning rate to 0.00018188696675751825
Updating learning rate to 9.094348337875913e-05
train 8209
vs, vt 0.12555694529278713 0.12268629382279786
need align? ->  True 0.12243310285901482
2023-08-08 18:45:32,498 - epoch:8, training loss:0.8961 validation loss:0.1256
Updating learning rate to 0.00019543179520238062
Updating learning rate to 9.771589760119031e-05
train 8209
vs, vt 0.12595919011668724 0.12290608950636604
need align? ->  True 0.12243310285901482
2023-08-08 18:47:10,882 - epoch:9, training loss:0.8774 validation loss:0.1260
Updating learning rate to 0.00019999992470120482
Updating learning rate to 9.999996235060241e-05
train 8209
vs, vt 0.1272484093735164 0.1224027643488212
need align? ->  True 0.1224027643488212
2023-08-08 18:48:50,564 - epoch:10, training loss:0.8653 validation loss:0.1272
Updating learning rate to 0.00019968203120099685
Updating learning rate to 9.984101560049843e-05
train 8209
vs, vt 0.12448282675309615 0.12281422816555608
need align? ->  True 0.1224027643488212
2023-08-08 18:50:32,524 - epoch:11, training loss:0.8456 validation loss:0.1245
Updating learning rate to 0.00019874956729692874
Updating learning rate to 9.937478364846437e-05
train 8209
vs, vt 0.12774414670738307 0.12155415837398985
need align? ->  True 0.12155415837398985
2023-08-08 18:52:13,357 - epoch:12, training loss:0.8248 validation loss:0.1277
Updating learning rate to 0.00019720828193904472
Updating learning rate to 9.860414096952236e-05
train 8209
vs, vt 0.1279324855316769 0.12451752681623805
need align? ->  True 0.12155415837398985
2023-08-08 18:53:53,518 - epoch:13, training loss:0.8334 validation loss:0.1279
Updating learning rate to 0.00019506767766410554
Updating learning rate to 9.753383883205277e-05
train 8209
vs, vt 0.1235210896384987 0.12106565589254553
need align? ->  True 0.12106565589254553
2023-08-08 18:55:33,985 - epoch:14, training loss:0.8192 validation loss:0.1235
Updating learning rate to 0.0001923409520092894
Updating learning rate to 9.61704760046447e-05
train 8209
vs, vt 0.13020123092626984 0.12086644811047749
need align? ->  True 0.12086644811047749
2023-08-08 18:57:15,023 - epoch:15, training loss:0.8294 validation loss:0.1302
Updating learning rate to 0.0001890449161449865
Updating learning rate to 9.452245807249325e-05
train 8209
vs, vt 0.12491923240436749 0.12277342870154163
need align? ->  True 0.12086644811047749
2023-08-08 18:58:54,674 - epoch:16, training loss:0.8359 validation loss:0.1249
Updating learning rate to 0.00018519989122834342
Updating learning rate to 9.259994561417171e-05
train 8209
vs, vt 0.12604334510185503 0.12105149945074861
need align? ->  True 0.12086644811047749
2023-08-08 19:00:38,533 - epoch:17, training loss:0.8220 validation loss:0.1260
Updating learning rate to 0.00018082958311657177
Updating learning rate to 9.041479155828588e-05
train 8209
vs, vt 0.1256686733527617 0.12205190050669691
need align? ->  True 0.12086644811047749
2023-08-08 19:02:17,752 - epoch:18, training loss:0.8152 validation loss:0.1257
Updating learning rate to 0.00017596093621245559
Updating learning rate to 8.798046810622779e-05
train 8209
vs, vt 0.1250686322931539 0.12329675189473412
need align? ->  True 0.12086644811047749
2023-08-08 19:03:57,223 - epoch:19, training loss:0.8091 validation loss:0.1251
Updating learning rate to 0.0001706239673431481
Updating learning rate to 8.531198367157406e-05
train 8209
vs, vt 0.12493161814795299 0.12172931889918717
need align? ->  True 0.12086644811047749
2023-08-08 19:05:36,471 - epoch:20, training loss:0.8045 validation loss:0.1249
Updating learning rate to 0.00016485158069645084
Updating learning rate to 8.242579034822542e-05
train 8209
vs, vt 0.12344442934475162 0.12153886986727064
need align? ->  True 0.12086644811047749
2023-08-08 19:07:18,849 - epoch:21, training loss:0.7996 validation loss:0.1234
Updating learning rate to 0.00015867936495555403
Updating learning rate to 7.933968247777702e-05
train 8209
vs, vt 0.12443111608312889 0.12128953440961512
need align? ->  True 0.12086644811047749
2023-08-08 19:09:01,479 - epoch:22, training loss:0.7960 validation loss:0.1244
Updating learning rate to 0.00015214537388297033
Updating learning rate to 7.607268694148517e-05
train 8209
vs, vt 0.12730909291316161 0.12204828883775255
need align? ->  True 0.12086644811047749
2023-08-08 19:10:44,198 - epoch:23, training loss:0.7923 validation loss:0.1273
Updating learning rate to 0.0001452898917064347
Updating learning rate to 7.264494585321735e-05
train 8209
vs, vt 0.1261332890188152 0.12207573618401181
need align? ->  True 0.12086644811047749
2023-08-08 19:12:27,161 - epoch:24, training loss:0.7916 validation loss:0.1261
Updating learning rate to 0.0001381551847532447
Updating learning rate to 6.907759237662235e-05
train 8209
vs, vt 0.12843058910220861 0.12349946017969739
need align? ->  True 0.12086644811047749
2023-08-08 19:14:10,461 - epoch:25, training loss:0.7874 validation loss:0.1284
Updating learning rate to 0.0001307852408642975
Updating learning rate to 6.539262043214875e-05
train 8209
vs, vt 0.12451429182494228 0.12272581822154197
need align? ->  True 0.12086644811047749
2023-08-08 19:15:53,717 - epoch:26, training loss:0.7863 validation loss:0.1245
Updating learning rate to 0.00012322549819442346
Updating learning rate to 6.161274909721173e-05
train 8209
vs, vt 0.13014974114908415 0.12212731934745204
need align? ->  True 0.12086644811047749
2023-08-08 19:17:35,844 - epoch:27, training loss:0.7826 validation loss:0.1301
Updating learning rate to 0.00011552256507105203
Updating learning rate to 5.776128253552601e-05
train 8209
vs, vt 0.12603935963389548 0.12284999002109874
need align? ->  True 0.12086644811047749
2023-08-08 19:19:18,757 - epoch:28, training loss:0.7823 validation loss:0.1260
Updating learning rate to 0.00010772393263837382
Updating learning rate to 5.386196631918691e-05
train 8209
vs, vt 0.1250824969769879 0.12224224006587808
need align? ->  True 0.12086644811047749
2023-08-08 19:21:02,894 - epoch:29, training loss:0.7772 validation loss:0.1251
Updating learning rate to 9.987768205864487e-05
Updating learning rate to 4.993884102932243e-05
train 8209
vs, vt 0.12760039359669795 0.1220644401725043
need align? ->  True 0.12086644811047749
2023-08-08 19:22:46,818 - epoch:30, training loss:0.7759 validation loss:0.1276
Updating learning rate to 9.203218807583219e-05
Updating learning rate to 4.601609403791609e-05
train 8209
vs, vt 0.12696294164793057 0.12264576215635646
need align? ->  True 0.12086644811047749
2023-08-08 19:24:32,293 - epoch:31, training loss:0.7753 validation loss:0.1270
Updating learning rate to 8.423582076923134e-05
Updating learning rate to 4.211791038461567e-05
train 8209
vs, vt 0.12662930380214343 0.12287590297108347
need align? ->  True 0.12086644811047749
2023-08-08 19:26:16,479 - epoch:32, training loss:0.7742 validation loss:0.1266
Updating learning rate to 7.653664733584277e-05
Updating learning rate to 3.826832366792138e-05
train 8209
vs, vt 0.12694042743268338 0.1229420407590541
need align? ->  True 0.12086644811047749
2023-08-08 19:28:00,131 - epoch:33, training loss:0.7734 validation loss:0.1269
Updating learning rate to 6.898213574011825e-05
Updating learning rate to 3.449106787005913e-05
train 8209
vs, vt 0.12607386242598295 0.12365357264537703
need align? ->  True 0.12086644811047749
2023-08-08 19:29:43,462 - epoch:34, training loss:0.7722 validation loss:0.1261
Updating learning rate to 6.161886205817543e-05
Updating learning rate to 3.0809431029087716e-05
train 8209
vs, vt 0.12729407821527935 0.12301611646332523
need align? ->  True 0.12086644811047749
2023-08-08 19:31:27,588 - epoch:35, training loss:0.7702 validation loss:0.1273
Updating learning rate to 5.449222332080049e-05
Updating learning rate to 2.7246111660400245e-05
train 8209
vs, vt 0.1262218009849841 0.12317210037938574
need align? ->  True 0.12086644811047749
2023-08-08 19:33:12,433 - epoch:36, training loss:0.7692 validation loss:0.1262
Updating learning rate to 4.764615762565717e-05
Updating learning rate to 2.3823078812828583e-05
train 8209
vs, vt 0.12649796124209056 0.12259045446460898
need align? ->  True 0.12086644811047749
2023-08-08 19:34:57,249 - epoch:37, training loss:0.7668 validation loss:0.1265
Updating learning rate to 4.1122873244303924e-05
Updating learning rate to 2.0561436622151962e-05
train 8209
vs, vt 0.12705612851476128 0.12260315093127164
need align? ->  True 0.12086644811047749
2023-08-08 19:36:42,454 - epoch:38, training loss:0.7684 validation loss:0.1271
Updating learning rate to 3.496258839416395e-05
Updating learning rate to 1.7481294197081977e-05
train 8209
vs, vt 0.12666512119837783 0.12298501113598997
need align? ->  True 0.12086644811047749
2023-08-08 19:38:28,900 - epoch:39, training loss:0.7670 validation loss:0.1267
Updating learning rate to 2.920328327984094e-05
Updating learning rate to 1.460164163992047e-05
train 8209
vs, vt 0.12765411380678415 0.12279620157046751
need align? ->  True 0.12086644811047749
2023-08-08 19:40:15,410 - epoch:40, training loss:0.7665 validation loss:0.1277
Updating learning rate to 2.3880465932528025e-05
Updating learning rate to 1.1940232966264013e-05
train 8209
vs, vt 0.12781774430451068 0.12303146440535784
need align? ->  True 0.12086644811047749
2023-08-08 19:42:00,795 - epoch:41, training loss:0.7647 validation loss:0.1278
Updating learning rate to 1.9026953291187736e-05
Updating learning rate to 9.513476645593868e-06
train 8209
vs, vt 0.12739745620638132 0.12322151770984585
need align? ->  True 0.12086644811047749
2023-08-08 19:43:45,659 - epoch:42, training loss:0.7665 validation loss:0.1274
Updating learning rate to 1.4672668875210677e-05
Updating learning rate to 7.336334437605339e-06
train 8209
vs, vt 0.12705969387157398 0.12311057424680753
need align? ->  True 0.12086644811047749
2023-08-08 19:45:31,980 - epoch:43, training loss:0.7645 validation loss:0.1271
Updating learning rate to 1.0844458295967848e-05
Updating learning rate to 5.422229147983924e-06
train 8209
vs, vt 0.12714020713147792 0.12312130426818674
need align? ->  True 0.12086644811047749
2023-08-08 19:47:16,804 - epoch:44, training loss:0.7640 validation loss:0.1271
Updating learning rate to 7.565923744689476e-06
Updating learning rate to 3.782961872344738e-06
train 8209
vs, vt 0.12765975398096172 0.12290299277413975
need align? ->  True 0.12086644811047749
2023-08-08 19:49:03,027 - epoch:45, training loss:0.7641 validation loss:0.1277
Updating learning rate to 4.857278477107554e-06
Updating learning rate to 2.428639238553777e-06
train 8209
vs, vt 0.12701808153228325 0.12312986405397003
need align? ->  True 0.12086644811047749
2023-08-08 19:50:49,681 - epoch:46, training loss:0.7629 validation loss:0.1270
Updating learning rate to 2.7352221920126874e-06
Updating learning rate to 1.3676110960063437e-06
train 8209
vs, vt 0.12718771398067474 0.12309050678529522
need align? ->  True 0.12086644811047749
2023-08-08 19:52:36,015 - epoch:47, training loss:0.7635 validation loss:0.1272
Updating learning rate to 1.212838072057814e-06
Updating learning rate to 6.06419036028907e-07
train 8209
vs, vt 0.12721245185556737 0.12310122850943696
need align? ->  True 0.12086644811047749
2023-08-08 19:54:21,847 - epoch:48, training loss:0.7639 validation loss:0.1272
Updating learning rate to 2.9951212158652577e-07
Updating learning rate to 1.4975606079326289e-07
train 8209
vs, vt 0.12716915289109404 0.12309498949484392
need align? ->  True 0.12086644811047749
2023-08-08 19:56:08,841 - epoch:49, training loss:0.7637 validation loss:0.1272
Updating learning rate to 8.752987951950934e-10
Updating learning rate to 4.376493975975467e-10
check exp/ECL-PatchTST2023-08-08-18:33:37.357624/0/0.1234_epoch_21.pkl  &  0.12086644811047749
2023-08-08 19:56:14,204 - [*] loss:0.2745
2023-08-08 19:56:14,207 - [*] phase 0, testing
2023-08-08 19:56:14,253 - T:96	MAE	0.334932	RMSE	0.274503	MAPE	133.282733
2023-08-08 19:56:14,254 - 96	mae	0.3349	
2023-08-08 19:56:14,254 - 96	rmse	0.2745	
2023-08-08 19:56:14,254 - 96	mape	133.2827	
2023-08-08 19:56:18,169 - [*] loss:0.2697
2023-08-08 19:56:18,172 - [*] phase 0, testing
2023-08-08 19:56:18,221 - T:96	MAE	0.329406	RMSE	0.270097	MAPE	130.129719
2023-08-08 19:56:23,183 - [*] loss:0.2721
2023-08-08 19:56:23,185 - [*] phase 0, testing
2023-08-08 19:56:23,222 - T:96	MAE	0.331094	RMSE	0.272343	MAPE	129.508781
2023-08-08 19:56:27,365 - [*] loss:0.2702
2023-08-08 19:56:27,368 - [*] phase 0, testing
2023-08-08 19:56:27,405 - T:96	MAE	0.328801	RMSE	0.270550	MAPE	129.180074
2023-08-08 19:56:27,406 - 96	mae	0.3288	
2023-08-08 19:56:27,407 - 96	rmse	0.2705	
2023-08-08 19:56:27,407 - 96	mape	129.1801	
