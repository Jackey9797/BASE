2023-09-04 01:55:17,339 - logger name:exp/ECL-PatchTST2023-09-04-01:55:17.339255/ECL-PatchTST.log
2023-09-04 01:55:17,339 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'abl_tmp_context': 0, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-04-01:55:17.339255', 'path': 'exp/ECL-PatchTST2023-09-04-01:55:17.339255', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 01:55:17,339 - [*] phase 0 start training
0 69680
train 34129
val 11425
test 11425
2023-09-04 01:55:18,112 - [*] phase 0 Dataset load!
2023-09-04 01:55:18,982 - [*] phase 0 Training start
train 34129
2023-09-04 01:56:53,879 - epoch:0, training loss:0.1882 validation loss:0.1820
train 34129
vs, vt 0.18196401625043815 0.18696492351591587
Updating learning rate to 1.0438661460315324e-05
Updating learning rate to 1.0438661460315324e-05
train 34129
vs, vt 0.16749209612607957 0.16712793711986806
need align? ->  True 0.16712793711986806
2023-09-04 02:01:12,173 - epoch:1, training loss:11.1624 validation loss:0.1675
Updating learning rate to 2.8027297449571736e-05
Updating learning rate to 2.8027297449571736e-05
train 34129
vs, vt 0.16765032642417485 0.16608818343116177
need align? ->  True 0.16608818343116177
2023-09-04 02:04:27,663 - epoch:2, training loss:6.5898 validation loss:0.1677
Updating learning rate to 5.204727160595503e-05
Updating learning rate to 5.204727160595503e-05
train 34129
vs, vt 0.1688392064637608 0.17217935774889256
need align? ->  True 0.16608818343116177
2023-09-04 02:07:44,018 - epoch:3, training loss:2.4293 validation loss:0.1688
Updating learning rate to 7.605456385119542e-05
Updating learning rate to 7.605456385119542e-05
train 34129
vs, vt 0.16222903807130123 0.1667939032945368
need align? ->  False 0.16608818343116177
2023-09-04 02:10:58,712 - epoch:4, training loss:1.6764 validation loss:0.1622
Updating learning rate to 9.360855637921138e-05
Updating learning rate to 9.360855637921138e-05
train 34129
vs, vt 0.16470406614243985 0.16475342280334895
need align? ->  False 0.16475342280334895
2023-09-04 02:14:15,347 - epoch:5, training loss:1.4996 validation loss:0.1647
Updating learning rate to 9.99999939458629e-05
Updating learning rate to 9.99999939458629e-05
train 34129
vs, vt 0.16396618348856767 0.16231438865264255
need align? ->  True 0.16231438865264255
2023-09-04 02:17:34,305 - epoch:6, training loss:1.2820 validation loss:0.1640
Updating learning rate to 9.956902716655286e-05
Updating learning rate to 9.956902716655286e-05
train 34129
vs, vt 0.15836201277044085 0.1585759768469466
need align? ->  False 0.1585759768469466
2023-09-04 02:20:52,234 - epoch:7, training loss:1.1238 validation loss:0.1584
Updating learning rate to 9.828992401134782e-05
Updating learning rate to 9.828992401134782e-05
train 34129
vs, vt 0.158704986423254 0.15948481654955282
need align? ->  True 0.1585759768469466
2023-09-04 02:24:05,634 - epoch:8, training loss:1.0659 validation loss:0.1587
Updating learning rate to 9.618457028986775e-05
Updating learning rate to 9.618457028986775e-05
train 34129
vs, vt 0.15785928116076522 0.15801854123257927
need align? ->  False 0.15801854123257927
2023-09-04 02:27:20,443 - epoch:9, training loss:0.9828 validation loss:0.1579
Updating learning rate to 9.32889891880015e-05
Updating learning rate to 9.32889891880015e-05
train 34129
vs, vt 0.1581435499091943 0.15724761897905004
need align? ->  True 0.15724761897905004
2023-09-04 02:30:34,936 - epoch:10, training loss:1.0001 validation loss:0.1581
Updating learning rate to 8.965272490120875e-05
Updating learning rate to 8.965272490120875e-05
train 34129
vs, vt 0.15735399739609823 0.15709140309029154
need align? ->  True 0.15709140309029154
2023-09-04 02:33:50,094 - epoch:11, training loss:0.9447 validation loss:0.1574
Updating learning rate to 8.533799491959945e-05
Updating learning rate to 8.533799491959945e-05
train 34129
vs, vt 0.15636705737560988 0.15557017171134552
need align? ->  True 0.15557017171134552
2023-09-04 02:37:06,275 - epoch:12, training loss:0.9329 validation loss:0.1564
Updating learning rate to 8.041862546942808e-05
Updating learning rate to 8.041862546942808e-05
train 34129
vs, vt 0.15622414938277668 0.1547945447266102
need align? ->  True 0.1547945447266102
2023-09-04 02:40:21,191 - epoch:13, training loss:0.9349 validation loss:0.1562
Updating learning rate to 7.497878832589398e-05
Updating learning rate to 7.497878832589398e-05
train 34129
vs, vt 0.1551378682255745 0.15427560400631693
need align? ->  True 0.15427560400631693
2023-09-04 02:43:34,284 - epoch:14, training loss:0.9323 validation loss:0.1551
Updating learning rate to 6.911156061073078e-05
Updating learning rate to 6.911156061073078e-05
train 34129
vs, vt 0.1556638458950652 0.15488206177122063
need align? ->  True 0.15427560400631693
2023-09-04 02:46:51,860 - epoch:15, training loss:0.9256 validation loss:0.1557
Updating learning rate to 6.291733221684778e-05
Updating learning rate to 6.291733221684778e-05
train 34129
vs, vt 0.15472944999734561 0.15328990177561838
need align? ->  True 0.15328990177561838
2023-09-04 02:50:10,159 - epoch:16, training loss:0.9085 validation loss:0.1547
Updating learning rate to 5.650208810942889e-05
Updating learning rate to 5.650208810942889e-05
train 34129
vs, vt 0.15515206079516147 0.15392505340278148
need align? ->  True 0.15328990177561838
2023-09-04 02:53:26,087 - epoch:17, training loss:0.9317 validation loss:0.1552
Updating learning rate to 4.9975594893793686e-05
Updating learning rate to 4.9975594893793686e-05
train 34129
vs, vt 0.15347224266992676 0.1545597315662437
need align? ->  True 0.15328990177561838
2023-09-04 02:56:43,833 - epoch:18, training loss:0.9180 validation loss:0.1535
Updating learning rate to 4.3449522678347527e-05
Updating learning rate to 4.3449522678347527e-05
train 34129
vs, vt 0.15466794984208213 0.1525188085105684
need align? ->  True 0.1525188085105684
2023-09-04 03:00:05,945 - epoch:19, training loss:0.9072 validation loss:0.1547
Updating learning rate to 3.7035534368065714e-05
Updating learning rate to 3.7035534368065714e-05
train 34129
vs, vt 0.15344630893733766 0.15240514282551076
need align? ->  True 0.15240514282551076
2023-09-04 03:03:25,562 - epoch:20, training loss:0.9483 validation loss:0.1534
Updating learning rate to 3.084337508123067e-05
Updating learning rate to 3.084337508123067e-05
train 34129
vs, vt 0.1538234466065963 0.15300697684288025
need align? ->  True 0.15240514282551076
2023-09-04 03:06:46,459 - epoch:21, training loss:0.9565 validation loss:0.1538
Updating learning rate to 2.497899438003108e-05
Updating learning rate to 2.497899438003108e-05
train 34129
vs, vt 0.1533436667174101 0.15248421087033218
need align? ->  True 0.15240514282551076
2023-09-04 03:10:06,034 - epoch:22, training loss:0.9488 validation loss:0.1533
Updating learning rate to 1.9542733444177923e-05
Updating learning rate to 1.9542733444177923e-05
train 34129
vs, vt 0.15370373328526815 0.15274418522086408
need align? ->  True 0.15240514282551076
2023-09-04 03:13:26,215 - epoch:23, training loss:0.9414 validation loss:0.1537
Updating learning rate to 1.4627608205499963e-05
Updating learning rate to 1.4627608205499963e-05
train 34129
vs, vt 0.15274297520518304 0.15266686193645002
need align? ->  True 0.15240514282551076
2023-09-04 03:16:47,465 - epoch:24, training loss:0.9363 validation loss:0.1527
Updating learning rate to 1.0317717819561129e-05
Updating learning rate to 1.0317717819561129e-05
train 34129
vs, vt 0.15338508383267455 0.15216488838195802
need align? ->  True 0.15216488838195802
2023-09-04 03:20:08,095 - epoch:25, training loss:0.9334 validation loss:0.1534
Updating learning rate to 6.686805705792195e-06
Updating learning rate to 6.686805705792195e-06
train 34129
vs, vt 0.15323919467628003 0.1525741638822688
need align? ->  True 0.15216488838195802
2023-09-04 03:23:28,305 - epoch:26, training loss:0.9581 validation loss:0.1532
Updating learning rate to 3.79699777713878e-06
Updating learning rate to 3.79699777713878e-06
train 34129
vs, vt 0.15298999684552353 0.15240709574686157
need align? ->  True 0.15216488838195802
2023-09-04 03:26:47,490 - epoch:27, training loss:0.9570 validation loss:0.1530
Updating learning rate to 1.6977394484662593e-06
Updating learning rate to 1.6977394484662593e-06
train 34129
vs, vt 0.15312436264422205 0.15228713183767265
need align? ->  True 0.15216488838195802
2023-09-04 03:30:17,244 - epoch:28, training loss:0.9563 validation loss:0.1531
Updating learning rate to 4.2494961180259127e-07
Updating learning rate to 4.2494961180259127e-07
train 34129
vs, vt 0.1531362422224548 0.15233270513514677
need align? ->  True 0.15216488838195802
2023-09-04 03:33:59,241 - epoch:29, training loss:0.9556 validation loss:0.1531
Updating learning rate to 4.0605413710007e-10
Updating learning rate to 4.0605413710007e-10
check exp/ECL-PatchTST2023-09-04-01:55:17.339255/0/0.1527_epoch_24.pkl  &  0.15216488838195802
2023-09-04 03:34:34,559 - [*] loss:0.2866
2023-09-04 03:34:34,571 - [*] phase 0, testing
2023-09-04 03:34:34,770 - T:96	MAE	0.332610	RMSE	0.288273	MAPE	206.962037
2023-09-04 03:34:34,771 - 96	mae	0.3326	
2023-09-04 03:34:34,771 - 96	rmse	0.2883	
2023-09-04 03:34:34,771 - 96	mape	206.9620	
----*-----
2023-09-04 03:35:09,628 - [*] loss:0.2866
2023-09-04 03:35:09,638 - [*] phase 0, testing
2023-09-04 03:35:09,821 - T:96	MAE	0.332610	RMSE	0.288273	MAPE	206.962037
2023-09-04 03:35:33,877 - [*] loss:0.3042
2023-09-04 03:35:33,887 - [*] phase 0, testing
2023-09-04 03:35:34,060 - T:96	MAE	0.357763	RMSE	0.305875	MAPE	224.535894
2023-09-04 03:36:01,665 - [*] loss:0.3896
2023-09-04 03:36:01,675 - [*] phase 0, testing
2023-09-04 03:36:01,855 - T:96	MAE	0.407437	RMSE	0.392036	MAPE	261.718678
2023-09-04 03:36:37,698 - [*] loss:0.2935
2023-09-04 03:36:37,708 - [*] phase 0, testing
2023-09-04 03:36:37,884 - T:96	MAE	0.342570	RMSE	0.295393	MAPE	227.484345
2023-09-04 03:37:10,722 - [*] loss:0.3584
2023-09-04 03:37:10,732 - [*] phase 0, testing
2023-09-04 03:37:10,931 - T:96	MAE	0.399331	RMSE	0.360316	MAPE	226.525116
2023-09-04 03:37:38,734 - [*] loss:0.3327
2023-09-04 03:37:38,744 - [*] phase 0, testing
2023-09-04 03:37:38,935 - T:96	MAE	0.374532	RMSE	0.334473	MAPE	196.679735
2023-09-04 03:38:07,826 - [*] loss:0.2921
2023-09-04 03:38:07,836 - [*] phase 0, testing
2023-09-04 03:38:08,014 - T:96	MAE	0.341429	RMSE	0.293766	MAPE	213.303161
2023-09-04 03:38:35,322 - [*] loss:0.3019
2023-09-04 03:38:35,332 - [*] phase 0, testing
2023-09-04 03:38:35,512 - T:96	MAE	0.353267	RMSE	0.303537	MAPE	198.508358
----*-----
2023-09-04 03:38:54,845 - [*] loss:0.2973
2023-09-04 03:38:54,855 - [*] phase 0, testing
2023-09-04 03:38:55,027 - T:96	MAE	0.350559	RMSE	0.297764	MAPE	202.579975
2023-09-04 03:39:21,535 - [*] loss:0.3128
2023-09-04 03:39:21,544 - [*] phase 0, testing
2023-09-04 03:39:21,722 - T:96	MAE	0.354170	RMSE	0.314537	MAPE	188.503230
2023-09-04 03:39:39,560 - [*] loss:0.2994
2023-09-04 03:39:39,569 - [*] phase 0, testing
2023-09-04 03:39:39,742 - T:96	MAE	0.345813	RMSE	0.300392	MAPE	200.280643
2023-09-04 03:39:39,745 - 96	mae	0.3458	
2023-09-04 03:39:39,745 - 96	rmse	0.3004	
2023-09-04 03:39:39,745 - 96	mape	200.2806	
2023-09-04 03:39:41,985 - logger name:exp/ECL-PatchTST2023-09-04-03:39:41.984843/ECL-PatchTST.log
2023-09-04 03:39:41,985 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'abl_tmp_context': 0, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-04-03:39:41.984843', 'path': 'exp/ECL-PatchTST2023-09-04-03:39:41.984843', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 03:39:41,986 - [*] phase 0 start training
0 69680
train 33889
val 11185
test 11185
2023-09-04 03:39:42,817 - [*] phase 0 Dataset load!
2023-09-04 03:39:43,814 - [*] phase 0 Training start
train 33889
2023-09-04 03:41:20,730 - epoch:0, training loss:0.2104 validation loss:0.2622
train 33889
vs, vt 0.2622243366627531 0.26576502138579433
Updating learning rate to 1.0438721218484657e-05
Updating learning rate to 1.0438721218484657e-05
train 33889
vs, vt 0.25501850145784294 0.2529570521278815
need align? ->  True 0.2529570521278815
2023-09-04 03:45:28,227 - epoch:1, training loss:11.0600 validation loss:0.2550
Updating learning rate to 2.802750441854847e-05
Updating learning rate to 2.802750441854847e-05
train 33889
vs, vt 0.25453988365320995 0.2530955165963281
need align? ->  True 0.2529570521278815
2023-09-04 03:48:42,308 - epoch:2, training loss:6.4380 validation loss:0.2545
Updating learning rate to 5.2047629950292354e-05
Updating learning rate to 5.2047629950292354e-05
train 33889
vs, vt 0.25208702408285305 0.25226840715516696
need align? ->  False 0.25226840715516696
2023-09-04 03:51:58,488 - epoch:3, training loss:2.6674 validation loss:0.2521
Updating learning rate to 7.605497731655361e-05
Updating learning rate to 7.605497731655361e-05
train 33889
vs, vt 0.2521563990210945 0.2501486540687355
need align? ->  True 0.2501486540687355
2023-09-04 03:55:14,232 - epoch:4, training loss:1.6142 validation loss:0.2522
Updating learning rate to 9.3608854147054e-05
Updating learning rate to 9.3608854147054e-05
train 33889
vs, vt 0.24829048955474387 0.2523909170519222
need align? ->  False 0.2501486540687355
2023-09-04 03:58:27,471 - epoch:5, training loss:1.4520 validation loss:0.2483
Updating learning rate to 9.99999938537861e-05
Updating learning rate to 9.99999938537861e-05
train 33889
vs, vt 0.2493668499148705 0.25115535742687906
need align? ->  False 0.2501486540687355
2023-09-04 04:01:47,129 - epoch:6, training loss:1.2670 validation loss:0.2494
Updating learning rate to 9.956900274488073e-05
Updating learning rate to 9.956900274488073e-05
train 33889
vs, vt 0.247414113572714 0.2531438745473596
need align? ->  False 0.2501486540687355
2023-09-04 04:05:16,836 - epoch:7, training loss:1.1228 validation loss:0.2474
Updating learning rate to 9.828987567794199e-05
Updating learning rate to 9.828987567794199e-05
train 33889
vs, vt 0.2461599834178659 0.25005325878208334
need align? ->  False 0.25005325878208334
2023-09-04 04:08:38,163 - epoch:8, training loss:1.0431 validation loss:0.2462
Updating learning rate to 9.618449887172618e-05
Updating learning rate to 9.618449887172618e-05
train 33889
vs, vt 0.24634692788293416 0.24916997153989293
need align? ->  False 0.24916997153989293
2023-09-04 04:11:54,589 - epoch:9, training loss:1.1100 validation loss:0.2463
Updating learning rate to 9.328889590710838e-05
Updating learning rate to 9.328889590710838e-05
train 33889
vs, vt 0.24582260436463085 0.2514237333512442
need align? ->  False 0.24916997153989293
2023-09-04 04:15:14,906 - epoch:10, training loss:1.0842 validation loss:0.2458
Updating learning rate to 8.965261135362604e-05
Updating learning rate to 8.965261135362604e-05
train 33889
vs, vt 0.24562830397520552 0.25226159033958206
need align? ->  False 0.24916997153989293
2023-09-04 04:18:31,288 - epoch:11, training loss:1.0277 validation loss:0.2456
Updating learning rate to 8.533786304815776e-05
Updating learning rate to 8.533786304815776e-05
train 33889
vs, vt 0.24660190800204873 0.25243592097169976
need align? ->  False 0.24916997153989293
2023-09-04 04:21:46,938 - epoch:12, training loss:0.9840 validation loss:0.2466
Updating learning rate to 8.041847753048434e-05
Updating learning rate to 8.041847753048434e-05
train 33889
vs, vt 0.24604563393884085 0.25122688824988226
need align? ->  False 0.24916997153989293
2023-09-04 04:25:05,289 - epoch:13, training loss:0.9499 validation loss:0.2460
Updating learning rate to 7.497862685072454e-05
Updating learning rate to 7.497862685072454e-05
train 33889
vs, vt 0.243774227543988 0.2523825317621231
need align? ->  False 0.24916997153989293
2023-09-04 04:28:23,797 - epoch:14, training loss:0.9278 validation loss:0.2438
Updating learning rate to 6.911138836222055e-05
Updating learning rate to 6.911138836222055e-05
train 33889
vs, vt 0.24584944393824448 0.2492428165860474
need align? ->  False 0.24916997153989293
2023-09-04 04:31:40,146 - epoch:15, training loss:0.9113 validation loss:0.2458
Updating learning rate to 6.291715214221653e-05
Updating learning rate to 6.291715214221653e-05
train 33889
vs, vt 0.24677992489358241 0.2516668203964152
need align? ->  False 0.24916997153989293
2023-09-04 04:34:58,661 - epoch:16, training loss:0.8970 validation loss:0.2468
Updating learning rate to 5.6501903289803477e-05
Updating learning rate to 5.6501903289803477e-05
train 33889
vs, vt 0.24363024431196126 0.2513725853271105
need align? ->  False 0.24916997153989293
2023-09-04 04:38:19,053 - epoch:17, training loss:0.8869 validation loss:0.2436
Updating learning rate to 4.997540849148917e-05
Updating learning rate to 4.997540849148917e-05
train 33889
vs, vt 0.24479128407653084 0.25052306635982613
need align? ->  False 0.24916997153989293
2023-09-04 04:41:39,689 - epoch:18, training loss:0.8810 validation loss:0.2448
Updating learning rate to 4.344933788275899e-05
Updating learning rate to 4.344933788275899e-05
train 33889
vs, vt 0.24615792913193052 0.2511152518405156
need align? ->  False 0.24916997153989293
2023-09-04 04:44:58,130 - epoch:19, training loss:0.8739 validation loss:0.2462
Updating learning rate to 3.703535434109692e-05
Updating learning rate to 3.703535434109692e-05
train 33889
vs, vt 0.24451487452130427 0.25119609241797164
need align? ->  False 0.24916997153989293
2023-09-04 04:48:20,565 - epoch:20, training loss:0.8686 validation loss:0.2445
Updating learning rate to 3.084320290319299e-05
Updating learning rate to 3.084320290319299e-05
train 33889
vs, vt 0.2451522117903964 0.25061543040316214
need align? ->  False 0.24916997153989293
2023-09-04 04:51:40,723 - epoch:21, training loss:0.8646 validation loss:0.2452
Updating learning rate to 2.4978832996938436e-05
Updating learning rate to 2.4978832996938436e-05
train 33889
vs, vt 0.24511656127023426 0.25097400013526733
need align? ->  False 0.24916997153989293
2023-09-04 04:55:00,316 - epoch:22, training loss:0.8610 validation loss:0.2451
Updating learning rate to 1.954258561733979e-05
Updating learning rate to 1.954258561733979e-05
train 33889
vs, vt 0.24637103148482062 0.25057828693058004
need align? ->  False 0.24916997153989293
2023-09-04 04:58:24,108 - epoch:23, training loss:0.8584 validation loss:0.2464
Updating learning rate to 1.4627476464274535e-05
Updating learning rate to 1.4627476464274535e-05
train 33889
vs, vt 0.245486241299659 0.2511472885327583
need align? ->  False 0.24916997153989293
2023-09-04 05:01:57,210 - epoch:24, training loss:0.8557 validation loss:0.2455
Updating learning rate to 1.0317604418077296e-05
Updating learning rate to 1.0317604418077296e-05
train 33889
vs, vt 0.24522659597410398 0.25081854982470925
need align? ->  False 0.24916997153989293
2023-09-04 05:05:22,366 - epoch:25, training loss:0.8549 validation loss:0.2452
Updating learning rate to 6.686712584380782e-06
Updating learning rate to 6.686712584380782e-06
train 33889
vs, vt 0.2457072320394218 0.25109928410330956
need align? ->  False 0.24916997153989293
2023-09-04 05:08:34,154 - epoch:26, training loss:0.8538 validation loss:0.2457
Updating learning rate to 3.7969265291329562e-06
Updating learning rate to 3.7969265291329562e-06
train 33889
vs, vt 0.2450734366747466 0.25115507896142925
need align? ->  False 0.24916997153989293
2023-09-04 05:11:51,131 - epoch:27, training loss:0.8530 validation loss:0.2451
Updating learning rate to 1.6976912929391648e-06
Updating learning rate to 1.6976912929391648e-06
train 33889
vs, vt 0.24536922019483012 0.2509368445978246
need align? ->  False 0.24916997153989293
2023-09-04 05:15:07,781 - epoch:28, training loss:0.8526 validation loss:0.2454
check exp/ECL-PatchTST2023-09-04-03:39:41.984843/0/0.2436_epoch_17.pkl  &  0.24916997153989293
2023-09-04 05:15:40,037 - [*] loss:0.3735
2023-09-04 05:15:40,072 - [*] phase 0, testing
2023-09-04 05:15:41,066 - T:336	MAE	0.386230	RMSE	0.373390	MAPE	232.289100
2023-09-04 05:15:41,067 - 336	mae	0.3862	
2023-09-04 05:15:41,067 - 336	rmse	0.3734	
2023-09-04 05:15:41,067 - 336	mape	232.2891	
----*-----
2023-09-04 05:16:10,314 - [*] loss:0.3735
2023-09-04 05:16:10,348 - [*] phase 0, testing
2023-09-04 05:16:11,011 - T:336	MAE	0.386230	RMSE	0.373390	MAPE	232.289100
2023-09-04 05:16:37,175 - [*] loss:0.3845
2023-09-04 05:16:37,222 - [*] phase 0, testing
2023-09-04 05:16:37,860 - T:336	MAE	0.399424	RMSE	0.384359	MAPE	242.675662
2023-09-04 05:17:03,878 - [*] loss:0.4256
2023-09-04 05:17:03,914 - [*] phase 0, testing
2023-09-04 05:17:04,999 - T:336	MAE	0.425862	RMSE	0.425658	MAPE	275.956750
2023-09-04 05:17:32,105 - [*] loss:0.3796
2023-09-04 05:17:32,138 - [*] phase 0, testing
2023-09-04 05:17:33,354 - T:336	MAE	0.393733	RMSE	0.379494	MAPE	248.384809
2023-09-04 05:18:04,145 - [*] loss:0.4410
2023-09-04 05:18:04,181 - [*] phase 0, testing
2023-09-04 05:18:05,548 - T:336	MAE	0.449732	RMSE	0.440721	MAPE	246.109033
2023-09-04 05:18:40,032 - [*] loss:0.4124
2023-09-04 05:18:40,066 - [*] phase 0, testing
2023-09-04 05:18:40,940 - T:336	MAE	0.421504	RMSE	0.412122	MAPE	211.297917
2023-09-04 05:19:13,548 - [*] loss:0.3765
2023-09-04 05:19:13,585 - [*] phase 0, testing
2023-09-04 05:19:14,378 - T:336	MAE	0.390181	RMSE	0.376304	MAPE	236.254311
2023-09-04 05:19:44,468 - [*] loss:0.3781
2023-09-04 05:19:44,503 - [*] phase 0, testing
2023-09-04 05:19:45,130 - T:336	MAE	0.394242	RMSE	0.377873	MAPE	220.776677
----*-----
2023-09-04 05:20:03,721 - [*] loss:0.3735
2023-09-04 05:20:03,755 - [*] phase 0, testing
2023-09-04 05:20:04,344 - T:336	MAE	0.392600	RMSE	0.373293	MAPE	222.926474
2023-09-04 05:20:30,587 - [*] loss:0.4253
2023-09-04 05:20:30,622 - [*] phase 0, testing
2023-09-04 05:20:31,376 - T:336	MAE	0.431222	RMSE	0.425275	MAPE	240.576458
2023-09-04 05:20:49,467 - [*] loss:0.3912
2023-09-04 05:20:49,501 - [*] phase 0, testing
2023-09-04 05:20:50,304 - T:336	MAE	0.399642	RMSE	0.391051	MAPE	226.265097
2023-09-04 05:20:50,305 - 336	mae	0.3996	
2023-09-04 05:20:50,305 - 336	rmse	0.3911	
2023-09-04 05:20:50,305 - 336	mape	226.2651	
2023-09-04 05:20:52,741 - logger name:exp/ECL-PatchTST2023-09-04-05:20:52.741333/ECL-PatchTST.log
2023-09-04 05:20:52,741 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'abl_tmp_context': 0, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-04-05:20:52.741333', 'path': 'exp/ECL-PatchTST2023-09-04-05:20:52.741333', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 05:20:52,741 - [*] phase 0 start training
0 69680
train 33889
val 11185
test 11185
2023-09-04 05:20:53,639 - [*] phase 0 Dataset load!
2023-09-04 05:20:54,590 - [*] phase 0 Training start
train 33889
2023-09-04 05:22:32,637 - epoch:0, training loss:0.2079 validation loss:0.2602
train 33889
vs, vt 0.2602407373064621 0.26377567885951564
Updating learning rate to 1.0438721218484657e-05
Updating learning rate to 1.0438721218484657e-05
train 33889
vs, vt 0.2550791253928434 0.2531480426815423
need align? ->  True 0.2531480426815423
2023-09-04 05:26:42,874 - epoch:1, training loss:10.9219 validation loss:0.2551
Updating learning rate to 2.802750441854847e-05
Updating learning rate to 2.802750441854847e-05
train 33889
vs, vt 0.2542053658769212 0.2528336205604402
need align? ->  True 0.2528336205604402
2023-09-04 05:29:58,491 - epoch:2, training loss:6.0570 validation loss:0.2542
Updating learning rate to 5.2047629950292354e-05
Updating learning rate to 5.2047629950292354e-05
train 33889
vs, vt 0.2520723933438686 0.2519698010503568
need align? ->  True 0.2519698010503568
2023-09-04 05:33:16,113 - epoch:3, training loss:2.3132 validation loss:0.2521
Updating learning rate to 7.605497731655361e-05
Updating learning rate to 7.605497731655361e-05
train 33889
vs, vt 0.2522860673594881 0.25132553855126555
need align? ->  True 0.25132553855126555
2023-09-04 05:36:31,275 - epoch:4, training loss:1.5793 validation loss:0.2523
Updating learning rate to 9.3608854147054e-05
Updating learning rate to 9.3608854147054e-05
train 33889
vs, vt 0.24808183680711823 0.2518507447517054
need align? ->  False 0.25132553855126555
2023-09-04 05:39:44,884 - epoch:5, training loss:1.3957 validation loss:0.2481
Updating learning rate to 9.99999938537861e-05
Updating learning rate to 9.99999938537861e-05
train 33889
vs, vt 0.24778201896697283 0.24907218148423868
need align? ->  False 0.24907218148423868
2023-09-04 05:43:00,572 - epoch:6, training loss:1.2114 validation loss:0.2478
Updating learning rate to 9.956900274488073e-05
Updating learning rate to 9.956900274488073e-05
train 33889
vs, vt 0.2472755146974867 0.25019151607358997
need align? ->  False 0.24907218148423868
2023-09-04 05:46:16,115 - epoch:7, training loss:1.1533 validation loss:0.2473
Updating learning rate to 9.828987567794199e-05
Updating learning rate to 9.828987567794199e-05
train 33889
vs, vt 0.2447996261868287 0.2519763908772306
need align? ->  False 0.24907218148423868
2023-09-04 05:49:32,157 - epoch:8, training loss:1.0447 validation loss:0.2448
Updating learning rate to 9.618449887172618e-05
Updating learning rate to 9.618449887172618e-05
train 33889
vs, vt 0.24713395091451026 0.25194906151260843
need align? ->  False 0.24907218148423868
2023-09-04 05:52:46,885 - epoch:9, training loss:0.9912 validation loss:0.2471
Updating learning rate to 9.328889590710838e-05
Updating learning rate to 9.328889590710838e-05
train 33889
vs, vt 0.2447593672285703 0.25117963704873214
need align? ->  False 0.24907218148423868
2023-09-04 05:56:00,600 - epoch:10, training loss:0.9552 validation loss:0.2448
Updating learning rate to 8.965261135362604e-05
Updating learning rate to 8.965261135362604e-05
train 33889
vs, vt 0.2453475441699001 0.25046834468164225
need align? ->  False 0.24907218148423868
2023-09-04 05:59:15,525 - epoch:11, training loss:0.9302 validation loss:0.2453
Updating learning rate to 8.533786304815776e-05
Updating learning rate to 8.533786304815776e-05
train 33889
vs, vt 0.24555878548628904 0.25230596371164377
need align? ->  False 0.24907218148423868
2023-09-04 06:02:31,381 - epoch:12, training loss:0.9103 validation loss:0.2456
Updating learning rate to 8.041847753048434e-05
Updating learning rate to 8.041847753048434e-05
train 33889
vs, vt 0.24439873821525412 0.25229760771617293
need align? ->  False 0.24907218148423868
2023-09-04 06:05:48,462 - epoch:13, training loss:0.8941 validation loss:0.2444
Updating learning rate to 7.497862685072454e-05
Updating learning rate to 7.497862685072454e-05
train 33889
vs, vt 0.24475434922020545 0.2531272876923057
need align? ->  False 0.24907218148423868
2023-09-04 06:09:10,580 - epoch:14, training loss:0.8798 validation loss:0.2448
Updating learning rate to 6.911138836222055e-05
Updating learning rate to 6.911138836222055e-05
train 33889
vs, vt 0.24547987405888058 0.2523756442506882
need align? ->  False 0.24907218148423868
2023-09-04 06:12:45,665 - epoch:15, training loss:0.8681 validation loss:0.2455
Updating learning rate to 6.291715214221653e-05
Updating learning rate to 6.291715214221653e-05
train 33889
vs, vt 0.24476720337671312 0.25318614241074433
need align? ->  False 0.24907218148423868
2023-09-04 06:16:07,001 - epoch:16, training loss:0.8587 validation loss:0.2448
Updating learning rate to 5.6501903289803477e-05
Updating learning rate to 5.6501903289803477e-05
train 33889
vs, vt 0.2443332573060285 0.25200577689842746
need align? ->  False 0.24907218148423868
2023-09-04 06:19:25,815 - epoch:17, training loss:0.8509 validation loss:0.2443
Updating learning rate to 4.997540849148917e-05
Updating learning rate to 4.997540849148917e-05
train 33889
vs, vt 0.24607044602320952 0.251376326915554
need align? ->  False 0.24907218148423868
2023-09-04 06:22:43,700 - epoch:18, training loss:0.8441 validation loss:0.2461
Updating learning rate to 4.344933788275899e-05
Updating learning rate to 4.344933788275899e-05
train 33889
vs, vt 0.24390153832394967 0.2517168127762323
need align? ->  False 0.24907218148423868
2023-09-04 06:25:58,902 - epoch:19, training loss:0.8383 validation loss:0.2439
Updating learning rate to 3.703535434109692e-05
Updating learning rate to 3.703535434109692e-05
train 33889
vs, vt 0.2442429724403403 0.251414435365322
need align? ->  False 0.24907218148423868
2023-09-04 06:29:17,576 - epoch:20, training loss:0.8337 validation loss:0.2442
Updating learning rate to 3.084320290319299e-05
Updating learning rate to 3.084320290319299e-05
train 33889
vs, vt 0.24658593924885447 0.2510067100044001
need align? ->  False 0.24907218148423868
2023-09-04 06:32:37,972 - epoch:21, training loss:0.8301 validation loss:0.2466
Updating learning rate to 2.4978832996938436e-05
Updating learning rate to 2.4978832996938436e-05
train 33889
vs, vt 0.2445295295170085 0.2522739050909877
need align? ->  False 0.24907218148423868
2023-09-04 06:35:58,204 - epoch:22, training loss:0.8267 validation loss:0.2445
Updating learning rate to 1.954258561733979e-05
Updating learning rate to 1.954258561733979e-05
train 33889
vs, vt 0.2465521782975305 0.25255105310035025
need align? ->  False 0.24907218148423868
2023-09-04 06:39:17,481 - epoch:23, training loss:0.8251 validation loss:0.2466
Updating learning rate to 1.4627476464274535e-05
Updating learning rate to 1.4627476464274535e-05
train 33889
vs, vt 0.24560664941302754 0.25251985116946424
need align? ->  False 0.24907218148423868
2023-09-04 06:42:58,429 - epoch:24, training loss:0.8229 validation loss:0.2456
Updating learning rate to 1.0317604418077296e-05
Updating learning rate to 1.0317604418077296e-05
train 33889
vs, vt 0.24513752936300906 0.252102978781543
need align? ->  False 0.24907218148423868
2023-09-04 06:46:29,012 - epoch:25, training loss:0.8215 validation loss:0.2451
Updating learning rate to 6.686712584380782e-06
Updating learning rate to 6.686712584380782e-06
train 33889
vs, vt 0.24426864769140427 0.25146968705071643
need align? ->  False 0.24907218148423868
2023-09-04 06:49:44,191 - epoch:26, training loss:0.8209 validation loss:0.2443
Updating learning rate to 3.7969265291329562e-06
Updating learning rate to 3.7969265291329562e-06
train 33889
vs, vt 0.2448446319692514 0.2512062727473676
need align? ->  False 0.24907218148423868
2023-09-04 06:53:03,739 - epoch:27, training loss:0.8207 validation loss:0.2448
Updating learning rate to 1.6976912929391648e-06
Updating learning rate to 1.6976912929391648e-06
train 33889
vs, vt 0.24492976408113132 0.25143858096139
need align? ->  False 0.24907218148423868
2023-09-04 06:56:22,318 - epoch:28, training loss:0.8201 validation loss:0.2449
Updating learning rate to 4.2492537270863806e-07
Updating learning rate to 4.2492537270863806e-07
train 33889
vs, vt 0.24498542152683844 0.25139259068634023
need align? ->  False 0.24907218148423868
2023-09-04 06:59:39,878 - epoch:29, training loss:0.8202 validation loss:0.2450
Updating learning rate to 4.0614621390542476e-10
Updating learning rate to 4.0614621390542476e-10
check exp/ECL-PatchTST2023-09-04-05:20:52.741333/0/0.2439_epoch_19.pkl  &  0.24907218148423868
2023-09-04 07:00:14,718 - [*] loss:0.3681
2023-09-04 07:00:14,763 - [*] phase 0, testing
2023-09-04 07:00:15,615 - T:336	MAE	0.386590	RMSE	0.367811	MAPE	234.223700
2023-09-04 07:00:15,616 - 336	mae	0.3866	
2023-09-04 07:00:15,616 - 336	rmse	0.3678	
2023-09-04 07:00:15,617 - 336	mape	234.2237	
----*-----
2023-09-04 07:00:42,044 - [*] loss:0.3681
2023-09-04 07:00:42,085 - [*] phase 0, testing
2023-09-04 07:00:42,689 - T:336	MAE	0.386590	RMSE	0.367811	MAPE	234.223700
2023-09-04 07:01:10,442 - [*] loss:0.3810
2023-09-04 07:01:10,484 - [*] phase 0, testing
2023-09-04 07:01:11,167 - T:336	MAE	0.400894	RMSE	0.380696	MAPE	248.956561
2023-09-04 07:01:36,606 - [*] loss:0.4234
2023-09-04 07:01:36,662 - [*] phase 0, testing
2023-09-04 07:01:37,298 - T:336	MAE	0.426684	RMSE	0.423243	MAPE	278.373766
2023-09-04 07:02:07,185 - [*] loss:0.3787
2023-09-04 07:02:07,227 - [*] phase 0, testing
2023-09-04 07:02:07,859 - T:336	MAE	0.398805	RMSE	0.378464	MAPE	257.306671
2023-09-04 07:02:43,018 - [*] loss:0.4393
2023-09-04 07:02:43,061 - [*] phase 0, testing
2023-09-04 07:02:43,748 - T:336	MAE	0.449812	RMSE	0.439043	MAPE	250.192738
2023-09-04 07:03:16,909 - [*] loss:0.4058
2023-09-04 07:03:16,953 - [*] phase 0, testing
2023-09-04 07:03:17,812 - T:336	MAE	0.418207	RMSE	0.405234	MAPE	214.562917
2023-09-04 07:03:49,284 - [*] loss:0.3720
2023-09-04 07:03:49,328 - [*] phase 0, testing
2023-09-04 07:03:50,057 - T:336	MAE	0.391155	RMSE	0.371701	MAPE	239.301658
2023-09-04 07:04:17,555 - [*] loss:0.3766
2023-09-04 07:04:17,599 - [*] phase 0, testing
2023-09-04 07:04:18,248 - T:336	MAE	0.396706	RMSE	0.376170	MAPE	222.504258
----*-----
2023-09-04 07:04:36,317 - [*] loss:0.3738
2023-09-04 07:04:36,357 - [*] phase 0, testing
2023-09-04 07:04:37,000 - T:336	MAE	0.395529	RMSE	0.373366	MAPE	226.045823
2023-09-04 07:05:03,383 - [*] loss:0.4261
2023-09-04 07:05:03,426 - [*] phase 0, testing
2023-09-04 07:05:04,037 - T:336	MAE	0.425508	RMSE	0.426110	MAPE	224.807787
2023-09-04 07:05:21,082 - [*] loss:0.3831
2023-09-04 07:05:21,124 - [*] phase 0, testing
2023-09-04 07:05:21,777 - T:336	MAE	0.398064	RMSE	0.382876	MAPE	223.453307
2023-09-04 07:05:21,778 - 336	mae	0.3981	
2023-09-04 07:05:21,778 - 336	rmse	0.3829	
2023-09-04 07:05:21,778 - 336	mape	223.4533	
2023-09-04 07:05:24,008 - logger name:exp/ECL-PatchTST2023-09-04-07:05:24.007863/ECL-PatchTST.log
2023-09-04 07:05:24,008 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTm1', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'abl_tmp_context': 0, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 36, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-04-07:05:24.007863', 'path': 'exp/ECL-PatchTST2023-09-04-07:05:24.007863', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 07:05:24,009 - [*] phase 0 start training
0 69680
train 33889
val 11185
test 11185
2023-09-04 07:05:24,866 - [*] phase 0 Dataset load!
2023-09-04 07:05:25,878 - [*] phase 0 Training start
train 33889
2023-09-04 07:07:03,780 - epoch:0, training loss:0.2094 validation loss:0.2606
train 33889
vs, vt 0.2606062770567157 0.26400578081269155
Updating learning rate to 1.0438721218484657e-05
Updating learning rate to 1.0438721218484657e-05
train 33889
vs, vt 0.2528427155180411 0.25367980962619185
need align? ->  False 0.25367980962619185
2023-09-04 07:11:12,510 - epoch:1, training loss:10.9370 validation loss:0.2528
Updating learning rate to 2.802750441854847e-05
Updating learning rate to 2.802750441854847e-05
train 33889
vs, vt 0.25449341687966476 0.25213400630111044
need align? ->  True 0.25213400630111044
2023-09-04 07:14:30,157 - epoch:2, training loss:6.2695 validation loss:0.2545
Updating learning rate to 5.2047629950292354e-05
Updating learning rate to 5.2047629950292354e-05
train 33889
vs, vt 0.2517876736819744 0.25314226043833926
need align? ->  False 0.25213400630111044
2023-09-04 07:17:44,599 - epoch:3, training loss:2.4347 validation loss:0.2518
Updating learning rate to 7.605497731655361e-05
Updating learning rate to 7.605497731655361e-05
train 33889
vs, vt 0.2526517893949693 0.25153435656631534
need align? ->  True 0.25153435656631534
2023-09-04 07:21:01,467 - epoch:4, training loss:1.6448 validation loss:0.2527
Updating learning rate to 9.3608854147054e-05
Updating learning rate to 9.3608854147054e-05
train 33889
vs, vt 0.2520092778246511 0.24997741834853182
need align? ->  True 0.24997741834853182
2023-09-04 07:24:17,498 - epoch:5, training loss:1.4366 validation loss:0.2520
Updating learning rate to 9.99999938537861e-05
Updating learning rate to 9.99999938537861e-05
train 33889
vs, vt 0.24668734207410703 0.2518797641704706
need align? ->  False 0.24997741834853182
2023-09-04 07:27:34,553 - epoch:6, training loss:1.2871 validation loss:0.2467
Updating learning rate to 9.956900274488073e-05
Updating learning rate to 9.956900274488073e-05
train 33889
vs, vt 0.2462173786691644 0.25020876044238155
need align? ->  False 0.24997741834853182
2023-09-04 07:30:50,340 - epoch:7, training loss:1.0953 validation loss:0.2462
Updating learning rate to 9.828987567794199e-05
Updating learning rate to 9.828987567794199e-05
train 33889
vs, vt 0.2441613691096956 0.2516565806317059
need align? ->  False 0.24997741834853182
2023-09-04 07:34:06,728 - epoch:8, training loss:1.0066 validation loss:0.2442
Updating learning rate to 9.618449887172618e-05
Updating learning rate to 9.618449887172618e-05
train 33889
vs, vt 0.24730854299427432 0.25040379856628453
need align? ->  False 0.24997741834853182
2023-09-04 07:37:23,219 - epoch:9, training loss:0.9605 validation loss:0.2473
Updating learning rate to 9.328889590710838e-05
Updating learning rate to 9.328889590710838e-05
train 33889
vs, vt 0.24897558597678487 0.25312413271008566
need align? ->  False 0.24997741834853182
2023-09-04 07:40:42,299 - epoch:10, training loss:0.9300 validation loss:0.2490
Updating learning rate to 8.965261135362604e-05
Updating learning rate to 8.965261135362604e-05
train 33889
vs, vt 0.24603818356990814 0.25464285795830865
need align? ->  False 0.24997741834853182
2023-09-04 07:44:04,319 - epoch:11, training loss:0.9090 validation loss:0.2460
Updating learning rate to 8.533786304815776e-05
Updating learning rate to 8.533786304815776e-05
train 33889
vs, vt 0.24551252597434955 0.2525062848475169
need align? ->  False 0.24997741834853182
2023-09-04 07:47:24,080 - epoch:12, training loss:0.8925 validation loss:0.2455
Updating learning rate to 8.041847753048434e-05
Updating learning rate to 8.041847753048434e-05
train 33889
vs, vt 0.24503000656312163 0.2523442483507097
need align? ->  False 0.24997741834853182
2023-09-04 07:50:45,431 - epoch:13, training loss:0.8793 validation loss:0.2450
Updating learning rate to 7.497862685072454e-05
Updating learning rate to 7.497862685072454e-05
train 33889
vs, vt 0.2465789202516052 0.2528068299659274
need align? ->  False 0.24997741834853182
2023-09-04 07:54:04,348 - epoch:14, training loss:0.8673 validation loss:0.2466
Updating learning rate to 6.911138836222055e-05
Updating learning rate to 6.911138836222055e-05
train 33889
vs, vt 0.24496311914514413 0.2515885704620318
need align? ->  False 0.24997741834853182
2023-09-04 07:57:22,293 - epoch:15, training loss:0.8575 validation loss:0.2450
Updating learning rate to 6.291715214221653e-05
Updating learning rate to 6.291715214221653e-05
train 33889
vs, vt 0.2434457731856541 0.2529742507348684
need align? ->  False 0.24997741834853182
2023-09-04 08:00:41,895 - epoch:16, training loss:0.8493 validation loss:0.2434
Updating learning rate to 5.6501903289803477e-05
Updating learning rate to 5.6501903289803477e-05
train 33889
vs, vt 0.24622938240116293 0.2514527405781502
need align? ->  False 0.24997741834853182
2023-09-04 08:04:03,885 - epoch:17, training loss:0.8416 validation loss:0.2462
Updating learning rate to 4.997540849148917e-05
Updating learning rate to 4.997540849148917e-05
train 33889
vs, vt 0.24553809865293177 0.25249755031175236
need align? ->  False 0.24997741834853182
2023-09-04 08:07:27,029 - epoch:18, training loss:0.8353 validation loss:0.2455
Updating learning rate to 4.344933788275899e-05
Updating learning rate to 4.344933788275899e-05
train 33889
vs, vt 0.24508096006783572 0.25151636625047435
need align? ->  False 0.24997741834853182
2023-09-04 08:10:50,908 - epoch:19, training loss:0.8302 validation loss:0.2451
Updating learning rate to 3.703535434109692e-05
Updating learning rate to 3.703535434109692e-05
train 33889
vs, vt 0.24473041520369324 0.2515812032348053
need align? ->  False 0.24997741834853182
2023-09-04 08:14:14,119 - epoch:20, training loss:0.8255 validation loss:0.2447
Updating learning rate to 3.084320290319299e-05
Updating learning rate to 3.084320290319299e-05
train 33889
vs, vt 0.2447025442749939 0.2515214453028007
need align? ->  False 0.24997741834853182
2023-09-04 08:18:11,361 - epoch:21, training loss:0.8227 validation loss:0.2447
Updating learning rate to 2.4978832996938436e-05
Updating learning rate to 2.4978832996938436e-05
train 33889
vs, vt 0.24547525063495745 0.2503062531098046
need align? ->  False 0.24997741834853182
2023-09-04 08:21:44,437 - epoch:22, training loss:0.8199 validation loss:0.2455
Updating learning rate to 1.954258561733979e-05
Updating learning rate to 1.954258561733979e-05
train 33889
vs, vt 0.2455073576843874 0.25135220075026155
need align? ->  False 0.24997741834853182
2023-09-04 08:25:01,709 - epoch:23, training loss:0.8177 validation loss:0.2455
Updating learning rate to 1.4627476464274535e-05
Updating learning rate to 1.4627476464274535e-05
train 33889
vs, vt 0.24539175570349803 0.251298690257086
need align? ->  False 0.24997741834853182
2023-09-04 08:28:18,254 - epoch:24, training loss:0.8158 validation loss:0.2454
Updating learning rate to 1.0317604418077296e-05
Updating learning rate to 1.0317604418077296e-05
train 33889
vs, vt 0.2452927397733385 0.2510522677647797
need align? ->  False 0.24997741834853182
2023-09-04 08:31:34,324 - epoch:25, training loss:0.8145 validation loss:0.2453
Updating learning rate to 6.686712584380782e-06
Updating learning rate to 6.686712584380782e-06
train 33889
vs, vt 0.2452499132857404 0.2511146284470504
need align? ->  False 0.24997741834853182
2023-09-04 08:34:46,850 - epoch:26, training loss:0.8137 validation loss:0.2452
Updating learning rate to 3.7969265291329562e-06
Updating learning rate to 3.7969265291329562e-06
train 33889
vs, vt 0.2455445788377388 0.25098223480480636
need align? ->  False 0.24997741834853182
2023-09-04 08:38:08,244 - epoch:27, training loss:0.8130 validation loss:0.2455
check exp/ECL-PatchTST2023-09-04-07:05:24.007863/0/0.2434_epoch_16.pkl  &  0.24997741834853182
2023-09-04 08:38:34,588 - [*] loss:0.3736
2023-09-04 08:38:34,626 - [*] phase 0, testing
2023-09-04 08:38:35,273 - T:336	MAE	0.385720	RMSE	0.373490	MAPE	233.427882
2023-09-04 08:38:35,273 - 336	mae	0.3857	
2023-09-04 08:38:35,273 - 336	rmse	0.3735	
2023-09-04 08:38:35,273 - 336	mape	233.4279	
----*-----
2023-09-04 08:39:04,280 - [*] loss:0.3736
2023-09-04 08:39:04,331 - [*] phase 0, testing
2023-09-04 08:39:04,952 - T:336	MAE	0.385720	RMSE	0.373490	MAPE	233.427882
2023-09-04 08:39:32,042 - [*] loss:0.3841
2023-09-04 08:39:32,076 - [*] phase 0, testing
2023-09-04 08:39:32,743 - T:336	MAE	0.399205	RMSE	0.383871	MAPE	244.937897
2023-09-04 08:40:01,472 - [*] loss:0.4125
2023-09-04 08:40:01,506 - [*] phase 0, testing
2023-09-04 08:40:02,120 - T:336	MAE	0.417692	RMSE	0.412395	MAPE	271.190000
2023-09-04 08:40:34,198 - [*] loss:0.3807
2023-09-04 08:40:34,231 - [*] phase 0, testing
2023-09-04 08:40:34,872 - T:336	MAE	0.394091	RMSE	0.380590	MAPE	250.352025
2023-09-04 08:41:06,591 - [*] loss:0.4375
2023-09-04 08:41:06,626 - [*] phase 0, testing
2023-09-04 08:41:07,382 - T:336	MAE	0.444950	RMSE	0.437308	MAPE	247.682381
2023-09-04 08:41:36,954 - [*] loss:0.4095
2023-09-04 08:41:36,988 - [*] phase 0, testing
2023-09-04 08:41:37,584 - T:336	MAE	0.418132	RMSE	0.409106	MAPE	210.761213
2023-09-04 08:42:07,404 - [*] loss:0.3768
2023-09-04 08:42:07,441 - [*] phase 0, testing
2023-09-04 08:42:08,139 - T:336	MAE	0.390088	RMSE	0.376661	MAPE	237.533402
2023-09-04 08:42:34,752 - [*] loss:0.3785
2023-09-04 08:42:34,787 - [*] phase 0, testing
2023-09-04 08:42:35,371 - T:336	MAE	0.395048	RMSE	0.378226	MAPE	219.603372
----*-----
2023-09-04 08:42:55,708 - [*] loss:0.3722
2023-09-04 08:42:55,746 - [*] phase 0, testing
2023-09-04 08:42:56,416 - T:336	MAE	0.392885	RMSE	0.371855	MAPE	220.708513
2023-09-04 08:43:23,061 - [*] loss:0.4213
2023-09-04 08:43:23,095 - [*] phase 0, testing
2023-09-04 08:43:23,733 - T:336	MAE	0.419956	RMSE	0.421260	MAPE	220.476699
2023-09-04 08:43:44,505 - [*] loss:0.3799
2023-09-04 08:43:44,551 - [*] phase 0, testing
2023-09-04 08:43:45,202 - T:336	MAE	0.397380	RMSE	0.379721	MAPE	219.474077
2023-09-04 08:43:45,203 - 336	mae	0.3974	
2023-09-04 08:43:45,203 - 336	rmse	0.3797	
2023-09-04 08:43:45,203 - 336	mape	219.4741	
2023-09-04 08:43:47,514 - logger name:exp/ECL-PatchTST2023-09-04-08:43:47.514630/ECL-PatchTST.log
2023-09-04 08:43:47,515 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'abl_tmp_context': 0, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 36, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-04-08:43:47.514630', 'path': 'exp/ECL-PatchTST2023-09-04-08:43:47.514630', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 08:43:47,515 - [*] phase 0 start training
0 17420
train 7585
val 2161
test 2161
2023-09-04 08:43:47,717 - [*] phase 0 Dataset load!
2023-09-04 08:43:48,716 - [*] phase 0 Training start
train 7585
2023-09-04 08:44:09,221 - epoch:0, training loss:0.3272 validation loss:0.2922
train 7585
vs, vt 0.2921844326397952 0.2930576025563128
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.26906805573140874 0.274128551430562
need align? ->  False 0.274128551430562
2023-09-04 08:45:06,628 - epoch:1, training loss:13.0066 validation loss:0.2691
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.2625954615719178 0.26323852249804663
need align? ->  False 0.26323852249804663
2023-09-04 08:45:50,565 - epoch:2, training loss:12.1929 validation loss:0.2626
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.26638512926943164 0.2617902562898748
need align? ->  True 0.2617902562898748
2023-09-04 08:46:36,060 - epoch:3, training loss:10.9289 validation loss:0.2664
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.2637556931551765 0.26861727106220584
need align? ->  True 0.2617902562898748
2023-09-04 08:47:20,741 - epoch:4, training loss:8.3156 validation loss:0.2638
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.26315638422966003 0.2716726673876538
need align? ->  True 0.2617902562898748
2023-09-04 08:48:04,726 - epoch:5, training loss:5.6710 validation loss:0.2632
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.25924968018251304 0.274801546598182
need align? ->  False 0.2617902562898748
2023-09-04 08:48:49,284 - epoch:6, training loss:4.2086 validation loss:0.2592
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.25796062586938634 0.2776349515599363
need align? ->  False 0.2617902562898748
2023-09-04 08:49:34,241 - epoch:7, training loss:3.6391 validation loss:0.2580
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.259738935267224 0.27657973152749676
need align? ->  False 0.2617902562898748
2023-09-04 08:50:18,475 - epoch:8, training loss:3.3434 validation loss:0.2597
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.25737306037369895 0.2790587794254808
need align? ->  False 0.2617902562898748
2023-09-04 08:51:03,122 - epoch:9, training loss:3.1150 validation loss:0.2574
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.25854139731210823 0.2788470374310718
need align? ->  False 0.2617902562898748
2023-09-04 08:51:47,054 - epoch:10, training loss:2.9856 validation loss:0.2585
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.261273510315839 0.28342023185070825
need align? ->  False 0.2617902562898748
2023-09-04 08:52:31,579 - epoch:11, training loss:2.9209 validation loss:0.2613
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.2590677996768671 0.287570018102141
need align? ->  False 0.2617902562898748
2023-09-04 08:53:15,659 - epoch:12, training loss:2.8658 validation loss:0.2591
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.2573666761026663 0.2854694504948223
need align? ->  False 0.2617902562898748
2023-09-04 08:54:00,089 - epoch:13, training loss:2.8268 validation loss:0.2574
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.2608513858388452 0.2854611641343902
need align? ->  False 0.2617902562898748
2023-09-04 08:54:44,129 - epoch:14, training loss:2.7905 validation loss:0.2609
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.2602670017410727 0.29020647310158787
need align? ->  False 0.2617902562898748
2023-09-04 08:55:27,306 - epoch:15, training loss:2.7511 validation loss:0.2603
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.2586829767507665 0.2895587494268137
need align? ->  False 0.2617902562898748
2023-09-04 08:56:11,206 - epoch:16, training loss:2.7270 validation loss:0.2587
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.2602638426948996 0.2882478267830961
need align? ->  False 0.2617902562898748
2023-09-04 08:56:59,525 - epoch:17, training loss:2.6920 validation loss:0.2603
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.2589717546806616 0.29125359566772685
need align? ->  False 0.2617902562898748
2023-09-04 08:57:47,034 - epoch:18, training loss:2.6710 validation loss:0.2590
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.2595740109682083 0.2909630146973273
need align? ->  False 0.2617902562898748
2023-09-04 08:58:29,506 - epoch:19, training loss:2.6433 validation loss:0.2596
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.25790417851770625 0.2922876148539431
need align? ->  False 0.2617902562898748
2023-09-04 08:59:14,880 - epoch:20, training loss:2.6208 validation loss:0.2579
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.2596452630618039 0.2911379446878153
need align? ->  False 0.2617902562898748
2023-09-04 08:59:59,561 - epoch:21, training loss:2.6058 validation loss:0.2596
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.2598060601774384 0.29307694557835073
need align? ->  False 0.2617902562898748
2023-09-04 09:00:45,224 - epoch:22, training loss:2.5909 validation loss:0.2598
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.26010867909473534 0.2928717925268061
need align? ->  False 0.2617902562898748
2023-09-04 09:01:29,810 - epoch:23, training loss:2.5729 validation loss:0.2601
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.25977470725774765 0.2923778435763191
need align? ->  False 0.2617902562898748
2023-09-04 09:02:14,220 - epoch:24, training loss:2.5627 validation loss:0.2598
check exp/ECL-PatchTST2023-09-04-08:43:47.514630/0/0.2574_epoch_13.pkl  &  0.2617902562898748
2023-09-04 09:02:20,540 - [*] loss:0.3879
2023-09-04 09:02:20,556 - [*] phase 0, testing
2023-09-04 09:02:20,807 - T:720	MAE	0.420931	RMSE	0.386239	MAPE	190.191853
2023-09-04 09:02:20,808 - 720	mae	0.4209	
2023-09-04 09:02:20,808 - 720	rmse	0.3862	
2023-09-04 09:02:20,808 - 720	mape	190.1919	
----*-----
2023-09-04 09:02:26,210 - [*] loss:0.3879
2023-09-04 09:02:26,224 - [*] phase 0, testing
2023-09-04 09:02:26,482 - T:720	MAE	0.420931	RMSE	0.386239	MAPE	190.191853
2023-09-04 09:02:31,741 - [*] loss:0.4060
2023-09-04 09:02:31,755 - [*] phase 0, testing
2023-09-04 09:02:32,010 - T:720	MAE	0.444419	RMSE	0.404482	MAPE	220.015836
2023-09-04 09:02:37,232 - [*] loss:0.3971
2023-09-04 09:02:37,246 - [*] phase 0, testing
2023-09-04 09:02:37,505 - T:720	MAE	0.431753	RMSE	0.395607	MAPE	193.402731
2023-09-04 09:02:43,071 - [*] loss:0.3835
2023-09-04 09:02:43,086 - [*] phase 0, testing
2023-09-04 09:02:43,327 - T:720	MAE	0.417026	RMSE	0.381605	MAPE	183.459282
2023-09-04 09:02:49,153 - [*] loss:0.4527
2023-09-04 09:02:49,167 - [*] phase 0, testing
2023-09-04 09:02:49,411 - T:720	MAE	0.479119	RMSE	0.451334	MAPE	269.463754
2023-09-04 09:02:55,325 - [*] loss:0.4667
2023-09-04 09:02:55,342 - [*] phase 0, testing
2023-09-04 09:02:55,598 - T:720	MAE	0.479117	RMSE	0.465194	MAPE	172.308123
2023-09-04 09:03:02,110 - [*] loss:0.3936
2023-09-04 09:03:02,125 - [*] phase 0, testing
2023-09-04 09:03:02,371 - T:720	MAE	0.429099	RMSE	0.392051	MAPE	200.548077
2023-09-04 09:03:08,647 - [*] loss:0.4195
2023-09-04 09:03:08,668 - [*] phase 0, testing
2023-09-04 09:03:08,954 - T:720	MAE	0.449766	RMSE	0.417362	MAPE	180.520082
----*-----
2023-09-04 09:03:12,905 - [*] loss:0.4187
2023-09-04 09:03:12,919 - [*] phase 0, testing
2023-09-04 09:03:13,165 - T:720	MAE	0.448508	RMSE	0.416642	MAPE	179.113853
2023-09-04 09:03:18,232 - [*] loss:0.4457
2023-09-04 09:03:18,250 - [*] phase 0, testing
2023-09-04 09:03:18,501 - T:720	MAE	0.465837	RMSE	0.443505	MAPE	179.645872
2023-09-04 09:03:22,135 - [*] loss:0.4557
2023-09-04 09:03:22,153 - [*] phase 0, testing
2023-09-04 09:03:22,401 - T:720	MAE	0.471689	RMSE	0.453667	MAPE	178.156662
2023-09-04 09:03:22,401 - 720	mae	0.4717	
2023-09-04 09:03:22,402 - 720	rmse	0.4537	
2023-09-04 09:03:22,402 - 720	mape	178.1567	
2023-09-04 09:03:24,661 - logger name:exp/ECL-PatchTST2023-09-04-09:03:24.661336/ECL-PatchTST.log
2023-09-04 09:03:24,661 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'mainrs': 0, 'abl': 1, 'abl_tmp_context': 0, 'abl_ae': 0, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'lr': 0.0001, 'lradj': 'TST', 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'd_ff': 256, 'd_model': 128, 'n_heads': 16, 'seq_len': 336, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'add_norm': 0, 'jitter_sigma': 0.4, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'summary': 0, 'omega': 1.0, 'theta': 1.5, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 1, 'add_FFN': 1, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, 'early_break': 0, 'early_stop': 10, '/* model related args*/': '//', 'model_name': 'PatchTST', 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-09-04-09:03:24.661336', 'path': 'exp/ECL-PatchTST2023-09-04-09:03:24.661336', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-09-04 09:03:24,661 - [*] phase 0 start training
0 17420
train 7585
val 2161
test 2161
2023-09-04 09:03:24,865 - [*] phase 0 Dataset load!
2023-09-04 09:03:25,884 - [*] phase 0 Training start
train 7585
2023-09-04 09:03:47,321 - epoch:0, training loss:0.3265 validation loss:0.2917
train 7585
vs, vt 0.2916712024632622 0.2926822401144925
Updating learning rate to 1.0466425117684725e-05
Updating learning rate to 1.0466425117684725e-05
train 7585
vs, vt 0.2697030766045346 0.27383003778317394
need align? ->  False 0.27383003778317394
2023-09-04 09:04:44,344 - epoch:1, training loss:13.0151 validation loss:0.2697
Updating learning rate to 2.812342322896289e-05
Updating learning rate to 2.812342322896289e-05
train 7585
vs, vt 0.2650350021088825 0.2625948076739031
need align? ->  True 0.2625948076739031
2023-09-04 09:05:29,700 - epoch:2, training loss:12.1821 validation loss:0.2650
Updating learning rate to 5.221359199676445e-05
Updating learning rate to 5.221359199676445e-05
train 7585
vs, vt 0.2632511722690919 0.2630271951065344
need align? ->  True 0.2625948076739031
2023-09-04 09:06:14,077 - epoch:3, training loss:10.8915 validation loss:0.2633
Updating learning rate to 7.624621173736545e-05
Updating learning rate to 7.624621173736545e-05
train 7585
vs, vt 0.257496896035531 0.26721811206901774
need align? ->  False 0.2625948076739031
2023-09-04 09:06:57,464 - epoch:4, training loss:8.9509 validation loss:0.2575
Updating learning rate to 9.374606845349967e-05
Updating learning rate to 9.374606845349967e-05
train 7585
vs, vt 0.26102878679247465 0.26377123550457116
need align? ->  False 0.2625948076739031
2023-09-04 09:07:41,878 - epoch:5, training loss:6.5366 validation loss:0.2610
Updating learning rate to 9.999987694158076e-05
Updating learning rate to 9.999987694158076e-05
train 7585
vs, vt 0.260798664215733 0.2705167054253466
need align? ->  False 0.2625948076739031
2023-09-04 09:08:27,840 - epoch:6, training loss:5.2857 validation loss:0.2608
Updating learning rate to 9.955764331963416e-05
Updating learning rate to 9.955764331963416e-05
train 7585
vs, vt 0.2585428116076133 0.27648276046795006
need align? ->  False 0.2625948076739031
2023-09-04 09:09:11,264 - epoch:7, training loss:4.7348 validation loss:0.2585
Updating learning rate to 9.826746810256955e-05
Updating learning rate to 9.826746810256955e-05
train 7585
vs, vt 0.25947918348452625 0.2739831608007936
need align? ->  False 0.2625948076739031
2023-09-04 09:09:55,439 - epoch:8, training loss:4.3982 validation loss:0.2595
Updating learning rate to 9.615142654605508e-05
Updating learning rate to 9.615142654605508e-05
train 7585
vs, vt 0.25862336202579383 0.27785460256478367
need align? ->  False 0.2625948076739031
2023-09-04 09:10:39,415 - epoch:9, training loss:4.1912 validation loss:0.2586
Updating learning rate to 9.32457247078002e-05
Updating learning rate to 9.32457247078002e-05
train 7585
vs, vt 0.2594962536412127 0.2828335512210341
need align? ->  False 0.2625948076739031
2023-09-04 09:11:24,369 - epoch:10, training loss:4.0694 validation loss:0.2595
Updating learning rate to 8.960007995187029e-05
Updating learning rate to 8.960007995187029e-05
train 7585
vs, vt 0.2593381593332571 0.2842913931783508
need align? ->  False 0.2625948076739031
2023-09-04 09:12:09,365 - epoch:11, training loss:3.9617 validation loss:0.2593
Updating learning rate to 8.527687027080292e-05
Updating learning rate to 8.527687027080292e-05
train 7585
vs, vt 0.26003748441443725 0.28407010173096375
need align? ->  False 0.2625948076739031
2023-09-04 09:12:52,804 - epoch:12, training loss:3.8640 validation loss:0.2600
Updating learning rate to 8.035006698086137e-05
Updating learning rate to 8.035006698086137e-05
train 7585
vs, vt 0.2592114635250148 0.2868109980926794
need align? ->  False 0.2625948076739031
2023-09-04 09:13:42,065 - epoch:13, training loss:3.7684 validation loss:0.2592
Updating learning rate to 7.490396905230444e-05
Updating learning rate to 7.490396905230444e-05
train 7585
vs, vt 0.26091892403714795 0.2884153947234154
need align? ->  False 0.2625948076739031
2023-09-04 09:14:30,919 - epoch:14, training loss:3.6625 validation loss:0.2609
Updating learning rate to 6.903176073063336e-05
Updating learning rate to 6.903176073063336e-05
train 7585
vs, vt 0.2566852854455219 0.2907510965186007
need align? ->  False 0.2625948076739031
2023-09-04 09:15:14,495 - epoch:15, training loss:3.5740 validation loss:0.2567
Updating learning rate to 6.283391712831566e-05
Updating learning rate to 6.283391712831566e-05
train 7585
vs, vt 0.25651682650341706 0.28942471172879725
need align? ->  False 0.2625948076739031
2023-09-04 09:15:58,875 - epoch:16, training loss:3.4827 validation loss:0.2565
Updating learning rate to 5.641648506775387e-05
Updating learning rate to 5.641648506775387e-05
train 7585
vs, vt 0.2569909849587609 0.29135456462116804
need align? ->  False 0.2625948076739031
2023-09-04 09:16:43,064 - epoch:17, training loss:3.4012 validation loss:0.2570
Updating learning rate to 4.98892685907525e-05
Updating learning rate to 4.98892685907525e-05
train 7585
vs, vt 0.25931035069858327 0.2902137339115143
need align? ->  False 0.2625948076739031
2023-09-04 09:17:34,423 - epoch:18, training loss:3.3368 validation loss:0.2593
Updating learning rate to 4.336395018091936e-05
Updating learning rate to 4.336395018091936e-05
train 7585
vs, vt 0.2576771913205876 0.2938995602376321
need align? ->  False 0.2625948076739031
2023-09-04 09:18:19,812 - epoch:19, training loss:3.2814 validation loss:0.2577
Updating learning rate to 3.695217984540676e-05
Updating learning rate to 3.695217984540676e-05
train 7585
vs, vt 0.2582183664335924 0.2907051676336457
need align? ->  False 0.2625948076739031
2023-09-04 09:19:04,157 - epoch:20, training loss:3.2436 validation loss:0.2582
Updating learning rate to 3.0763664752333844e-05
Updating learning rate to 3.0763664752333844e-05
train 7585
vs, vt 0.2583179688628982 0.29219262871672125
need align? ->  False 0.2625948076739031
2023-09-04 09:19:48,016 - epoch:21, training loss:3.2030 validation loss:0.2583
Updating learning rate to 2.490429211072368e-05
Updating learning rate to 2.490429211072368e-05
train 7585
vs, vt 0.2580108857330154 0.29228422527804093
need align? ->  False 0.2625948076739031
2023-09-04 09:20:33,199 - epoch:22, training loss:3.1762 validation loss:0.2580
Updating learning rate to 1.9474317410999204e-05
Updating learning rate to 1.9474317410999204e-05
train 7585
vs, vt 0.25771360975854535 0.2929103825898731
need align? ->  False 0.2625948076739031
2023-09-04 09:21:16,686 - epoch:23, training loss:3.1557 validation loss:0.2577
Updating learning rate to 1.4566649025746091e-05
Updating learning rate to 1.4566649025746091e-05
train 7585
vs, vt 0.2585901464609539 0.2939906400792739
need align? ->  False 0.2625948076739031
2023-09-04 09:22:04,033 - epoch:24, training loss:3.1434 validation loss:0.2586
Updating learning rate to 1.0265258521698795e-05
Updating learning rate to 1.0265258521698795e-05
train 7585
