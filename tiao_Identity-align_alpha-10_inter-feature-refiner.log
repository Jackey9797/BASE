2023-08-07 15:14:13,123 - logger name:exp/ECL-PatchTST2023-08-07-15:14:13.123108/ECL-PatchTST.log
2023-08-07 15:14:13,123 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.3, 'feature_jittering': 0, 'mid_dim': 64, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-07-15:14:13.123108', 'path': 'exp/ECL-PatchTST2023-08-07-15:14:13.123108', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-07 15:14:13,123 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-07 15:14:13,311 - [*] phase 0 Dataset load!
2023-08-07 15:14:14,162 - [*] phase 0 Training start
train 8209
2023-08-07 15:14:47,738 - epoch:0, training loss:0.2125 validation loss:0.1682
train 8209
vs, vt 0.1682073539969596 0.16755852729759432
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13908135264434598 0.14059841260313988
need align? ->  False 0.14059841260313988
2023-08-07 15:16:04,932 - epoch:1, training loss:1.7089 validation loss:0.1391
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.13092464801262726 0.1293165311217308
need align? ->  True 0.1293165311217308
2023-08-07 15:17:15,902 - epoch:2, training loss:1.4690 validation loss:0.1309
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12559962645173073 0.12570744058625263
need align? ->  False 0.12570744058625263
2023-08-07 15:18:27,331 - epoch:3, training loss:1.2347 validation loss:0.1256
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12722265043041922 0.12531331520188937
need align? ->  True 0.12531331520188937
2023-08-07 15:19:38,942 - epoch:4, training loss:1.0040 validation loss:0.1272
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12834825629199093 0.12632657621394505
need align? ->  True 0.12531331520188937
2023-08-07 15:20:51,092 - epoch:5, training loss:0.8525 validation loss:0.1283
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12942305994643408 0.12412294770844957
need align? ->  True 0.12412294770844957
2023-08-07 15:22:03,185 - epoch:6, training loss:0.8292 validation loss:0.1294
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.1275930507955226 0.1226709337735718
need align? ->  True 0.1226709337735718
2023-08-07 15:23:15,195 - epoch:7, training loss:0.7446 validation loss:0.1276
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.1258099903775887 0.1233955378728834
need align? ->  True 0.1226709337735718
2023-08-07 15:24:27,250 - epoch:8, training loss:0.7086 validation loss:0.1258
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12639367013153704 0.12292691108516672
need align? ->  True 0.1226709337735718
2023-08-07 15:25:39,134 - epoch:9, training loss:0.6914 validation loss:0.1264
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12722791037098927 0.12295146472752094
need align? ->  True 0.1226709337735718
2023-08-07 15:26:51,366 - epoch:10, training loss:0.6855 validation loss:0.1272
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12459476114335385 0.12220867460762913
need align? ->  True 0.12220867460762913
2023-08-07 15:28:03,175 - epoch:11, training loss:0.6833 validation loss:0.1246
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12474183010106737 0.12193028129298579
need align? ->  True 0.12193028129298579
2023-08-07 15:29:15,882 - epoch:12, training loss:0.6587 validation loss:0.1247
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12508409216322683 0.12280687511983243
need align? ->  True 0.12193028129298579
2023-08-07 15:30:28,217 - epoch:13, training loss:0.6268 validation loss:0.1251
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.1266681941395456 0.12313591248609802
need align? ->  True 0.12193028129298579
2023-08-07 15:31:41,895 - epoch:14, training loss:0.6257 validation loss:0.1267
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12529437387870115 0.12339338143779473
need align? ->  True 0.12193028129298579
2023-08-07 15:32:53,875 - epoch:15, training loss:0.6278 validation loss:0.1253
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.1259818385270509 0.12350975662808526
need align? ->  True 0.12193028129298579
2023-08-07 15:34:05,963 - epoch:16, training loss:0.6170 validation loss:0.1260
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.1253904691812667 0.12385765065185049
need align? ->  True 0.12193028129298579
2023-08-07 15:35:17,956 - epoch:17, training loss:0.6195 validation loss:0.1254
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.1261537028476596 0.12340943337502805
need align? ->  True 0.12193028129298579
2023-08-07 15:36:30,095 - epoch:18, training loss:0.6135 validation loss:0.1262
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.1263336721299724 0.12384741804139181
need align? ->  True 0.12193028129298579
2023-08-07 15:37:41,953 - epoch:19, training loss:0.6151 validation loss:0.1263
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12534111069346016 0.123588960533115
need align? ->  True 0.12193028129298579
2023-08-07 15:38:53,768 - epoch:20, training loss:0.6127 validation loss:0.1253
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.1254110891710628 0.12348450644111092
need align? ->  True 0.12193028129298579
2023-08-07 15:40:06,134 - epoch:21, training loss:0.6167 validation loss:0.1254
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.1256053081120957 0.12373767962509935
need align? ->  True 0.12193028129298579
2023-08-07 15:41:19,111 - epoch:22, training loss:0.6140 validation loss:0.1256
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12554267328232527 0.12382066842507232
need align? ->  True 0.12193028129298579
2023-08-07 15:42:31,600 - epoch:23, training loss:0.6133 validation loss:0.1255
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12587192773141645 0.12381511164659803
need align? ->  True 0.12193028129298579
2023-08-07 15:43:43,662 - epoch:24, training loss:0.6120 validation loss:0.1259
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.1257420571690256 0.12368947724727067
need align? ->  True 0.12193028129298579
2023-08-07 15:44:56,071 - epoch:25, training loss:0.6129 validation loss:0.1257
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12557796134867452 0.12380243719301441
need align? ->  True 0.12193028129298579
2023-08-07 15:46:08,259 - epoch:26, training loss:0.6158 validation loss:0.1256
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12532752176577394 0.12355356096205386
need align? ->  True 0.12193028129298579
2023-08-07 15:47:20,375 - epoch:27, training loss:0.6143 validation loss:0.1253
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.1254196223718199 0.12364536794749173
need align? ->  True 0.12193028129298579
2023-08-07 15:48:33,090 - epoch:28, training loss:0.6140 validation loss:0.1254
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12537199749865316 0.12386289941654964
need align? ->  True 0.12193028129298579
2023-08-07 15:49:45,456 - epoch:29, training loss:0.6108 validation loss:0.1254
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-07-15:14:13.123108/0/0.1246_epoch_11.pkl  &  0.12193028129298579
2023-08-07 15:49:47,195 - [*] loss:0.2773
2023-08-07 15:49:47,198 - [*] phase 0, testing
2023-08-07 15:49:47,236 - T:96	MAE	0.336708	RMSE	0.277535	MAPE	131.326890
2023-08-07 15:49:47,238 - 96	mae	0.3367	
2023-08-07 15:49:47,238 - 96	rmse	0.2775	
2023-08-07 15:49:47,238 - 96	mape	131.3269	
2023-08-07 15:49:48,110 - [*] loss:0.2743
2023-08-07 15:49:48,113 - [*] phase 0, testing
2023-08-07 15:49:48,151 - T:96	MAE	0.330264	RMSE	0.274494	MAPE	133.286703
2023-08-07 15:49:48,153 - 96	mae	0.3303	
2023-08-07 15:49:48,153 - 96	rmse	0.2745	
2023-08-07 15:49:48,153 - 96	mape	133.2867	
2023-08-07 15:49:50,212 - logger name:exp/ECL-PatchTST2023-08-07-15:49:50.212423/ECL-PatchTST.log
2023-08-07 15:49:50,213 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.3, 'feature_jittering': 0, 'mid_dim': 64, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-07-15:49:50.212423', 'path': 'exp/ECL-PatchTST2023-08-07-15:49:50.212423', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-07 15:49:50,213 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-07 15:49:50,415 - [*] phase 0 Dataset load!
2023-08-07 15:49:51,310 - [*] phase 0 Training start
train 8209
2023-08-07 15:50:26,963 - epoch:0, training loss:0.2142 validation loss:0.1697
train 8209
vs, vt 0.1696690827269446 0.1704733408987522
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13908583823252807 0.1414803998036818
need align? ->  False 0.1414803998036818
2023-08-07 15:51:49,649 - epoch:1, training loss:1.7328 validation loss:0.1391
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12931374849920924 0.1286817980422215
need align? ->  True 0.1286817980422215
2023-08-07 15:53:05,375 - epoch:2, training loss:1.4933 validation loss:0.1293
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.1266856544905088 0.1268137658692219
need align? ->  False 0.1268137658692219
2023-08-07 15:54:19,095 - epoch:3, training loss:1.2690 validation loss:0.1267
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.1256256294860081 0.12336157457056371
need align? ->  True 0.12336157457056371
2023-08-07 15:55:31,729 - epoch:4, training loss:1.0498 validation loss:0.1256
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.1265831102024425 0.12417649960314686
need align? ->  True 0.12336157457056371
2023-08-07 15:56:43,727 - epoch:5, training loss:0.9035 validation loss:0.1266
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12612880994989115 0.12365925540639595
need align? ->  True 0.12336157457056371
2023-08-07 15:57:55,455 - epoch:6, training loss:0.8741 validation loss:0.1261
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.13398478789763016 0.12469970587302338
need align? ->  True 0.12336157457056371
2023-08-07 15:59:07,272 - epoch:7, training loss:0.8607 validation loss:0.1340
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12894435997375034 0.1250571600923484
need align? ->  True 0.12336157457056371
2023-08-07 16:00:19,163 - epoch:8, training loss:0.8471 validation loss:0.1289
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12856862202964045 0.12357266560535539
need align? ->  True 0.12336157457056371
2023-08-07 16:01:30,877 - epoch:9, training loss:0.8381 validation loss:0.1286
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12742978165095503 0.12354918061332269
need align? ->  True 0.12336157457056371
2023-08-07 16:02:43,203 - epoch:10, training loss:0.8326 validation loss:0.1274
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.1293614044446837 0.12394007938829335
need align? ->  True 0.12336157457056371
2023-08-07 16:03:55,103 - epoch:11, training loss:0.8296 validation loss:0.1294
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12861756362359633 0.12396485180678693
need align? ->  True 0.12336157457056371
2023-08-07 16:05:06,953 - epoch:12, training loss:0.8250 validation loss:0.1286
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12874950968067755 0.12299483235586774
need align? ->  True 0.12299483235586774
2023-08-07 16:06:18,658 - epoch:13, training loss:0.8248 validation loss:0.1287
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12739265981045636 0.12285465239123865
need align? ->  True 0.12285465239123865
2023-08-07 16:07:30,876 - epoch:14, training loss:0.7809 validation loss:0.1274
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12475727777928114 0.12207546432248571
need align? ->  True 0.12207546432248571
2023-08-07 16:08:43,083 - epoch:15, training loss:0.6738 validation loss:0.1248
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12471866404468362 0.12311440613120794
need align? ->  True 0.12207546432248571
2023-08-07 16:09:55,357 - epoch:16, training loss:0.6587 validation loss:0.1247
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12536471819674427 0.12266016074202278
need align? ->  True 0.12207546432248571
2023-08-07 16:11:07,015 - epoch:17, training loss:0.6502 validation loss:0.1254
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12455120733515783 0.12210811462930658
need align? ->  True 0.12207546432248571
2023-08-07 16:12:18,539 - epoch:18, training loss:0.6487 validation loss:0.1246
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12358892031691292 0.12295198372819206
need align? ->  True 0.12207546432248571
2023-08-07 16:13:30,254 - epoch:19, training loss:0.6479 validation loss:0.1236
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12469366514547305 0.12295631149953062
need align? ->  True 0.12207546432248571
2023-08-07 16:14:42,479 - epoch:20, training loss:0.6456 validation loss:0.1247
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12523308091542937 0.12290914272042838
need align? ->  True 0.12207546432248571
2023-08-07 16:15:54,423 - epoch:21, training loss:0.6410 validation loss:0.1252
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12484986208040606 0.12303643665191802
need align? ->  True 0.12207546432248571
2023-08-07 16:17:06,655 - epoch:22, training loss:0.6459 validation loss:0.1248
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12531873126598922 0.12311225291341543
need align? ->  True 0.12207546432248571
2023-08-07 16:18:18,144 - epoch:23, training loss:0.6411 validation loss:0.1253
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.124825009314174 0.12304670363664627
need align? ->  True 0.12207546432248571
2023-08-07 16:19:29,883 - epoch:24, training loss:0.6358 validation loss:0.1248
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12533267646689306 0.12316125944595445
need align? ->  True 0.12207546432248571
2023-08-07 16:20:41,903 - epoch:25, training loss:0.6358 validation loss:0.1253
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12474104872142727 0.12311556114053185
need align? ->  True 0.12207546432248571
2023-08-07 16:21:53,356 - epoch:26, training loss:0.6423 validation loss:0.1247
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.1247631437568502 0.12314634025096893
need align? ->  True 0.12207546432248571
2023-08-07 16:23:05,110 - epoch:27, training loss:0.6380 validation loss:0.1248
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12482769355516542 0.12321235010908409
need align? ->  True 0.12207546432248571
2023-08-07 16:24:17,369 - epoch:28, training loss:0.6383 validation loss:0.1248
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12472121552987532 0.12313829912719401
need align? ->  True 0.12207546432248571
2023-08-07 16:25:29,039 - epoch:29, training loss:0.6367 validation loss:0.1247
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-07-15:49:50.212423/0/0.1236_epoch_19.pkl  &  0.12207546432248571
2023-08-07 16:25:30,637 - [*] loss:0.2750
2023-08-07 16:25:30,641 - [*] phase 0, testing
2023-08-07 16:25:30,679 - T:96	MAE	0.334556	RMSE	0.275429	MAPE	129.430985
2023-08-07 16:25:30,681 - 96	mae	0.3346	
2023-08-07 16:25:30,681 - 96	rmse	0.2754	
2023-08-07 16:25:30,681 - 96	mape	129.4310	
2023-08-07 16:25:31,675 - [*] loss:0.2740
2023-08-07 16:25:31,678 - [*] phase 0, testing
2023-08-07 16:25:31,716 - T:96	MAE	0.329839	RMSE	0.274200	MAPE	132.710969
2023-08-07 16:25:31,717 - 96	mae	0.3298	
2023-08-07 16:25:31,717 - 96	rmse	0.2742	
2023-08-07 16:25:31,717 - 96	mape	132.7110	
2023-08-07 16:25:33,707 - logger name:exp/ECL-PatchTST2023-08-07-16:25:33.707259/ECL-PatchTST.log
2023-08-07 16:25:33,707 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.5, 'feature_jittering': 0, 'mid_dim': 64, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-07-16:25:33.707259', 'path': 'exp/ECL-PatchTST2023-08-07-16:25:33.707259', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-07 16:25:33,707 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-07 16:25:33,899 - [*] phase 0 Dataset load!
2023-08-07 16:25:34,738 - [*] phase 0 Training start
train 8209
2023-08-07 16:26:09,384 - epoch:0, training loss:0.2099 validation loss:0.1695
train 8209
vs, vt 0.1695170769975944 0.16770821792835539
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13796308094804938 0.14258680915967983
need align? ->  False 0.14258680915967983
2023-08-07 16:27:29,139 - epoch:1, training loss:1.7331 validation loss:0.1380
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12965433756736192 0.1296973627229983
need align? ->  False 0.1296973627229983
2023-08-07 16:28:41,423 - epoch:2, training loss:1.4837 validation loss:0.1297
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.1261934550817717 0.12577987081286582
need align? ->  True 0.12577987081286582
2023-08-07 16:29:53,671 - epoch:3, training loss:1.2504 validation loss:0.1262
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.1272133532911539 0.12590243252502245
need align? ->  True 0.12577987081286582
2023-08-07 16:31:05,945 - epoch:4, training loss:1.0105 validation loss:0.1272
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12886715773493052 0.1268565088341182
need align? ->  True 0.12577987081286582
2023-08-07 16:32:18,239 - epoch:5, training loss:0.9667 validation loss:0.1289
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.13146146259863267 0.12607251547954298
need align? ->  True 0.12577987081286582
2023-08-07 16:33:30,282 - epoch:6, training loss:0.9469 validation loss:0.1315
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12730151516470042 0.12378864921629429
need align? ->  True 0.12378864921629429
2023-08-07 16:34:42,819 - epoch:7, training loss:0.9314 validation loss:0.1273
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12665324759754268 0.12412407613274726
need align? ->  True 0.12378864921629429
2023-08-07 16:35:54,835 - epoch:8, training loss:0.7587 validation loss:0.1267
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12635920493101532 0.1236997621811249
need align? ->  True 0.1236997621811249
2023-08-07 16:37:07,060 - epoch:9, training loss:0.6906 validation loss:0.1264
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12607392465526407 0.12411844213916497
need align? ->  True 0.1236997621811249
2023-08-07 16:38:19,683 - epoch:10, training loss:0.6612 validation loss:0.1261
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.125380845063112 0.12328200643374161
need align? ->  True 0.12328200643374161
2023-08-07 16:39:31,680 - epoch:11, training loss:0.6303 validation loss:0.1254
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.1248450308022174 0.12302251414141872
need align? ->  True 0.12302251414141872
2023-08-07 16:40:43,962 - epoch:12, training loss:0.6237 validation loss:0.1248
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12527745809744706 0.12375812139362097
need align? ->  True 0.12302251414141872
2023-08-07 16:41:56,795 - epoch:13, training loss:0.6251 validation loss:0.1253
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12618531422181564 0.12409681657498534
need align? ->  True 0.12302251414141872
2023-08-07 16:43:08,650 - epoch:14, training loss:0.6220 validation loss:0.1262
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12531986011361534 0.12457473322071812
need align? ->  True 0.12302251414141872
2023-08-07 16:44:20,511 - epoch:15, training loss:0.6255 validation loss:0.1253
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12584769192405723 0.12444758872416886
need align? ->  True 0.12302251414141872
2023-08-07 16:45:32,445 - epoch:16, training loss:0.6180 validation loss:0.1258
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12542379156432368 0.12507220916450024
need align? ->  True 0.12302251414141872
2023-08-07 16:46:44,803 - epoch:17, training loss:0.6216 validation loss:0.1254
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12622061566534368 0.12428333940492435
need align? ->  True 0.12302251414141872
2023-08-07 16:47:56,917 - epoch:18, training loss:0.6167 validation loss:0.1262
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12645064031874592 0.12456018155948682
need align? ->  True 0.12302251414141872
2023-08-07 16:49:09,607 - epoch:19, training loss:0.6166 validation loss:0.1265
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12549022034826604 0.1244028326631947
need align? ->  True 0.12302251414141872
2023-08-07 16:50:21,488 - epoch:20, training loss:0.6168 validation loss:0.1255
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12567447405308485 0.12434781723740426
need align? ->  True 0.12302251414141872
2023-08-07 16:51:33,551 - epoch:21, training loss:0.6157 validation loss:0.1257
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.1258205617354675 0.12466238684613597
need align? ->  True 0.12302251414141872
2023-08-07 16:52:45,963 - epoch:22, training loss:0.6150 validation loss:0.1258
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12557786059650508 0.12503013226457618
need align? ->  True 0.12302251414141872
2023-08-07 16:53:57,977 - epoch:23, training loss:0.6152 validation loss:0.1256
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12588446485725316 0.1248545702546835
need align? ->  True 0.12302251414141872
2023-08-07 16:55:10,178 - epoch:24, training loss:0.6149 validation loss:0.1259
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.1258063582195477 0.1248052403838797
need align? ->  True 0.12302251414141872
2023-08-07 16:56:24,076 - epoch:25, training loss:0.6143 validation loss:0.1258
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.1255860451439565 0.12486546876078303
need align? ->  True 0.12302251414141872
2023-08-07 16:57:36,239 - epoch:26, training loss:0.6177 validation loss:0.1256
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.1254478945312175 0.12471967165104368
need align? ->  True 0.12302251414141872
2023-08-07 16:58:48,449 - epoch:27, training loss:0.6136 validation loss:0.1254
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12538200489838014 0.12484037122604522
need align? ->  True 0.12302251414141872
2023-08-07 17:00:01,038 - epoch:28, training loss:0.6145 validation loss:0.1254
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12542055098509247 0.12512741043147715
need align? ->  True 0.12302251414141872
2023-08-07 17:01:13,806 - epoch:29, training loss:0.6110 validation loss:0.1254
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-07-16:25:33.707259/0/0.1248_epoch_12.pkl  &  0.12302251414141872
2023-08-07 17:01:15,369 - [*] loss:0.2780
2023-08-07 17:01:15,373 - [*] phase 0, testing
2023-08-07 17:01:15,410 - T:96	MAE	0.336824	RMSE	0.278558	MAPE	133.527148
2023-08-07 17:01:15,411 - 96	mae	0.3368	
2023-08-07 17:01:15,411 - 96	rmse	0.2786	
2023-08-07 17:01:15,411 - 96	mape	133.5271	
2023-08-07 17:01:16,435 - [*] loss:0.2783
2023-08-07 17:01:16,438 - [*] phase 0, testing
2023-08-07 17:01:16,474 - T:96	MAE	0.332351	RMSE	0.278467	MAPE	134.949172
2023-08-07 17:01:16,475 - 96	mae	0.3324	
2023-08-07 17:01:16,475 - 96	rmse	0.2785	
2023-08-07 17:01:16,475 - 96	mape	134.9492	
2023-08-07 17:01:18,430 - logger name:exp/ECL-PatchTST2023-08-07-17:01:18.429525/ECL-PatchTST.log
2023-08-07 17:01:18,430 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.5, 'feature_jittering': 0, 'mid_dim': 64, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-07-17:01:18.429525', 'path': 'exp/ECL-PatchTST2023-08-07-17:01:18.429525', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-07 17:01:18,430 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-07 17:01:18,618 - [*] phase 0 Dataset load!
2023-08-07 17:01:19,480 - [*] phase 0 Training start
train 8209
2023-08-07 17:01:54,281 - epoch:0, training loss:0.2106 validation loss:0.1706
train 8209
vs, vt 0.17060050402175297 0.1707417114891789
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13750637805258686 0.14324062639339405
need align? ->  False 0.14324062639339405
2023-08-07 17:03:13,542 - epoch:1, training loss:1.7555 validation loss:0.1375
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12856697367334907 0.12937943459573117
need align? ->  False 0.12937943459573117
2023-08-07 17:04:25,663 - epoch:2, training loss:1.5066 validation loss:0.1286
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12667940108274872 0.12724463616243817
need align? ->  False 0.12724463616243817
2023-08-07 17:05:37,799 - epoch:3, training loss:1.2830 validation loss:0.1267
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12566070868210358 0.12332008567384699
need align? ->  True 0.12332008567384699
2023-08-07 17:07:11,789 - epoch:4, training loss:1.0483 validation loss:0.1257
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12594350596720522 0.12436621195890686
need align? ->  True 0.12332008567384699
2023-08-07 17:08:48,830 - epoch:5, training loss:0.8922 validation loss:0.1259
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12591219244694168 0.12380463773892685
need align? ->  True 0.12332008567384699
2023-08-07 17:10:24,473 - epoch:6, training loss:0.8604 validation loss:0.1259
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.13657354097813368 0.12424738104031845
need align? ->  True 0.12332008567384699
2023-08-07 17:11:59,490 - epoch:7, training loss:0.8466 validation loss:0.1366
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12855539750307798 0.12554678761146285
need align? ->  True 0.12332008567384699
2023-08-07 17:13:32,625 - epoch:8, training loss:0.8332 validation loss:0.1286
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12905957829207182 0.12444496976042335
need align? ->  True 0.12332008567384699
2023-08-07 17:15:04,059 - epoch:9, training loss:0.8222 validation loss:0.1291
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12696212902665138 0.1242933992127126
need align? ->  True 0.12332008567384699
2023-08-07 17:16:35,474 - epoch:10, training loss:0.8148 validation loss:0.1270
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12855538167059422 0.12461979128420353
need align? ->  True 0.12332008567384699
2023-08-07 17:18:06,304 - epoch:11, training loss:0.8085 validation loss:0.1286
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.129240265539424 0.12465760890733112
need align? ->  True 0.12332008567384699
2023-08-07 17:19:37,455 - epoch:12, training loss:0.8039 validation loss:0.1292
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12785170574418522 0.12341438665647399
need align? ->  True 0.12332008567384699
2023-08-07 17:21:08,334 - epoch:13, training loss:0.8025 validation loss:0.1279
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12833715890618888 0.1239472868936983
need align? ->  True 0.12332008567384699
2023-08-07 17:22:38,878 - epoch:14, training loss:0.7980 validation loss:0.1283
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.1276693255217238 0.12375678528438915
need align? ->  True 0.12332008567384699
2023-08-07 17:24:11,285 - epoch:15, training loss:0.7928 validation loss:0.1277
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.1293374854563312 0.12419863697141409
need align? ->  True 0.12332008567384699
2023-08-07 17:25:42,650 - epoch:16, training loss:0.7876 validation loss:0.1293
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12906456484713338 0.12417240102182735
need align? ->  True 0.12332008567384699
2023-08-07 17:27:14,425 - epoch:17, training loss:0.7879 validation loss:0.1291
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12915324301204897 0.12414994404058564
need align? ->  True 0.12332008567384699
2023-08-07 17:28:45,344 - epoch:18, training loss:0.7848 validation loss:0.1292
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12884159191426905 0.1246951346031644
need align? ->  True 0.12332008567384699
2023-08-07 17:30:17,205 - epoch:19, training loss:0.7812 validation loss:0.1288
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.1291428707032041 0.12469493394548242
need align? ->  True 0.12332008567384699
2023-08-07 17:31:47,009 - epoch:20, training loss:0.7820 validation loss:0.1291
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12927344322881915 0.12462175027890639
need align? ->  True 0.12332008567384699
2023-08-07 17:33:17,514 - epoch:21, training loss:0.7766 validation loss:0.1293
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12871696131134575 0.12471336024728688
need align? ->  True 0.12332008567384699
2023-08-07 17:34:49,415 - epoch:22, training loss:0.7777 validation loss:0.1287
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.1295537150210955 0.1247697098349983
need align? ->  True 0.12332008567384699
2023-08-07 17:36:21,004 - epoch:23, training loss:0.7746 validation loss:0.1296
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12949381184510209 0.1252951148728078
need align? ->  True 0.12332008567384699
2023-08-07 17:37:52,057 - epoch:24, training loss:0.7735 validation loss:0.1295
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12901618018407712 0.12493496896191077
need align? ->  True 0.12332008567384699
2023-08-07 17:39:23,295 - epoch:25, training loss:0.7727 validation loss:0.1290
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12935976675626906 0.12503521305254914
need align? ->  True 0.12332008567384699
2023-08-07 17:40:54,877 - epoch:26, training loss:0.7728 validation loss:0.1294
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12913480045443232 0.12497941277582537
need align? ->  True 0.12332008567384699
2023-08-07 17:42:28,213 - epoch:27, training loss:0.7710 validation loss:0.1291
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12920695975084195 0.1249919981611046
need align? ->  True 0.12332008567384699
2023-08-07 17:44:01,851 - epoch:28, training loss:0.7712 validation loss:0.1292
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12920998798852618 0.12507254393263298
need align? ->  True 0.12332008567384699
2023-08-07 17:45:37,485 - epoch:29, training loss:0.7727 validation loss:0.1292
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-07-17:01:18.429525/0/0.1257_epoch_4.pkl  &  0.12332008567384699
2023-08-07 17:45:42,347 - [*] loss:0.2794
2023-08-07 17:45:42,372 - [*] phase 0, testing
2023-08-07 17:45:42,499 - T:96	MAE	0.337721	RMSE	0.279613	MAPE	136.143816
2023-08-07 17:45:42,501 - 96	mae	0.3377	
2023-08-07 17:45:42,501 - 96	rmse	0.2796	
2023-08-07 17:45:42,501 - 96	mape	136.1438	
2023-08-07 17:45:45,864 - [*] loss:0.2747
2023-08-07 17:45:45,915 - [*] phase 0, testing
2023-08-07 17:45:46,029 - T:96	MAE	0.333494	RMSE	0.274773	MAPE	136.885345
2023-08-07 17:45:46,031 - 96	mae	0.3335	
2023-08-07 17:45:46,031 - 96	rmse	0.2748	
2023-08-07 17:45:46,031 - 96	mape	136.8853	
2023-08-07 17:45:48,114 - logger name:exp/ECL-PatchTST2023-08-07-17:45:48.114423/ECL-PatchTST.log
2023-08-07 17:45:48,114 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 2, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'mid_dim': 64, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-07-17:45:48.114423', 'path': 'exp/ECL-PatchTST2023-08-07-17:45:48.114423', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-07 17:45:48,115 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-07 17:45:48,313 - [*] phase 0 Dataset load!
2023-08-07 17:45:49,232 - [*] phase 0 Training start
train 8209
2023-08-07 17:47:10,351 - epoch:0, training loss:0.2123 validation loss:0.1679
train 8209
vs, vt 0.16794292120770973 0.16740228489718653
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13800957121632315 0.13959727114574474
need align? ->  False 0.13959727114574474
2023-08-07 17:49:45,701 - epoch:1, training loss:1.6752 validation loss:0.1380
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.1294626726514914 0.1276789310980927
need align? ->  True 0.1276789310980927
2023-08-07 17:52:09,969 - epoch:2, training loss:1.4369 validation loss:0.1295
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12793405989015644 0.12509875659915534
need align? ->  True 0.12509875659915534
2023-08-07 17:54:37,442 - epoch:3, training loss:1.2063 validation loss:0.1279
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12865284085273743 0.12435497868467461
need align? ->  True 0.12435497868467461
2023-08-07 17:57:00,603 - epoch:4, training loss:0.9851 validation loss:0.1287
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.126094289124012 0.12466439418494701
need align? ->  True 0.12435497868467461
2023-08-07 17:59:23,255 - epoch:5, training loss:0.8538 validation loss:0.1261
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12779091874306853 0.12228413552723148
need align? ->  True 0.12228413552723148
2023-08-07 18:01:46,991 - epoch:6, training loss:0.8292 validation loss:0.1278
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12703883131458002 0.12355388997291977
need align? ->  True 0.12228413552723148
2023-08-07 18:04:09,545 - epoch:7, training loss:0.7522 validation loss:0.1270
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12759592036970638 0.122727689079263
need align? ->  True 0.12228413552723148
2023-08-07 18:06:32,409 - epoch:8, training loss:0.7324 validation loss:0.1276
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.1282032426785339 0.12254015542566776
need align? ->  True 0.12228413552723148
2023-08-07 18:08:55,171 - epoch:9, training loss:0.7232 validation loss:0.1282
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12949280127544294 0.12159693334251642
need align? ->  True 0.12159693334251642
2023-08-07 18:11:18,382 - epoch:10, training loss:0.7203 validation loss:0.1295
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12791639947417108 0.12347864918410778
need align? ->  True 0.12159693334251642
2023-08-07 18:13:45,412 - epoch:11, training loss:0.6973 validation loss:0.1279
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.1277831836857579 0.12344304040413011
need align? ->  True 0.12159693334251642
2023-08-07 18:16:08,919 - epoch:12, training loss:0.6406 validation loss:0.1278
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.1278738578442823 0.12334380984644998
need align? ->  True 0.12159693334251642
2023-08-07 18:18:33,868 - epoch:13, training loss:0.6307 validation loss:0.1279
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.1318141750314019 0.12279306953264908
need align? ->  True 0.12159693334251642
2023-08-07 18:20:57,502 - epoch:14, training loss:0.6360 validation loss:0.1318
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.1293960293246941 0.12270078079944308
need align? ->  True 0.12159693334251642
2023-08-07 18:23:21,042 - epoch:15, training loss:0.6279 validation loss:0.1294
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12688583084805446 0.12296895682811737
need align? ->  True 0.12159693334251642
2023-08-07 18:25:43,547 - epoch:16, training loss:0.6224 validation loss:0.1269
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.1298368816179308 0.12251473218202591
need align? ->  True 0.12159693334251642
2023-08-07 18:28:08,496 - epoch:17, training loss:0.6211 validation loss:0.1298
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12907671395011924 0.12318123936314475
need align? ->  True 0.12159693334251642
2023-08-07 18:30:30,449 - epoch:18, training loss:0.6203 validation loss:0.1291
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12884683449837295 0.12288340833038092
need align? ->  True 0.12159693334251642
2023-08-07 18:32:54,774 - epoch:19, training loss:0.6196 validation loss:0.1288
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12907686981965194 0.12395007417283276
need align? ->  True 0.12159693334251642
2023-08-07 18:35:18,407 - epoch:20, training loss:0.6141 validation loss:0.1291
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12858118379319255 0.12295017349110408
need align? ->  True 0.12159693334251642
2023-08-07 18:37:42,238 - epoch:21, training loss:0.6203 validation loss:0.1286
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12822824487970633 0.1232087289575826
need align? ->  True 0.12159693334251642
2023-08-07 18:40:05,427 - epoch:22, training loss:0.6206 validation loss:0.1282
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12952310041609136 0.12348519570448181
need align? ->  True 0.12159693334251642
2023-08-07 18:42:31,734 - epoch:23, training loss:0.6213 validation loss:0.1295
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.1294737456535751 0.12367007102478635
need align? ->  True 0.12159693334251642
2023-08-07 18:44:54,126 - epoch:24, training loss:0.6169 validation loss:0.1295
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.1296531083908948 0.12345142281529578
need align? ->  True 0.12159693334251642
2023-08-07 18:47:21,698 - epoch:25, training loss:0.6201 validation loss:0.1297
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.1291871861639348 0.12335184309631586
need align? ->  True 0.12159693334251642
2023-08-07 18:49:44,615 - epoch:26, training loss:0.6163 validation loss:0.1292
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12935041331432082 0.12344685925001447
need align? ->  True 0.12159693334251642
2023-08-07 18:52:10,744 - epoch:27, training loss:0.6180 validation loss:0.1294
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.1291889235885306 0.12340948925438253
need align? ->  True 0.12159693334251642
2023-08-07 18:54:33,441 - epoch:28, training loss:0.6174 validation loss:0.1292
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.1292949297719381 0.12337708871134302
need align? ->  True 0.12159693334251642
2023-08-07 18:56:59,193 - epoch:29, training loss:0.6159 validation loss:0.1293
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-07-17:45:48.114423/0/0.1261_epoch_5.pkl  &  0.12159693334251642
2023-08-07 18:57:04,638 - [*] loss:0.2813
2023-08-07 18:57:04,642 - [*] phase 0, testing
2023-08-07 18:57:04,681 - T:96	MAE	0.335897	RMSE	0.281110	MAPE	129.814780
2023-08-07 18:57:04,683 - 96	mae	0.3359	
2023-08-07 18:57:04,683 - 96	rmse	0.2811	
2023-08-07 18:57:04,683 - 96	mape	129.8148	
2023-08-07 18:57:09,088 - [*] loss:0.2750
2023-08-07 18:57:09,092 - [*] phase 0, testing
2023-08-07 18:57:09,130 - T:96	MAE	0.330356	RMSE	0.275328	MAPE	133.634341
2023-08-07 18:57:09,132 - 96	mae	0.3304	
2023-08-07 18:57:09,132 - 96	rmse	0.2753	
2023-08-07 18:57:09,132 - 96	mape	133.6343	
2023-08-07 18:57:11,216 - logger name:exp/ECL-PatchTST2023-08-07-18:57:11.216440/ECL-PatchTST.log
2023-08-07 18:57:11,216 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 2, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'mid_dim': 64, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-07-18:57:11.216440', 'path': 'exp/ECL-PatchTST2023-08-07-18:57:11.216440', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-07 18:57:11,217 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-07 18:57:11,416 - [*] phase 0 Dataset load!
2023-08-07 18:57:12,326 - [*] phase 0 Training start
train 8209
2023-08-07 18:58:30,662 - epoch:0, training loss:0.2126 validation loss:0.1683
train 8209
vs, vt 0.1682687196880579 0.16603690081022002
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.1381678394973278 0.13866675560447303
need align? ->  False 0.13866675560447303
2023-08-07 19:01:05,680 - epoch:1, training loss:1.6962 validation loss:0.1382
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.1291449059816924 0.12852855094454504
need align? ->  True 0.12852855094454504
2023-08-07 19:03:28,287 - epoch:2, training loss:1.4554 validation loss:0.1291
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12817375565117056 0.12678409274667501
need align? ->  True 0.12678409274667501
2023-08-07 19:05:50,531 - epoch:3, training loss:1.2249 validation loss:0.1282
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12531788198446686 0.12392189319838177
need align? ->  True 0.12392189319838177
2023-08-07 19:08:13,622 - epoch:4, training loss:0.9918 validation loss:0.1253
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12620654422789812 0.12261509709060192
need align? ->  True 0.12261509709060192
2023-08-07 19:10:35,025 - epoch:5, training loss:0.8504 validation loss:0.1262
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12690017778765073 0.1223555188626051
need align? ->  True 0.1223555188626051
2023-08-07 19:12:57,525 - epoch:6, training loss:0.7792 validation loss:0.1269
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12747305834835226 0.12228893903507428
need align? ->  True 0.12228893903507428
2023-08-07 19:15:21,163 - epoch:7, training loss:0.7361 validation loss:0.1275
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.1263858602297577 0.12294488840482452
need align? ->  True 0.12228893903507428
2023-08-07 19:17:43,437 - epoch:8, training loss:0.6860 validation loss:0.1264
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12648709389296445 0.12206194854595444
need align? ->  True 0.12206194854595444
2023-08-07 19:20:08,150 - epoch:9, training loss:0.6706 validation loss:0.1265
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.1278309582478621 0.12196828678927639
need align? ->  True 0.12196828678927639
2023-08-07 19:22:31,705 - epoch:10, training loss:0.6215 validation loss:0.1278
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12563822503116998 0.12207817574116317
need align? ->  True 0.12196828678927639
2023-08-07 19:24:56,421 - epoch:11, training loss:0.5922 validation loss:0.1256
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.1281285180964253 0.12280449338934639
need align? ->  True 0.12196828678927639
2023-08-07 19:27:19,177 - epoch:12, training loss:0.5825 validation loss:0.1281
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12678600153462452 0.12196349826726047
need align? ->  True 0.12196349826726047
2023-08-07 19:29:44,303 - epoch:13, training loss:0.5781 validation loss:0.1268
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12592999408529562 0.12299604680050504
need align? ->  True 0.12196349826726047
2023-08-07 19:32:07,060 - epoch:14, training loss:0.5781 validation loss:0.1259
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12669849353419108 0.12271584443409335
need align? ->  True 0.12196349826726047
2023-08-07 19:34:35,040 - epoch:15, training loss:0.5602 validation loss:0.1267
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12579897020689465 0.1222774630243128
need align? ->  True 0.12196349826726047
2023-08-07 19:36:57,497 - epoch:16, training loss:0.5625 validation loss:0.1258
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12657642499967056 0.12251545116305351
need align? ->  True 0.12196349826726047
2023-08-07 19:39:24,357 - epoch:17, training loss:0.5613 validation loss:0.1266
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12571607868779788 0.12247023875401779
need align? ->  True 0.12196349826726047
2023-08-07 19:41:46,403 - epoch:18, training loss:0.5543 validation loss:0.1257
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.1263852815397761 0.1225451643324711
need align? ->  True 0.12196349826726047
2023-08-07 19:44:12,211 - epoch:19, training loss:0.5575 validation loss:0.1264
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.1264192589812658 0.12220514311709187
need align? ->  True 0.12196349826726047
2023-08-07 19:46:34,919 - epoch:20, training loss:0.5526 validation loss:0.1264
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12662517228587108 0.12248011669990691
need align? ->  True 0.12196349826726047
2023-08-07 19:49:00,798 - epoch:21, training loss:0.5560 validation loss:0.1266
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12627917096357455 0.12247907073998993
need align? ->  True 0.12196349826726047
2023-08-07 19:51:23,681 - epoch:22, training loss:0.5549 validation loss:0.1263
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12645641070875255 0.12259964382445271
need align? ->  True 0.12196349826726047
2023-08-07 19:53:49,155 - epoch:23, training loss:0.5558 validation loss:0.1265
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12674423239447855 0.12244072581895372
need align? ->  True 0.12196349826726047
2023-08-07 19:56:11,823 - epoch:24, training loss:0.5536 validation loss:0.1267
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12667719207026742 0.12257120411165735
need align? ->  True 0.12196349826726047
2023-08-07 19:58:36,245 - epoch:25, training loss:0.5562 validation loss:0.1267
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12707333004271443 0.12246603738855232
need align? ->  True 0.12196349826726047
2023-08-07 20:00:58,508 - epoch:26, training loss:0.5546 validation loss:0.1271
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12667682716114956 0.1224786215885119
need align? ->  True 0.12196349826726047
2023-08-07 20:03:22,574 - epoch:27, training loss:0.5558 validation loss:0.1267
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12689399685372005 0.12247808624736288
need align? ->  True 0.12196349826726047
2023-08-07 20:05:43,186 - epoch:28, training loss:0.5557 validation loss:0.1269
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12697779565033587 0.12246852258050983
need align? ->  True 0.12196349826726047
2023-08-07 20:08:07,718 - epoch:29, training loss:0.5530 validation loss:0.1270
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-07-18:57:11.216440/0/0.1253_epoch_4.pkl  &  0.12196349826726047
2023-08-07 20:08:14,056 - [*] loss:0.2779
2023-08-07 20:08:14,060 - [*] phase 0, testing
2023-08-07 20:08:14,097 - T:96	MAE	0.337761	RMSE	0.277660	MAPE	131.135523
2023-08-07 20:08:14,098 - 96	mae	0.3378	
2023-08-07 20:08:14,098 - 96	rmse	0.2777	
2023-08-07 20:08:14,098 - 96	mape	131.1355	
2023-08-07 20:08:17,682 - [*] loss:0.2741
2023-08-07 20:08:17,686 - [*] phase 0, testing
2023-08-07 20:08:17,723 - T:96	MAE	0.330021	RMSE	0.274320	MAPE	134.997690
2023-08-07 20:08:17,724 - 96	mae	0.3300	
2023-08-07 20:08:17,724 - 96	rmse	0.2743	
2023-08-07 20:08:17,724 - 96	mape	134.9977	
2023-08-07 20:08:19,858 - logger name:exp/ECL-PatchTST2023-08-07-20:08:19.858466/ECL-PatchTST.log
2023-08-07 20:08:19,859 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 3, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'mid_dim': 64, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-07-20:08:19.858466', 'path': 'exp/ECL-PatchTST2023-08-07-20:08:19.858466', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-07 20:08:19,859 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-07 20:08:20,064 - [*] phase 0 Dataset load!
2023-08-07 20:08:20,961 - [*] phase 0 Training start
train 8209
2023-08-07 20:10:02,803 - epoch:0, training loss:0.2099 validation loss:0.1690
train 8209
vs, vt 0.16899879937144843 0.16504523683000694
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13690770315853032 0.13894220644777472
need align? ->  False 0.13894220644777472
2023-08-07 20:13:30,038 - epoch:1, training loss:1.6737 validation loss:0.1369
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.13075363432819193 0.12936609805646268
need align? ->  True 0.12936609805646268
2023-08-07 20:16:33,987 - epoch:2, training loss:1.4368 validation loss:0.1308
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12760770278559488 0.1255599862303246
need align? ->  True 0.1255599862303246
2023-08-07 20:19:29,000 - epoch:3, training loss:1.1890 validation loss:0.1276
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12645705264400353 0.12582379892807116
need align? ->  True 0.1255599862303246
2023-08-07 20:22:24,008 - epoch:4, training loss:0.9471 validation loss:0.1265
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12864126383580946 0.12400167528539896
need align? ->  True 0.12400167528539896
2023-08-07 20:25:19,267 - epoch:5, training loss:0.9067 validation loss:0.1286
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12844041311605411 0.12350257138975641
need align? ->  True 0.12350257138975641
2023-08-07 20:28:14,324 - epoch:6, training loss:0.7690 validation loss:0.1284
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12543412958356467 0.12258937590840188
need align? ->  True 0.12258937590840188
2023-08-07 20:31:09,396 - epoch:7, training loss:0.7219 validation loss:0.1254
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12792787159031088 0.12262787771495906
need align? ->  True 0.12258937590840188
2023-08-07 20:34:04,791 - epoch:8, training loss:0.6856 validation loss:0.1279
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.1266876771538095 0.12282404735345732
need align? ->  True 0.12258937590840188
2023-08-07 20:36:59,520 - epoch:9, training loss:0.6684 validation loss:0.1267
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12921190134842286 0.12215050157498229
need align? ->  True 0.12215050157498229
2023-08-07 20:39:55,730 - epoch:10, training loss:0.6609 validation loss:0.1292
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12649202609265392 0.1221841557628729
need align? ->  True 0.12215050157498229
2023-08-07 20:42:50,300 - epoch:11, training loss:0.6185 validation loss:0.1265
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.13006035636433147 0.12272091413086111
need align? ->  True 0.12215050157498229
2023-08-07 20:45:46,413 - epoch:12, training loss:0.5890 validation loss:0.1301
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12925678186795928 0.12276506567881866
need align? ->  True 0.12215050157498229
2023-08-07 20:48:41,171 - epoch:13, training loss:0.5868 validation loss:0.1293
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.1285231928764419 0.1225750483572483
need align? ->  True 0.12215050157498229
2023-08-07 20:51:35,294 - epoch:14, training loss:0.5867 validation loss:0.1285
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12768565634773535 0.12311972855505618
need align? ->  True 0.12215050157498229
2023-08-07 20:54:29,746 - epoch:15, training loss:0.5835 validation loss:0.1277
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12705976329743862 0.12347281546416608
need align? ->  True 0.12215050157498229
2023-08-07 20:57:25,420 - epoch:16, training loss:0.5834 validation loss:0.1271
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.1259937807917595 0.12241442577744072
need align? ->  True 0.12215050157498229
2023-08-07 21:00:19,746 - epoch:17, training loss:0.5813 validation loss:0.1260
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12736422509293666 0.12259490432387049
need align? ->  True 0.12215050157498229
2023-08-07 21:03:14,401 - epoch:18, training loss:0.5765 validation loss:0.1274
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12834488050165502 0.12301768641918898
need align? ->  True 0.12215050157498229
2023-08-07 21:06:08,526 - epoch:19, training loss:0.5733 validation loss:0.1283
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12698821029202503 0.12328461980955167
need align? ->  True 0.12215050157498229
2023-08-07 21:09:02,828 - epoch:20, training loss:0.5729 validation loss:0.1270
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12940896260128779 0.12288187622007998
need align? ->  True 0.12215050157498229
2023-08-07 21:11:57,445 - epoch:21, training loss:0.5696 validation loss:0.1294
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12904045696962962 0.1236610474403609
need align? ->  True 0.12215050157498229
2023-08-07 21:14:52,208 - epoch:22, training loss:0.5687 validation loss:0.1290
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.1287557924674316 0.12331635437228462
need align? ->  True 0.12215050157498229
2023-08-07 21:17:46,599 - epoch:23, training loss:0.5734 validation loss:0.1288
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12887445718727328 0.12342543298886581
need align? ->  True 0.12215050157498229
2023-08-07 21:20:41,256 - epoch:24, training loss:0.5739 validation loss:0.1289
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12877464641562916 0.12327777501195669
need align? ->  True 0.12215050157498229
2023-08-07 21:23:35,699 - epoch:25, training loss:0.5716 validation loss:0.1288
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12898393170061437 0.12343776971101761
need align? ->  True 0.12215050157498229
2023-08-07 21:26:30,237 - epoch:26, training loss:0.5701 validation loss:0.1290
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12861250662668186 0.12328451694074002
need align? ->  True 0.12215050157498229
2023-08-07 21:29:24,518 - epoch:27, training loss:0.5672 validation loss:0.1286
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12884228109297427 0.12329464058645746
need align? ->  True 0.12215050157498229
2023-08-07 21:32:18,608 - epoch:28, training loss:0.5723 validation loss:0.1288
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12910949396477503 0.1232657413083044
need align? ->  True 0.12215050157498229
2023-08-07 21:35:13,238 - epoch:29, training loss:0.5701 validation loss:0.1291
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-07-20:08:19.858466/0/0.1254_epoch_7.pkl  &  0.12215050157498229
2023-08-07 21:35:16,162 - [*] loss:0.2789
2023-08-07 21:35:16,165 - [*] phase 0, testing
2023-08-07 21:35:16,214 - T:96	MAE	0.336277	RMSE	0.278726	MAPE	131.081378
2023-08-07 21:35:16,214 - 96	mae	0.3363	
2023-08-07 21:35:16,214 - 96	rmse	0.2787	
2023-08-07 21:35:16,214 - 96	mape	131.0814	
2023-08-07 21:35:17,182 - [*] loss:0.2739
2023-08-07 21:35:17,185 - [*] phase 0, testing
2023-08-07 21:35:17,223 - T:96	MAE	0.330512	RMSE	0.273875	MAPE	131.777716
2023-08-07 21:35:17,224 - 96	mae	0.3305	
2023-08-07 21:35:17,224 - 96	rmse	0.2739	
2023-08-07 21:35:17,224 - 96	mape	131.7777	
2023-08-07 21:35:19,296 - logger name:exp/ECL-PatchTST2023-08-07-21:35:19.295914/ECL-PatchTST.log
2023-08-07 21:35:19,296 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 3, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'mid_dim': 64, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-07-21:35:19.295914', 'path': 'exp/ECL-PatchTST2023-08-07-21:35:19.295914', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-07 21:35:19,296 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-07 21:35:19,488 - [*] phase 0 Dataset load!
2023-08-07 21:35:20,364 - [*] phase 0 Training start
train 8209
2023-08-07 21:36:45,508 - epoch:0, training loss:0.2099 validation loss:0.1700
train 8209
vs, vt 0.16999682817946782 0.1695252853361043
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13927468403496526 0.14114428701048548
need align? ->  False 0.14114428701048548
2023-08-07 21:39:46,649 - epoch:1, training loss:1.6941 validation loss:0.1393
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.127777408469807 0.12972838533195583
need align? ->  False 0.12972838533195583
2023-08-07 21:42:40,562 - epoch:2, training loss:1.4501 validation loss:0.1278
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12960496223108334 0.12500753880224444
need align? ->  True 0.12500753880224444
2023-08-07 21:45:34,455 - epoch:3, training loss:1.2048 validation loss:0.1296
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12495522090995853 0.1225872662087733
need align? ->  True 0.1225872662087733
2023-08-07 21:48:28,135 - epoch:4, training loss:0.9647 validation loss:0.1250
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.1292597638781775 0.12335481824861332
need align? ->  True 0.1225872662087733
2023-08-07 21:51:21,984 - epoch:5, training loss:0.8283 validation loss:0.1293
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.1279141024940393 0.12227969117123973
need align? ->  True 0.12227969117123973
2023-08-07 21:54:15,687 - epoch:6, training loss:0.8002 validation loss:0.1279
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.1306881836022843 0.12308750064535574
need align? ->  True 0.12227969117123973
2023-08-07 21:57:09,699 - epoch:7, training loss:0.7322 validation loss:0.1307
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12962842334739186 0.12300728320736777
need align? ->  True 0.12227969117123973
2023-08-07 22:00:04,117 - epoch:8, training loss:0.7054 validation loss:0.1296
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.1303869183598594 0.12385002616792917
need align? ->  True 0.12227969117123973
2023-08-07 22:02:57,904 - epoch:9, training loss:0.6969 validation loss:0.1304
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12761160299520602 0.1235293521630493
need align? ->  True 0.12227969117123973
2023-08-07 22:05:51,579 - epoch:10, training loss:0.6937 validation loss:0.1276
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.1272810563783754 0.1228830809281631
need align? ->  True 0.12227969117123973
2023-08-07 22:08:45,240 - epoch:11, training loss:0.6878 validation loss:0.1273
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12714934145862405 0.12281977537680756
need align? ->  True 0.12227969117123973
2023-08-07 22:11:39,189 - epoch:12, training loss:0.6817 validation loss:0.1271
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12721040430055422 0.12271085246042772
need align? ->  True 0.12227969117123973
2023-08-07 22:14:32,892 - epoch:13, training loss:0.6775 validation loss:0.1272
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12758238893002272 0.12218422358008948
need align? ->  True 0.12218422358008948
2023-08-07 22:17:27,152 - epoch:14, training loss:0.6744 validation loss:0.1276
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12774629814719612 0.12314060067927296
need align? ->  True 0.12218422358008948
2023-08-07 22:20:21,049 - epoch:15, training loss:0.6778 validation loss:0.1277
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.1259127859534188 0.12236696185374801
need align? ->  True 0.12218422358008948
2023-08-07 22:23:14,636 - epoch:16, training loss:0.5698 validation loss:0.1259
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12639514246786182 0.12230653298849409
need align? ->  True 0.12218422358008948
2023-08-07 22:26:08,239 - epoch:17, training loss:0.5682 validation loss:0.1264
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.1278070699084889 0.12258482140234926
need align? ->  True 0.12218422358008948
2023-08-07 22:29:02,293 - epoch:18, training loss:0.5601 validation loss:0.1278
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12699142953550274 0.12237165377221325
need align? ->  True 0.12218422358008948
2023-08-07 22:31:56,485 - epoch:19, training loss:0.5582 validation loss:0.1270
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12558667556467382 0.12249388376420195
need align? ->  True 0.12218422358008948
2023-08-07 22:34:50,594 - epoch:20, training loss:0.5649 validation loss:0.1256
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.1262522851201621 0.12238888264718381
need align? ->  True 0.12218422358008948
2023-08-07 22:37:44,600 - epoch:21, training loss:0.5539 validation loss:0.1263
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.1268663021989844 0.1227450009266084
need align? ->  True 0.12218422358008948
2023-08-07 22:40:38,664 - epoch:22, training loss:0.5597 validation loss:0.1269
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.1269006370143457 0.12274745089763944
need align? ->  True 0.12218422358008948
2023-08-07 22:43:32,281 - epoch:23, training loss:0.5597 validation loss:0.1269
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.1277544702487913 0.12260190947827967
need align? ->  True 0.12218422358008948
2023-08-07 22:46:26,167 - epoch:24, training loss:0.5579 validation loss:0.1278
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12732879309491676 0.12294394099576907
need align? ->  True 0.12218422358008948
2023-08-07 22:49:19,977 - epoch:25, training loss:0.5607 validation loss:0.1273
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12677429751916366 0.1227257563309236
need align? ->  True 0.12218422358008948
2023-08-07 22:52:14,269 - epoch:26, training loss:0.5586 validation loss:0.1268
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.126975993541154 0.12269569416953759
need align? ->  True 0.12218422358008948
2023-08-07 22:55:10,145 - epoch:27, training loss:0.5604 validation loss:0.1270
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12695241092958234 0.12271637308665297
need align? ->  True 0.12218422358008948
2023-08-07 22:58:03,971 - epoch:28, training loss:0.5564 validation loss:0.1270
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12697215877812018 0.12269141313365915
need align? ->  True 0.12218422358008948
2023-08-07 23:00:57,675 - epoch:29, training loss:0.5566 validation loss:0.1270
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-07-21:35:19.295914/0/0.125_epoch_4.pkl  &  0.12218422358008948
2023-08-07 23:01:00,517 - [*] loss:0.2772
2023-08-07 23:01:00,521 - [*] phase 0, testing
2023-08-07 23:01:00,559 - T:96	MAE	0.336245	RMSE	0.277145	MAPE	129.226732
2023-08-07 23:01:00,560 - 96	mae	0.3362	
2023-08-07 23:01:00,560 - 96	rmse	0.2771	
2023-08-07 23:01:00,560 - 96	mape	129.2267	
2023-08-07 23:01:01,605 - [*] loss:0.2745
2023-08-07 23:01:01,608 - [*] phase 0, testing
2023-08-07 23:01:01,655 - T:96	MAE	0.330013	RMSE	0.274776	MAPE	132.495689
2023-08-07 23:01:01,655 - 96	mae	0.3300	
2023-08-07 23:01:01,655 - 96	rmse	0.2748	
2023-08-07 23:01:01,655 - 96	mape	132.4957	
2023-08-07 23:01:03,656 - logger name:exp/ECL-PatchTST2023-08-07-23:01:03.656480/ECL-PatchTST.log
2023-08-07 23:01:03,657 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-07-23:01:03.656480', 'path': 'exp/ECL-PatchTST2023-08-07-23:01:03.656480', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-07 23:01:03,657 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-07 23:01:03,848 - [*] phase 0 Dataset load!
2023-08-07 23:01:04,709 - [*] phase 0 Training start
train 8209
2023-08-07 23:01:42,657 - epoch:0, training loss:0.2137 validation loss:0.1644
train 8209
vs, vt 0.1643989621238275 0.16545222818174146
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13739830170842735 0.1361093885180625
need align? ->  True 0.1361093885180625
2023-08-07 23:03:08,267 - epoch:1, training loss:1.6327 validation loss:0.1374
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12792578534307805 0.12793565981767394
need align? ->  False 0.12793565981767394
2023-08-07 23:04:27,155 - epoch:2, training loss:1.3886 validation loss:0.1279
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12693988929756664 0.12356217027726499
need align? ->  True 0.12356217027726499
2023-08-07 23:05:45,364 - epoch:3, training loss:1.1570 validation loss:0.1269
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12726971329274503 0.12232605190100995
need align? ->  True 0.12232605190100995
2023-08-07 23:07:03,501 - epoch:4, training loss:0.9682 validation loss:0.1273
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12574424899437212 0.12258899313482371
need align? ->  True 0.12232605190100995
2023-08-07 23:08:21,644 - epoch:5, training loss:0.8511 validation loss:0.1257
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12665461393242533 0.12312514521181583
need align? ->  True 0.12232605190100995
2023-08-07 23:09:39,888 - epoch:6, training loss:0.8287 validation loss:0.1267
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12554199917411263 0.12343093600462783
need align? ->  True 0.12232605190100995
2023-08-07 23:10:58,296 - epoch:7, training loss:0.8167 validation loss:0.1255
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12497524519196966 0.1219108577140353
need align? ->  True 0.1219108577140353
2023-08-07 23:12:16,594 - epoch:8, training loss:0.8089 validation loss:0.1250
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12700914024290713 0.12225186638534069
need align? ->  True 0.1219108577140353
2023-08-07 23:13:34,607 - epoch:9, training loss:0.7276 validation loss:0.1270
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12433382140641863 0.12166042067110538
need align? ->  True 0.12166042067110538
2023-08-07 23:14:52,753 - epoch:10, training loss:0.6971 validation loss:0.1243
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12363642123951153 0.12268006251278249
need align? ->  True 0.12166042067110538
2023-08-07 23:16:10,911 - epoch:11, training loss:0.6626 validation loss:0.1236
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.1254271390763196 0.12249087085100738
need align? ->  True 0.12166042067110538
2023-08-07 23:17:29,525 - epoch:12, training loss:0.6480 validation loss:0.1254
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.1252316270362247 0.1221147443760525
need align? ->  True 0.12166042067110538
2023-08-07 23:18:47,767 - epoch:13, training loss:0.6441 validation loss:0.1252
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12603140529245138 0.12253330343148926
need align? ->  True 0.12166042067110538
2023-08-07 23:20:06,034 - epoch:14, training loss:0.6378 validation loss:0.1260
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.1247354257343845 0.1226178425448862
need align? ->  True 0.12166042067110538
2023-08-07 23:21:24,126 - epoch:15, training loss:0.6334 validation loss:0.1247
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.1255214995789257 0.12256664283234965
need align? ->  True 0.12166042067110538
2023-08-07 23:22:42,515 - epoch:16, training loss:0.6356 validation loss:0.1255
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12452440209348094 0.1224596170200543
need align? ->  True 0.12166042067110538
2023-08-07 23:24:00,742 - epoch:17, training loss:0.6364 validation loss:0.1245
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12380731656131419 0.12242370979352431
need align? ->  True 0.12166042067110538
2023-08-07 23:25:18,751 - epoch:18, training loss:0.6291 validation loss:0.1238
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12579601605168797 0.12237745057791471
need align? ->  True 0.12166042067110538
2023-08-07 23:26:36,767 - epoch:19, training loss:0.6270 validation loss:0.1258
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12531025064262477 0.12253344761715694
need align? ->  True 0.12166042067110538
2023-08-07 23:27:54,852 - epoch:20, training loss:0.6322 validation loss:0.1253
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12417610226707025 0.12254425722428343
need align? ->  True 0.12166042067110538
2023-08-07 23:29:13,459 - epoch:21, training loss:0.6339 validation loss:0.1242
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12459993167695674 0.12223139439116824
need align? ->  True 0.12166042067110538
2023-08-07 23:30:31,751 - epoch:22, training loss:0.6295 validation loss:0.1246
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.1245974176986651 0.12254042567854578
need align? ->  True 0.12166042067110538
2023-08-07 23:31:50,074 - epoch:23, training loss:0.6285 validation loss:0.1246
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12416906197640029 0.12243700797923586
need align? ->  True 0.12166042067110538
2023-08-07 23:33:08,448 - epoch:24, training loss:0.6278 validation loss:0.1242
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12446657949211923 0.12237536399201913
need align? ->  True 0.12166042067110538
2023-08-07 23:34:27,771 - epoch:25, training loss:0.6235 validation loss:0.1245
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12443106181242249 0.12238261730156162
need align? ->  True 0.12166042067110538
2023-08-07 23:35:46,445 - epoch:26, training loss:0.6292 validation loss:0.1244
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12435566092079336 0.12236850463192571
need align? ->  True 0.12166042067110538
2023-08-07 23:37:04,995 - epoch:27, training loss:0.6290 validation loss:0.1244
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12442466997626153 0.12236185126345266
need align? ->  True 0.12166042067110538
2023-08-07 23:38:23,269 - epoch:28, training loss:0.6252 validation loss:0.1244
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12451927787200971 0.12239324707876552
need align? ->  True 0.12166042067110538
2023-08-07 23:39:41,787 - epoch:29, training loss:0.6282 validation loss:0.1245
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-07-23:01:03.656480/0/0.1236_epoch_11.pkl  &  0.12166042067110538
2023-08-07 23:39:43,901 - [*] loss:0.2747
2023-08-07 23:39:43,904 - [*] phase 0, testing
2023-08-07 23:39:43,942 - T:96	MAE	0.334620	RMSE	0.274928	MAPE	130.729973
2023-08-07 23:39:43,943 - 96	mae	0.3346	
2023-08-07 23:39:43,943 - 96	rmse	0.2749	
2023-08-07 23:39:43,943 - 96	mape	130.7300	
2023-08-07 23:39:44,922 - [*] loss:0.2724
2023-08-07 23:39:44,925 - [*] phase 0, testing
2023-08-07 23:39:44,962 - T:96	MAE	0.329626	RMSE	0.272671	MAPE	134.141970
2023-08-07 23:39:44,963 - 96	mae	0.3296	
2023-08-07 23:39:44,964 - 96	rmse	0.2727	
2023-08-07 23:39:44,964 - 96	mape	134.1420	
2023-08-07 23:39:46,950 - logger name:exp/ECL-PatchTST2023-08-07-23:39:46.950402/ECL-PatchTST.log
2023-08-07 23:39:46,950 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-07-23:39:46.950402', 'path': 'exp/ECL-PatchTST2023-08-07-23:39:46.950402', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-07 23:39:46,950 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-07 23:39:47,146 - [*] phase 0 Dataset load!
2023-08-07 23:39:48,014 - [*] phase 0 Training start
train 8209
2023-08-07 23:40:26,081 - epoch:0, training loss:0.2144 validation loss:0.1649
train 8209
vs, vt 0.1648892340334979 0.16521862877363508
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13646198402751575 0.13540264981036837
need align? ->  True 0.13540264981036837
2023-08-07 23:41:51,299 - epoch:1, training loss:1.6275 validation loss:0.1365
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12807118189944464 0.1274372594939037
need align? ->  True 0.1274372594939037
2023-08-07 23:43:12,588 - epoch:2, training loss:1.3809 validation loss:0.1281
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12619630983945998 0.12525263437154618
need align? ->  True 0.12525263437154618
2023-08-07 23:44:31,673 - epoch:3, training loss:1.1471 validation loss:0.1262
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12439151658591899 0.12272347374395891
need align? ->  True 0.12272347374395891
2023-08-07 23:45:50,226 - epoch:4, training loss:0.9372 validation loss:0.1244
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12291774102909998 0.12265473858199337
need align? ->  True 0.12265473858199337
2023-08-07 23:47:11,476 - epoch:5, training loss:0.8227 validation loss:0.1229
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12531991226767952 0.12284119164740498
need align? ->  True 0.12265473858199337
2023-08-07 23:48:29,929 - epoch:6, training loss:0.7694 validation loss:0.1253
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.128421674313193 0.12373775811019269
need align? ->  True 0.12265473858199337
2023-08-07 23:49:48,530 - epoch:7, training loss:0.7558 validation loss:0.1284
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.13132723005996508 0.12459361138330265
need align? ->  True 0.12265473858199337
2023-08-07 23:51:07,166 - epoch:8, training loss:0.7495 validation loss:0.1313
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12712068398567763 0.12342826556414366
need align? ->  True 0.12265473858199337
2023-08-07 23:52:25,876 - epoch:9, training loss:0.7437 validation loss:0.1271
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12415820241651752 0.12193020881915634
need align? ->  True 0.12193020881915634
2023-08-07 23:53:44,632 - epoch:10, training loss:0.7422 validation loss:0.1242
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12321759353984486 0.12130332103168423
need align? ->  True 0.12130332103168423
2023-08-07 23:55:05,433 - epoch:11, training loss:0.6875 validation loss:0.1232
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12380213849246502 0.12237278778444637
need align? ->  True 0.12130332103168423
2023-08-07 23:56:24,023 - epoch:12, training loss:0.6209 validation loss:0.1238
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12457332125102932 0.12213768318972805
need align? ->  True 0.12130332103168423
2023-08-07 23:57:42,398 - epoch:13, training loss:0.6154 validation loss:0.1246
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.1241235046393492 0.12184401169757951
need align? ->  True 0.12130332103168423
2023-08-07 23:59:20,868 - epoch:14, training loss:0.6118 validation loss:0.1241
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12406265972690149 0.12181583309376781
need align? ->  True 0.12130332103168423
2023-08-08 00:00:58,025 - epoch:15, training loss:0.6131 validation loss:0.1241
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12258033953945745 0.12182467396963727
need align? ->  True 0.12130332103168423
2023-08-08 00:02:32,627 - epoch:16, training loss:0.6109 validation loss:0.1226
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12301691689274528 0.12194575741887093
need align? ->  True 0.12130332103168423
2023-08-08 00:04:10,954 - epoch:17, training loss:0.6067 validation loss:0.1230
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12415672533891418 0.12178025987337936
need align? ->  True 0.12130332103168423
2023-08-08 00:05:49,626 - epoch:18, training loss:0.6069 validation loss:0.1242
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12335398581556299 0.12169191148132086
need align? ->  True 0.12130332103168423
2023-08-08 00:07:27,136 - epoch:19, training loss:0.6085 validation loss:0.1234
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12336989991705526 0.12160077081485228
need align? ->  True 0.12130332103168423
2023-08-08 00:09:04,204 - epoch:20, training loss:0.6025 validation loss:0.1234
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12321296655996279 0.12167103876444427
need align? ->  True 0.12130332103168423
2023-08-08 00:10:41,436 - epoch:21, training loss:0.6010 validation loss:0.1232
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.1233077139847658 0.12205943245102059
need align? ->  True 0.12130332103168423
2023-08-08 00:12:18,991 - epoch:22, training loss:0.6009 validation loss:0.1233
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12364086593416604 0.12186713626777584
need align? ->  True 0.12130332103168423
2023-08-08 00:13:55,759 - epoch:23, training loss:0.5982 validation loss:0.1236
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12421477814628319 0.12207191860811277
need align? ->  True 0.12130332103168423
2023-08-08 00:15:32,872 - epoch:24, training loss:0.6017 validation loss:0.1242
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12368250130252405 0.12196381779556925
need align? ->  True 0.12130332103168423
2023-08-08 00:17:10,416 - epoch:25, training loss:0.5987 validation loss:0.1237
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12367362393574281 0.12194516221908006
need align? ->  True 0.12130332103168423
2023-08-08 00:18:47,304 - epoch:26, training loss:0.5981 validation loss:0.1237
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12384041703560135 0.12190424828705462
need align? ->  True 0.12130332103168423
2023-08-08 00:20:25,145 - epoch:27, training loss:0.5997 validation loss:0.1238
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12365674794736234 0.1218599546700716
need align? ->  True 0.12130332103168423
2023-08-08 00:22:02,622 - epoch:28, training loss:0.5998 validation loss:0.1237
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12412153963338245 0.12200302893126552
need align? ->  True 0.12130332103168423
2023-08-08 00:23:41,038 - epoch:29, training loss:0.6003 validation loss:0.1241
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-07-23:39:46.950402/0/0.1226_epoch_16.pkl  &  0.12130332103168423
2023-08-08 00:23:46,046 - [*] loss:0.2726
2023-08-08 00:23:46,051 - [*] phase 0, testing
2023-08-08 00:23:46,114 - T:96	MAE	0.332177	RMSE	0.272978	MAPE	129.761887
2023-08-08 00:23:46,115 - 96	mae	0.3322	
2023-08-08 00:23:46,115 - 96	rmse	0.2730	
2023-08-08 00:23:46,115 - 96	mape	129.7619	
2023-08-08 00:23:50,267 - [*] loss:0.2724
2023-08-08 00:23:50,270 - [*] phase 0, testing
2023-08-08 00:23:50,323 - T:96	MAE	0.329742	RMSE	0.272514	MAPE	134.108794
2023-08-08 00:23:50,325 - 96	mae	0.3297	
2023-08-08 00:23:50,325 - 96	rmse	0.2725	
2023-08-08 00:23:50,325 - 96	mape	134.1088	
2023-08-08 00:23:52,556 - logger name:exp/ECL-PatchTST2023-08-08-00:23:52.556436/ECL-PatchTST.log
2023-08-08 00:23:52,556 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'rec_intra_feature': 0, 'rec_ori': 0, 'mid_dim': 32, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-00:23:52.556436', 'path': 'exp/ECL-PatchTST2023-08-08-00:23:52.556436', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 00:23:52,557 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 00:23:52,777 - [*] phase 0 Dataset load!
2023-08-08 00:23:53,766 - [*] phase 0 Training start
train 8209
2023-08-08 00:24:49,117 - epoch:0, training loss:0.2162 validation loss:0.1667
train 8209
vs, vt 0.16666690480302682 0.1665126999670809
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13916144215247847 0.1365407593548298
need align? ->  True 0.1365407593548298
2023-08-08 00:26:36,886 - epoch:1, training loss:1.6884 validation loss:0.1392
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12976968711750073 0.12782008336348968
need align? ->  True 0.12782008336348968
2023-08-08 00:28:13,316 - epoch:2, training loss:1.4395 validation loss:0.1298
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12488027154044672 0.12472918036986481
need align? ->  True 0.12472918036986481
2023-08-08 00:29:47,219 - epoch:3, training loss:1.2002 validation loss:0.1249
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.1271335855126381 0.1241307749666951
need align? ->  True 0.1241307749666951
2023-08-08 00:31:18,316 - epoch:4, training loss:0.9828 validation loss:0.1271
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12302177026867867 0.12372981680726464
need align? ->  False 0.12372981680726464
2023-08-08 00:32:48,249 - epoch:5, training loss:0.8481 validation loss:0.1230
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12368417141789739 0.12287193680690094
need align? ->  True 0.12287193680690094
2023-08-08 00:34:17,857 - epoch:6, training loss:0.7866 validation loss:0.1237
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12566452761265365 0.12198365352709185
need align? ->  True 0.12198365352709185
2023-08-08 00:35:47,478 - epoch:7, training loss:0.7515 validation loss:0.1257
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12518453217026862 0.12271730999716303
need align? ->  True 0.12198365352709185
2023-08-08 00:37:17,474 - epoch:8, training loss:0.7256 validation loss:0.1252
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.1256282546303489 0.12269207428802144
need align? ->  True 0.12198365352709185
2023-08-08 00:38:47,473 - epoch:9, training loss:0.7174 validation loss:0.1256
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.1253222016278993 0.12268175684254277
need align? ->  True 0.12198365352709185
2023-08-08 00:40:17,414 - epoch:10, training loss:0.7179 validation loss:0.1253
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.1260915303772146 0.12207651333036748
need align? ->  True 0.12198365352709185
2023-08-08 00:41:48,041 - epoch:11, training loss:0.7168 validation loss:0.1261
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12778808091851798 0.12320179763165387
need align? ->  True 0.12198365352709185
2023-08-08 00:43:19,336 - epoch:12, training loss:0.7178 validation loss:0.1278
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12561275115744633 0.12267513547770002
need align? ->  True 0.12198365352709185
2023-08-08 00:44:48,650 - epoch:13, training loss:0.7170 validation loss:0.1256
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.1275844369083643 0.12315597503700039
need align? ->  True 0.12198365352709185
2023-08-08 00:46:18,282 - epoch:14, training loss:0.7135 validation loss:0.1276
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.126797764392739 0.122758768922226
need align? ->  True 0.12198365352709185
2023-08-08 00:47:49,260 - epoch:15, training loss:0.7154 validation loss:0.1268
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12761152832006867 0.12323147083886644
need align? ->  True 0.12198365352709185
2023-08-08 00:49:22,575 - epoch:16, training loss:0.7158 validation loss:0.1276
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12471392318945039 0.12280723588033156
need align? ->  True 0.12198365352709185
2023-08-08 00:50:59,574 - epoch:17, training loss:0.7151 validation loss:0.1247
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.1266117775812745 0.1233848554336212
need align? ->  True 0.12198365352709185
2023-08-08 00:52:30,827 - epoch:18, training loss:0.7138 validation loss:0.1266
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.1272760255431587 0.1228803467344154
need align? ->  True 0.12198365352709185
2023-08-08 00:54:04,195 - epoch:19, training loss:0.7114 validation loss:0.1273
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.1264707331799648 0.12290866672992706
need align? ->  True 0.12198365352709185
2023-08-08 00:55:39,080 - epoch:20, training loss:0.7110 validation loss:0.1265
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12704171003265816 0.1228586897592653
need align? ->  True 0.12198365352709185
2023-08-08 00:57:17,896 - epoch:21, training loss:0.7096 validation loss:0.1270
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12762182476845654 0.12341277216645805
need align? ->  True 0.12198365352709185
2023-08-08 00:58:51,633 - epoch:22, training loss:0.7084 validation loss:0.1276
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12680739452215758 0.123343317176808
need align? ->  True 0.12198365352709185
2023-08-08 01:00:26,498 - epoch:23, training loss:0.7080 validation loss:0.1268
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12695762896063653 0.12298822242089293
need align? ->  True 0.12198365352709185
2023-08-08 01:01:55,429 - epoch:24, training loss:0.7085 validation loss:0.1270
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.126994735899974 0.12289744344624606
need align? ->  True 0.12198365352709185
2023-08-08 01:03:25,988 - epoch:25, training loss:0.7082 validation loss:0.1270
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12675627490336244 0.12303555087948387
need align? ->  True 0.12198365352709185
2023-08-08 01:04:56,789 - epoch:26, training loss:0.7074 validation loss:0.1268
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.1271295083517378 0.12302993898364631
need align? ->  True 0.12198365352709185
2023-08-08 01:06:26,882 - epoch:27, training loss:0.7075 validation loss:0.1271
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12701042597605425 0.1231027397073128
need align? ->  True 0.12198365352709185
2023-08-08 01:07:55,490 - epoch:28, training loss:0.7084 validation loss:0.1270
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12689428801902317 0.1230602345683358
need align? ->  True 0.12198365352709185
2023-08-08 01:09:25,161 - epoch:29, training loss:0.7058 validation loss:0.1269
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-00:23:52.556436/0/0.123_epoch_5.pkl  &  0.12198365352709185
2023-08-08 01:09:30,135 - [*] loss:0.2731
2023-08-08 01:09:30,138 - [*] phase 0, testing
2023-08-08 01:09:30,178 - T:96	MAE	0.333219	RMSE	0.273387	MAPE	134.348881
2023-08-08 01:09:30,179 - 96	mae	0.3332	
2023-08-08 01:09:30,179 - 96	rmse	0.2734	
2023-08-08 01:09:30,179 - 96	mape	134.3489	
2023-08-08 01:09:34,652 - [*] loss:0.2723
2023-08-08 01:09:34,656 - [*] phase 0, testing
2023-08-08 01:09:34,695 - T:96	MAE	0.329466	RMSE	0.272607	MAPE	132.455540
2023-08-08 01:09:34,696 - 96	mae	0.3295	
2023-08-08 01:09:34,696 - 96	rmse	0.2726	
2023-08-08 01:09:34,696 - 96	mape	132.4555	
2023-08-08 01:09:37,034 - logger name:exp/ECL-PatchTST2023-08-08-01:09:37.033796/ECL-PatchTST.log
2023-08-08 01:09:37,034 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'rec_intra_feature': 0, 'rec_ori': 0, 'mid_dim': 32, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-01:09:37.033796', 'path': 'exp/ECL-PatchTST2023-08-08-01:09:37.033796', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 01:09:37,034 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 01:09:37,233 - [*] phase 0 Dataset load!
2023-08-08 01:09:38,231 - [*] phase 0 Training start
train 8209
2023-08-08 01:10:33,684 - epoch:0, training loss:0.2149 validation loss:0.1659
train 8209
vs, vt 0.16594391129910946 0.16546715711328117
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.138376307758418 0.13616233462975783
need align? ->  True 0.13616233462975783
2023-08-08 01:12:22,857 - epoch:1, training loss:1.6403 validation loss:0.1384
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.128444233993915 0.1270079134370793
need align? ->  True 0.1270079134370793
2023-08-08 01:13:59,379 - epoch:2, training loss:1.4035 validation loss:0.1284
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12536666516891934 0.1240081200376153
need align? ->  True 0.1240081200376153
2023-08-08 01:15:33,822 - epoch:3, training loss:1.1502 validation loss:0.1254
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12577504427595573 0.12372380630536513
need align? ->  True 0.12372380630536513
2023-08-08 01:17:05,312 - epoch:4, training loss:0.9332 validation loss:0.1258
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.1251663105900992 0.12271004093980248
need align? ->  True 0.12271004093980248
2023-08-08 01:18:34,966 - epoch:5, training loss:0.8161 validation loss:0.1252
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12431515600870956 0.12321626699783585
need align? ->  True 0.12271004093980248
2023-08-08 01:20:03,721 - epoch:6, training loss:0.7587 validation loss:0.1243
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.1253495140170509 0.12226543473926457
need align? ->  True 0.12226543473926457
2023-08-08 01:21:33,116 - epoch:7, training loss:0.7447 validation loss:0.1253
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.1252900189296766 0.12314932539381764
need align? ->  True 0.12226543473926457
2023-08-08 01:23:02,886 - epoch:8, training loss:0.7049 validation loss:0.1253
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12613846006041224 0.12221590962938288
need align? ->  True 0.12221590962938288
2023-08-08 01:24:32,313 - epoch:9, training loss:0.6871 validation loss:0.1261
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12492513004690409 0.12255652494389903
need align? ->  True 0.12221590962938288
2023-08-08 01:26:02,043 - epoch:10, training loss:0.6476 validation loss:0.1249
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12523426428775897 0.12236140287396582
need align? ->  True 0.12221590962938288
2023-08-08 01:27:31,390 - epoch:11, training loss:0.6297 validation loss:0.1252
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12475035331127318 0.1223209211602807
need align? ->  True 0.12221590962938288
2023-08-08 01:29:01,397 - epoch:12, training loss:0.6317 validation loss:0.1248
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12596807493404907 0.12268288738348267
need align? ->  True 0.12221590962938288
2023-08-08 01:30:31,632 - epoch:13, training loss:0.6279 validation loss:0.1260
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12428655729375103 0.12269616626541723
need align? ->  True 0.12221590962938288
2023-08-08 01:32:00,990 - epoch:14, training loss:0.6276 validation loss:0.1243
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12596919006583365 0.1219128549776294
need align? ->  True 0.1219128549776294
2023-08-08 01:33:33,857 - epoch:15, training loss:0.6212 validation loss:0.1260
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.1235869402404536 0.12318964632736011
need align? ->  True 0.1219128549776294
2023-08-08 01:35:08,976 - epoch:16, training loss:0.5978 validation loss:0.1236
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12366698597642509 0.12234515078704465
need align? ->  True 0.1219128549776294
2023-08-08 01:36:46,368 - epoch:17, training loss:0.5716 validation loss:0.1237
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.1236834189939228 0.12303914637728171
need align? ->  True 0.1219128549776294
2023-08-08 01:38:21,159 - epoch:18, training loss:0.5704 validation loss:0.1237
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12361239591105418 0.12296902489933101
need align? ->  True 0.1219128549776294
2023-08-08 01:39:53,369 - epoch:19, training loss:0.5677 validation loss:0.1236
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12436940952796828 0.12308601476252079
need align? ->  True 0.1219128549776294
2023-08-08 01:41:29,407 - epoch:20, training loss:0.5663 validation loss:0.1244
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12416425804522904 0.12301092146133835
need align? ->  True 0.1219128549776294
2023-08-08 01:43:00,306 - epoch:21, training loss:0.5635 validation loss:0.1242
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12349416180090471 0.12304972264577042
need align? ->  True 0.1219128549776294
2023-08-08 01:44:31,772 - epoch:22, training loss:0.5626 validation loss:0.1235
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.1234808550639586 0.12294546353884718
need align? ->  True 0.1219128549776294
2023-08-08 01:46:02,214 - epoch:23, training loss:0.5603 validation loss:0.1235
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12400366611440074 0.12314373407174241
need align? ->  True 0.1219128549776294
2023-08-08 01:47:31,700 - epoch:24, training loss:0.5657 validation loss:0.1240
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12418300362134521 0.12315441845831546
need align? ->  True 0.1219128549776294
2023-08-08 01:49:01,793 - epoch:25, training loss:0.5630 validation loss:0.1242
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12379793797365644 0.12304520852525126
need align? ->  True 0.1219128549776294
2023-08-08 01:50:31,201 - epoch:26, training loss:0.5582 validation loss:0.1238
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12367146538401191 0.12304033872417429
need align? ->  True 0.1219128549776294
2023-08-08 01:52:02,800 - epoch:27, training loss:0.5570 validation loss:0.1237
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12383008189499378 0.12315712750635364
need align? ->  True 0.1219128549776294
2023-08-08 01:53:38,914 - epoch:28, training loss:0.5623 validation loss:0.1238
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12392199318856001 0.12317578486082228
need align? ->  True 0.1219128549776294
2023-08-08 01:55:10,297 - epoch:29, training loss:0.5571 validation loss:0.1239
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-01:09:37.033796/0/0.1235_epoch_23.pkl  &  0.1219128549776294
2023-08-08 01:55:16,299 - [*] loss:0.2754
2023-08-08 01:55:16,304 - [*] phase 0, testing
2023-08-08 01:55:16,344 - T:96	MAE	0.332764	RMSE	0.275667	MAPE	128.874183
2023-08-08 01:55:16,345 - 96	mae	0.3328	
2023-08-08 01:55:16,346 - 96	rmse	0.2757	
2023-08-08 01:55:16,346 - 96	mape	128.8742	
2023-08-08 01:55:22,198 - [*] loss:0.2723
2023-08-08 01:55:22,201 - [*] phase 0, testing
2023-08-08 01:55:22,240 - T:96	MAE	0.329834	RMSE	0.272402	MAPE	133.231020
2023-08-08 01:55:22,242 - 96	mae	0.3298	
2023-08-08 01:55:22,242 - 96	rmse	0.2724	
2023-08-08 01:55:22,242 - 96	mape	133.2310	
2023-08-08 01:55:24,977 - logger name:exp/ECL-PatchTST2023-08-08-01:55:24.976545/ECL-PatchTST.log
2023-08-08 01:55:24,977 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 2, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'rec_intra_feature': 0, 'rec_ori': 0, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-01:55:24.976545', 'path': 'exp/ECL-PatchTST2023-08-08-01:55:24.976545', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 01:55:24,977 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 01:55:25,300 - [*] phase 0 Dataset load!
2023-08-08 01:55:26,347 - [*] phase 0 Training start
train 8209
2023-08-08 01:56:46,314 - epoch:0, training loss:0.2122 validation loss:0.1674
train 8209
vs, vt 0.16737469370392236 0.16722329299558292
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
2023-08-08 01:57:07,398 - logger name:exp/ECL-PatchTST2023-08-08-01:57:07.397649/ECL-PatchTST.log
2023-08-08 01:57:07,398 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 2, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 0, 'rec_intra_feature': 0, 'rec_ori': 0, 'mid_dim': 128, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-01:57:07.397649', 'path': 'exp/ECL-PatchTST2023-08-08-01:57:07.397649', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 01:57:07,399 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 01:57:07,661 - [*] phase 0 Dataset load!
2023-08-08 01:57:08,722 - [*] phase 0 Training start
train 8209
2023-08-08 01:58:30,349 - epoch:0, training loss:0.2108 validation loss:0.1664
train 8209
vs, vt 0.16638522188771854 0.1651341368190267
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
2023-08-08 01:58:51,167 - logger name:exp/ECL-PatchTST2023-08-08-01:58:51.166819/ECL-PatchTST.log
2023-08-08 01:58:51,167 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 0, 'mid_dim': 64, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-01:58:51.166819', 'path': 'exp/ECL-PatchTST2023-08-08-01:58:51.166819', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 01:58:51,168 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 01:58:51,440 - [*] phase 0 Dataset load!
2023-08-08 01:58:52,376 - [*] phase 0 Training start
train 8209
2023-08-08 01:59:43,064 - epoch:0, training loss:0.2157 validation loss:0.1663
train 8209
vs, vt 0.1663396390662952 0.1671547761017626
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13967661508782345 0.13788628137924455
need align? ->  True 0.13788628137924455
2023-08-08 02:01:27,132 - epoch:1, training loss:1.7618 validation loss:0.1397
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12926127592271025 0.12768637499010022
need align? ->  True 0.12768637499010022
2023-08-08 02:02:58,797 - epoch:2, training loss:1.5203 validation loss:0.1293
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.12729729432612658 0.12474266532808542
need align? ->  True 0.12474266532808542
2023-08-08 02:04:30,495 - epoch:3, training loss:1.2574 validation loss:0.1273
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.1252102613957091 0.12269180945374748
need align? ->  True 0.12269180945374748
2023-08-08 02:06:01,850 - epoch:4, training loss:1.0489 validation loss:0.1252
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12572993168776686 0.1218169370496815
need align? ->  True 0.1218169370496815
2023-08-08 02:07:38,709 - epoch:5, training loss:0.9378 validation loss:0.1257
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.1269468188455159 0.1223316074941646
need align? ->  True 0.1218169370496815
2023-08-08 02:09:17,878 - epoch:6, training loss:0.8788 validation loss:0.1269
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12680315632711758 0.12269790665331212
need align? ->  True 0.1218169370496815
2023-08-08 02:10:51,438 - epoch:7, training loss:0.8589 validation loss:0.1268
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12508524607189678 0.1225024980746887
need align? ->  True 0.1218169370496815
2023-08-08 02:12:23,989 - epoch:8, training loss:0.8480 validation loss:0.1251
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12784648923711342 0.12328219168226827
need align? ->  True 0.1218169370496815
2023-08-08 02:13:56,439 - epoch:9, training loss:0.8398 validation loss:0.1278
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12610861396586354 0.12260927852581847
need align? ->  True 0.1218169370496815
2023-08-08 02:15:27,641 - epoch:10, training loss:0.8317 validation loss:0.1261
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12724728492850607 0.1220540279014544
need align? ->  True 0.1218169370496815
2023-08-08 02:16:59,701 - epoch:11, training loss:0.8224 validation loss:0.1272
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12628865902396766 0.12295585582879456
need align? ->  True 0.1218169370496815
2023-08-08 02:18:30,935 - epoch:12, training loss:0.8191 validation loss:0.1263
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.1303495028987527 0.12276179140264337
need align? ->  True 0.1218169370496815
2023-08-08 02:20:04,567 - epoch:13, training loss:0.8140 validation loss:0.1303
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12715093706819144 0.12258331723172557
need align? ->  True 0.1218169370496815
2023-08-08 02:21:36,470 - epoch:14, training loss:0.8100 validation loss:0.1272
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12651377577673306 0.1228707233782519
need align? ->  True 0.1218169370496815
2023-08-08 02:23:13,662 - epoch:15, training loss:0.8073 validation loss:0.1265
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.1276509098877961 0.12305356062610041
need align? ->  True 0.1218169370496815
2023-08-08 02:24:52,252 - epoch:16, training loss:0.8028 validation loss:0.1277
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12785799225622957 0.12305070874704556
need align? ->  True 0.1218169370496815
2023-08-08 02:26:26,404 - epoch:17, training loss:0.8003 validation loss:0.1279
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.1268727555024353 0.12275445385074074
need align? ->  True 0.1218169370496815
2023-08-08 02:27:58,125 - epoch:18, training loss:0.7984 validation loss:0.1269
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.13018904287706723 0.12336451983587308
need align? ->  True 0.1218169370496815
2023-08-08 02:29:29,062 - epoch:19, training loss:0.7964 validation loss:0.1302
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.1290342335335233 0.12329267761246725
need align? ->  True 0.1218169370496815
2023-08-08 02:31:00,700 - epoch:20, training loss:0.7936 validation loss:0.1290
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.1286517712710933 0.12347624019127
need align? ->  True 0.1218169370496815
2023-08-08 02:32:32,980 - epoch:21, training loss:0.7935 validation loss:0.1287
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12943354731594975 0.1234450436789881
need align? ->  True 0.1218169370496815
2023-08-08 02:34:04,906 - epoch:22, training loss:0.7909 validation loss:0.1294
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12750967092473398 0.12339905099096624
need align? ->  True 0.1218169370496815
2023-08-08 02:35:37,369 - epoch:23, training loss:0.7895 validation loss:0.1275
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.1282247110185298 0.12311839921907945
need align? ->  True 0.1218169370496815
2023-08-08 02:37:08,799 - epoch:24, training loss:0.7904 validation loss:0.1282
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12821155371652407 0.12308778876269405
need align? ->  True 0.1218169370496815
2023-08-08 02:38:40,816 - epoch:25, training loss:0.7880 validation loss:0.1282
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12801489360969176 0.1231114926155318
need align? ->  True 0.1218169370496815
2023-08-08 02:40:14,365 - epoch:26, training loss:0.7874 validation loss:0.1280
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12829739405688914 0.1230770382522182
need align? ->  True 0.1218169370496815
2023-08-08 02:41:52,027 - epoch:27, training loss:0.7863 validation loss:0.1283
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.1282091916284778 0.1231095137243921
need align? ->  True 0.1218169370496815
2023-08-08 02:43:30,278 - epoch:28, training loss:0.7863 validation loss:0.1282
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12807125894522126 0.12306469509547407
need align? ->  True 0.1218169370496815
2023-08-08 02:45:05,324 - epoch:29, training loss:0.7865 validation loss:0.1281
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-01:58:51.166819/0/0.1251_epoch_8.pkl  &  0.1218169370496815
2023-08-08 02:45:12,563 - [*] loss:0.2796
2023-08-08 02:45:12,568 - [*] phase 0, testing
2023-08-08 02:45:12,629 - T:96	MAE	0.334833	RMSE	0.279679	MAPE	131.732154
2023-08-08 02:45:12,631 - 96	mae	0.3348	
2023-08-08 02:45:12,631 - 96	rmse	0.2797	
2023-08-08 02:45:12,632 - 96	mape	131.7322	
2023-08-08 02:45:18,239 - [*] loss:0.2717
2023-08-08 02:45:18,242 - [*] phase 0, testing
2023-08-08 02:45:18,283 - T:96	MAE	0.330532	RMSE	0.271978	MAPE	133.487928
2023-08-08 02:45:18,285 - 96	mae	0.3305	
2023-08-08 02:45:18,285 - 96	rmse	0.2720	
2023-08-08 02:45:18,286 - 96	mape	133.4879	
2023-08-08 02:45:20,702 - logger name:exp/ECL-PatchTST2023-08-08-02:45:20.702063/ECL-PatchTST.log
2023-08-08 02:45:20,703 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 35, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 0, 'mid_dim': 64, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-08-02:45:20.702063', 'path': 'exp/ECL-PatchTST2023-08-08-02:45:20.702063', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-08 02:45:20,703 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-08 02:45:20,966 - [*] phase 0 Dataset load!
2023-08-08 02:45:22,010 - [*] phase 0 Training start
train 8209
2023-08-08 02:46:12,068 - epoch:0, training loss:0.2181 validation loss:0.1683
train 8209
vs, vt 0.16827843710780144 0.1700424634936181
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.1397082544863224 0.13838067583062433
need align? ->  True 0.13838067583062433
2023-08-08 02:47:57,616 - epoch:1, training loss:1.7872 validation loss:0.1397
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12924844399094582 0.12700668544593183
need align? ->  True 0.12700668544593183
2023-08-08 02:49:30,289 - epoch:2, training loss:1.5472 validation loss:0.1292
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.1260361767119982 0.12396007776260376
need align? ->  True 0.12396007776260376
2023-08-08 02:51:01,884 - epoch:3, training loss:1.2801 validation loss:0.1260
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12579191865568812 0.12349105143750255
need align? ->  True 0.12349105143750255
2023-08-08 02:52:34,400 - epoch:4, training loss:1.0649 validation loss:0.1258
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12452644676986066 0.12296468061818318
need align? ->  True 0.12296468061818318
2023-08-08 02:54:09,644 - epoch:5, training loss:0.9458 validation loss:0.1245
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12429604349149899 0.1221837517382069
need align? ->  True 0.1221837517382069
2023-08-08 02:55:46,083 - epoch:6, training loss:0.8911 validation loss:0.1243
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12532670609652996 0.12249511361799457
need align? ->  True 0.1221837517382069
2023-08-08 02:57:23,978 - epoch:7, training loss:0.8595 validation loss:0.1253
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12439881112765182 0.12249938923526894
need align? ->  True 0.1221837517382069
2023-08-08 02:58:57,260 - epoch:8, training loss:0.8433 validation loss:0.1244
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12688660494644532 0.12263631575148214
need align? ->  True 0.1221837517382069
2023-08-08 03:00:29,663 - epoch:9, training loss:0.8345 validation loss:0.1269
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12869570378891446 0.12285657565702092
need align? ->  True 0.1221837517382069
2023-08-08 03:02:01,854 - epoch:10, training loss:0.8264 validation loss:0.1287
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.1261009374972094 0.12222359295595776
need align? ->  True 0.1221837517382069
2023-08-08 03:03:34,744 - epoch:11, training loss:0.8204 validation loss:0.1261
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.125586963174018 0.1229247096079317
need align? ->  True 0.1221837517382069
2023-08-08 03:05:06,935 - epoch:12, training loss:0.8144 validation loss:0.1256
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.1268664621324702 0.12267249407754703
need align? ->  True 0.1221837517382069
2023-08-08 03:06:40,930 - epoch:13, training loss:0.8097 validation loss:0.1269
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.1262702587991953 0.122160102579404
need align? ->  True 0.122160102579404
2023-08-08 03:08:13,125 - epoch:14, training loss:0.8056 validation loss:0.1263
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12570155411958694 0.12220280702141198
need align? ->  True 0.122160102579404
2023-08-08 03:09:44,973 - epoch:15, training loss:0.8102 validation loss:0.1257
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12505308868871493 0.12213613152165305
need align? ->  True 0.12213613152165305
2023-08-08 03:11:21,514 - epoch:16, training loss:0.7833 validation loss:0.1251
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12860087576237592 0.12246492792936889
need align? ->  True 0.12213613152165305
2023-08-08 03:13:05,494 - epoch:17, training loss:0.8000 validation loss:0.1286
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12525498198175972 0.12214969996024262
need align? ->  True 0.12213613152165305
2023-08-08 03:14:45,237 - epoch:18, training loss:0.7898 validation loss:0.1253
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12837799325246702 0.12202779576182365
need align? ->  True 0.12202779576182365
2023-08-08 03:16:18,083 - epoch:19, training loss:0.7839 validation loss:0.1284
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.1254905259914019 0.12204396606168964
need align? ->  True 0.12202779576182365
2023-08-08 03:17:50,014 - epoch:20, training loss:0.8086 validation loss:0.1255
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.1269417739557949 0.12231856178153645
need align? ->  True 0.12202779576182365
2023-08-08 03:19:22,462 - epoch:21, training loss:0.7963 validation loss:0.1269
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12511153578419576 0.12203220167959278
need align? ->  True 0.12202779576182365
2023-08-08 03:20:54,629 - epoch:22, training loss:0.7941 validation loss:0.1251
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.1259618059478023 0.12219693681055849
need align? ->  True 0.12202779576182365
2023-08-08 03:22:26,637 - epoch:23, training loss:0.7910 validation loss:0.1260
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12585896719247103 0.12223307314244183
need align? ->  True 0.12202779576182365
2023-08-08 03:23:58,249 - epoch:24, training loss:0.7893 validation loss:0.1259
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12607970655980436 0.12215861991386522
need align? ->  True 0.12202779576182365
2023-08-08 03:25:29,583 - epoch:25, training loss:0.7882 validation loss:0.1261
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12582813770594922 0.12220390344207938
need align? ->  True 0.12202779576182365
2023-08-08 03:27:01,554 - epoch:26, training loss:0.7873 validation loss:0.1258
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.1256601679223505 0.12211515610529618
need align? ->  True 0.12202779576182365
2023-08-08 03:28:33,328 - epoch:27, training loss:0.7875 validation loss:0.1257
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12568007815967908 0.12217256553809751
need align? ->  True 0.12202779576182365
2023-08-08 03:30:09,451 - epoch:28, training loss:0.7874 validation loss:0.1257
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12581826695664364 0.12214616576040332
need align? ->  True 0.12202779576182365
2023-08-08 03:31:47,951 - epoch:29, training loss:0.7863 validation loss:0.1258
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-08-02:45:20.702063/0/0.1243_epoch_6.pkl  &  0.12202779576182365
2023-08-08 03:31:52,815 - [*] loss:0.2768
2023-08-08 03:31:52,820 - [*] phase 0, testing
2023-08-08 03:31:52,857 - T:96	MAE	0.334785	RMSE	0.276855	MAPE	132.374620
2023-08-08 03:31:52,858 - 96	mae	0.3348	
2023-08-08 03:31:52,858 - 96	rmse	0.2769	
2023-08-08 03:31:52,858 - 96	mape	132.3746	
2023-08-08 03:31:57,021 - [*] loss:0.2725
2023-08-08 03:31:57,025 - [*] phase 0, testing
2023-08-08 03:31:57,062 - T:96	MAE	0.330474	RMSE	0.272849	MAPE	135.721183
2023-08-08 03:31:57,063 - 96	mae	0.3305	
2023-08-08 03:31:57,063 - 96	rmse	0.2728	
2023-08-08 03:31:57,063 - 96	mape	135.7212	
