2023-08-23 23:41:57,604 - logger name:exp/ECL-PatchTST2023-08-23-23:41:57.603952/ECL-PatchTST.log
2023-08-23 23:41:57,604 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-22-20:19:56.785959/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 64, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 2.0, 'slope_rate': 0.01, 'slope_range': 0.5, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'omega': 1.0, 'theta': 1.1, 'mask_border': 1, 'sup_weight': 20.0, 'rec_length_ratio': 0.95, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-23-23:41:57.603952', 'path': 'exp/ECL-PatchTST2023-08-23-23:41:57.603952', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-23 23:41:57,604 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-23 23:41:57,795 - [*] phase 0 Dataset load!
2023-08-23 23:41:58,785 - [*] phase 0 Training start
train 8209
2023-08-23 23:42:34,289 - epoch:0, training loss:0.2113 validation loss:0.1488
train 8209
vs, vt 0.14877593839033085 0.15270698942582717
Updating learning rate to 1.0447174097965174e-05
Updating learning rate to 1.0447174097965174e-05
train 8209
vs, vt 0.13092856109142303 0.1353443286127665
need align? ->  False 0.1353443286127665
2023-08-23 23:43:28,277 - epoch:1, training loss:4.4202 validation loss:0.1309
Updating learning rate to 2.8056777481465998e-05
Updating learning rate to 2.8056777481465998e-05
train 8209
vs, vt 0.12668049178848212 0.12796781402589244
need align? ->  False 0.12796781402589244
2023-08-23 23:44:16,158 - epoch:2, training loss:3.0794 validation loss:0.1267
Updating learning rate to 5.209830270010471e-05
Updating learning rate to 5.209830270010471e-05
train 8209
vs, vt 0.1244522894478657 0.12401930374008688
need align? ->  True 0.12401930374008688
2023-08-23 23:45:04,219 - epoch:3, training loss:2.1645 validation loss:0.1245
Updating learning rate to 7.611342064283194e-05
Updating learning rate to 7.611342064283194e-05
train 8209
vs, vt 0.12436974624341185 0.12340294739062135
need align? ->  True 0.12340294739062135
2023-08-23 23:45:52,280 - epoch:4, training loss:1.7119 validation loss:0.1244
Updating learning rate to 9.365089604465064e-05
Updating learning rate to 9.365089604465064e-05
train 8209
vs, vt 0.12305153753947128 0.12425911989571019
need align? ->  False 0.12340294739062135
2023-08-23 23:46:40,219 - epoch:5, training loss:1.5226 validation loss:0.1231
Updating learning rate to 9.9999973854584e-05
Updating learning rate to 9.9999973854584e-05
train 8209
vs, vt 0.12384484128349206 0.12357654828916896
need align? ->  True 0.12340294739062135
2023-08-23 23:47:28,008 - epoch:6, training loss:1.4657 validation loss:0.1238
Updating learning rate to 9.956554473013382e-05
Updating learning rate to 9.956554473013382e-05
train 8209
vs, vt 0.12355449131097306 0.12410132756287401
need align? ->  True 0.12340294739062135
2023-08-23 23:48:24,455 - epoch:7, training loss:1.4250 validation loss:0.1236
Updating learning rate to 9.828303881524126e-05
Updating learning rate to 9.828303881524126e-05
train 8209
vs, vt 0.12443861289119179 0.12435275557535616
need align? ->  True 0.12340294739062135
2023-08-23 23:49:12,546 - epoch:8, training loss:1.3919 validation loss:0.1244
Updating learning rate to 9.617440014168803e-05
Updating learning rate to 9.617440014168803e-05
train 8209
vs, vt 0.12266136595810001 0.12530887156555598
need align? ->  False 0.12340294739062135
2023-08-23 23:50:00,855 - epoch:9, training loss:1.3653 validation loss:0.1227
Updating learning rate to 9.327570810180364e-05
Updating learning rate to 9.327570810180364e-05
train 8209
vs, vt 0.12432860680432482 0.1239146647382189
need align? ->  True 0.12340294739062135
2023-08-23 23:50:48,855 - epoch:10, training loss:1.3356 validation loss:0.1243
Updating learning rate to 8.963656012005982e-05
Updating learning rate to 8.963656012005982e-05
train 8209
vs, vt 0.12449061845175245 0.12527996526015076
need align? ->  True 0.12340294739062135
2023-08-23 23:51:36,836 - epoch:11, training loss:1.3188 validation loss:0.1245
Updating learning rate to 8.531922302738664e-05
Updating learning rate to 8.531922302738664e-05
train 8209
vs, vt 0.12314395301721313 0.12526021465997805
need align? ->  False 0.12340294739062135
2023-08-23 23:52:25,536 - epoch:12, training loss:1.2993 validation loss:0.1231
Updating learning rate to 8.03975676584317e-05
Updating learning rate to 8.03975676584317e-05
train 8209
vs, vt 0.12356991642578082 0.1242569408125498
need align? ->  True 0.12340294739062135
2023-08-23 23:53:38,449 - epoch:13, training loss:1.2777 validation loss:0.1236
Updating learning rate to 7.49558049010985e-05
Updating learning rate to 7.49558049010985e-05
train 8209
vs, vt 0.12316313919357279 0.12521760902282866
need align? ->  False 0.12340294739062135
2023-08-23 23:54:26,533 - epoch:14, training loss:1.2653 validation loss:0.1232
Updating learning rate to 6.908704482490666e-05
Updating learning rate to 6.908704482490666e-05
train 8209
vs, vt 0.12335182692516934 0.125150781463493
need align? ->  False 0.12340294739062135
2023-08-23 23:55:15,177 - epoch:15, training loss:1.2510 validation loss:0.1234
Updating learning rate to 6.289170354188751e-05
Updating learning rate to 6.289170354188751e-05
train 8209
vs, vt 0.123727861283855 0.12517554909837517
need align? ->  True 0.12340294739062135
2023-08-23 23:56:03,933 - epoch:16, training loss:1.2402 validation loss:0.1237
Updating learning rate to 5.647578505906667e-05
Updating learning rate to 5.647578505906667e-05
train 8209
vs, vt 0.12372537499124353 0.12507704251699828
need align? ->  True 0.12340294739062135
2023-08-23 23:57:17,350 - epoch:17, training loss:1.2265 validation loss:0.1237
Updating learning rate to 4.9949067520513824e-05
Updating learning rate to 4.9949067520513824e-05
train 8209
vs, vt 0.12373050149868835 0.125937666405331
need align? ->  True 0.12340294739062135
2023-08-23 23:58:05,684 - epoch:18, training loss:1.2214 validation loss:0.1237
Updating learning rate to 4.3423224872861584e-05
Updating learning rate to 4.3423224872861584e-05
train 8209
vs, vt 0.12323855816132644 0.12475597384301099
need align? ->  False 0.12340294739062135
2023-08-23 23:58:54,212 - epoch:19, training loss:1.2129 validation loss:0.1232
Updating learning rate to 3.70099160931167e-05
Updating learning rate to 3.70099160931167e-05
train 8209
vs, vt 0.12335232027213681 0.12534491806714373
need align? ->  False 0.12340294739062135
2023-08-23 23:59:43,137 - epoch:20, training loss:1.2040 validation loss:0.1234
Updating learning rate to 3.081887467260569e-05
Updating learning rate to 3.081887467260569e-05
train 8209
vs, vt 0.12323748857968232 0.12569373765621672
need align? ->  False 0.12340294739062135
2023-08-24 00:00:32,390 - epoch:21, training loss:1.1963 validation loss:0.1232
Updating learning rate to 2.4956031046514483e-05
Updating learning rate to 2.4956031046514483e-05
train 8209
vs, vt 0.12357083225453441 0.12521830243481832
need align? ->  True 0.12340294739062135
2023-08-24 00:01:21,038 - epoch:22, training loss:1.1951 validation loss:0.1236
Updating learning rate to 1.952170009477285e-05
Updating learning rate to 1.952170009477285e-05
train 8209
vs, vt 0.12291101251982829 0.12549412292851644
need align? ->  False 0.12340294739062135
2023-08-24 00:02:09,176 - epoch:23, training loss:1.1938 validation loss:0.1229
Updating learning rate to 1.4608864726646246e-05
Updating learning rate to 1.4608864726646246e-05
train 8209
vs, vt 0.12301906401460821 0.12536452498964287
need align? ->  False 0.12340294739062135
2023-08-24 00:02:57,673 - epoch:24, training loss:1.1883 validation loss:0.1230
Updating learning rate to 1.0301584917378652e-05
Updating learning rate to 1.0301584917378652e-05
train 8209
vs, vt 0.12343299541283738 0.1252237606200982
need align? ->  True 0.12340294739062135
2023-08-24 00:03:46,407 - epoch:25, training loss:1.1858 validation loss:0.1234
Updating learning rate to 6.673559418710186e-06
Updating learning rate to 6.673559418710186e-06
train 8209
vs, vt 0.12320177146995609 0.12547399967231535
need align? ->  False 0.12340294739062135
2023-08-24 00:04:34,876 - epoch:26, training loss:1.1872 validation loss:0.1232
Updating learning rate to 3.7868647528017672e-06
Updating learning rate to 3.7868647528017672e-06
train 8209
vs, vt 0.12324685992842371 0.12576989533210342
need align? ->  False 0.12340294739062135
2023-08-24 00:05:23,228 - epoch:27, training loss:1.1842 validation loss:0.1232
Updating learning rate to 1.690893065730064e-06
Updating learning rate to 1.690893065730064e-06
train 8209
vs, vt 0.1232036432589997 0.12492846629836342
need align? ->  False 0.12340294739062135
2023-08-24 00:06:11,598 - epoch:28, training loss:1.1831 validation loss:0.1232
Updating learning rate to 4.2150701417399644e-07
Updating learning rate to 4.2150701417399644e-07
train 8209
vs, vt 0.12312123094770042 0.12518523879010568
need align? ->  False 0.12340294739062135
2023-08-24 00:07:00,143 - epoch:29, training loss:1.1842 validation loss:0.1231
Updating learning rate to 4.26145415999806e-10
Updating learning rate to 4.26145415999806e-10
check exp/ECL-PatchTST2023-08-23-23:41:57.603952/0/0.1227_epoch_9.pkl  &  0.12340294739062135
2023-08-24 00:07:04,967 - [*] loss:0.2718
2023-08-24 00:07:04,971 - [*] phase 0, testing
2023-08-24 00:07:05,022 - T:96	MAE	0.333248	RMSE	0.273000	MAPE	133.553708
2023-08-24 00:07:05,023 - 96	mae	0.3332	
2023-08-24 00:07:05,023 - 96	rmse	0.2730	
2023-08-24 00:07:05,023 - 96	mape	133.5537	
2023-08-24 00:07:06,158 - [*] loss:0.2743
2023-08-24 00:07:06,162 - [*] phase 0, testing
2023-08-24 00:07:06,211 - T:96	MAE	0.334399	RMSE	0.275559	MAPE	135.701025
2023-08-24 00:07:07,373 - [*] loss:0.2744
2023-08-24 00:07:07,377 - [*] phase 0, testing
2023-08-24 00:07:07,428 - T:96	MAE	0.333264	RMSE	0.276097	MAPE	135.479450
2023-08-24 00:07:07,430 - 96	mae	0.3333	
2023-08-24 00:07:07,430 - 96	rmse	0.2761	
2023-08-24 00:07:07,430 - 96	mape	135.4795	
