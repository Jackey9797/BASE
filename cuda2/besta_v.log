2023-08-26 00:40:51,999 - logger name:exp/ECL-PatchTST2023-08-26-00:40:51.999068/ECL-PatchTST.log
2023-08-26 00:40:51,999 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 336, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-23-11:43:26.865832/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 64, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 2.0, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'omega': 1.0, 'theta': 1.1, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-26-00:40:51.999068', 'path': 'exp/ECL-PatchTST2023-08-26-00:40:51.999068', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-26 00:40:51,999 - [*] phase 0 start training
0 17420
train 7969
val 2545
test 2545
2023-08-26 00:40:52,187 - [*] phase 0 Dataset load!
2023-08-26 00:40:53,191 - [*] phase 0 Training start
train 7969
2023-08-26 00:41:04,387 - epoch:0, training loss:0.2637 validation loss:0.1805
train 7969
vs, vt 0.18049050234258174 0.1824018396437168
Updating learning rate to 1.044770395452216e-05
Updating learning rate to 1.044770395452216e-05
train 7969
vs, vt 0.163856665417552 0.16983951367437838
need align? ->  False 0.16983951367437838
2023-08-26 00:41:59,139 - epoch:1, training loss:10.1183 validation loss:0.1639
Updating learning rate to 2.8058612222873615e-05
Updating learning rate to 2.8058612222873615e-05
train 7969
vs, vt 0.16241810349747537 0.16236437698826195
need align? ->  True 0.16236437698826195
2023-08-26 00:42:46,440 - epoch:2, training loss:5.4895 validation loss:0.1624
Updating learning rate to 5.2101478018196974e-05
Updating learning rate to 5.2101478018196974e-05
train 7969
vs, vt 0.15942223398014904 0.1651505962945521
need align? ->  False 0.16236437698826195
2023-08-26 00:43:33,741 - epoch:3, training loss:3.5791 validation loss:0.1594
Updating learning rate to 7.611708130438597e-05
Updating learning rate to 7.611708130438597e-05
train 7969
vs, vt 0.1602392105385661 0.1619716899469495
need align? ->  False 0.1619716899469495
2023-08-26 00:44:21,909 - epoch:4, training loss:3.0872 validation loss:0.1602
Updating learning rate to 9.365352623649903e-05
Updating learning rate to 9.365352623649903e-05
train 7969
vs, vt 0.1619848213158548 0.16759056253358723
need align? ->  True 0.1619716899469495
2023-08-26 00:45:10,561 - epoch:5, training loss:2.5776 validation loss:0.1620
Updating learning rate to 9.999997214057666e-05
Updating learning rate to 9.999997214057666e-05
train 7969
vs, vt 0.1628300542011857 0.16665652990341187
need align? ->  True 0.1619716899469495
2023-08-26 00:45:58,506 - epoch:6, training loss:2.3682 validation loss:0.1628
Updating learning rate to 9.956532773642211e-05
Updating learning rate to 9.956532773642211e-05
train 7969
vs, vt 0.16061400091275574 0.17029894925653935
need align? ->  False 0.1619716899469495
2023-08-26 00:46:46,410 - epoch:7, training loss:2.2254 validation loss:0.1606
Updating learning rate to 9.828261025464778e-05
Updating learning rate to 9.828261025464778e-05
train 7969
vs, vt 0.16157061085104943 0.17203894415870308
need align? ->  False 0.1619716899469495
2023-08-26 00:47:34,014 - epoch:8, training loss:2.1352 validation loss:0.1616
Updating learning rate to 9.617376734700332e-05
Updating learning rate to 9.617376734700332e-05
train 7969
vs, vt 0.1614985613152385 0.17180262859910728
need align? ->  False 0.1619716899469495
2023-08-26 00:48:22,653 - epoch:9, training loss:2.0668 validation loss:0.1615
Updating learning rate to 9.327488190032024e-05
Updating learning rate to 9.327488190032024e-05
train 7969
vs, vt 0.16268557775765657 0.17379705738276244
need align? ->  True 0.1619716899469495
2023-08-26 00:49:11,015 - epoch:10, training loss:1.9931 validation loss:0.1627
Updating learning rate to 8.963555464831415e-05
Updating learning rate to 8.963555464831415e-05
train 7969
vs, vt 0.15940784653648735 0.17382773160934448
need align? ->  False 0.1619716899469495
2023-08-26 00:50:28,638 - epoch:11, training loss:1.9171 validation loss:0.1594
Updating learning rate to 8.531805548927904e-05
Updating learning rate to 8.531805548927904e-05
train 7969
vs, vt 0.163071911316365 0.17211951576173307
need align? ->  True 0.1619716899469495
2023-08-26 00:52:11,435 - epoch:12, training loss:1.8713 validation loss:0.1631
Updating learning rate to 8.039625803086289e-05
Updating learning rate to 8.039625803086289e-05
train 7969
vs, vt 0.1598341559059918 0.1797305792570114
need align? ->  False 0.1619716899469495
2023-08-26 00:53:46,946 - epoch:13, training loss:1.8285 validation loss:0.1598
Updating learning rate to 7.495437559215929e-05
Updating learning rate to 7.495437559215929e-05
train 7969
vs, vt 0.16203171713277698 0.17456246465444564
need align? ->  True 0.1619716899469495
2023-08-26 00:55:16,942 - epoch:14, training loss:1.7944 validation loss:0.1620
Updating learning rate to 6.908552029046927e-05
Updating learning rate to 6.908552029046927e-05
train 7969
vs, vt 0.1617179809138179 0.17816064264625311
need align? ->  False 0.1619716899469495
2023-08-26 00:56:45,697 - epoch:15, training loss:1.7594 validation loss:0.1617
Updating learning rate to 6.289010986715887e-05
Updating learning rate to 6.289010986715887e-05
train 7969
vs, vt 0.15884908819571136 0.17765940483659506
need align? ->  False 0.1619716899469495
2023-08-26 00:58:30,328 - epoch:16, training loss:1.7343 validation loss:0.1588
Updating learning rate to 5.64741495122632e-05
Updating learning rate to 5.64741495122632e-05
train 7969
vs, vt 0.15990425134077668 0.1743749801069498
need align? ->  False 0.1619716899469495
2023-08-26 01:00:05,923 - epoch:17, training loss:1.7128 validation loss:0.1599
Updating learning rate to 4.9947418086294814e-05
Updating learning rate to 4.9947418086294814e-05
train 7969
vs, vt 0.16096155112609267 0.1745268389582634
need align? ->  False 0.1619716899469495
2023-08-26 01:01:36,353 - epoch:18, training loss:1.6937 validation loss:0.1610
Updating learning rate to 4.3421589773503834e-05
Updating learning rate to 4.3421589773503834e-05
train 7969
vs, vt 0.16024305569007993 0.17639278937131167
need align? ->  False 0.1619716899469495
2023-08-26 01:03:24,479 - epoch:19, training loss:1.6737 validation loss:0.1602
Updating learning rate to 3.700832330562355e-05
Updating learning rate to 3.700832330562355e-05
train 7969
vs, vt 0.1615765797905624 0.17673362102359533
need align? ->  False 0.1619716899469495
2023-08-26 01:04:55,719 - epoch:20, training loss:1.6631 validation loss:0.1616
Updating learning rate to 3.0817351450012735e-05
Updating learning rate to 3.0817351450012735e-05
train 7969
vs, vt 0.15935890544205905 0.17866811491549014
need align? ->  False 0.1619716899469495
2023-08-26 01:06:23,767 - epoch:21, training loss:1.6428 validation loss:0.1594
Updating learning rate to 2.495460345158263e-05
Updating learning rate to 2.495460345158263e-05
train 7969
vs, vt 0.15930626513436436 0.1773105390369892
need align? ->  False 0.1619716899469495
2023-08-26 01:08:16,212 - epoch:22, training loss:1.6371 validation loss:0.1593
Updating learning rate to 1.9520392554047177e-05
Updating learning rate to 1.9520392554047177e-05
train 7969
vs, vt 0.1593388456851244 0.1779119001701474
need align? ->  False 0.1619716899469495
2023-08-26 01:09:48,374 - epoch:23, training loss:1.6313 validation loss:0.1593
Updating learning rate to 1.4607699612511084e-05
Updating learning rate to 1.4607699612511084e-05
train 7969
vs, vt 0.15938531719148158 0.17672957256436347
need align? ->  False 0.1619716899469495
2023-08-26 01:11:17,229 - epoch:24, training loss:1.6269 validation loss:0.1594
Updating learning rate to 1.0300582165259884e-05
Updating learning rate to 1.0300582165259884e-05
train 7969
vs, vt 0.1597879427485168 0.17677316162735224
need align? ->  False 0.1619716899469495
2023-08-26 01:13:03,101 - epoch:25, training loss:1.6216 validation loss:0.1598
Updating learning rate to 6.672736185974589e-06
Updating learning rate to 6.672736185974589e-06
train 7969
vs, vt 0.16003923267126083 0.17705768551677464
need align? ->  False 0.1619716899469495
2023-08-26 01:14:34,395 - epoch:26, training loss:1.6183 validation loss:0.1600
Updating learning rate to 3.786235125189677e-06
Updating learning rate to 3.786235125189677e-06
train 7969
vs, vt 0.15977216875180603 0.17717944663017987
need align? ->  False 0.1619716899469495
2023-08-26 01:16:09,070 - epoch:27, training loss:1.6151 validation loss:0.1598
Updating learning rate to 1.6904678163444962e-06
Updating learning rate to 1.6904678163444962e-06
train 7969
vs, vt 0.1598074103705585 0.17695784643292428
need align? ->  False 0.1619716899469495
2023-08-26 01:17:50,893 - epoch:28, training loss:1.6164 validation loss:0.1598
Updating learning rate to 4.2129341914984563e-07
Updating learning rate to 4.2129341914984563e-07
train 7969
vs, vt 0.1605176106095314 0.1772021332755685
need align? ->  False 0.1619716899469495
2023-08-26 01:19:20,560 - epoch:29, training loss:1.6135 validation loss:0.1605
Updating learning rate to 4.278594233409897e-10
Updating learning rate to 4.278594233409897e-10
check exp/ECL-PatchTST2023-08-26-00:40:51.999068/0/0.1588_epoch_16.pkl  &  0.1619716899469495
2023-08-26 01:19:31,542 - [*] loss:0.3733
2023-08-26 01:19:31,549 - [*] phase 0, testing
2023-08-26 01:19:31,697 - T:336	MAE	0.398855	RMSE	0.368323	MAPE	162.531877
2023-08-26 01:19:31,697 - 336	mae	0.3989	
2023-08-26 01:19:31,697 - 336	rmse	0.3683	
2023-08-26 01:19:31,697 - 336	mape	162.5319	
2023-08-26 01:19:38,343 - [*] loss:0.3739
2023-08-26 01:19:38,351 - [*] phase 0, testing
2023-08-26 01:19:38,501 - T:336	MAE	0.397387	RMSE	0.368821	MAPE	161.799335
2023-08-26 01:19:49,284 - [*] loss:0.3893
2023-08-26 01:19:49,292 - [*] phase 0, testing
2023-08-26 01:19:49,434 - T:336	MAE	0.408188	RMSE	0.384344	MAPE	165.923250
2023-08-26 01:19:56,015 - [*] loss:0.3854
2023-08-26 01:19:56,023 - [*] phase 0, testing
2023-08-26 01:19:56,158 - T:336	MAE	0.404300	RMSE	0.380381	MAPE	161.598241
2023-08-26 01:19:56,159 - 336	mae	0.4043	
2023-08-26 01:19:56,159 - 336	rmse	0.3804	
2023-08-26 01:19:56,159 - 336	mape	161.5982	
