2023-08-26 13:08:31,838 - logger name:exp/ECL-PatchTST2023-08-26-13:08:31.837686/ECL-PatchTST.log
2023-08-26 13:08:31,838 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'weather', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 720, 'noise_rate': 0.5, 'device': device(type='cuda', index=1), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-23-11:43:26.865832/0/best_model.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'rec_block_num': 1, 'enhance': 1, 'enhance_type': 5, 'seed': 34, 'batch_size': 32, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 2.0, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'omega': 1.0, 'theta': 1.1, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-26-13:08:31.837686', 'path': 'exp/ECL-PatchTST2023-08-26-13:08:31.837686', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-26 13:08:31,838 - [*] phase 0 start training
0 52696
36887 5270 10539 0.7 0.2 52696
train 35832
36887 5270 10539 0.7 0.2 52696
val 4551
36887 5270 10539 0.7 0.2 52696
test 9820
2023-08-26 13:08:32,630 - [*] phase 0 Dataset load!
2023-08-26 13:08:33,645 - [*] phase 0 Training start
36887 5270 10539 0.7 0.2 52696
train 35832
2023-08-26 13:15:04,782 - epoch:0, training loss:0.2292 validation loss:0.2123
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.21225117350166495 0.21228459946342282
Updating learning rate to 1.0432652690361703e-05
Updating learning rate to 1.0432652690361703e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.2122610964766749 0.20965424034145327
need align? ->  True 0.20965424034145327
2023-08-26 13:33:11,603 - epoch:1, training loss:2.5549 validation loss:0.2123
Updating learning rate to 2.800648490166273e-05
Updating learning rate to 2.800648490166273e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.2096887994687874 0.20570420973992848
need align? ->  True 0.20570420973992848
2023-08-26 13:48:34,684 - epoch:2, training loss:1.5511 validation loss:0.2097
Updating learning rate to 5.2011231673320136e-05
Updating learning rate to 5.2011231673320136e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.2067777450372289 0.2063245405996596
need align? ->  True 0.20570420973992848
2023-08-26 14:04:02,300 - epoch:3, training loss:1.2445 validation loss:0.2068
Updating learning rate to 7.601296805107756e-05
Updating learning rate to 7.601296805107756e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20873629135060143 0.2077933213823325
need align? ->  True 0.20570420973992848
2023-08-26 14:19:28,924 - epoch:4, training loss:1.1549 validation loss:0.2087
Updating learning rate to 9.357857594811273e-05
Updating learning rate to 9.357857594811273e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20660744883588977 0.2041764592790937
need align? ->  True 0.2041764592790937
2023-08-26 14:35:00,284 - epoch:5, training loss:1.1186 validation loss:0.2066
Updating learning rate to 9.999999965789821e-05
Updating learning rate to 9.999999965789821e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20677748515264138 0.20440894083960073
need align? ->  True 0.2041764592790937
2023-08-26 14:50:31,694 - epoch:6, training loss:1.2250 validation loss:0.2068
Updating learning rate to 9.957148100130167e-05
Updating learning rate to 9.957148100130167e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20606321820637563 0.20502544225721092
need align? ->  True 0.2041764592790937
2023-08-26 15:06:02,645 - epoch:7, training loss:1.1187 validation loss:0.2061
Updating learning rate to 9.829478398301725e-05
Updating learning rate to 9.829478398301725e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20486212735409504 0.20474274336666495
need align? ->  True 0.2041764592790937
2023-08-26 15:21:35,442 - epoch:8, training loss:1.0933 validation loss:0.2049
Updating learning rate to 9.61917532429951e-05
Updating learning rate to 9.61917532429951e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.2046311710785319 0.2052154321666364
need align? ->  True 0.2041764592790937
2023-08-26 15:37:05,648 - epoch:9, training loss:1.0763 validation loss:0.2046
Updating learning rate to 9.329837222026726e-05
Updating learning rate to 9.329837222026726e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.2027404287478307 0.20600010241036648
need align? ->  False 0.2041764592790937
2023-08-26 15:52:44,030 - epoch:10, training loss:1.0620 validation loss:0.2027
Updating learning rate to 8.966414746632942e-05
Updating learning rate to 8.966414746632942e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20340820792671685 0.2049099157948594
need align? ->  False 0.2041764592790937
2023-08-26 16:08:45,290 - epoch:11, training loss:1.0522 validation loss:0.2034
Updating learning rate to 8.535126157431886e-05
Updating learning rate to 8.535126157431886e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20015149771958798 0.20603412999348206
need align? ->  False 0.2041764592790937
2023-08-26 16:25:09,637 - epoch:12, training loss:1.0418 validation loss:0.2002
Updating learning rate to 8.043350921760577e-05
Updating learning rate to 8.043350921760577e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20227583617597192 0.2065471326346164
need align? ->  False 0.2041764592790937
2023-08-26 16:41:52,177 - epoch:13, training loss:1.0340 validation loss:0.2023
Updating learning rate to 7.499503450247206e-05
Updating learning rate to 7.499503450247206e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.2009800979932705 0.20858595447315204
need align? ->  False 0.2041764592790937
2023-08-26 16:58:15,305 - epoch:14, training loss:1.0259 validation loss:0.2010
Updating learning rate to 6.912889123912373e-05
Updating learning rate to 6.912889123912373e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.19998261702435832 0.2070434000733849
need align? ->  False 0.2041764592790937
2023-08-26 17:14:20,763 - epoch:15, training loss:1.0205 validation loss:0.2000
Updating learning rate to 6.293545076519881e-05
Updating learning rate to 6.293545076519881e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20239301187383546 0.20668017108748843
need align? ->  False 0.2041764592790937
2023-08-26 17:30:10,433 - epoch:16, training loss:1.0156 validation loss:0.2024
Updating learning rate to 5.652068456435232e-05
Updating learning rate to 5.652068456435232e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20189293974137806 0.20907725503186245
need align? ->  False 0.2041764592790937
2023-08-26 17:45:47,132 - epoch:17, training loss:1.0095 validation loss:0.2019
Updating learning rate to 4.99943510647899e-05
Updating learning rate to 4.99943510647899e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20083591836940992 0.2061138543319869
need align? ->  False 0.2041764592790937
2023-08-26 18:01:18,495 - epoch:18, training loss:1.0060 validation loss:0.2008
Updating learning rate to 4.346811764213053e-05
Updating learning rate to 4.346811764213053e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20216694786832048 0.20662822303446857
need align? ->  False 0.2041764592790937
2023-08-26 18:17:08,131 - epoch:19, training loss:1.0023 validation loss:0.2022
Updating learning rate to 3.705364995964971e-05
Updating learning rate to 3.705364995964971e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20047806911952012 0.20833378306010386
need align? ->  False 0.2041764592790937
2023-08-26 18:32:57,341 - epoch:20, training loss:0.9995 validation loss:0.2005
Updating learning rate to 3.0860701337821077e-05
Updating learning rate to 3.0860701337821077e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20162758877227357 0.2081174039757335
need align? ->  False 0.2041764592790937
2023-08-26 18:48:28,473 - epoch:21, training loss:0.9966 validation loss:0.2016
Updating learning rate to 2.4995234844573852e-05
Updating learning rate to 2.4995234844573852e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20083310204994428 0.2084666321744452
need align? ->  False 0.2041764592790937
2023-08-26 19:04:00,740 - epoch:22, training loss:0.9938 validation loss:0.2008
Updating learning rate to 1.9557610237822065e-05
Updating learning rate to 1.9557610237822065e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20218114935226375 0.20755719851989013
need align? ->  False 0.2041764592790937
2023-08-26 19:19:36,948 - epoch:23, training loss:0.9923 validation loss:0.2022
Updating learning rate to 1.4640866782181578e-05
Updating learning rate to 1.4640866782181578e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20178795116139459 0.20776950797209373
need align? ->  False 0.2041764592790937
2023-08-26 19:35:13,284 - epoch:24, training loss:0.9900 validation loss:0.2018
Updating learning rate to 1.0329131321357218e-05
Updating learning rate to 1.0329131321357218e-05
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20129448305685205 0.20732926415813552
need align? ->  False 0.2041764592790937
2023-08-26 19:51:13,268 - epoch:25, training loss:0.9892 validation loss:0.2013
Updating learning rate to 6.696178844522622e-06
Updating learning rate to 6.696178844522622e-06
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20188598543197125 0.20703173611130748
need align? ->  False 0.2041764592790937
2023-08-26 20:07:37,027 - epoch:26, training loss:0.9879 validation loss:0.2019
Updating learning rate to 3.8041701758011355e-06
Updating learning rate to 3.8041701758011355e-06
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.20190649594251925 0.20824746605191197
need align? ->  False 0.2041764592790937
2023-08-26 20:24:18,969 - epoch:27, training loss:0.9871 validation loss:0.2019
Updating learning rate to 1.7025883853308567e-06
Updating learning rate to 1.7025883853308567e-06
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.2019807820866158 0.20819619626223623
need align? ->  False 0.2041764592790937
2023-08-26 20:35:35,231 - epoch:28, training loss:0.9867 validation loss:0.2020
Updating learning rate to 4.273921202153066e-07
Updating learning rate to 4.273921202153066e-07
36887 5270 10539 0.7 0.2 52696
train 35832
vs, vt 0.2020157529757573 0.2076805213635618
need align? ->  False 0.2041764592790937
2023-08-26 20:45:33,083 - epoch:29, training loss:0.9867 validation loss:0.2020
Updating learning rate to 4.0034210179960347e-10
Updating learning rate to 4.0034210179960347e-10
check exp/ECL-PatchTST2023-08-26-13:08:31.837686/0/0.2_epoch_15.pkl  &  0.2041764592790937
2023-08-26 20:46:16,526 - [*] loss:0.3204
2023-08-26 20:46:17,625 - [*] phase 0, testing
2023-08-26 20:46:32,343 - T:720	MAE	0.328947	RMSE	0.320366	MAPE	1432.721615
2023-08-26 20:46:32,391 - 720	mae	0.3289	
2023-08-26 20:46:32,391 - 720	rmse	0.3204	
2023-08-26 20:46:32,391 - 720	mape	1432.7216	
2023-08-26 20:46:44,362 - [*] loss:0.3186
2023-08-26 20:46:44,634 - [*] phase 0, testing
2023-08-26 20:47:03,984 - T:720	MAE	0.326856	RMSE	0.318527	MAPE	1443.423271
2023-08-26 20:47:45,744 - [*] loss:0.3252
2023-08-26 20:47:45,924 - [*] phase 0, testing
2023-08-26 20:48:05,662 - T:720	MAE	0.332360	RMSE	0.325094	MAPE	1505.612278
2023-08-26 20:48:17,318 - [*] loss:0.3244
2023-08-26 20:48:17,503 - [*] phase 0, testing
2023-08-26 20:48:29,498 - T:720	MAE	0.329941	RMSE	0.324298	MAPE	1536.742783
2023-08-26 20:48:29,500 - 720	mae	0.3299	
2023-08-26 20:48:29,500 - 720	rmse	0.3243	
2023-08-26 20:48:29,500 - 720	mape	1536.7428	
