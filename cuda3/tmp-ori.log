2023-08-26 00:49:52,467 - logger name:exp/ECL-PatchTST2023-08-26-00:49:52.466897/ECL-PatchTST.log
2023-08-26 00:49:52,467 - params : {'loss': 'mse', 'conf': 'ECL-PatchTST', 'data_name': 'weather', 'iteration': 1, 'train': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_residual': 0, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-08-23-11:43:26.865832/0/best_model.pkl', 'idx': -1, 'aligner': 0, 'always_align': 1, 'refiner': 0, 'rec_block_num': 1, 'enhance': 0, 'enhance_type': 5, 'seed': 34, 'batch_size': 128, 'share_head': 0, 'add_noise': 1, 'jitter_sigma': 2.0, 'slope_rate': 0.01, 'slope_range': 0.2, 'alpha': 10.0, 'beta': 1.0, 'gamma': 0.15, 'feature_jittering': 1, 'rec_intra_feature': 0, 'rec_ori': 1, 'mid_dim': 128, 'test_en': 0, 'debugger': 0, 'omega': 1.0, 'theta': 1.1, 'mask_border': 1, 'sup_weight': 10.0, 'rec_length_ratio': 0.8, 'ref_dropout': 0.0, 'ref_block_num': 2, 'add_FFN': 0, 'add_residual': 0, 'rec_all': 0, 'e_layers': 3, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': -1, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-26-00:49:52.466897', 'path': 'exp/ECL-PatchTST2023-08-26-00:49:52.466897', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-26 00:49:52,468 - [*] phase 0 start training
0 52696
36887 5270 10539 0.7 0.2 52696
train 36456
36887 5270 10539 0.7 0.2 52696
val 5175
36887 5270 10539 0.7 0.2 52696
test 10444
2023-08-26 00:49:53,297 - [*] phase 0 Dataset load!
2023-08-26 00:49:54,281 - [*] phase 0 Training start
36887 5270 10539 0.7 0.2 52696
train 36456
2023-08-26 00:52:42,669 - epoch:0, training loss:0.7346 validation loss:0.5114
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.5113576021863193 0.5038872855465587
Updating learning rate to 1.043816154401052e-05
Updating learning rate to 1.043816154401052e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.45159382227717376 0.43803975240486426
need align? ->  True 0.43803975240486426
2023-08-26 00:58:53,321 - epoch:1, training loss:0.6086 validation loss:0.4516
Updating learning rate to 2.802556600659304e-05
Updating learning rate to 2.802556600659304e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.4248741198240257 0.42643995746606733
need align? ->  False 0.42643995746606733
2023-08-26 01:03:32,107 - epoch:2, training loss:0.5136 validation loss:0.4249
Updating learning rate to 5.204427375983271e-05
Updating learning rate to 5.204427375983271e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.4090614533279 0.41684466800311715
need align? ->  False 0.41684466800311715
2023-08-26 01:08:15,610 - epoch:3, training loss:0.4840 validation loss:0.4091
Updating learning rate to 7.605110477899055e-05
Updating learning rate to 7.605110477899055e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.40340789716418196 0.40879356206917183
need align? ->  False 0.40879356206917183
2023-08-26 01:12:52,893 - epoch:4, training loss:0.4727 validation loss:0.4034
Updating learning rate to 9.360606505318738e-05
Updating learning rate to 9.360606505318738e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.39712630394028453 0.4038742382715388
need align? ->  False 0.4038742382715388
2023-08-26 01:17:28,322 - epoch:5, training loss:0.4667 validation loss:0.3971
Updating learning rate to 9.999999468896889e-05
Updating learning rate to 9.999999468896889e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.393404958633388 0.3989582551506961
need align? ->  False 0.3989582551506961
2023-08-26 01:22:05,224 - epoch:6, training loss:0.4598 validation loss:0.3934
Updating learning rate to 9.956923145608515e-05
Updating learning rate to 9.956923145608515e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3944131368544044 0.39580182594860475
need align? ->  False 0.39580182594860475
2023-08-26 01:26:43,099 - epoch:7, training loss:0.4550 validation loss:0.3944
Updating learning rate to 9.82903283518559e-05
Updating learning rate to 9.82903283518559e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3940954889829566 0.39289630022717686
need align? ->  True 0.39289630022717686
2023-08-26 01:31:25,372 - epoch:8, training loss:0.4520 validation loss:0.3941
Updating learning rate to 9.618516776297342e-05
Updating learning rate to 9.618516776297342e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.39319513629122477 0.3932350063469352
need align? ->  True 0.39289630022717686
2023-08-26 01:36:03,032 - epoch:9, training loss:0.4492 validation loss:0.3932
Updating learning rate to 9.328976957077426e-05
Updating learning rate to 9.328976957077426e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.39168267351825065 0.39373660469200555
need align? ->  False 0.39289630022717686
2023-08-26 01:40:45,992 - epoch:10, training loss:0.4459 validation loss:0.3917
Updating learning rate to 8.9653674841083e-05
Updating learning rate to 8.9653674841083e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.39029562164370607 0.39545460154370565
need align? ->  False 0.39289630022717686
2023-08-26 01:45:21,549 - epoch:11, training loss:0.4433 validation loss:0.3903
Updating learning rate to 8.533909816284057e-05
Updating learning rate to 8.533909816284057e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3948620297923321 0.39600705864225944
need align? ->  True 0.39289630022717686
2023-08-26 01:48:57,379 - epoch:12, training loss:0.4407 validation loss:0.3949
Updating learning rate to 8.041986313923835e-05
Updating learning rate to 8.041986313923835e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3919655070072267 0.3979274539685831
need align? ->  False 0.39289630022717686
2023-08-26 01:52:29,377 - epoch:13, training loss:0.4390 validation loss:0.3920
Updating learning rate to 7.498013924539976e-05
Updating learning rate to 7.498013924539976e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3901433893820135 0.3963939754701242
need align? ->  False 0.39289630022717686
2023-08-26 01:56:00,487 - epoch:14, training loss:0.4366 validation loss:0.3901
Updating learning rate to 6.91130016653248e-05
Updating learning rate to 6.91130016653248e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3891116062678942 0.3958209227861428
need align? ->  False 0.39289630022717686
2023-08-26 01:59:29,992 - epoch:15, training loss:0.4354 validation loss:0.3891
Updating learning rate to 6.291883874968641e-05
Updating learning rate to 6.291883874968641e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3890217101065124 0.39474697883536175
need align? ->  False 0.39289630022717686
2023-08-26 02:03:00,043 - epoch:16, training loss:0.4328 validation loss:0.3890
Updating learning rate to 5.650363434331756e-05
Updating learning rate to 5.650363434331756e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3889289091273052 0.3953925429320917
need align? ->  False 0.39289630022717686
2023-08-26 02:06:29,614 - epoch:17, training loss:0.4325 validation loss:0.3889
Updating learning rate to 4.997715437224189e-05
Updating learning rate to 4.997715437224189e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.38740293663449404 0.395652046836004
need align? ->  False 0.39289630022717686
2023-08-26 02:09:58,368 - epoch:18, training loss:0.4314 validation loss:0.3874
Updating learning rate to 4.345106871824663e-05
Updating learning rate to 4.345106871824663e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3903034093176446 0.3943529785042856
need align? ->  False 0.39289630022717686
2023-08-26 02:13:27,684 - epoch:19, training loss:0.4296 validation loss:0.3903
Updating learning rate to 3.7037040516244397e-05
Updating learning rate to 3.7037040516244397e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.38832824459163157 0.3951073632371135
need align? ->  False 0.39289630022717686
2023-08-26 02:16:58,078 - epoch:20, training loss:0.4293 validation loss:0.3883
Updating learning rate to 3.0844815567076016e-05
Updating learning rate to 3.0844815567076016e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.38931662025975017 0.3940020278096199
need align? ->  False 0.39289630022717686
2023-08-26 02:20:28,869 - epoch:21, training loss:0.4293 validation loss:0.3893
Updating learning rate to 2.4980344556430884e-05
Updating learning rate to 2.4980344556430884e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3889091999792471 0.3947660597722705
need align? ->  False 0.39289630022717686
2023-08-26 02:23:57,863 - epoch:22, training loss:0.4286 validation loss:0.3889
Updating learning rate to 1.9543970209239668e-05
Updating learning rate to 1.9543970209239668e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.38830327515195057 0.39463248812570806
need align? ->  False 0.39289630022717686
2023-08-26 02:27:26,897 - epoch:23, training loss:0.4264 validation loss:0.3883
Updating learning rate to 1.4628710397830516e-05
Updating learning rate to 1.4628710397830516e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3889022714844564 0.3948697029817395
need align? ->  False 0.39289630022717686
2023-08-26 02:30:56,043 - epoch:24, training loss:0.4269 validation loss:0.3889
Updating learning rate to 1.031866658034415e-05
Updating learning rate to 1.031866658034415e-05
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.38783326890410447 0.39500071708022094
need align? ->  False 0.39289630022717686
2023-08-26 02:34:25,701 - epoch:25, training loss:0.4268 validation loss:0.3878
Updating learning rate to 6.687584801467642e-06
Updating learning rate to 6.687584801467642e-06
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.38805196597808744 0.3941795475235799
need align? ->  False 0.39289630022717686
2023-08-26 02:37:55,265 - epoch:26, training loss:0.4260 validation loss:0.3881
Updating learning rate to 3.7975938771636264e-06
Updating learning rate to 3.7975938771636264e-06
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.3878466866728736 0.39447465066502735
need align? ->  False 0.39289630022717686
2023-08-26 02:41:25,764 - epoch:27, training loss:0.4256 validation loss:0.3878
Updating learning rate to 1.6981423534038182e-06
Updating learning rate to 1.6981423534038182e-06
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.388012195505747 0.3944093692230015
need align? ->  False 0.39289630022717686
2023-08-26 02:44:56,022 - epoch:28, training loss:0.4252 validation loss:0.3880
Updating learning rate to 4.251524278376747e-07
Updating learning rate to 4.251524278376747e-07
36887 5270 10539 0.7 0.2 52696
train 36456
vs, vt 0.38798350459191855 0.394473849091588
need align? ->  False 0.39289630022717686
2023-08-26 02:48:24,038 - epoch:29, training loss:0.4262 validation loss:0.3880
Updating learning rate to 4.053110311148044e-10
Updating learning rate to 4.053110311148044e-10
check exp/ECL-PatchTST2023-08-26-00:49:52.466897/0/0.3874_epoch_18.pkl  &  0.39289630022717686
2023-08-26 02:48:32,148 - [*] loss:0.1499
2023-08-26 02:48:32,172 - [*] phase 0, testing
2023-08-26 02:48:32,646 - T:96	MAE	0.197217	RMSE	0.150396	MAPE	1234.909725
2023-08-26 02:48:32,646 - 96	mae	0.1972	
2023-08-26 02:48:32,646 - 96	rmse	0.1504	
2023-08-26 02:48:32,646 - 96	mape	1234.9097	
2023-08-26 02:48:40,633 - [*] loss:0.1499
2023-08-26 02:48:40,657 - [*] phase 0, testing
2023-08-26 02:48:41,117 - T:96	MAE	0.197217	RMSE	0.150396	MAPE	1234.909725
2023-08-26 02:48:49,093 - [*] loss:0.1630
2023-08-26 02:48:49,117 - [*] phase 0, testing
2023-08-26 02:48:49,578 - T:96	MAE	0.215570	RMSE	0.163497	MAPE	1012.482262
2023-08-26 02:48:57,681 - [*] loss:0.1542
2023-08-26 02:48:57,705 - [*] phase 0, testing
2023-08-26 02:48:58,167 - T:96	MAE	0.197882	RMSE	0.154726	MAPE	1147.084904
2023-08-26 02:48:58,168 - 96	mae	0.1979	
2023-08-26 02:48:58,168 - 96	rmse	0.1547	
2023-08-26 02:48:58,168 - 96	mape	1147.0849	
