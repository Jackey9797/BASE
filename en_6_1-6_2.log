2023-08-02 15:27:15,359 - logger name:exp/ECL-PatchTST2023-08-02-15:27:15.359672/ECL-PatchTST.log
2023-08-02 15:27:15,360 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 6, 'seed': 42033, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 1.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-02-15:27:15.359672', 'path': 'exp/ECL-PatchTST2023-08-02-15:27:15.359672', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-02 15:27:15,360 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-02 15:27:15,555 - [*] phase 0 Dataset load!
2023-08-02 15:27:16,391 - [*] phase 0 Training start
train 8209
2023-08-02 15:27:29,056 - epoch:0, training loss:0.2071 validation loss:0.1610
train 8209
vs, vt 0.160981329835274 0.16870617849582975
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13820190355181694 0.1533685133538463
2023-08-02 15:28:00,331 - epoch:1, training loss:2.9240 validation loss:0.1382
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12946503126824443 0.14018399640917778
2023-08-02 15:28:24,562 - epoch:2, training loss:2.2762 validation loss:0.1295
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.1253083815628832 0.13417309751226145
2023-08-02 15:28:48,806 - epoch:3, training loss:1.4943 validation loss:0.1253
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12441825680434704 0.12944835873151367
2023-08-02 15:29:13,308 - epoch:4, training loss:0.9876 validation loss:0.1244
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12366352873769673 0.12489768913523717
2023-08-02 15:29:38,103 - epoch:5, training loss:0.8106 validation loss:0.1237
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.12758723985065112 0.12828788584606213
2023-08-02 15:30:02,223 - epoch:6, training loss:0.7367 validation loss:0.1276
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.1258537063205784 0.1260229805484414
2023-08-02 15:30:25,717 - epoch:7, training loss:0.6854 validation loss:0.1259
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.12527468597347086 0.12332583930004727
2023-08-02 15:30:49,769 - epoch:8, training loss:0.6404 validation loss:0.1253
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.1250037384134802 0.12372207802466371
2023-08-02 15:31:13,453 - epoch:9, training loss:0.5857 validation loss:0.1250
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.125184577466412 0.12445906380360777
2023-08-02 15:31:37,725 - epoch:10, training loss:0.5703 validation loss:0.1252
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12371427862142975 0.12237773073667829
2023-08-02 15:32:03,308 - epoch:11, training loss:0.5123 validation loss:0.1237
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12411523068493063 0.12301167523996397
2023-08-02 15:32:27,676 - epoch:12, training loss:0.5081 validation loss:0.1241
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12363674847239797 0.12380541742525318
2023-08-02 15:32:52,153 - epoch:13, training loss:0.4982 validation loss:0.1236
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12369601504707878 0.12402864266186953
2023-08-02 15:33:16,760 - epoch:14, training loss:0.4928 validation loss:0.1237
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12315078587694601 0.1233106976036321
2023-08-02 15:33:41,377 - epoch:15, training loss:0.4799 validation loss:0.1232
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12310504515401342 0.12324415291236206
2023-08-02 15:34:05,483 - epoch:16, training loss:0.4811 validation loss:0.1231
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12312960683960807 0.12347791801122102
2023-08-02 15:34:29,897 - epoch:17, training loss:0.4776 validation loss:0.1231
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12377990502864122 0.1241372844881632
2023-08-02 15:34:53,410 - epoch:18, training loss:0.4649 validation loss:0.1238
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.12356735342605547 0.12372510025108402
2023-08-02 15:35:18,047 - epoch:19, training loss:0.4701 validation loss:0.1236
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.12337848366322843 0.1237541272050955
2023-08-02 15:35:44,426 - epoch:20, training loss:0.4614 validation loss:0.1234
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.123024472204799 0.12389335188676011
2023-08-02 15:36:13,592 - epoch:21, training loss:0.4661 validation loss:0.1230
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.12327702021734281 0.12378608249127865
2023-08-02 15:36:37,589 - epoch:22, training loss:0.4605 validation loss:0.1233
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12340652620927854 0.12393977357582613
2023-08-02 15:37:02,358 - epoch:23, training loss:0.4591 validation loss:0.1234
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.123209677467292 0.12424882518296892
2023-08-02 15:37:26,560 - epoch:24, training loss:0.4515 validation loss:0.1232
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12335545925254171 0.12418971777978269
2023-08-02 15:37:50,732 - epoch:25, training loss:0.4570 validation loss:0.1234
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.12328940579159693 0.12407118386842987
2023-08-02 15:38:14,870 - epoch:26, training loss:0.4581 validation loss:0.1233
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12325221088460901 0.12409320820800283
2023-08-02 15:38:39,141 - epoch:27, training loss:0.4589 validation loss:0.1233
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12328068793497303 0.12401950740340081
2023-08-02 15:39:05,404 - epoch:28, training loss:0.4564 validation loss:0.1233
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12333367129957135 0.12392145352946086
2023-08-02 15:39:29,917 - epoch:29, training loss:0.4583 validation loss:0.1233
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-02-15:27:15.359672/0/0.123_epoch_21.pkl  &  0.12237773073667829
2023-08-02 15:39:31,875 - [*] loss:0.2732
2023-08-02 15:39:31,878 - [*] phase 0, testing
2023-08-02 15:39:31,917 - T:96	MAE	0.332428	RMSE	0.273523	MAPE	131.653941
2023-08-02 15:39:31,919 - 96	mae	0.3324	
2023-08-02 15:39:31,919 - 96	rmse	0.2735	
2023-08-02 15:39:31,919 - 96	mape	131.6539	
2023-08-02 15:39:32,863 - [*] loss:0.2725
2023-08-02 15:39:32,866 - [*] phase 0, testing
2023-08-02 15:39:32,904 - T:96	MAE	0.332222	RMSE	0.272453	MAPE	130.231333
2023-08-02 15:39:32,905 - 96	mae	0.3322	
2023-08-02 15:39:32,905 - 96	rmse	0.2725	
2023-08-02 15:39:32,905 - 96	mape	130.2313	
2023-08-02 15:39:35,161 - logger name:exp/ECL-PatchTST2023-08-02-15:39:35.161679/ECL-PatchTST.log
2023-08-02 15:39:35,162 - params : {'loss': 'huber', 'conf': 'ECL-PatchTST', 'data_name': 'ETTh2', 'iteration': 1, 'load': True, 'build_graph': False, 'same_init': True, 'grad_norm': False, 'refiner_no_residual': False, 'root_path': '', 'exp_path': 'exp/', 'val_test_mix': False, 'pred_len': 96, 'noise_rate': 0.5, 'device': device(type='cuda', index=0), 'test_model_path': '/Disk/fhyega/code/BASE/exp/ECL-PatchTST2023-07-23-21:59:42.618606/0/0.0379_epoch_25.pkl', 'idx': -1, 'aligner': 1, 'always_align': 1, 'refiner': 1, 'refiner_block_num': 1, 'enhance': 1, 'enhance_type': 6, 'seed': 42033, 'batch_size': 128, 'share_head': 0, 'jitter_sigma': 0.2, 'slope_rate': 0.01, 'slope_range': 0.5, 'beta': 2.0, '/* model related args*/': '//', 'model_name': 'PatchTST', 'seq_len': 336, 'label_len': 48, 'features': 'M', 'target': 'OT', 'graph_input': False, 'individual': False, 'e_layers': 3, 'd_layers': 1, 'factor': 1, 'n_heads': 16, 'd_model': 128, 'd_ff': 256, 'dropout': 0.2, 'fc_dropout': 0.2, 'head_dropout': 0.0, 'patch_len': 16, 'stride': 8, 'des': 'Exp', 'padding_patch': 'end', 'revin': 1, 'affine': 0, 'subtract_last': 0, 'decomposition': 0, 'kernel_size': 25, 'output_attention': 0, 'embed_type': 0, 'activation': 'gelu', 'lradj': 'TST', 'distil': 1, 'linear_output': 1, 'pct_start': 0.2, '/*train related args*/': '//', 'train': True, 'epoch': 30, 'lr': 0.0001, '/*dataset related args*/': '//', 'save_data_path': 'data/ECL/', 'data_process': True, 'embed': 'timeF', 'freq': 'h', 'begin_phase': 0, 'end_phase': 1, 'phase_len': 26304, 'val_ratio': 0.33, 'graph_size': 321, '/*strategy related args*/': '//', 'strategy': 'incremental', 'increase': False, 'detect': False, 'detect_strategy': 'feature', 'replay': False, 'replay_strategy': 'random', 'repaly_num_samples': 100, 'ewc': False, 'ewc_strategy': 'ewc', 'ewc_lambda': 0.0001, 'subgraph_train': False, 'num_hops': 2, 'logname': 'ECL-PatchTST', 'time': '2023-08-02-15:39:35.161679', 'path': 'exp/ECL-PatchTST2023-08-02-15:39:35.161679', 'num_workers': 4, 'start_train': 0, 'train_mode': 'pretrain', 'get_score': False, 'use_cm': True, 'logger': <Logger __main__ (INFO)>}
2023-08-02 15:39:35,162 - [*] phase 0 start training
0 26304
train 8209
val 2785
test 2785
2023-08-02 15:39:35,365 - [*] phase 0 Dataset load!
2023-08-02 15:39:36,248 - [*] phase 0 Training start
train 8209
2023-08-02 15:39:49,059 - epoch:0, training loss:0.2071 validation loss:0.1610
train 8209
vs, vt 0.160981329835274 0.16870617849582975
Updating learning rate to 1.0463629820836433e-05
Updating learning rate to 1.0463629820836433e-05
train 8209
vs, vt 0.13846086964688517 0.15637552026997914
2023-08-02 15:40:20,991 - epoch:1, training loss:3.2991 validation loss:0.1385
Updating learning rate to 2.8113748014145436e-05
Updating learning rate to 2.8113748014145436e-05
train 8209
vs, vt 0.12958764606578785 0.14072404784912412
2023-08-02 15:40:45,647 - epoch:2, training loss:2.5951 validation loss:0.1296
Updating learning rate to 5.219686165094539e-05
Updating learning rate to 5.219686165094539e-05
train 8209
vs, vt 0.1253862279382619 0.1352261547845873
2023-08-02 15:41:10,810 - epoch:3, training loss:1.7582 validation loss:0.1254
Updating learning rate to 7.622695691951079e-05
Updating learning rate to 7.622695691951079e-05
train 8209
vs, vt 0.12445488910783421 0.12912668372419747
2023-08-02 15:41:34,890 - epoch:4, training loss:1.1882 validation loss:0.1245
Updating learning rate to 9.373229880419829e-05
Updating learning rate to 9.373229880419829e-05
train 8209
vs, vt 0.12407074462283742 0.12500736701556228
2023-08-02 15:42:00,676 - epoch:5, training loss:0.9869 validation loss:0.1241
Updating learning rate to 9.999989541836333e-05
Updating learning rate to 9.999989541836333e-05
train 8209
vs, vt 0.1284472028809515 0.1281720552254807
2023-08-02 15:42:25,404 - epoch:6, training loss:0.9006 validation loss:0.1284
Updating learning rate to 9.955879284407972e-05
Updating learning rate to 9.955879284407972e-05
train 8209
vs, vt 0.12615200055932457 0.12644038527187976
2023-08-02 15:42:49,907 - epoch:7, training loss:0.8404 validation loss:0.1262
Updating learning rate to 9.826972900599613e-05
Updating learning rate to 9.826972900599613e-05
train 8209
vs, vt 0.1259492946450006 0.12372309858487411
2023-08-02 15:43:14,221 - epoch:8, training loss:0.7692 validation loss:0.1259
Updating learning rate to 9.615476014377818e-05
Updating learning rate to 9.615476014377818e-05
train 8209
vs, vt 0.12527304591441696 0.12425746853378686
2023-08-02 15:43:38,218 - epoch:9, training loss:0.7115 validation loss:0.1253
Updating learning rate to 9.325007396103859e-05
Updating learning rate to 9.325007396103859e-05
train 8209
vs, vt 0.12510951870883052 0.12461207349869338
2023-08-02 15:44:02,350 - epoch:10, training loss:0.7262 validation loss:0.1251
Updating learning rate to 8.960537044369518e-05
Updating learning rate to 8.960537044369518e-05
train 8209
vs, vt 0.12359747663140297 0.12314036547799002
2023-08-02 15:44:26,869 - epoch:11, training loss:0.6685 validation loss:0.1236
Updating learning rate to 8.528301147943237e-05
Updating learning rate to 8.528301147943237e-05
train 8209
vs, vt 0.12432287912815809 0.12344415113329887
2023-08-02 15:44:51,187 - epoch:12, training loss:0.6622 validation loss:0.1243
Updating learning rate to 8.035695382851308e-05
Updating learning rate to 8.035695382851308e-05
train 8209
vs, vt 0.12364992457018657 0.12424440343271602
2023-08-02 15:45:18,077 - epoch:13, training loss:0.6559 validation loss:0.1236
Updating learning rate to 7.49114837031057e-05
Updating learning rate to 7.49114837031057e-05
train 8209
vs, vt 0.12370486557483673 0.1242183308032426
2023-08-02 15:45:42,437 - epoch:14, training loss:0.6562 validation loss:0.1237
Updating learning rate to 6.90397746068255e-05
Updating learning rate to 6.90397746068255e-05
train 8209
vs, vt 0.12332504352724011 0.12344233623959801
2023-08-02 15:46:06,696 - epoch:15, training loss:0.6436 validation loss:0.1233
Updating learning rate to 6.284229311025519e-05
Updating learning rate to 6.284229311025519e-05
train 8209
vs, vt 0.12330645559863611 0.12362857535481453
2023-08-02 15:46:30,834 - epoch:16, training loss:0.6485 validation loss:0.1233
Updating learning rate to 5.642507984006751e-05
Updating learning rate to 5.642507984006751e-05
train 8209
vs, vt 0.12338659218089147 0.1233458910137415
2023-08-02 15:46:57,967 - epoch:17, training loss:0.6395 validation loss:0.1234
Updating learning rate to 4.989793509450307e-05
Updating learning rate to 4.989793509450307e-05
train 8209
vs, vt 0.12401622906327248 0.12434103754772381
2023-08-02 15:47:23,730 - epoch:18, training loss:0.6286 validation loss:0.1240
Updating learning rate to 4.337254012982485e-05
Updating learning rate to 4.337254012982485e-05
train 8209
vs, vt 0.1231778073547916 0.12386804886839607
2023-08-02 15:47:48,832 - epoch:19, training loss:0.6345 validation loss:0.1232
Updating learning rate to 3.696054626305981e-05
Updating learning rate to 3.696054626305981e-05
train 8209
vs, vt 0.1234336645257744 0.12421942663125017
2023-08-02 15:48:17,569 - epoch:20, training loss:0.6298 validation loss:0.1234
Updating learning rate to 3.0771664487008854e-05
Updating learning rate to 3.0771664487008854e-05
train 8209
vs, vt 0.12303883540020748 0.12406472235240719
2023-08-02 15:48:42,070 - epoch:21, training loss:0.6300 validation loss:0.1230
Updating learning rate to 2.491178828474237e-05
Updating learning rate to 2.491178828474237e-05
train 8209
vs, vt 0.1232551017945463 0.12393421163274483
2023-08-02 15:49:07,537 - epoch:22, training loss:0.6303 validation loss:0.1233
Updating learning rate to 1.9481181762745777e-05
Updating learning rate to 1.9481181762745777e-05
train 8209
vs, vt 0.12330831147053024 0.12422321609814059
2023-08-02 15:49:35,706 - epoch:23, training loss:0.6258 validation loss:0.1233
Updating learning rate to 1.4572764104259043e-05
Updating learning rate to 1.4572764104259043e-05
train 8209
vs, vt 0.12309148992327126 0.12445703758434816
2023-08-02 15:50:02,926 - epoch:24, training loss:0.6212 validation loss:0.1231
Updating learning rate to 1.0270519696289322e-05
Updating learning rate to 1.0270519696289322e-05
train 8209
vs, vt 0.12324434383348985 0.12453303122046319
2023-08-02 15:50:30,586 - epoch:25, training loss:0.6234 validation loss:0.1232
Updating learning rate to 6.648061133464466e-06
Updating learning rate to 6.648061133464466e-06
train 8209
vs, vt 0.1231938593766906 0.12431248260492628
2023-08-02 15:50:54,841 - epoch:26, training loss:0.6274 validation loss:0.1232
Updating learning rate to 3.7673696861296922e-06
Updating learning rate to 3.7673696861296922e-06
train 8209
vs, vt 0.12320213460109451 0.12440719514746558
2023-08-02 15:51:24,444 - epoch:27, training loss:0.6249 validation loss:0.1232
Updating learning rate to 1.6777347836274343e-06
Updating learning rate to 1.6777347836274343e-06
train 8209
vs, vt 0.12319854740053415 0.12428132275288756
2023-08-02 15:51:49,381 - epoch:28, training loss:0.6243 validation loss:0.1232
Updating learning rate to 4.1491065849575754e-07
Updating learning rate to 4.1491065849575754e-07
train 8209
vs, vt 0.12325124112381176 0.1241405104202303
2023-08-02 15:52:14,450 - epoch:29, training loss:0.6223 validation loss:0.1233
Updating learning rate to 5.045816366600914e-10
Updating learning rate to 5.045816366600914e-10
check exp/ECL-PatchTST2023-08-02-15:39:35.161679/0/0.123_epoch_21.pkl  &  0.12314036547799002
2023-08-02 15:52:16,420 - [*] loss:0.2733
2023-08-02 15:52:16,424 - [*] phase 0, testing
2023-08-02 15:52:16,462 - T:96	MAE	0.332530	RMSE	0.273643	MAPE	131.606150
2023-08-02 15:52:16,463 - 96	mae	0.3325	
2023-08-02 15:52:16,463 - 96	rmse	0.2736	
2023-08-02 15:52:16,463 - 96	mape	131.6061	
2023-08-02 15:52:18,597 - [*] loss:0.2736
2023-08-02 15:52:18,600 - [*] phase 0, testing
2023-08-02 15:52:18,637 - T:96	MAE	0.334463	RMSE	0.273338	MAPE	130.786002
2023-08-02 15:52:18,638 - 96	mae	0.3345	
2023-08-02 15:52:18,638 - 96	rmse	0.2733	
2023-08-02 15:52:18,638 - 96	mape	130.7860	
